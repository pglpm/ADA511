{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be9db6f-c71b-40d2-9ff1-1301f75ba94f",
   "metadata": {},
   "source": [
    "We shall compare the results obtained in some numerical simulations by using\n",
    "\n",
    "- a Machine-Learning Classifier trained to do most successful guesses\n",
    "\n",
    "- a prototype “Optimal Predictor Machine” trained to make the optimal decision\n",
    "\n",
    "For the moment we treat both as “black boxes”, that is, we don’t study yet how they’re calculating their outputs (although you may already have a good guess at how the Optimal Predictor Machine works).\n",
    "\n",
    "Their operation is implemented in the R function defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d8d4c6-a095-4861-93b2-8f27ebfafa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def hitsvsgain(ntrials, chooseAtrueA, chooseAtrueB, chooseBtrueB, chooseBtrueA, probsA=[0.5]):\n",
    "    ## Recycle & shuffle the given probabilities for the number of trials\n",
    "    probsArepeated = random.choices(probsA, k=ntrials)\n",
    "    \n",
    "    ## \"Magic\" parameters used in making the optimal decision\n",
    "    threshold1 = (chooseAtrueA - chooseAtrueB + chooseBtrueB - chooseBtrueA)\n",
    "    threshold2 = (chooseBtrueB - chooseAtrueB)\n",
    "    \n",
    "    ## Initialize total \"hits\" and gains\n",
    "    ## 'mlc' refers to the Machine-Learning Classifier\n",
    "    ## 'opm' refers to the Optimal Predictor Machine\n",
    "    mlchits = 0\n",
    "    mlcgain = 0\n",
    "    opmhits = 0\n",
    "    opmgain = 0\n",
    "    \n",
    "    ##\n",
    "    ## Loop through the trials and their probabilities\n",
    "    for probabilityA in probsArepeated:\n",
    "        ## Output of the MLC, based on the current probability\n",
    "        if probabilityA > 0.5:\n",
    "            mlcchoice = 'A'\n",
    "        elif probabilityA < 0.5:\n",
    "            mlcchoice = 'B'\n",
    "        else:\n",
    "            mlcchoice = random.choice(['A', 'B']) # A or B with 50%/50% prob.\n",
    "        \n",
    "        ## Output of the OPM, based on the current probability\n",
    "        ## try to understand where this inequality comes from\n",
    "        if (chooseAtrueA - chooseAtrueB + chooseBtrueB - chooseBtrueA) * probabilityA > (chooseBtrueB - chooseAtrueB):\n",
    "            opmchoice = 'A'\n",
    "        elif (chooseAtrueA - chooseAtrueB + chooseBtrueB - chooseBtrueA) * probabilityA < (chooseBtrueB - chooseAtrueB):\n",
    "if threshold1 * probabilityA > threshold2:\n",
    "            opmchoice = 'B'\n",
    "        else:\n",
    "            opmchoice = random.choice(['A', 'B']) # A or B with 50%/50% prob.\n",
    "        \n",
    "        ##\n",
    "        ## Correct answer for the current trial\n",
    "        trueitem = random.choices(['A', 'B'], weights=[probabilityA, 1-probabilityA], k=1)[0]\n",
    "        \n",
    "        ##\n",
    "        ## MLC: add one \"hit\" if correct guess, and add gain/loss\n",
    "        if mlcchoice == trueitem:\n",
    "            mlchits += 1 # one success\n",
    "            if trueitem == 'A':\n",
    "                mlcgain += chooseAtrueA\n",
    "            else:\n",
    "                mlcgain += chooseBtrueB\n",
    "        else:\n",
    "            if trueitem == 'B':\n",
    "                mlcgain += chooseAtrueB\n",
    "            else:\n",
    "                mlcgain += chooseBtrueA\n",
    "        \n",
    "        ##\n",
    "        ## OPM: add one \"hit\" if correct guess, and add gain/loss\n",
    "        if opmchoice == trueitem:\n",
    "            opmhits += 1 # one success\n",
    "            if trueitem == 'A':\n",
    "                opmgain += chooseAtrueA\n",
    "            else:\n",
    "                opmgain += chooseBtrueB\n",
    "        else:\n",
    "            if trueitem == 'B':\n",
    "                opmgain += chooseAtrueB\n",
    "            else:\n",
    "                opmgain += chooseBtrueA\n",
    "    \n",
    "    ## end of loop\n",
    "    ##\n",
    "    ## Output total number of hits and total gain or loss produced\n",
    "    print('\\nTrials:', ntrials)\n",
    "    print('Machine-Learning Classifier: successes', mlchits, '(', format(mlchits/ntrials*100, '.3f'), '%)',\n",
    "          '| total gain', mlcgain)\n",
    "    print('Optimal Predictor Machine:   successes', opmhits, '(', format(opmhits/ntrials*100, '.3f'), '%)',\n",
    "          '| total gain', opmgain)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1615756-ce76-4600-b3df-91aa7d0d0ea1",
   "metadata": {},
   "source": [
    "The function above has 6 arguments:\n",
    "\n",
    "- `ntrials`: how many simulations of guesses to make\n",
    "- `chooseAtrueA`: utility gained by guessing `A` when the successful guess is indeed `A`\n",
    "- `chooseAtrueB`: utility gained by guessing `A` when the successful guess is `B` instead\n",
    "- `chooseBtrueB`: utility gained by guessing `B` when the successful guess is indeed `B`\n",
    "- `chooseBtrueA`: utility gained by guessing `B` when the successful guess is `A` instead\n",
    "- `probsA`: a tuple of probabilities (between `0` and `1`) to be used in the simulations (recycling it if necessary), for the successful guess being `A`; the corresponding probabilities for `B` are therefore `1-probsA`. If this argument is omitted it defaults to `0.5` (not very interesting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab59deb-f9a9-47d6-ba5f-8be1a01d6d27",
   "metadata": {},
   "source": [
    "## Example 1: electronic component\n",
    "\n",
    "Let's apply our two classifiers to the *Accept or discard?* problem. Call `A` the alternative in which the element won't fail before one year, and should therefore be accepted *if this alternative were known at the time of the decision*. Call `B` the alternative in which the element will fail within a year, and should therefore be discarded *if this alternative were known at the time of the decision*. (Remember that the crucial point here is that the classifiers *don't* have this information at the moment of making the decision.)\n",
    "\n",
    "We simulate this decision for 100000 components (\"trials\"), assuming that the probabilities of failure can be `0.05`, `0.20`, `0.80`, `0.95`. The values of the arguments should be clear:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8175cc4d-bae0-4941-baca-e1d469a4de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trials: 100000\n",
      "Machine-Learning Classifier: successes 87410 ( 87.410 %) | total gain -26140\n",
      "Optimal Predictor Machine:   successes 72535 ( 72.535 %) | total gain 9475\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hitsvsgain(ntrials=100000,\n",
    "           chooseAtrueA=+1, chooseAtrueB=-11,\n",
    "           chooseBtrueB=0, chooseBtrueA=0,\n",
    "           probsA=[0.05, 0.20, 0.80, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d46df-efad-4a78-ba8b-29bdf54e4437",
   "metadata": {},
   "source": [
    "Which classifier makes most *successful* guesses?\n",
    "\n",
    "Which classifier gives the highest monetary gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179812d-3d57-4edb-871a-8ab037cf0507",
   "metadata": {},
   "source": [
    "## Example 2: find Aladdin! (image recognition)\n",
    "\n",
    "A typical use of machine-learning classifiers is for image recognition: for instance, the classifier guesses whether a particular subject is present in the image or not.\n",
    "\n",
    "Intuitively one may think that \"guessing successfully\" should be the best goal here. But exceptions to this may be more common than one thinks. Consider the following scenario:\n",
    "\n",
    "> Bianca has a computer folder with 10000 photos. Some of these include her beloved cat Aladdin, who sadly passed away recently. She would like to select all photos that include Aladdin and save them in a separate \"Aladdin\" folder. Doing this by hand would take too long, if at all possible; so Bianca wants to employ a machine-learning classifier.\n",
    "> \n",
    "> For Bianca it's important that no photo with Aladdin goes missing, so she would be very sad if any photo with him weren't correctly recognized; on the other hand she doesn't mind if some photos without him end up in the \"Aladdin\" folder -- she can delete them herself afterwards.\n",
    "\n",
    "Let's apply and compare our two classifiers to this image-recognition problem, using again the `hitsvsgain()` function. We call `A` the case where Aladdin is present in a photo, and `B` where he isn't. To reflect Bianca's preferences, let's use these \"emotional utilities\":\n",
    "\n",
    "- `chooseAisA = +2`: Aladdin is correctly recognized\n",
    "- `chooseBisA = -2`: Aladdin is not recognized and photo goes missing\n",
    "- `chooseBisB = +1`: absence of Aladding is correctly recognized\n",
    "- `chooseAisB = -1`: photo without Aladding end up in \"Aladding\" folder\n",
    "\n",
    "and let's say that the photos may have probabilities `0.3`, `0.4`, `0.6`, `0.7` of including Aladding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a83427-673a-4853-98cb-627be7a42f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trials: 10000\n",
      "Machine-Learning Classifier: successes 6513 ( 65.130 %) | total gain 4565\n",
      "Optimal Predictor Machine:   successes 5994 ( 59.940 %) | total gain 5443\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hitsvsgain(ntrials=10000,\n",
    "           chooseAtrueA=+2, chooseAtrueB=-1,\n",
    "           chooseBtrueB=1, chooseBtrueA=-2,\n",
    "           probsA=[0.3, 0.4, 0.6, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70f91f-ba7f-439e-9b66-61bac4a99c08",
   "metadata": {},
   "source": [
    "Again we see that the machine-learning classifier makes more successful guesses than the optimal predictor machine, but the latter yields a higher \"emotional utility\".\n",
    "\n",
    "You may sensibly object that this result could depend on the peculiar utilities or probabilities chosen for this example. The next exercise helps answering your objection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9d447-b1e6-42d0-8942-c8c0ffd486a4",
   "metadata": {},
   "source": [
    "## Now play and experiment!\n",
    "\n",
    "- Is there any case in which the optimal predictor machine yields a strictly lower utility than the machine-learning classifier?\n",
    "    + Try using different utilities, for instance using `±5` instead of `±2`, or whatever other values you please.\n",
    "    + Try using different probabilities as well.\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "- As in the previous exercise, try to understand what's happening. Consider this question: *how many photos including Aladdin did each classifier miss?*\n",
    "    \n",
    "    Modify the `hitsvsgain()` function to output this result.\n",
    "\n",
    "- Do the comparison using the following utilities: `chooseAtrueA=+1, chooseAtrueB=-1, chooseBtrueB=1, chooseBtrueA=-1`. What's the result? what does this tell you about the relationship between the machine-learning classifier and the optimal predictor machine?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39845053-9b96-4ec3-b856-a594c76d4a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
