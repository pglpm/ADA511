# [Exchangeable beliefs]{.green} {#sec-exchangeable-beliefs}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

## Recap {#sec-recap-before-exchang}

In the chapters of part [Inference I]{.green} we had an overview of how an agent can draw inferences and make predictions of the most general kind, expressed by general sentences, using the four fundamental rules of inference.

Then, in part [Inference II]{.green}, we successively narrowed our focus on more and more specialized kinds of inference, typical of engineering and data-science problems and of machine-learning algorithms. First we considered inferences about measurements and observations, then inferences about multiple instances of similar measurements and observations. The idea was that an agent can arrive at sharper degrees of belief  -- that is, it can *learn* -- by using information about "similar instances".

For these purposes we introduced a specialized language about quantities and data types in part [Data I]{.yellow}, and about "populations" of similar "units" in part [Data II]{.yellow}.

In the [Machine learning]{.red} part we took an overview of current machine-learning methods, and then focused on several types of tasks that popular machine-learning algorithms, such as deep networks and random forests, purport to solve. We found a remarkable result: a perfect agent -- one that operates according to Probability Theory -- can in principle perform *any and all* of those tasks by using the joint probability distribution

$$
\P(\blue
Y_{N+1}\mo y_{N+1}
\and
X_{N+1}\mo x_{N+1}
\and \dotsb \and
Y_{1}\mo y_{1}
\and
X_{1}\mo x_{1}
\black \| \yI)
$$

The agent only needs to do some calculations with this joint distribution, involving sums and divisions. This distribution must be specified for all possible values of $\blue x_{1}, \dotsc, x_{N+1}$, $\blue y_{1}, \dotsc, y_{N+1}$, and $N$.

Take "supervised learning" for example, that is, the task  of predicting some variates (predictands) for a new unit, from knowledge of  other variates (predictors) for the same unit and of all variates for $N$ other units. Solving this task corresponds to calculating

:::{.column-page-inset-right}
$$
\begin{aligned}
    &\P\bigl(
	{\red Y_{N+1} \mo y}
	\pmb{\|[\big]} 
	{\green X_{N+1} \mo x_{N+1}}\, \and\,
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\and {\yI} \bigr)
	\\[2ex]
	&\qquad{}=
	\frac{
	    \P\bigl(
	\red Y_{N+1} \mo y \and
	\green X_{N+1} \mo x_{N+1}
	\black
		\and
    \green Y_N \mo y_N \and X_N \mo x_N
	\and
	\dotsb \and 
	\green Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]} {\yI} \bigr)
}{
	 \sum_{\red y} \P\bigl(
	{\red Y_{N+1} \mo y} \and
	{\green X_{N+1} \mo x_{N+1}}
		\and
    \green Y_N \mo y_N \and X_N \mo x_N \and
	\dotsb \and 
	Y_1 \mo y_1 \and X_1 \mo x_1 
    \black\pmb{\|[\big]}  {\yI} \bigr)
}
\end{aligned}
$$
:::

\
To build an AI agent that deals with these kinds of task we must: (1) choose a joint distribution according to reasonable assumptions and background information, (2) encode it in a computationally feasible way.

In order to reach these two goals we shall now narrow our focus further, upon inferences satisfying a condition that greatly simplifies the calculations, and that is also reasonable in many real inference problems -- and it is moreover typical of many "supervised" and "unsupervised" machine-learning applications.

\

## States of knowledge with symmetries {#sec-excheable-beliefs}

An agent's degrees of belief about a particular population may satisfy a special symmetry called [**exchangeability**]{.blue}. This symmetry can be understood from different points of view. Let's start from one of these viewpoints, and then make connections with alternative ones.

Take again two populations briefly mentioned in [§@sec-collections]:

- Stock exchange
: The daily change in closing price of a stock during 1000 consecutive days. Each day the change can be positive or zero: $\ypl$, or negative: $\ymi$.

:::{.column-margin}
![](stock_course.jpg){width=50%}
:::


- Mars prospecting
: A collection of 1000 similar-sized rocks gathered from a specific, large crater on Mars. Each rock either contains haematite: $\yy$, or it doesn't: $\yn$.

:::{.column-margin}
![](mars_crater2.jpg){width=50%}
:::

\
Suppose that, in each of these populations, you (the agent) don't know the variate value for unit [#735]{.midgrey}, and for some reason would like to infer it. You are given the variate values for 100 other units, which you can use to improve your inference. Now consider this question:

> [*How much does the relative order of the 100 known units and the unknown unit matter to you, for drawing your inference?*]{.blue}

We know, from information theory, that it never hurts having extra information, such as the units' order. But you probably judge the units' order to be much more important for your inference in the stock-exchange case than in the Mars-prospecting one. In the stock-exchange case it would be more informative to have data from units temporally close to unit [#735]{.midgrey}; for example units [#635--#734]{.midgrey}, or [#736--#835]{.midgrey}, or [#685--#734]{.midgrey} & [#736--#785]{.midgrey}, or similar ranges. But in the Mars-prospecting case you might find it acceptable if the 100 known units were picked up in some unsystematic way from the catalogue of remaining 999 units. There are reasons, boiling down to physics, behind this kind of judgement.

The question above could also be replaced by others, slightly different but still connected to the same issue. For example:

> [*How strongly would you like to be able to choose which 100 units you can have data from, in order to draw your inference?*]{.blue}

or

> [*How much would you be upset if the original order of the population units were destroyed by accidental shuffling?*]{.blue}

or

> [*Would it be acceptable to you if only the **frequencies** of the values ($\set{\ypl,\ymi}$ in one case, $\set{\yy,\yn}$ in the other) for the 100 known units were given to you?*]{.blue}


:::{.callout-caution}


- Find examples of populations where the units have some kind of ordering that you think would be *very* important for drawing inferences about some units, given other units. Examine why you judge such ordering to be important. (The ordering doesn't need to be one-dimensional. For instance, the pixel intensities of an image also have a two-dimensional relative order or position: is that important if you want to draw inferences about the intensities of some pixels from those of other pixels?)

- Find examples of populations where any potential ordering of the units would *not* be very important for drawing inferences about some units, given other units. Or, put it otherwise, you wouldn't be excessively upset or worried if such order were lost owing to accidental shuffling of the units.
:::


Many kinds of inference considered in data science and engineering, and all inferences done with "supervised" or "unsupervised" machine-learning algorithms, are examples where *any ordering of the data used for learning is deemed irrelevant* and is, in fact, often lost. This irrelevance is clear from the data-shuffling involved in many procedures that accompany these algorithms.

We shall thus restrict our attention to situations and kinds of background information where this judgement of irrelevance is considered appropriate. In reality this is not a black-or-white situation: it is possible that some kind of ordering information would improve our inferences; what we are assuming here is that this improvement is so small that it can be neglected altogether.


## Exchangeable probability distributions {#sec-exchaneable-distr}

Let's take the Mars-prospecting problem as a concrete example. Denote by $H$ the variate expressing haematite presence, with domain $\set{\yy, \yn}$.

If an agent's background information or assumption $\yI$ says that the relative order of units -- rocks in this case -- is irrelevant for inferences about other units, then it means that a probability such as

$$
\P(\yR_{735}\mo\yy \|
\yR_{734}\mo\yn \and
\yR_{733}\mo\yy \and
\yR_{732}\mo\yy \and
\yR_{731}\mo\yn \and
\yR_{730}\mo\yy \and
\yI)
$$

must be equal to the probability

$$
\P(\yR_{735}\mo\yy \|
\yR_{87}\mo\yn \and
\yR_{7}\mo\yy \and
\yR_{16}\mo\yy \and
\yR_{52}\mo\yn \and
\yR_{988}\mo\yy \and
\yI)
$$

and in fact to any probability like

$$
\P(\yR_{i}\mo\yy \|
\yR_{j}\mo\yy \and
\yR_{k}\mo\yy \and
\yR_{l}\mo\yy \and
\yR_{m}\mo\yn \and
\yR_{n}\mo\yn \and
\yI)
$$

for instance 

$$
\P(\yR_{356}\mo\yy \|
\yR_{952}\mo\yn \and
\yR_{103}\mo\yy \and
\yR_{69}\mo\yy \and
\yR_{740}\mo\yn \and
\yR_{679}\mo\yy \and
\yI)
$$

where $i$, $j$, and so on are different but otherwise arbitrary indices.

In other words, the probability depends on whether we are inferring $\yy$ or $\yn$, and on how many $\yy$ and $\yn$ appear in the conditional; in the example above, [three $\yy$]{.green .m} and [two $\yn$.]{.red .m} This property should also apply if the agent makes inferences about more than one unit, conditional on any number of units. It can be proven that this property is equivalent, in our present example, to this general requirement:

> The value of a joint probability such as
>
> $$\P(\yR_{\scriptscriptstyle\dotso}\mo\yy \and\ 
\dotso\ \and
\yR_{\scriptscriptstyle\dotso}\mo\yn \and\ \dotso \|
\yI)
$$
>
> depends only on the **total number of $\yy$ values**  and **total number of $\yn$ values** that appear in it, or, equivalently, on the **absolute frequencies** of the values that appear in it.

\

Leaving the Mars-specific example and generalizing, we can define the following property, called [**exchangeability**]{.blue}:

:::{.callout-note style="font-size:120%"}
##  
::::{style="font-size:120%"}
A joint probability distribution is called [**exchangeable**]{.blue} if the probabilities for any number of units depend only on the absolute frequencies of the values appearing in them.
::::
:::

Let's see a couple more examples.

:::{.column-margin}
[Don't forget that\
$\P(X\mo x \and Y\mo y \| \yI)$\
and\
$\P(Y\mo y \and X\mo x \| \yI)$\
**mean exactly the same**, because `and` (symbol ["$,$")]{.m} is commutative!]{.red}
:::


a. Consider an infinite population with variate $Y$ having domain $\set{\lo,\me,\hi}$. If the background information $\yJ$ guarantees exchangeability, then these three joint probabilities must have the same value:
    
    $$\begin{aligned}
    &\quad\P(
	Y_{3}\mo\hi \and 
	Y_{4}\mo\lo \and 
	Y_{5}\mo\me \and 
	Y_{6}\mo\lo \| \yJ
	)
	\\[2ex]
	&=\P(
	Y_{6}\mo\me \and 
	Y_{5}\mo\hi \and 
	Y_{3}\mo\lo \and 
	Y_{4}\mo\lo \| \yJ
	)
	\\[2ex]
	&=\P(
	Y_{283}\mo\hi \and 
	Y_{91}\mo\lo \and 
	Y_{72}\mo\me \and 
	Y_{1838}\mo\lo \| \yJ
	)
	\end{aligned}
$$
    
    because they all have [one $\hi$]{.green}, [one $\me$]{.yellow}, [two $\lo$]{.red}. The same is true for these two probabilities:
    
    $$\begin{aligned}
    &\quad\P(
	Y_{99}\mo\me \and 
	Y_{1}\mo\me \and 
	Y_{3024}\mo\lo \| \yJ
	)
	\\[2ex]
	&=\P(
	Y_{26}\mo\me \and 
	Y_{611}\mo\lo \and 
	Y_{78}\mo\me \| \yJ
	)
	\end{aligned}
$$
    
    because both have [zero $\hi$]{.green}, [two $\me$]{.yellow}, [one $\lo$]{.red}.

:::{.column-page-inset-right}
b. Consider an infinite population with variates $(U,V)$ having joint domain $\set{\yfa,\ypa} \times \set{\yva, \yvb, \yvc}$\ \ (six possible joint values). If the background information $\yK$ guarantees exchangeability, then these two joint probabilities must have the same value:
    
    $$\begin{aligned}
    &\quad\P(
	U_{14}\mo\ypa\and V_{14}\mo\yvc \ \and\ 
	U_{337}\mo\ypa\and V_{337}\mo\yva \ \and\ 
	U_{8}\mo\yfa\and V_{8}\mo\yvb \ \and\ 
	U_{43}\mo\yfa\and V_{43}\mo\yva \and{}
	\\
	&\qquad\qquad\qquad\quad
	U_{825}\mo\ypa\and V_{825}\mo\yva \ \and\ 
	U_{66}\mo\ypa\and V_{66}\mo\yva \ \and\ 
	U_{700}\mo\yfa\and V_{700}\mo\yvb
	\| \yK)
	\\[3ex]
	&=\P(
	U_{421}\mo\ypa\and V_{421}\mo\yva \ \and\ 
	U_{55}\mo\yfa\and V_{55}\mo\yvb \ \and\ 
	U_{43}\mo\ypa\and V_{43}\mo\yvc \ \and\ 
	U_{14}\mo\yfa\and V_{14}\mo\yva \and{}
	\\
	&\qquad\qquad\qquad\quad
	U_{928}\mo\ypa\and V_{928}\mo\yva \ \and\ 
	U_{700}\mo\yfa\and V_{700}\mo\yvb \ \and\ 
	U_{39}\mo\ypa\and V_{39}\mo\yva
	\| \yK)
	\end{aligned}
    $$
    
    because both have: [one $(\yfa,\yva)$,]{.m}\ \ [two $(\yfa,\yvb)$,]{.m}\ \ [zero $(\yfa,\yvc)$,]{.m}\ \ [three $(\ypa,\yva)$,]{.m}\ \ [zero $(\ypa,\yvb)$,]{.m}\ \ [one $(\ypa,\yvc)$.]{.m} From this example, note that it's important to count the occurrences of the *joint* values, not of the values of the single variates independently.
:::

:::{.callout-caution}


- First let's check that you haven't forgotten the basics about connectives ([§@sec-connecting-sentences]), Boolean algebra [§@sec-boolean]), and the four fundamental rules of inference ([§@sec-fundamental]):
    
    + How much is\ \ [$\P(Y_4\mo\lo \| Y_4\mo\lo \and \yJ )$ ?]{.m}
	
	+ Simplify the probability
	
	$$\P(X_{9}\mo\yy \and X_{28}\mo\yn \and X_{2}\mo \yy \and X_{28}\mo\yn \| \yI)$$
	
	what are the absolute frequencies of the values $\yy$ and $\yn$ among the units in the probability above?

\

- For each collection of probabilities below (the sentences $\se{I'}, \se{I''}, \se{J'}\dotsc$ indicate different states of knowledge), say whether they *cannot* come from an exchangeable probability distribution, or if they *might* [(to guarantee exchangeability, one has to check an infinite number of inequalities, so we can't be sure about it unless they give us a general formula for the joint probabilities)]{.small .midgrey}:
    
	+ $\begin{aligned}[c]
	&\P(C_{2}\mo -1 \| \se{I'}) = 31.6\%
	\\
	&\P(C_{7}\mo -1\| \se{I'}) = 24.8\%
	\end{aligned}$
	
	+ $\begin{aligned}[c]
	&\P(Z_{2}\mo\yof \and Z_{53}\mo\yon \| \se{I''}) = 9.7\%
	\\
	&\P(Z_{3904}\mo\yon \and Z_{29}\mo\yof \| \se{I''}) = 9.7\%
	\end{aligned}$
	
	+ $\begin{aligned}[c]
	&\P(A_{1}\mo\yn \and A_{87}\mo\yy \and A_{3}\mo\yn \| \se{J'}) = 6.2\%
	\\
	&\P(A_{99}\mo\yy \and A_{10}\mo\yn \and A_{13}\mo\yn \| \se{J'}) = 8.9\%
	\end{aligned}$
	
	+ $\begin{aligned}[c]
	&\P(W_{4}\mo\ymi \and W_{97}\mo\ypl \and W_{300}\mo\ymi \| \se{J''}) = 6.2\%
	\\
	&\P(W_{1}\mo\ymi \and W_{86}\mo\ymi \and W_{107}\mo\ymi \| \se{J''}) = 8.9\%
	\end{aligned}$
	
	+ $\begin{aligned}[c]
	&\P(B_{1190}\mo\ypl \and B_{1152}\mo\ymi \and B_{233}\mo\ymi \| \se{K'}) = 7.5\%
	\\
	&\P(B_{1185}\mo\ypl \and B_{424}\mo\ymi \and B_{424}\mo\ymi \| \se{K'}) = 12.3\%
	\end{aligned}$
	
	+ $\begin{aligned}[c]
	&\P(S_{21}\mo\ypl \and T_{21}\mo\lo \ \and\  S_{33}\mo\ymi \and T_{33}\mo\hi \| \se{K''}) = 5.0\%
	\\
	&\P(S_{5}\mo\ymi \and T_{5}\mo\lo \ \and\  S_{102}\mo\ypl \and T_{102}\mo\hi \| \se{K''}) = 2.9\%
	\end{aligned}$

:::

### Further constraints

The exchangeability property greatly reduces the number of probabilities that an agent needs to specify. For a population with a binary variate, a joint probability distribution for 1000 units would require the specification of around $2^{1000} \approx 10^{300}$ probabilities. But if this distribution is exchangeable, only $1000$ probabilities need to be specified (the absolute frequency of one of the two values, ranging between 0 and 1000; minus one because of normalization).^[For the general case of a variate with $n$ values, and $k$ units, [the number of independent probabilities is $\binom{n+k-1}{k}$](https://mathworld.wolfram.com/Multichoose.html).]


Moreover, the exchangeable joint distributions for different numbers of units satisfy additional restrictions, owing to the fact that each of them is the marginal distribution of all distributions with a larger number of units. In the Mars-prospecting case, for instance, if $\P(\yR_{1}\mo\yy\|\yI)$ is the degree of belief that rock #1 contains haematite, we must also have

$$
\begin{aligned}
\P(\yR_{1}\mo\yy\|\yI)
&=\P(\yR_{a}\mo\yy\|\yI)
\\
&= \P(\yR_{a}\mo\yy \and \yR_{b}\mo\yy \| \yI) + \P(\yR_{a}\mo\yy \and \yR_{b}\mo\yn \| \yI)
\end{aligned}
$$

for any two different units [#$a$]{.m} and [#$b$]{.m}. Therefore, if the agent has specified $\P(\yR_{1}\mo\yy\|\yI)$, then three of these four probabilities

$$
\begin{gathered}
\P(\yR_{a}\mo\yy \and \yR_{b}\mo\yy \| \yI)\qquad
\P(\yR_{a}\mo\yy \and \yR_{b}\mo\yn \| \yI)\\[1ex]
\P(\yR_{a}\mo\yn \and \yR_{b}\mo\yy \| \yI)\qquad
\P(\yR_{a}\mo\yn \and \yR_{b}\mo\yn \| \yI)
\end{gathered}
$$

are completely determined if we specify just one of them.

:::{.callout-caution}


Assume that the state of knowledge $\yI$ implies exchangeability, and a population has binary variate $\yR \in \set{\yn,\yy}$.

If

$$\P(\yR_{1}\mo\yy\|\yI) = 0.75 \qquad \P(\yR_{4}\mo\yy \and \yR_{9}\mo\yy \|\yI) = 0.60$$

Then how much are the probabilities

$$\P(\yR_{15}\mo\yy \and \yR_{3}\mo\yn \|\yI) = \mathord{?} \qquad
\P(\yR_{7}\mo\yn \and \yR_{11}\mo\yn \|\yI) = \mathord{?}$$

<!-- 0.15, 0.10 -->
:::

