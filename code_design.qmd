# [Implementing an OPM]{.red} {#sec-code-design}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}
{{< include macros_opm.qmd >}}

We now try to build up a real prototype AI agent from basic principles, using the formulae summarized in [ch. @sec-summary-formulae] and in the previous chapter. By design, this agent is as close to optimal as theoretically possible; so let's call it an

![](optimal_predictor_machine.png){width=75%}

or *OPM* for short.

Before starting, let's agree on some terminology so as not to get confused in the discussion below.

- We shall call [*task*]{.blue} a *repetitive* inference problem with a specified set of units and variates. For instance, a task could be the consecutive prediction of the urgency of several incoming patients, given their mean of transportation.
- We shall call [*application*]{.blue} or [*instance*]{.blue} of the task a *single* inference about a new unit, for example one new incoming patient.

\

## Desired characteristics of the OPM {#sec-characteristics-opm}

We design our Optimal Predictor Machine with the following specific characteristics:

- It handles variates of *nominal* type ([§@sec-basic-types]).

- It handles inferences and decisions about approximately infinite populations, and its beliefs about the population are *exchangeable* ([ch. @sec-exchangeable-beliefs]).

- Its initial beliefs about the population frequencies are represented by a *Dirichlet-mixture distribution* ([ch. @sec-dirichlet-mix]).

- Before deployment, it learns from a set of $N$ units.
\

### Example of what kind of agent we want

Let's give an example of what we want our agent to be able to do. Suppose we have a population having three nominal variates $Z = (Y \and X \and W)$.\  Abbreviate the set of $N$ training data as

$\data \defd
( Z_{N}\mo z_{N} \and \dotsb \and
Z_{2}\mo z_{2} \and
Z_{1}\mo z_{1} )$    

:::{.column-margin}
Recall that $Z$ denotes all (nominal) variates of the population
:::

where $z_N, \dotsc, z_2, z_1$ are specific values, stored in some training dataset. To simplify things, we assume that no values are missing.

We want an agent that can draw inferences like the following ones, as often as required:

- $P(X\mo\dotso\|\data \and \yD)$,\  $P(Y\mo\dotso\|\data \and \yD)$,\ etc.: inference about a predictand variate, without knowledge of any predictors.

- $P(Y\mo\dotso \and W\mo\dotso\|\data \and \yD)$: same but for any two predictand variates.

- $P(Y\mo\dotso \and X\mo\dotso \and W\mo\dotso\|\data \and \yD)$: same but for all three variates.

- $P(X\mo\dotso\|Y\mo\dotso \and \data \and \yD)$: inference about any one predictand variate, given information about one predictor variate.

- $P(X\mo\dotso\| Y\mo\dotso \and W\mo\dotso \and\data \and  \yD)$: same, but given information about any pair of predictors.

- $P(Y\mo\dotso \and W\mo\dotso\|X\mo\dotso \and \data \and \yD)$: inference about any two predictand variates, given information about one predictor.

Note that *we are not fixing beforehand which variates are predictands and which are predictors*. Once the agent has learnt from the training data, we want to be able to change *on the fly, at each new application*, what the predictands are, and what the predictors are (if any).

Pause for a second and *ponder about the flexibility that we are requesting from our prototype agent!* Consider that virtually all present-day machine-learning algorithms only work one way: a machine-learning algorithm designed to guess a label from some features cannot guess features from a label. Will we really manage to build an agent with the amazing versatility illustrated above?


## Computations needed and mathematical challenges in the code {#sec-code-computations}

The examples above of requested inferences show that the OPM agent must essentially use formulae (@eq-dmagent-y)--(@eq-dmagent-aux) from [§@sec-formulae-dirmix], which we repeat here:

::::{.callout-note}
##  

$\P(Y\mo y \| \data, \yD)
=
\sum_{\ya = \amin}^{\amax}
\Bigl(\tfrac{2^\ya}{M_Y} + \#y\Bigr)
\cdot \aux(\ya)$   (@eq-dmagent-y)

$\P(Y\mo y \| X\mo x \and \data, \yD)
=
\frac{
\sum_{\ya = \amin}^{\amax}
\Bigl(\tfrac{2^\ya}{M_Y \cdot M_X} + \#(y,x)\Bigr)
\cdot \aux(\ya)
}{
\sum_{y}\sum_{\ya = \amin}^{\amax}
\Bigl(\tfrac{2^\ya}{M_Y \cdot M_X} + \#(y,x)\Bigr)
\cdot \aux(\ya)
}$   (@eq-dmagent-yx)

with\ \ $\aux(\ya)
\defd
\frac{
\frac{
\prod_{x,y,w} \Bigl(\frac{2^{\ya}}{M} + \#(x,y,w) - 1\Bigr)!
}{
\bigl(2^{\ya} + N -1 \bigr)!
}
\cdot
\frac{
\bigl(2^{\ya} -1 \bigr)!
}{
{\Bigl(\frac{2^{\ya}}{M} - 1\Bigr)!}^M
}
}{
\sum_{\yb = \amin}^{\amax}
\frac{
\prod_{x,y,w} \Bigl(\frac{2^{\yb}}{M} + \#(x,y,w) - 1\Bigr)!
}{
\bigl(2^{\yb} + N \bigr)!
}
\cdot
\frac{
\bigl(2^{\yb} -1 \bigr)!
}{
{\Bigl(\frac{2^{\yb}}{M} - 1\Bigr)!}^M
}
}$   (@eq-dmagent-aux)
::::

The values of $\aux(\ya)$ can be calculated just once, when the OPM agent is built, and stored. Subsequently the agent will draw inferences by using (@eq-dmagent-y) or (@eq-dmagent-yx) as needed. To use those formulae, the agent needs to store the counts $\#(x,y,w,\dotsc)$, which it found in the training data, for all combinations of values $x,y,w,\dotsc$.
\

This kind of storage and computation could be implemented in a straightforward way if we had unlimited storage and computation precision. But in a real implementation we must face the following difficulties:

[{{< fa circle-exclamation >}} Finite precision]{.red}
: Owing to finite precision, the operations in the formulae may easily lead to overflow or underflow: large numbers are treated as `infinity`, and small non-zero numbers as `0`. For instance this is what happens if we directly compute something like $(2^{10})! / (2^{10})!$, obviously equal to $1$:
```{r}
factorial(2^10) / factorial(2^10)
```
One way to bypass this problem is by rewriting the formulae in ways that are mathematically equivalent but less prone to over- and under-flow. For example we can use identities like

$$
x / y = \exp\bigl(\ln x - \ln y\bigr)\ ,\quad x, y > 0 \ .
$$

Now indeed it works; note that `lfactorial()` is `log(factorial())` in R:
```{r}
exp( lfactorial(2^10) - lfactorial(2^10) )
```
Another useful identity that avoids over- and under-flow, if $\pmb{x}$ is a vector of positive numbers, is the following:

$$
\frac{\pmb{x}}{\operatorname{\texttt{sum}}(\pmb{x})}
=
\frac{
\operatorname{\texttt{exp}}\bigl(\operatorname{\texttt{log}}(\pmb{x})
- \operatorname{\texttt{max}}(\operatorname{\texttt{log}}(\pmb{x}))\bigr)
}{\operatorname{\texttt{sum}}\bigl(
\operatorname{\texttt{exp}}\bigl(\operatorname{\texttt{log}}(\pmb{x})
- \operatorname{\texttt{max}}(\operatorname{\texttt{log}}(\pmb{x}))\bigr)
\bigr)}
$$

\

[{{< fa circle-exclamation >}} Storage]{.red}
: With many variates and large domains, we may run out of memory in storing all possible counts $\#(x,y,w,\dotsc)$. For instance if we have four variates with 20 possible values each, we would need to store $4^{20}$ integers, which would take more than 4 000 GB:
```{r}
try( x <- integer(length = 4^20) )
```
We can bypass this problem again by using smart mathematical manipulations. In the case of formulae (@eq-dmagent-y)--(@eq-dmagent-aux), the product over all possible values $(x,y,w,\dotsc)$ can be rewritten in one over all different *values of the counts*, which usually has much fewer terms. For example, if we have $N=10000$ datapoints, and $4^{20} -1$ counts are equal to $9000$, while one count is equal to $1000$, then we only need to store these *four* numbers rather than $4^{20}$ numbers!

Many such mathematical tricks are used in finite-precision computations. Unfortunately their knowledge requires a separate course.

\

[{{< fa circle-exclamation >}} Speed]{.red}
: The formulae that the agent uses may involve sums over many terms, or repeated computations for many different variate values. Thus computation speed may become an issue.






## Range of use of the code {#sec-code-range}

The concrete formulae discussed in the previous [chapter @sec-dirichlet-mix] can be put into code, for use in different tasks involving only nominal variates. Software of this kind can in principle be written to allow for some or all of the versatility discussed in §§ [-@sec-categ-probtheory]--[-@sec-underlying-distribution], for example the possibility of taking care (in a first-principled way!) of partially missing training data. But the more versatile we make the software, the more memory, processing power, and computation time it will require.

Roughly speaking, more versatility corresponds to calculations of the joint probability

::::{.column-page-right}
:::{.callout-note}
##  

$$
\P(
\blue 
Z_{L}\mo  z_{L}
\and
\dotsb \and
Z_{1}\mo z_1
\black
\| \yD
)
=
\frac{1}{\amax-\amin+1}
\sum_{\ya=\amin}^{\amax}
\frac{
\prod_{\bz} \bigl(\frac{2^{\ya}}{M} + \#\bz - 1\bigr)!
}{
\bigl(2^{\ya} + L -1 \bigr)!
}
\cdot
\frac{
\bigl(2^{\ya} -1 \bigr)!
}{
{\bigl(\frac{2^{\ya}}{M} - 1\bigr)!}^M
}
\quad
$$ {#eq-main-joint}

:::
::::

for more values of the quantities $\blue Z_1, Z_2, \dotsc$. For instance, if data about unit #4 are missing, then we need to calculate the joint probability above for several (possibly all) values of $\blue Z_4$. If data about two units are missing, then we need to do an analogous calculation for all possible *combinations* of values; and so on.

For our prototype, let's forgo versatility about units used as training data. From now on we abbreviate the set of training data as

:::{.column-margin}
Recall that $\bZ$ denotes all (nominal) variates of the population
:::

$$
\data \defd
(
Z_{N}\mo z_{N} \and \dotsb \and
Z_{2}\mo z_2 \and
Z_{1}\mo z_{1}
)
$$

where $\blue z_N, \dotsc, z_2, z_1$ are specific values, stored in some training dataset. No values are missing.

Since the training $\data$ are given and fixed in a task, we omit the suffix "${}_{N+1}$" that we have often used to indicate a "new" unit. So "$\blue Z\mo z$" simply refers to the variate $\bZ$ in a new application of the task.

We allow for full versatility in every new instance. This means that we can accommodate, *on the spot at each new instance*, what the predictand variates are, and what the predictor variates (if any) are. For example, if the population has three variates $\bZ=(\bA \and \bB \and \bC)$, our prototype can calculate, at each new application, inferences such as

- $P(\bB\mo\dotso\|\data \and \yD)$: any one predictand variate, no predictors

- $P(\bA\mo\dotso \and \bC\mo\dotso\|\data \and \yD)$: any two predictand variates, no predictors

- $P(\bA\mo\dotso \and \bB\mo\dotso \and \bC\mo\dotso\|\data \and \yD)$: all three variates

- $P(\bB\mo\dotso\|\bA\mo\dotso \and \data \and \yD)$: any one predictand variate, any other one predictor

- $P(\bB\mo\dotso\| \bA\mo\dotso \and \bC\mo\dotso \and\data \and  \yD)$: any one predictand variate, any other two predictors

- $P(\bA\mo\dotso \and \bC\mo\dotso\|\bB\mo\dotso \and \data \and \yD)$: any two predictand variates, any other one predictor


## Code design and computations needed {#sec-code-computations2}


To enjoy the versatility discussed above, the code needs to compute

::::{.column-page-right}
:::{.callout-note}
##  

$$
\P(
\blue Z \mo z
\and
\green\data
\black \| \yD)
=
\frac{1}{\amax-\amin+1}
\sum_{\ya=\amin}^{\amax}
\Biggl(\frac{2^{\ya}}{M} + {\green\#}\bz\Biggr)
\cdot
\frac{
\prod_{\bz} \bigl(\frac{2^{\ya}}{M} + {\green\# z} - 1\bigr)!
}{
\bigl(2^{\ya} + N \bigr)!
}
\cdot
\frac{
\bigl(2^{\ya} -1 \bigr)!
}{
{\bigl(\frac{2^{\ya}}{M} - 1\bigr)!}^M
}
$$ {#eq-objectP}

for all possible values $\bz$, where ${\green\#}\bz$ is the number of times value $\bz$ appears **in the training [data]{.green}**, and $N = \sum_{\green z}{\green\# z}$ is the number of training data
:::
::::

This formula is just a rewriting of formula (@eq-main-joint) for $L=N+1$, simplified by using the property of the factorial

$$(a+1)! = (a+1) \cdot a!$$



But the computation of formula (@eq-objectP) (for all values of $\bz$) must be done *only once* for a given task. For a new application we only need to combine these already-computed probabilities via sums and fractions. For example, in the three-variate case above, if in a new application we need to forecast $\red A\mo a$ given $\yellow C\mo c$, then we calculate

::::{.column-page-inset-right}
:::{.callout-note}
##  

(example with $Z \defd ({\red A}, {\blue B}, {\yellow C})$)

$$
P(\red A\mo a \black \|\yellow C\mo c \black \and \data \and \yD)
=
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \yellow C\mo c \black \and \data \| \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \yellow C\mo c \black \and \data \| \yD)
}
\quad
$$ {#eq-forecast}

:::
::::

where all $P(\red A\mo\dotso \black \and \blue B\mo\dotso \black \and \yellow C\mo\dotso \black \and \data \| \yD)$ are already computed.

\

Our prototype software must therefore include two main functions, which we can call as follows:

<!-- you can add url to specific line n by appending #Ln -->

- `buildagent()` ([see code](https://github.com/pglpm/ADA511/blob/code/OPM_nominal.R))
: computes $\green\#\bz$ for all values $\bz$, as well as the multiplicative factors
    
	$$
	\frac{
	\bigl(2^{\ya} -1 \bigr)!
}{
\bigl(2^{\ya} + N \bigr)!
\cdot
{\bigl(\frac{2^{\ya}}{M} - 1\bigr)!}^M
}
$$
    
    for all $k$, in (@eq-objectP). This computation is done once and for all in a given task, using the training $\data$ and the metadata $\yD$ provided. The result can be stored in an array or similar object, which we shall call an `agent`-class object.

- `infer()` ([see code](https://github.com/pglpm/ADA511/blob/master/code/OPM_nominal.R))
: computes probabilities such as (@eq-forecast) at each new instance, using the stored `agent`-class object as well as the predictor variates and values provided with that instance, and the predictand variates requested at that instance.

\

We shall also include four additional functions for convenience:

- `guessmetadata()`
: builds a preliminary metadata file, encoding the background information $\yD$, from some dataset.

- `decide()`
: makes a decision according to expected-utility maximization ([chapter @sec-basic-decisions]), using probabilities calculated with `infer()` and utilities.

- `rF()`
: draws one or more possible full-population frequency distribution $\vf$, according to the updated degree of belief $\p(F\mo\vf \| \data \and \yD)$

- `plotFsamples1D()`
: plots, as a generalized scatter plot, the possible full-population marginal frequency distributions for a single (not joint) predictand variate. If required it also also the final probability obtained with `infer()`.

- `mutualinfo()`
: calculates the mutual information ([§@sec-entropy-mutualinfo]) between any two sets of variates.

::::{.column-body-outset-right}
:::{.callout-caution}



Using the `and`-rule, prove (pay attention to the conditional "$\|$" bar):

$$
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \yellow C\mo c \black \and \data \| \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \yellow C\mo c \black \and \data \| \yD)
}
=
\frac{
\sum_{\blue b}
P(\red A\mo a \black \and \blue B\mo b \black \and \yellow C\mo c \black \| \data \and \yD)
}{
\sum_{\purple \alpha}\sum_{\blue b}
P(\red A\mo {\purple \alpha} \black \and \blue B\mo b \black \and \yellow C\mo c \black \| \data \and \yD)
}
$$

\

This exercise shows that instead of

$$\P(\blue Z \mo z \black \and \green\data \black \| \yD)$$

we could calculate

$$
\P(
\blue Z \mo z
\black \|
\green\data
\black \and  \yD)
$$

once for all possible values $\bz$, and use that. Mathematically and logically the two ways are completely equivalent. Numerically they can be different as regards precision or possible overflow errors. Using $\P( \blue Z \mo z \black \| \green\data \black \and \yD)$ would be convenient if our basic formula (@eq-main-joint) didn't contain the sum $\sum_k$ over the $k$ index. Our code shall instead use $\P(\blue Z \mo z \black \and \green\data \black \| \yD)$ because it leads to slightly more precision and speed in some tasks.

:::
::::




## Code optimization {#sec-code-optim}

The formulae of [chapter @sec-dirichlet-mix], if used as-written, easily lead to two kinds of computation problems. First, they generate overflows and `NaN`, owing to factorials and their divisions. Second, the products over variates may involve so many terms as to require a long computation time. In the end we would have to wait a long time just to receive a string of `NaN`s.

The first problem is dealt with by rewriting the formulae in terms of logarithms, and renormalizing numerators and denominators of fractions. See for example the lines defining `auxalphas` in the [`buildagent()`](https://github.com/pglpm/ADA511/blob/master/code/OPM_nominal.R) function, and the line that redefines `counts` one last time in the [`infer()`](https://github.com/pglpm/ADA511/blob/master/code/OPM_nominal.R) function.

The second problem is dealt with by reorganizing the sums as multiples of identical summands; see the lines working with `freqscounts` in the `buildagent()`function.

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
§6.1 in [*Numerical Recipes*](references.html)
:::
::::



