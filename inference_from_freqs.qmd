# [Inferences from frequencies]{.green} {#sec-inference-from-freqs}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

## If the population frequencies were known {#sec-pop-freq-known}

Let's now see how the exchangeability of an agent's degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where *the agent knows the joint frequency distribution* (§§[-@sec-freq-distr], [-@sec-joint-freq], [-@sec-limit-freqs]) *for the full population*. Second, in the more general case where the agent lacks this population-frequency information.

When the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical "drawing balls from an urn". We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned "by intuition", but actually fully determined by the two basic pieces of knowledge or assumptions: *exchangeability* and *known population frequencies*. Some simple proof sketches of this will also be given.

We consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. In this and the following chapters we shall often use the simplified [income]{.midgrey} dataset (file [`income_data_nominal_nomissing.csv`](datasets/income_data_nominal_nomissing.csv) and its underlying population as an example. This population has nine nominal variates. The variates, their domain sizes, and their possible values are listed [at this link](https://github.com/pglpm/ADA511/blob/master/datasets/meta_income_data_nominal_nomissing.csv).


### Notation recap

We shall mainly use the notation introduced in [§@sec-categ-probtheory]:

All population variates, jointly, are denoted $\bZ$. In the case of the income dataset, for instance, the variate $\bZ$ stands for the joint variate with nine components:

$$
\begin{aligned}
\bZ &\defd (\blue
\mathit{workclass} \and 
\mathit{education} \and 
\mathit{marital\_status} \and 
\mathit{occupation} \and 
{}\\ &\qquad
\blue\mathit{relationship} \and
\mathit{race} \and 
\mathit{sex} \and 
\mathit{native\_country} \and 
\mathit{income}
\black)
\end{aligned}
$$

When we write $\blue Z \mo z$, the symbol $\blue z$ stands for some definite joint values, for instance $({\blue\cat{Without-pay} \and \cat{Doctorate} \and \dotsb \and \cat{Ireland} \and \cat{>50K}})$.

In applications where the agent wants to infer the values of some predictand variates, given the observation of predictor variates, the former are denoted $\bY$, the latter $\bX$. In the [income]{.midgrey} problem, for instance, the agent (some USA census agency) would like to infer the $\blue\mathit{income}$ variate of a person from the other eight demographic characteristics $\blue\mathit{workclass} \and \mathit{education} \and \dotsb$ of that person. So in this inference problem we define

$$
\begin{aligned}
\bY &\defd {\blue\mathit{income}}
\\[1ex]
\bX &\defd ({\blue\mathit{workclass} \and \mathit{education} \and \dotsb \and \mathit{sex} \and \mathit{native\_country}})
\end{aligned}
$$

We shall, however, also consider slightly different inference problems, for example with $({\blue\mathit{race} \and \mathit{sex}})$ as predictand and the remaining seven variates $({\blue\mathit{workclass} \and \dotsb \and \mathit{income}})$ as predictors.

Often we shall use [red]{.red} for quantities that are not known in the problem, and [green]{.green} for quantities that are known.


## Knowing the full-population frequency distribution {#sec-know-freq}

Now suppose that the agent knows the *full-population joint frequency distribution*. Let's make clearer what this means. In the [income]{.midgrey} problem, for instance, consider these two different joint values for the joint variate $\bZ$:

<!-- `example_freqdistr.rds`, items 2 and 26 in reverse-ordered frequencies -->
$$
\begin{aligned}
{\blue z^{*}}&\defd (
\cat{Private} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and \cat{Machine-op-inspct} \and {}
\\
&\qquad\cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\\[2ex]
{\blue z^{**}}&\defd (
\cat{Self-emp-not-inc} \and \cat{HS-grad} \and \cat{Married-civ-spouse} \and {}
\\
&\qquad \cat{Farming-fishing} \and \cat{Husband} \and \cat{White} \and \cat{Male} \and \cat{United-States} \and \cat{<=50K}
)
\end{aligned}
$$

The agent knows that the value $\blue Z\mo z^{*}$ occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency $0.860 369\%$; it also knows that the value $\blue Z\mo z^{**}$ occurs with a relative frequency $0.260 058\%$. We write this as follows:

$$
f({\blue Z\mo z^{*}}) = 0.860 369\% \ ,
\qquad
f({\blue Z\mo z^{**}}) = 0.260 058\%
$$

The agent knows not only the frequencies of the two particular joint values $\blue z^{*}$, $\blue z^{**}$, but for *all possible* joint values, that is, for all possible combinations of values from the single variate. In the [income]{.midgrey} example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for $\bZ$, which we denote collectively with $\vf$ (note the boldface). Let's introduce the quantity $F$, denoting the full-population frequency distribution. Knowledge that the frequencies are $\vf$ is then expressed by the sentence $F\mo\vf$.

:::{.callout-important}
## {{< fa exclamation-triangle >}} Don't confuse the full population with a sample from it
Note that the frequencies reported above are *not* the ones found in the [`income_data_nominal_nomissing.csv`](datasets/income_data_nominal_nomissing.csv) dataset, because that dataset is only a *sample* from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).
:::



In other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range.

## Inference about a single unit {#sec-1unit-freq-known}

Now imagine that the agent, given the information $F\mo\vf$ about the frequencies and some background information $\yI$, must infer all $\bZ$ variates for a specific unit $u$. In the [income]{.midgrey} case, it would be an inference about a specific USA citizen. This unit $u$ could have any particular combination of variate values; in the [income]{.midgrey} case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.^[Remember that this is what we mean when we say "drawing an inference"! (See [chap. @sec-what-inference] and [§@sec-distribute-prob])] Which probability values should it assign?

Intuitively we would say that the probability for a particular value $\blue Z\mo z$ should be equal to the frequency of that value in the full population:


:::{.callout-note style="font-size:120%"}
##  
::::{style="font-size:120%"}

if $\yI$ leads to an exchangeable probability distribution, then

$$
\P(\bZ_{u}\mo {\blue z} \| F\mo\vf \and \yI) = f(\bZ\mo {\blue z})
$$

for any unit $u$.
::::
:::



For instance, the probabilities that unit $u$ has the values $\blue z^{*}$ or $\blue z^{**}$ above is

$$
\begin{aligned}
&\P(\bZ_{u}\mo {\blue z^{*}} \| F\mo\vf \and \yI) =
f(\bZ \mo {\blue z^{*}}) = 0.860 369\%
\\[1ex]
&\P(\bZ_{u}\mo {\blue z^{**}} \| F\mo\vf \and \yI) =
f(\bZ \mo {\blue z^{**}}) = 0.260 058\%
\end{aligned}
$$

This intuition is the same as in drawing balls, which may have different sets of labels, from a collection, given that we know the proportion of balls with each possible label set.

But the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let's examine a very simple case to get an idea of how this proof works.

### Exact calculation of the probabilities in a simple case

Suppose we have three rocks from our Mars-prospecting collection. They are marked #1, #2, #3. They look alike, but we know that two of them have haematite, so $\yR\mo\yy$ for them, and one doesn't, so $\yR\mo\yn$ for that rock. This background information -- let's call it $\yut$ -- is a simple case of a finite population with three units and a binary variate $\yR$. We know that the frequency distribution for this population is

:::{.column-margin}
![](mars_crater2.jpg){width=50%}
:::


$$f(\yR\mo\yy) = 2/3 \qquad f(\yR\mo\yn) = 1/3$$

Our information $F\mo\vf$ about the frequencies corresponds to the following composite sentence:

:::{.column-page-inset-right}
$$
 F\mo\vf \ \Longleftrightarrow\ 
(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn)
\lor
(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy)
\lor
(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy)
$$
:::

Given $F\mo\vf$, we know that $F\mo\vf$ is true: $\P( F\mo\vf \| F\mo\vf \and \yut)=1$, which means

:::{.column-page-inset-right}
$$
\P\bigl[(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn)
\lor
(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy)
\lor
(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy)
\|[\big] F\mo\vf , \yut\bigr] = 1
$$
:::

Now use the `or`-rule, considering that the three `or`ed sentences are mutually exclusive:

:::{.column-page-right}
$$
\begin{aligned}
1&=\P\bigl[(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn)
\lor
(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy)
\lor
(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy)
\|[\big] F\mo\vf \and \yut\bigr]
\\[2ex]
&=
\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn \| F\mo\vf \and \yut)
+{}
\\&\qquad
\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy \| F\mo\vf \and \yut)
+{}
\\&\qquad
\P(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy \| F\mo\vf \and \yut)
\end{aligned}
$$
:::

According to our background information $\yut$, our degrees of belief are exchangeable. This means that the three probabilities summed up above must all have the same value, because in each of them $\yy$ appears twice and $\yn$ once. But if we are summing the same value thrice, and the sum is $1$, that that value must be $1/3$. Hence we find that

$$
\begin{aligned}
&\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn \| F\mo\vf \and \yut)
= 1/3
\\
&\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy \| F\mo\vf \and \yut)
= 1/3
\\
&\P(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy \| F\mo\vf \and \yut)
= 1/3
\\[1ex]
&\text{all other probabilities are zero}
\end{aligned}
$$

Now let's find the probability that a rock, say #1, has haematite ($\yy$), given that we haven't observed any other rocks: $\P(\yR_1\mo\yy \| F\mo\vf \and \yut)$. This is a marginal probability ([§@sec-marginal-probs]), so it's given by the sum

$$
\begin{aligned}
\P(\yR_1\mo\yy \| F\mo\vf \and \yut) &=
\sum_{i=\yy}^{\yn}\sum_{j=\yy}^{\yn}
\P(\yR_1\mo\yy \and \yR_2\mo i \and \yR_3\mo j \| F\mo\vf \and \yut)
\\[1ex]
&=
\P(\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yy \| F\mo\vf \and \yut) + {}
\\ &\qquad
\P(\yR_1\mo\yy \and \yR_2\mo\yy \and \yR_3\mo\yn \| F\mo\vf \and \yut) + {}
\\ &\qquad
\P(\yR_1\mo\yy \and \yR_2\mo\yn \and \yR_3\mo\yy \| F\mo\vf \and \yut) + {}
\\ &\qquad
\P(\yR_1\mo\yy \and \yR_2\mo\yn \and \yR_3\mo\yn \| F\mo\vf \and \yut)
\\[1ex]
&= 0 + 1/3 + 1/3 + 0
\\[1ex]
&= 2/3
\end{aligned}
$$

which is indeed equal to $f(\yR\mo\yy)$.

\

This simple example gives you an idea why our intuition for equating -- in specific circumstances -- probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) **exchangeability**.

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
- Calculate $\P(\yR_2\mo\yy \| F\mo\vf \and \yut)$, that is, the probability that rock #2 has haematite, given that we don't know the haematite content of any other rock. Is it different from $\P(\yR_1\mo\yy \| F\mo\vf \and \yut)$, or not? Why?

- Build a similar proof for a slightly different case; for example four rocks; or two units from a population with a variate having three possible values (instead of just the two $\set{\yy,\yn}$).

- Consider the same calculation we did above, but in the case of background knowledge $\se{K}_{\text{NE}}$ where our degrees of belief are [**$\text{N}$ot $\text{E}$xchangeable**.]{.m} For instance, give three *different* values to the probabilities
    
    $$
\begin{gathered}
\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yn \| F\mo\vf \and \se{K}_{\text{NE}})
\\
\P(\yR_{1}\mo\yy \and 
\yR_{2}\mo\yn \and 
\yR_{3}\mo\yy \| F\mo\vf \and \se{K}_{\text{NE}})
\\
\P(\yR_{1}\mo\yn \and 
\yR_{2}\mo\yy \and 
\yR_{3}\mo\yy \| F\mo\vf \and \se{K}_{\text{NE}})
\end{gathered}
$$
    
    in such a way that they still sum up to $1$. Then find by marginalization the probability that rock #1 contains haematite ($\yy$). Is this probability still equal to the relative frequency of $\yy$?
:::


## Inference about several units {#sec-moreunit-freq-known}

Let's continue with the Mars-prospecting example of the previous section, with just three rocks. We found that the probability that rock #1 has haematite ($\yy$) was $2/3$, given that we haven't observed any other rocks. This probability was equal to the frequency of $\yy$-rocks in the urn.

Now suppose that we observe rock #2, and it turns out to have haematite ($\yR_2\mo\yy$). What is the probability that rock #1 has haematite?

The probability we are asking about is $\P(\yR_{1}\mo\yy \| \yR_{2}\mo\yy \and F\mo\vf \and \yut)$, and it can be calculated with the usual rules. The result is again the same as the frequency of the $\yy$-rocks, but *with respect to the new situation*: there are now two rocks left in front of us, and one must contain haematite, while the other doesn't. The probability is therefore $1/2$, a value different from that we found before, $2/3$:

$$
\P(\yR_{1}\mo\yy \| F\mo\vf \and \yut) = 2/3
\qquad
\P(\yR_{1}\mo\yy \| \yR_{2}\mo\yy \and F\mo\vf \and \yut) = 1/2
$$

This situation is quite general: in a collection of many rocks, the probabilities for new observations change accordingly to information about previous observations (and also subsequent ones, if already known).

\

But consider now the case $\yul$ of a large collection of 3 000 000 rocks, 2 000 000 of which have haematite and the rest doesn't.^[Note how this scenario becomes very similar to that of coin tosses.] The population's relative frequencies are exactly as in the case with three rocks, and for the probability that rock #1 contains haematite we still have

$$
\P(\yR_1\mo\yy \| F\mo\vf \and \yul) = 
f(\yR\mo\yy) = \frac{2 000 000}{3 000 000} = 2/3
$$

Now suppose we examine rock #2 and find haematite. What is the probability that rock #1 also contains haematite? In this case we find

$$
\P(\yR_1\mo\yy \| \yR_2\mo\yy \and F\mo\vf \and \yul) = 
\frac{1 999 999}{2 999 999} \approx 2/3
$$

with an absolute error of only $0.000 000 1$. That is, the probability and frequency are almost the same as before examining rock #2. The reason is clear: the number of rocks is so large that observing some of them doesn't practically change the content and proportions of the whole collection.

The joint probability that rock #2 contains haematite and rock #1 doesn't is therefore, by the `and`-rule,

:::{.column-page-inset-right}
$$
\begin{aligned}
\P(\yR_2\mo\yy \and \yR_1\mo\yy \| F\mo\vf \and \yul) &=
\P(\yR_1\mo\yy \| \yR_2\mo\yy \and F\mo\vf \and \yul) \cdot
\P(\yR_2\mo\yy \| F\mo\vf \and \yul)
\\[1ex]
&\approx f(\yR\mo\yy) \cdot f(\yR\mo\yy)
\end{aligned}
$$
:::

the approximation being the better, the larger the collection of rocks.

It is easy to see that this will hold for more observations, and for different and more complex variate domains, as long as the number of units considered is enough small compared with the population size. For instance

:::{.column-page-inset-right}
$$
\P(\yR_4\mo\yy \and \yR_3\mo\yn \and \yR_2\mo\yn \and \yR_1\mo\yy \| F\mo\vf \and \yul)
\approx 
f(\yR\mo\yy) \cdot f(\yR\mo\yn) \cdot f(\yR\mo\yn) \cdot f(\yR\mo\yy)
$$
:::

where $\vf$ is the *initial* frequency distribution for the population.

\

This situation applies to more general populations: if the full-population frequencies are known, the agent's beliefs are exchangeable, and the population is practically infinite, then the joint probability that some units have a particular set of values is equal to the product of the frequencies of those values.

:::::{.column-page-right}
:::{.callout-note style="font-size:120%"}
##  
::::{style="font-size:120%"}

If an agent:

- has background information $\yI$ about a population saying that
    + beliefs about units are exchangeable
    + the population size is practically infinite
- has full information $F\mo\vf$ about the population frequencies $\vf$ for the variate $\bZ$

then

$$
\P(
\bZ_{u'}\mo {\blue z'} \and 
\bZ_{u''}\mo {\blue z''} \and 
\bZ_{u'''}\mo {\blue z'''} \and 
\dotsb
\| F\mo\vf \and \yI) 
\approx
f(\bZ\mo {\blue z'}) \cdot
f(\bZ\mo {\blue z''}) \cdot
f(\bZ\mo {\blue z'''}) \cdot
\dotsb
$$

for any (different) units $u', u'', u''', \dotsc$ and any (even equal) values $\blue z', z'', z''', \dotsc$.
::::
:::
:::::


The formula above solves our initial problem: *How to calculate and encode the joint probability distribution for the full population?*, although it does so only in the case where the full-population frequencies $\vf$ are known. In this case this probability is encoded in the $\vf$ itself (which can be represented as a multidimensional array), and can be calculated for any desired joint probability distribution just by a multiplication.

In the [income]{.midgrey} example from [§@sec-know-freq], the probability that two units (citizens) [#$a$,]{.m} [#$c$]{.m} have value $\blue Z\mo z^{**}$ and one unit [#$b$]{.m} has value $\blue Z \mo z^{*}$ is

:::{.column-page-inset-right}
$$
\begin{aligned}
\P(
\bZ_{a}\mo {\blue z^{**}} \and
\bZ_{b}\mo {\blue z^{*}} \and
\bZ_{c}\mo {\blue z^{**}}
\| F\mo\vf \and \yI)
&\approx
f(\bZ \mo {\blue z^{**}}) \cdot
f(\bZ \mo {\blue z^{*}}) \cdot
f(\bZ \mo {\blue z^{**}})
\\[1ex]
&=
0.260 058\% \cdot
0.860 369\% \cdot
0.260 058\%
\\
&= 0.000 005 818 7\%
\end{aligned}
$$
:::

:::{.callout-important}
## {{< fa exclamation-triangle >}} Always check whether the approximate formula above is appropriate
As we have seen, the product formula above is strictly speaking *only approximate*. In situations where the full population has practically infinite size compared to (1) the number of units that the agent uses for learning, and (2) the number of units the agent will draw inferences about, then the formula can be used as if it were exact.

But how much is "practically infinite"? No general answer is possible: it depends on the precision required in the specific problem. In some problems, even if learning and inference involve 10% of the units from the full population, the approximation might still be acceptable; but in other problems it might not be. If learning and inference involve 50% or more units from the full population, then the formula above is hardly acceptable.

The probability calculus and the four fundamental rules allow us to handle problems with any population size exactly (see the [Study reading]{.yellow} below), but the exact computation becomes involved and expensive. This is why the approximate product formula above is valuable, when it can be properly used.
:::

## No learning when full-population frequencies are known {#sec-no-learn-freqs}

Imagine an agent with exchangeable beliefs $\yI$ and knowledge $F\mo\vf$ of the full-population frequencies, who also has observed several units with values (possibly some identical) $\blue Z_{u'}\mo z' \and Z_{u''}\mo z'' \and Z_{u'''}\mo z''' \and \dotsb$. What is this agent's degree of belief that a new unit [#$u$]{.m} has value $\blue Z\mo z$?

From our basic formula for this question,

$$
\begin{aligned}
&\P(\red
Z_{u}\mo z 
\black\|\green 
Z_{u'}\mo z' \and 
Z_{u''}\mo z'' \and 
Z_{u'''}\mo z''' \and 
\dotsb \black \and F\mo\vf \and \yI)
\\[2ex]
&\qquad{}=\frac{
\P(\red
Z_{u}\mo z 
\black\and
\green Z_{u'}\mo z' \and 
Z_{u''}\mo z'' \and 
Z_{u'''}\mo z''' \and 
\dotsb \black \| F\mo\vf \and \yI) 
}{
\sum_{\purple z}
\P(
\red Z_{u}\mo \purple z 
\black\and
\green Z_{u'}\mo z' \and 
Z_{u''}\mo z'' \and 
Z_{u'''}\mo z''' \and 
\dotsb \black\| F\mo\vf \and \yI) 
}
\\[2ex]
&\qquad{}\approx\frac{
f({\red Z\mo z}) \cdot
f({\blue {\green Z\mo z'}}) \cdot 
f({\green Z\mo z''}) \cdot 
f({\green Z\mo z'''}) \cdot
\dotsb
}{
\sum_{\purple z}
f({\red Z\mo \purple z}) \cdot
f({\green Z\mo z'}) \cdot 
f({\green Z\mo z''}) \cdot 
f({\green Z\mo z'''}) \cdot
\dotsb
}
\\[2ex]
&\qquad{}=\frac{
f({\red Z\mo z}) \cdot
\cancel{f({\green Z\mo z'})} \cdot 
\cancel{f({\green Z\mo z''})} \cdot 
\cancel{f({\green Z\mo z'''})} \cdot
\cancel{\dotsb}
}{
\underbracket[0.2ex]{\sum_{\purple z}
f({\red Z\mo \purple z})}_{{}=1} \cdot
\cancel{f({\green Z\mo z'})} \cdot 
\cancel{f({\green Z\mo z''})} \cdot 
\cancel{f({\green Z\mo z'''})} \cdot
\cancel{\dotsb}
}
\\[2ex]
&\qquad{}=
f({\red Z\mo z})
\\[3ex]
&\qquad{}\equiv
\P(\red Z_{u}\mo z \black \| F\mo\vf \and \yI)
\end{aligned}
$$

so the information from the units $u'$, $u''$, and so on is *irrelevant* to this agent. In other words, this agent's inferences about some units are not affected  by the  observation of other units.

The reason for this irrelevance is that the agent *already knows the full-population frequencies*. So the observation of the frequencies of some values provides no new information to the agent.

Obviously this is not what we desire. But it is not a problem: the crucial point is that knowledge of full-population frequencies is only a hypothetical, idealized situation. In the next chapter we shall see that learning occurs when we go beyond this idealization.

:::{.callout-important}
## {{< fa exclamation-triangle >}} "Learning" about what?
In this and the following sections, and sometimes in the following chapters, when we say "the agent is learning" or "the agent is not learning" we mean specifically the change in an agent's beliefs [**about observation of variates of some units which had not yet been observed**]{.purple}.

Note that there is always learning about something whenever we put new information in the conditional of a probability. In the Mars-prospecting example above, for example, we have

$$
\P(\yR_{1} \mo \yy \|  \yul) = 2/3
\qquad
\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and \yul) = 2/3
$$

and the agent has (practically) not learned anything [about the sentence $\yR_1\mo\yy$]{.purple} from the sentence $\yR_2\mo\yn$.

But we also have

$$
\P(\yR_{2} \mo \yy \|  \yul) = 2/3
\qquad
\P(\yR_{2} \mo \yy \| \yR_{2} \mo \yn \and \yul) = 0
$$

the probability for the sentence $\yR_2\mo\yy$ has changed. So the agent has learned something: that rock #2 doesn't contain haematite ($\yn$).
:::

\

----

\

::: {.callout-warning}
## {{< fa book >}} Study reading
Ch. 3 of Jaynes: [*Probability Theory*](references.html). This chapter is extremely instructive in general to understand how probability theory works.
:::
