# [Third connection with machine learning]{.midgrey} {#sec-3-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-2-connection-ML] we made a second *tentative* connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance)]{.green}, and is assessing a [not-previously known piece of information (other partial aspects of a new task instance)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

The correspondence about [training data]{.green} and [architecture]{.yellow} seems somewhat convincing, the one about [outcome]{.red} needs more exploration.

Having introduced the notion of quantity in the latest chapters [-@sec-quantities-types-basic] and [-@sec-quantities-types-multi], we recognize that "training data" are nothing else but quantities with given values. So a datum $\se{D}_i$ can be expressed by a sentence like $Z_i\mo z_i$, where

- $i$ is the instance: $1,2,\dotsc,N, N+1$
- $Z_i$, a quantity, describes the type of data at instance $i$, for example "128 × 128 image with 24-bit colour depth, with a character label"
- $z_i$ is the value of the quantity $Z_i$ at instance $i$, for example the specific image & label enclosed here:

:::{.column-margin}
![label = "Saitama"](saitama_smile.png){width=128 fig-cap-location="center"}
:::

We can therefore rewrite the correspondence above as follows:

$$
\P(\underbracket[0ex]{\red Z_{N+1} \mo z_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{ Z_N \mo z_N \and \dotsb \and  Z_2 \mo z_2 \and  Z_1 \mo z_1}_{\mathclap{\green\text{training data?}}} 
\black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

This is the kind of inference that we explored in the "next-three-patients" scenario of [§@sec-conditional-joint-sim] and in some of its following sections. In [chapter @sec-beyond-ML], after a review of conventional machine-learning methods and terminology, we shall discuss with more care what these inferences are about, what kind of information they use, and how they can be concretely calculated.

But we have been speaking of "task instances" and "instances of quantities" quite vaguely so far. These are important notions: the whole idea of "learning from examples" hinges on them. In the next few chapters we shall therefore make them more rigorous. The theory at makes them rigorous is *Statistics*. As a bonus we shall find out that a rigorous analysis of the notion of "instances" also leads to concrete formulae to calculate the probabilities discussed above.
