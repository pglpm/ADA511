# [Third connection with machine learning]{.midgrey} {#sec-3-connection-ML}
{{< include macros.qmd >}}
{{< include macros_prob_inference.qmd >}}

In [chapter @sec-2-connection-ML] we made a second *tentative* connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some [built-in background information (corresponding to the algorithm's architecture)]{.yellow}, has received [pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance)]{.green}, and is assessing a [not-previously known piece of information (other partial aspects of a new task instance)]{.red}:

$$
\P(\underbracket[0ex]{\red\se{D}_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{\se{D}_N \land \dotsb \land \se{D}_2 \land \se{D}_1}_{\mathclap{\green\text{training data?}}} 
\black\land \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

The correspondence about [training data]{.green} and [architecture]{.yellow} seems somewhat convincing, the one about [outcome]{.red} needs more exploration.

Having introduced the notion of quantity in the latest chapters [-@sec-quantities-types-basic] and [-@sec-quantities-types-multi], we recognize that "training data" are just quantities, the values of which the agent has learned. So a datum $\se{D}_i$ can be expressed by a sentence like $Z_i\mo z_i$, where

- $i$ is the instance: $1,2,\dotsc,N, N+1$.
- $Z_i$, a quantity, describes the type of data at instance $i$, for example "128 × 128 image with 24-bit colour depth, with a character label".
- $z_i$ is the value of the quantity $Z_i$ at instance $i$, for example the specific image & label displayed here:

:::{.column-margin}
![label = "Saitama"](saitama_smile.png){width=128 fig-cap-location="center"}
:::

We can therefore rewrite the correspondence above as follows:

$$
\P(\underbracket[0ex]{\red Z_{N+1} \mo z_{N+1}}_{\mathclap{\red\text{outcome?}}} \| 
\green\underbracket[0ex]{ Z_N \mo z_N \and \dotsb \and  Z_2 \mo z_2 \and  Z_1 \mo z_1}_{\mathclap{\green\text{training data?}}} 
\black\and \underbracket[0ex]{\yellow\yI}_{\mathrlap{\yellow\uparrow\ \text{architecture?}}})
$$

This is the kind of inference that we explored in the "next-three-patients" scenario of [§@sec-conditional-joint-sim] and in some of the subsequent sections. In [chapter @sec-kinds-inferences], after a review of conventional machine-learning methods and terminology, we shall discuss with more care what these inferences are about, what kind of information they use, and how they can be concretely calculated.

\
In the last sections we have often been speaking about "instances", "instances of similar quantities", "task instances", and similar expression. What do with mean with "instance", more exactly? It is time that we make this and related notions more precise: the whole idea of "learning from examples" hinges on them. In the next few chapters we shall therefore make these ideas more rigorous and quantifiable. *Statistics* is the theory that deals with these ideas. As a bonus we shall find out that a rigorous analysis of the notion of "instances" also leads to concrete formulae for calculating the kind of probabilities discussed in the present chapter.
