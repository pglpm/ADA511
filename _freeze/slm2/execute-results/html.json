{
  "hash": "f39f27ec29c326d2583383c8205917c6",
  "result": {
    "engine": "knitr",
    "markdown": "# [The Optimal Predictor Machine generates text]{.red} {#sec-example-slm2}\n::: {.hidden}\n<!-- $$\\require{mathtools}$$ -->\n\n\\providecommand{\\ul}{\\uline}\n\\providecommand{\\and}{\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}}\n\\renewcommand*{\\|}[1][]{\\nonscript\\:#1\\vert\\nonscript\\:\\mathopen{}}\n\\providecommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\renewcommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\providecommand*{\\prq}[1]{\\textsf{\\small #1}}\n\\providecommand*{\\se}[1]{\\mathsfit{#1}}\n\\renewcommand{\\se}[1]{\\mathsfit{#1}}\n\\providecommand*{\\sei}[1]{\\mathsfit{\\small #1}}\n<!-- \\providecommand{\\cat}[1]{\\texttt{\\small #1}} -->\n\\providecommand{\\cat}[1]{{\\small\\verb;#1;}}\n\\providecommand{\\vec}[1]{\\boldsymbol{#1}}\n\\providecommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\P}{\\mathrm{P}}\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA}\n\\definecolor{quarto-callout-note-color-frame}{HTML}{4477AA}\n\\definecolor{quarto-callout-important-color}{HTML}{AA3377}\n\\definecolor{quarto-callout-important-color-frame}{HTML}{AA3377}\n\\definecolor{quarto-callout-warning-color}{HTML}{EE6677}\n\\definecolor{quarto-callout-warning-color-frame}{HTML}{EE6677}\n\\definecolor{quarto-callout-tip-color}{HTML}{228833}\n\\definecolor{quarto-callout-tip-color-frame}{HTML}{228833}\n\\definecolor{quarto-callout-caution-color}{HTML}{CCBB44}\n\\definecolor{quarto-callout-caution-color-frame}{HTML}{CCBB44}\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\nonscript\\mkern-3mu\\textrm{\\small#1}\\nonscript\\mkern-3mu}} -->\n\\providecommand*{\\mo}[1][=]{\\mathclose{}\\mathord{\\nonscript\\mkern0mu\\textrm{\\small#1}\\nonscript\\mkern0mu}\\mathopen{}}\n\\providecommand*{\\yX}{\\se{X}}\n\\providecommand*{\\yY}{\\se{Y}}\n\\providecommand*{\\yI}{\\se{I}}\n\\providecommand*{\\yi}[1][]{\\se{I}_{\\text{#1}}}\n\\providecommand{\\di}{\\mathrm{d}}\n\\providecommand{\\defd}{\\coloneqq}\n\\providecommand{\\blue}{\\color[RGB]{68,119,170}}\n\\providecommand{\\red}{\\color[RGB]{238,102,119}}\n\\providecommand{\\purple}{\\color[RGB]{170,51,119}}\n\\providecommand{\\green}{\\color[RGB]{34,136,51}}\n\\providecommand{\\yellow}{\\color[RGB]{204,187,68}}\n\\providecommand{\\lblue}{\\color[RGB]{102,204,238}}\n\\providecommand{\\grey}{\\color[RGB]{187,187,187}}\n\\providecommand{\\midgrey}{\\color[RGB]{119,119,119}}\n\\providecommand{\\black}{\\color[RGB]{0,0,0}}\n\\providecommand*{\\e}{\\mathrm{e}}\n\\providecommand*{\\pu}{\\text{π}}\n\\providecommand*{\\RR}{\\mathbf{R}}\n\n$\\DeclarePairedDelimiter{\\set}{\\{}{\\}}$\n$\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}$\n\\providecommand*{\\argmax}{\\operatorname{argmax}\\limits}\n<!-- \\DeclareMathOperator*{\\argmax}{argmax} -->\n\n<!-- \\renewcommand*{\\prq}[1]{\\textsf{\\small #1}} -->\n<!-- \\definecolor{lightblue}{HTML}{66CCEE} -->\n<!-- \\sethlcolor{lightblue} -->\n<!-- \\providecommand*{\\moo}[1][=]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\mkern-4mu#1\\mkern-4mu}} -->\n<!-- \\providecommand*{\\mo}[1][\\textrm{\\small=}]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n\n:::\n\n::: {.hidden}\n\\providecommand*{\\yon}{{\\green\\cat{on}}}\n\\providecommand*{\\yof}{{\\red\\cat{off}}}\n\\providecommand*{\\yy}{{\\lblue\\cat{Y}}}\n\\providecommand*{\\yn}{{\\yellow\\cat{N}}}\n\\providecommand{\\ypl}{{\\green\\cat{+}}}\n\\providecommand{\\ymi}{{\\red\\cat{-}}}\n\\providecommand{\\ypa}{{\\green\\cat{pass}}}\n\\providecommand{\\yfa}{{\\red\\cat{fail}}}\n<!-- \\providecommand{\\ypl}{\\mathord{\\green\\boldsymbol{+}}} -->\n<!-- \\providecommand{\\ymi}{\\mathord{\\red\\boldsymbol{-}}} -->\n\\providecommand{\\hi}{{\\green\\cat{high}}}\n\\providecommand{\\me}{{\\yellow\\cat{medium}}}\n\\providecommand{\\lo}{{\\red\\cat{low}}}\n\\providecommand*{\\yJ}{\\se{J}}\n\\providecommand{\\yva}{{\\lblue-1}}\n\\providecommand{\\yvb}{{\\midgrey0}}\n\\providecommand{\\yvc}{{\\yellow1}}\n\\providecommand*{\\yK}{\\se{K}}\n\\providecommand*{\\yL}{\\se{L}}\n\n\\providecommand*{\\yR}{R}\n\n\\providecommand*{\\bZ}{{\\blue Z}}\n\\providecommand*{\\bz}{{\\blue z}}\n\\providecommand*{\\rY}{{\\red Y}}\n\\providecommand*{\\bY}{{\\blue Y}}\n\\providecommand*{\\ry}{{\\red y}}\n\\providecommand*{\\gX}{{\\green X}}\n\\providecommand*{\\bX}{{\\blue X}}\n\\providecommand*{\\gx}{{\\green x}}\n\\providecommand*{\\vf}{\\vec{f}}\n<!-- \\providecommand*{\\if}{\\se{F}} -->\n\\providecommand*{\\yut}{\\se{K}_{\\textsf{3}}}\n\\providecommand*{\\yul}{\\se{K}}\n\n\\providecommand*{\\bA}{{\\blue A}}\n\\providecommand*{\\bB}{{\\blue B}}\n\\providecommand*{\\bC}{{\\blue C}}\n\n\n\\providecommand*{\\vfa}{\\vf'}\n\\providecommand*{\\vfb}{\\vf''}\n\n:::\n\n\n::: {.hidden}\n\n\\providecommand*{\\data}{\\se{\\green data}}\n\\providecommand*{\\yD}{\\se{I}_{\\textrm{d}}}\n\\providecommand*{\\ya}{k}\n\\providecommand*{\\yb}{l}\n\\providecommand*{\\amin}{\\ya_{\\text{mi}}}\n\\providecommand*{\\amax}{\\ya_{\\text{ma}}}\n\\providecommand*{\\aux}{\\operatorname{aux}}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Agents that generate text: what's the decision-making problem? {#sec-what-decision-llm}\n\nIn [§@sec-language-inference] we saw that present-day Large Language Model determine their belief in what the next word or token should be, only based on the sequence of words seen so far, and on the frequencies of such sequences in a huge collection of texts. These beliefs are not determined on possible future outcomes of their choice of words, as instead is the case in human conversation. We managed to use our Optimal Predictor Machine in the same way in a simplified setting.\n\nLarge Language Models do not output probabilities, however: they output words. So at every step they are effectively choosing one of the possible words about which they calculated their beliefs. This is decision, and to be optimal and self-consistent it should be based on some outcomes and their utilities.\n\nWhat are the outcomes and utilities underlying word-choice in Large Language Models? This is still an open question. The approaches followed so far in the literature have been based more on intuition and on \"playing around\" rather than framing the problem in a systematic way. This means that there's a lot of room for an AI engineer to bring forth a better understanding and major improvements.\n\\\n\nLet's consider our OPM agent used as a \"small language model\" in [§@sec-opm-as-slm]. It was only calculating degrees of belief about the $n$th token of a string of $n$ tokens. First of all let's ask again: beliefs about what? We could say: the belief that this $n$th token is the correct one, in this particular sequence. Keep in mind that this is a suspicious point of view; can we really say that there's a \"correct\" token?\n\nIf we want our OPM agent to also *choose* one of the possible tokens, we need to find the appropriate set of outcomes and their utilities for this decision problem.\n\n:::{.callout-caution}\n\nOn your own or in group, think about the problem above.\n\n- What kind of meaningful *outcomes* are there in this problem?\n\n- Can they be easily assessed? Do they depend on the choice of present token alone, or in future ones as well?\n\n- What are the utility values for the outcomes? How to assess them?\n\n- Can we find alternative points of view about outcomes and utilities -- points of view that do not reflect natural language but may make sense somehow in the present context?\n:::\n\n## A tentative decision-making point of view {#sec-tentative-decision-llm}\n\nA *tentative* and somewhat vague point of view is the following: the agent should generate, on the long run, text that \"looks natural\". Now if we assign a given utility, say $+1$, to the \"correct\" choice of token, the agent should at each step choose the token that always has highest probability. This, however, eventually leads to a circular repetition, as soon as a string of $n-1$ token appears again. Let's see an example of this phenomenon with our OPM agent.\n\nLet's build again an agent that does inference about 3-grams from the United Nation's [*Universal Declaration of Human Rights*](https://www.un.org/en/about-us/universal-declaration-of-human-rights):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load main functions\nsource('tplotfunctions.R')\nsource('OPM_nominal.R')\nsource('textpreparation.R')\n\n## Set seed for reproducibility\nset.seed(700)\n\n## Prepare metadata and training data\nngramfiles <- preparengramfiles(\n    inputfile = 'texts/human_rights.txt',\n    outsuffix = 'rights',\n    n = 3,\n    maxtokens = Inf\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUnique tokens:  506.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nData:  1904  3-grams.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFiles saved.\n```\n\n\n:::\n\n```{.r .cell-code}\n## Create the OPM agent\nopmSLM <- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data,\n    savememory = TRUE\n)\n```\n:::\n\n\nNow let's operate the agent as follows:\n\n0. We give an initial prompt of two tokens: `ARTICLE`, `x`.\n1. We let the agent calculate the degrees of belief about the next token.\n2. The next token is *chosen* as the one having highest probability. This corresponds to a decision process with a *unit utility matrix*: correct token choices have utility $+1$; and wrong choices, $0$.\n3. The first token is discarded, the second becomes first, and the last token generated becomes the second.\n4. Repeat from 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Starting tokens\nword1 <- 'ARTICLE'\nword2 <- 'x'\n\n## Convenience variable to wrap text\ntextlength <- 0\n\nfor(i in 1:100){\n    ## Print the starting tokens if first iteration\n    if(i == 1){ cat('\\n', word1, word2, '') }\n\n    ## Calculate beliefs about next token\n    probs <- infer(agent = opmSLM,\n        predictor = list(word1 = word1, word2 = word2),\n        predictand = 'word3')\n\n    ## Decision: next token\n    ## is the one with max prob.\n    word3 <- names(which.max(probs))\n\n    ## Wrap text if it'll get too long\n    textlength <- textlength + nchar(word3) + 1\n    if(textlength > 50){\n        cat('\\n')\n        textlength <- 0\n    }\n\n    ## Print chosen token\n    cat(word3, '')\n\n    ## Use last two tokens for new prediction\n    word1 <- word2\n    word2 <- word3\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ARTICLE x EVERYONE HAS THE RIGHT TO FREEDOM OF OPINION AND \nEXPRESSION ; THIS RIGHT INCLUDES FREEDOM TO CHANGE HIS \nRELIGION OR BELIEF , AND THE RIGHT TO FREEDOM OF OPINION \nAND EXPRESSION ; THIS RIGHT INCLUDES FREEDOM TO \nCHANGE HIS RELIGION OR BELIEF , AND THE RIGHT TO FREEDOM \nOF OPINION AND EXPRESSION ; THIS RIGHT INCLUDES \nFREEDOM TO CHANGE HIS RELIGION OR BELIEF , AND THE RIGHT \nTO FREEDOM OF OPINION AND EXPRESSION ; THIS RIGHT \nINCLUDES FREEDOM TO CHANGE HIS RELIGION OR BELIEF , AND \nTHE RIGHT TO FREEDOM OF OPINION AND EXPRESSION ; THIS \nRIGHT INCLUDES FREEDOM TO \n```\n\n\n:::\n:::\n\n\nAs you see, the agent started looping at \"`THE` `RIGHT`\". The text is doesn't look natural from this point of view. The set of decisions and of utilities we chose for the agent are not appropriate.\n\\\n\nLet's try to define a little more precisely what we mean by \"look natural\".\n\n- One condition is the absence of text loops like the one above. The text should be to us somewhat *unpredictable*.\n- Also, the frequencies with which 3-grams appear in the output text should reflect those of natural language -- like the frequencies in the texts used to train the agent.\n\nThese conditions suggest that we use an \"*unpredictable decision*\", in the sense discussed in [§@sec-unpred-decisions].\n\nLet's introduce the following decision:\n\n[\"Unpredictably select and output the next token according to its belief\"]{.yellow}.\n\nIn the long run, making this decision will produce text that is unpredictable, and also whose 3-gram frequencies reflect those of the training texts, because the agent bases its beliefs on those (though beliefs and frequencies are not exactly equal). Therefore it seems that, in the long run, this decision will lead to the highest utility.\n\\\n\nThe function `generatetext()` in the file [`textpreparation.R`](https://github.com/pglpm/ADA511/blob/master/code/textpreparation.R) applies the heuristic strategy above and outputs the resulting text. We saw an example output in [§@sec-preview-gentext]. Let's see another example output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- generatetext(\n    agent = opmSLM,\n    prompt = c('ARTICLE', 'x'),\n    stopat = 70,\n    online = FALSE\n)\n\nwrapprint(text, wrapat = 50)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n ARTICLE x EVERYONE HAS THE RIGHT TO JUST AND\n FAVOURABLE CONDITIONS OF WORK AND TO SHARE IN SCIENTIFIC\n ADVANCEMENT AND ITS BENEFITS. EVERYONE HAS THE RIGHT TO\n FREEDOM OF MOVEMENT AND RESIDENCE WITHIN THE BORDERS OF\n EACH STATE. EVERYONE HAS THE RIGHT TO BE PRESUMED\n INNOCENT UNTIL PROVED GUILTY ACCORDING TO LAW IN A\n DEMOCRATIC SOCIETY. THESE RIGHTS AND FREEDOMS, EVERYONE\n SHALL BE GIVEN TO THEIR ...\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-important}\n##  \n\nThe discussion above is **not a rigorous application of the principle of maximal expected utility**. We did not list properly the decisions, or the outcomes, or the utility values. The reasoning was purely heuristic and intuitive.\n\nFor this reason it is quite possible that the strategy adopted is **not** optimal.\n:::\n\n:::{.callout-caution}\n\n- Try to formalize our heuristic reasoning above and to derive the strategy rigorously, from the principle of maximal expected utility. [(This is not an easy task, but why not try?)]{.midgrey}\n\n- Reverse-engineer the function `generatetext()` and verify that it indeed implements the decision discussed above.\n\n- Try generating text with the OPM agent using other prompts. You should notice that sometimes the output text suddenly becomes gibberish, or is gibberish right from the start. It is also possible that gibberish output suddenly becomes more coherent again.\n    \n    Can you explain why these transitions from between sensible and insensible text occur?\n\n- Use other text to train the OPM agent and explore its output. You can prepare texts yourself, or use the ones in the directory [`code/texts`](https://github.com/pglpm/ADA511/tree/master/code/texts). In size order:\n    \n    - [`human_rights.txt`](https://github.com/pglpm/ADA511/blob/master/code/texts/human_rights.txt): [*Universal Declaration of Human Rights*](https://www.un.org/en/about-us/universal-declaration-of-human-rights).\n    - [`EUparliament.txt`](https://github.com/pglpm/ADA511/blob/master/code/texts/EUparliament.txt): sample of [*European Parliament Proceedings*](https://www.statmt.org/europarl/).\n    - [sherlock_holmes.txt](https://github.com/pglpm/ADA511/blob/master/code/texts/sherlock_holmes.txt): [*The Complete Sherlock Holmes*](https://sherlock-holm.es).\n    - [`ABC.txt`](https://github.com/pglpm/ADA511/blob/master/code/texts/ABC.txt): [*Australian Broadcasting Commission*](http://www.abc.net.au/).\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}