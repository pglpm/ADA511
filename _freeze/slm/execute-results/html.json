{
  "hash": "c723881282310c4d9adfa1cf0623f3be",
  "result": {
    "engine": "knitr",
    "markdown": "# [Example application: \"small language model\"]{.red} {#sec-example-slm}\n::: {.hidden}\n<!-- $$\\require{mathtools}$$ -->\n\n\\providecommand{\\ul}{\\uline}\n\\providecommand{\\and}{\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}}\n\\renewcommand*{\\|}[1][]{\\nonscript\\:#1\\vert\\nonscript\\:\\mathopen{}}\n\\providecommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\renewcommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\providecommand*{\\prq}[1]{\\textsf{\\small #1}}\n\\providecommand*{\\se}[1]{\\mathsfit{#1}}\n\\renewcommand{\\se}[1]{\\mathsfit{#1}}\n\\providecommand*{\\sei}[1]{\\mathsfit{\\small #1}}\n<!-- \\providecommand{\\cat}[1]{\\texttt{\\small #1}} -->\n\\providecommand{\\cat}[1]{{\\small\\verb;#1;}}\n\\providecommand{\\vec}[1]{\\boldsymbol{#1}}\n\\providecommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\P}{\\mathrm{P}}\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA}\n\\definecolor{quarto-callout-note-color-frame}{HTML}{4477AA}\n\\definecolor{quarto-callout-important-color}{HTML}{AA3377}\n\\definecolor{quarto-callout-important-color-frame}{HTML}{AA3377}\n\\definecolor{quarto-callout-warning-color}{HTML}{EE6677}\n\\definecolor{quarto-callout-warning-color-frame}{HTML}{EE6677}\n\\definecolor{quarto-callout-tip-color}{HTML}{228833}\n\\definecolor{quarto-callout-tip-color-frame}{HTML}{228833}\n\\definecolor{quarto-callout-caution-color}{HTML}{CCBB44}\n\\definecolor{quarto-callout-caution-color-frame}{HTML}{CCBB44}\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\nonscript\\mkern-3mu\\textrm{\\small#1}\\nonscript\\mkern-3mu}} -->\n\\providecommand*{\\mo}[1][=]{\\mathclose{}\\mathord{\\nonscript\\mkern0mu\\textrm{\\small#1}\\nonscript\\mkern0mu}\\mathopen{}}\n\\providecommand*{\\yX}{\\se{X}}\n\\providecommand*{\\yY}{\\se{Y}}\n\\providecommand*{\\yI}{\\se{I}}\n\\providecommand*{\\yi}[1][]{\\se{I}_{\\text{#1}}}\n\\providecommand{\\di}{\\mathrm{d}}\n\\providecommand{\\defd}{\\coloneqq}\n\\providecommand{\\blue}{\\color[RGB]{68,119,170}}\n\\providecommand{\\red}{\\color[RGB]{238,102,119}}\n\\providecommand{\\purple}{\\color[RGB]{170,51,119}}\n\\providecommand{\\green}{\\color[RGB]{34,136,51}}\n\\providecommand{\\yellow}{\\color[RGB]{204,187,68}}\n\\providecommand{\\lblue}{\\color[RGB]{102,204,238}}\n\\providecommand{\\grey}{\\color[RGB]{187,187,187}}\n\\providecommand{\\midgrey}{\\color[RGB]{119,119,119}}\n\\providecommand{\\black}{\\color[RGB]{0,0,0}}\n\\providecommand*{\\e}{\\mathrm{e}}\n\\providecommand*{\\pu}{\\text{π}}\n\\providecommand*{\\RR}{\\mathbf{R}}\n\n$\\DeclarePairedDelimiter{\\set}{\\{}{\\}}$\n\\providecommand*{\\argmax}{\\operatorname{argmax}}\n<!-- \\DeclareMathOperator*{\\argmax}{argmax} -->\n\n<!-- \\renewcommand*{\\prq}[1]{\\textsf{\\small #1}} -->\n<!-- \\definecolor{lightblue}{HTML}{66CCEE} -->\n<!-- \\sethlcolor{lightblue} -->\n<!-- \\providecommand*{\\moo}[1][=]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\mkern-4mu#1\\mkern-4mu}} -->\n<!-- \\providecommand*{\\mo}[1][\\textrm{\\small=}]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n\n:::\n\n::: {.hidden}\n\\providecommand*{\\yon}{{\\green\\cat{on}}}\n\\providecommand*{\\yof}{{\\red\\cat{off}}}\n\\providecommand*{\\yy}{{\\lblue\\cat{Y}}}\n\\providecommand*{\\yn}{{\\yellow\\cat{N}}}\n\\providecommand{\\ypl}{{\\green\\cat{+}}}\n\\providecommand{\\ymi}{{\\red\\cat{-}}}\n\\providecommand{\\ypa}{{\\green\\cat{pass}}}\n\\providecommand{\\yfa}{{\\red\\cat{fail}}}\n<!-- \\providecommand{\\ypl}{\\mathord{\\green\\boldsymbol{+}}} -->\n<!-- \\providecommand{\\ymi}{\\mathord{\\red\\boldsymbol{-}}} -->\n\\providecommand{\\hi}{{\\green\\cat{high}}}\n\\providecommand{\\me}{{\\yellow\\cat{medium}}}\n\\providecommand{\\lo}{{\\red\\cat{low}}}\n\\providecommand*{\\yJ}{\\se{J}}\n\\providecommand{\\yva}{{\\lblue-1}}\n\\providecommand{\\yvb}{{\\midgrey0}}\n\\providecommand{\\yvc}{{\\yellow1}}\n\\providecommand*{\\yK}{\\se{K}}\n\\providecommand*{\\yL}{\\se{L}}\n\n\\providecommand*{\\yR}{R}\n\n\\providecommand*{\\bZ}{{\\blue Z}}\n\\providecommand*{\\bz}{{\\blue z}}\n\\providecommand*{\\rY}{{\\red Y}}\n\\providecommand*{\\bY}{{\\blue Y}}\n\\providecommand*{\\ry}{{\\red y}}\n\\providecommand*{\\gX}{{\\green X}}\n\\providecommand*{\\bX}{{\\blue X}}\n\\providecommand*{\\gx}{{\\green x}}\n\\providecommand*{\\vf}{\\vec{f}}\n<!-- \\providecommand*{\\if}{\\se{F}} -->\n\\providecommand*{\\yut}{\\se{K}_{\\textsf{3}}}\n\\providecommand*{\\yul}{\\se{K}}\n\n\\providecommand*{\\bA}{{\\blue A}}\n\\providecommand*{\\bB}{{\\blue B}}\n\\providecommand*{\\bC}{{\\blue C}}\n\n\n\\providecommand*{\\vfa}{\\vf'}\n\\providecommand*{\\vfb}{\\vf''}\n\n:::\n\n\n::: {.hidden}\n\n\\providecommand*{\\data}{\\se{\\green data}}\n\\providecommand*{\\yD}{\\se{I}_{\\textrm{d}}}\n\\providecommand*{\\ya}{k}\n\\providecommand*{\\yb}{l}\n\\providecommand*{\\amin}{\\ya_{\\text{mi}}}\n\\providecommand*{\\amax}{\\ya_{\\text{ma}}}\n\\providecommand*{\\aux}{\\operatorname{aux}}\n\n:::\n\n\n::: {.hidden}\n\n\\providecommand*{\\worda}{\\mathit{word1}}\n\\providecommand*{\\wordb}{\\mathit{word2}}\n\\providecommand*{\\wordc}{\\mathit{word3}}\n\\providecommand*{\\wordn}{\\mathit{word}n}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Natural language as inference and decision {#sec-language-inference}\n\nIn [§@sec-meaning-decision] we remarked that even a process like speaking is a continuous decision-making process. Each uttered word is chosen among an infinity of other possible ones. The choice depends not only on the object or idea's noun that has to be communicated at the moment, but also on long-term goals and consequences. Is the conversation's purpose to be funny? Is it to explain something in the most precise way possible? What's the probability that this particular choice of words offend the listener? What's the probability that this possible sentence is misunderstood, and what would happen in that case?\n\nThe decision-making underlying speaking is remarkably complex, and can only be represented by a deep sequence of decisions and outcomes (we'll discuss decision sequences in [ch. @sec-making-decisions]), trying to forecast short-term and long-term future consequences. Most of this decision-making takes place almost unconsciously in humans, by means of heuristic, approximate procedures. But it's nevertheless a decision-making process.\n\n:::{.column-margin}\n![](decision_tree_raiffa.jpg){width=100%}\n:::\n\nBuilding an AI agent that can speak *in the sense above* is still out of reach. Speaking in that sense requires an agent to have long-term goals and a set of values. But speaking can remarkably be *mimicked*, at least for short time spans, by a different inference and decision process. In human speech the word choice depends on future goals as much or more than past factors; but we can try to draw an inference based on past factors alone.\n\nIn the simplest inference of this kind, an agent can assign a degree of belief about the word that \"should follow next\", **based on the words that precede it**. This is how the next-word predictors of some smartphone keyboards work, and it is essentially also the way Large Language Models (LLMs) and General Pretrained Transformers (GPTs) work. The inference is based on the frequencies of many different sequences of words, learned from as many written or spoken texts as possible.\n\n::::{.column-margin}\n::: {.callout-tip}\n## {{< fa rocket >}} For the extra curious\n\n[*Large language models as Markov chains*](references.html)\n\n:::\n::::\n\nThis way of operation is also the reason why we can't really say that large language models *understand* language, despite some literature stating that they do. They don't do any kind of forecast of the consequences of their word choice, nor any inferences about the intentions of their interlocutor; not even approximate, heuristic ones. They have simply learned word frequencies in a huge variety of contexts.\n\n\n## The OPM as a small language model {#sec-opm-as-slm}\n\nAlgorithms that implement the kind of word inference just described usually do so in an *exchangeable* way. They use the frequencies of different word sequences, without regard to whether most of those sequences occur, say, in older rather than newer texts.\n\nOur OPM is also designed to use built-in exchangeable beliefs, and in the previous chapter we saw its extreme versatility with exchangeable inferences of nominal variates -- just what words, more or less, are. So let's try to use it for word inference, and see if it can mimic speech. Of course we cannot expect spectacular results: large language models manage to make their remarkable inferences after learning more than tens of trillions (10 000 000 000 000) of words, which require huge amounts of memory and time. Owing to our memory and time limitations, our OPM can act as a \"small language model\".\n\nTo use the OPM this way, we define a population of $n$ variates, let's say $n = 3$. The three variates are defined as **consecutive** words in some specific set of texts. We can call them $\\worda$, $\\wordb$, $\\wordc$, and so on. Instead of words it's actually best to use [*tokens*]{.blue}, which comprise not only words, but punctuation marks like `,.;:!?` and other signs -- the exact definition of token depends on the specific application. The sequence of $n$ tokens is often called an **n-gram**.\n\n### Preparing the learning data from some text {#sec-prepare-text}\n\nIn order to prepare the learning data for our OPM agent, let's load some R functions defined in the [`textpreparation.R`](https://github.com/pglpm/ADA511/blob/master/code/textpreparation.R) file, as well as the OPM functions, from the usual directory\\\n[https://github.com/pglpm/ADA511/blob/master/code](https://github.com/pglpm/ADA511/blob/master/code):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource('tplotfunctions.R')\nsource('OPM_nominal.R')\nsource('textpreparation.R')\n\nset.seed(100)\n```\n:::\n\n\nThe function `preparengramfiles()` takes three input arguments:\n\n- `inputfile`: the name of a text `.txt` file, used to train the agent;\n- `outsuffix`: the suffix for the output files (see below);\n- `n`: the $n$ of the n-grams, default `n = 3`;\n- `maxtokens`: the maximum number of unique tokens to use -- the \"vocabulary\" -- in case we need to save memory; default is `maxtokens = Inf`, which uses all tokens from the input text.\n\nThe function creates two `csv` files: one containing the training data with all n-grams from the input text, and one containing the metadata, consisting in the vocabulary. It outputs the file names as list with elements `metadata` and `data`.\n\nFor simplicity, the function converts all text to upper case and ASCII characters, it replaces every sequence of digits 0–9 with the single lowercase letter `n`, and only keeps the following punctuation: `,.;:?!%$&@+'/-`.\n\nAs a concrete example let's start with a simple text: the lyrics from Daft Punk's song [*Around the World*](https://www.youtube.com/watch?v=K0HSD_i2DvA):\n\n:::{.column-margin}\n<!-- {{< video https://www.youtube.com/watch?v=lFIYKmos3-s >}} -->\n{{< video https://www.youtube.com/watch?v=K0HSD_i2DvA >}}\n:::\n\n> Around the world, around the world.  \n> Around the world, around the world.\n\nThese two lines are stored in the file [`texts/around_the_world.txt`](https://github.com/pglpm/ADA511/blob/master/code/texts/around_the_world.txt).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngramfiles <- preparengramfiles(\n    inputfile = 'texts/around_the_world.txt',\n    outsuffix = 'around',\n    n = 3,\n    maxtokens = Inf\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUnique tokens: 5.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFiles saved.\n```\n\n\n:::\n:::\n\n\nThe function tells us that it used five unique tokens. Let's take a look at the learning-data file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.csv(ngramfiles$data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    word1  word2  word3\n1  AROUND    THE  WORLD\n2     THE  WORLD      ,\n3   WORLD      , AROUND\n4       , AROUND    THE\n5  AROUND    THE  WORLD\n6     THE  WORLD      .\n7   WORLD      . AROUND\n8       . AROUND    THE\n9  AROUND    THE  WORLD\n10    THE  WORLD      ,\n11  WORLD      , AROUND\n12      , AROUND    THE\n13 AROUND    THE  WORLD\n14    THE  WORLD      .\n15  WORLD      . AROUND\n16      . AROUND    THE\n17 AROUND    THE  WORLD\n18    THE  WORLD      ,\n19  WORLD      , AROUND\n20      , AROUND    THE\n21 AROUND    THE  WORLD\n22    THE  WORLD      .\n```\n\n\n:::\n:::\n\n\nWe see that the token vocabulary consists of\\ \\ {`AROUND` `THE` `WORLD` `,` `.`}.\\ \\ The learning data consist of all sequences, 22 in total, of three consecutive tokens present in the text. Note a specific word in the original text therefore usually appears in three different datapoints, except for the very first and very last two words.\n\n\nLet's build an `opmSLM` agent and make it learn from these metadata and data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopmSLM <- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data\n)\n```\n:::\n\n\n### Token inferences\n\nWe ask the `opmSLM` agent to draw an inference about $\\worda$, and sort the results in order of decreasing probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM, predictand = 'word1')\n\nsort(probs, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword1\n  AROUND      THE    WORLD        ,        . \n0.262502 0.262502 0.223438 0.145311 0.106248 \n```\n\n\n:::\n:::\n\n\nThese degrees of belief make sense: in the learning text, if we take any token that can be followed by two more, we find `AROUND`, `THE`, and `WORLD` with approximately equal frequencies, somewhat less for `WORLD` because it also occurs in second-last position. The two punctuation marks occur with around half those frequencies.\n\nThe agent learned from just two lines, though. Let's inspect the agent's beliefs about the frequencies in a possibly larger text, by means of the `plotFsamples1D()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotFsamples1D(agent = opmSLM,\n    n = 200,\n    predictand = 'word1',\n    sort = +Inf,\n    ylim = c(0,1))\n```\n\n::: {.cell-output-display}\n![](slm_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nso the agent is open to possible changes in the frequencies, if more text were added to its learning data.\n\nFinally let's use the agent as a small language model. If a sequence of two tokens is \"`AROUND` `THE`\", what should be the next word?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'AROUND', word2 = 'THE'))\n\nsort(probs, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword3\n     WORLD          ,          .     AROUND        THE \n0.98125345 0.00468664 0.00468664 0.00468664 0.00468664 \n```\n\n\n:::\n:::\n\n\nThe agent believe at 98% that it should be `WORLD`!\n\nOur agent can actually also do inferences that many large language models are not designed to do; for instance it can guess the token *preceding* a sequence of two tokens. What should come before \"`THE` `WORLD`\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word1',\n    predictor = list(word2 = 'THE', word3 = 'WORLD'))\n\nsort(probs, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword1\n    AROUND          ,          .        THE      WORLD \n0.98125345 0.00468664 0.00468664 0.00468664 0.00468664 \n```\n\n\n:::\n:::\n\n\nIt can also guess the middle token. What should be between `THE` and `,`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word2',\n    predictor = list(word1 = 'THE', word3 = ','))\n\nsort(probs, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword2\n     WORLD          ,          .     AROUND        THE \n0.96336537 0.00915866 0.00915866 0.00915866 0.00915866 \n```\n\n\n:::\n:::\n\n\n\n## A more demanding example {#sec-slm-large-example}\n\nThe simple text we chose was just for testing. Now we use a slightly larger text: the United Nation's [*Universal Declaration of Human Rights*](https://www.un.org/en/about-us/universal-declaration-of-human-rights), stored in [`texts/human_rights.txt`](https://github.com/pglpm/ADA511/blob/master/code/texts/human_rights.txt). The core of this document is a set of Articles:\n\n> Article 1  \n> All human beings are born free and equal [...]\n> \n> Article 2  \n> Everyone is entitled to [...]\n\n\nPrepare the metadata and learning-data files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngramfiles <- preparengramfiles(\n    inputfile = 'texts/human_rights.txt',\n    outsuffix = 'around',\n    n = 3,\n    maxtokens = Inf\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUnique tokens: 506.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFiles saved.\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(read.csv(ngramfiles$data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1904\n```\n\n\n:::\n:::\n\n\nBuild an agent from this information; we call it again `opmSLM` (rewriting the previous one). To avoid memory problems, we use the option `savememory = TRUE`, which stores the learning information in a memory-efficient way -- at the cost of slightly longer inference-computation times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopmSLM <- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data,\n    savememory = TRUE\n)\n```\n:::\n\n\nNow some inferences. Recall that the text is converted to uppercase, and all numbers are replaced with `n`. Many pieces of text thus begin with \"`ARTICLE` `n`\". Ask the agent what should follow after these two tokens. Let's just display the five most probable tokens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'ARTICLE', word2 = 'n'))\n\nsort(probs, decreasing = TRUE)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword3\n EVERYONE        NO       ALL       MEN   NOTHING \n0.7329435 0.1332633 0.0666322 0.0333166 0.0333166 \n```\n\n\n:::\n:::\n\n\nRoughly 75% of Articles begin with \"everyone...\", and this is roughly the agent's belief.\n\n\n### Apparent grammar knowledge\n\nLarge language models seem to have good grammar knowledge. This knowledge comes, not from the study of grammar rules, but again simply from the frequencies of particular word combinations in the training text. Let's see if our small language model displays some rudimentary grammar.\n\nWhich word should follow the *noun* \"right\", as in \"the right...\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'THE', word2 = 'RIGHT'))\n\nsort(probs, decreasing = TRUE)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nword3\n          TO           OF       FREELY          THE          AND \n0.9280423224 0.0356950206 0.0356950206 0.0000011285 0.0000011285 \n```\n\n\n:::\n:::\n\n\nThe `opmSLM` agent believes at 93% that the preposition \"to\" should follow \"the right\".\n\nAnother grammar question: which is correct, \"everyone has\", or \"everyone have\"? Let's see the agent's beliefs about either sequence. To check them, we must use `infer()` to calculate the probabilities of all tokens after `EVERYONE`, and then select the particular tokens `HAS` and `HAVE`. We can compare the two probabilities against each other:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- infer(agent = opmSLM,\n    predictand = 'word2',\n    predictor = list(word1 = 'EVERYONE'))\n\nprobHas <- probs['HAS']\n\nprobHave <- probs['HAVE']\n\n## probability of each, assuming that one of them is true\n\nc(probHas, probHave) / (probHas + probHave)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        HAS        HAVE \n0.999273886 0.000726114 \n```\n\n\n:::\n:::\n\n\nA 99.9% belief that \"everyone has\" is correct between the two!\n\n\n:::{.callout-caution}\n\nUsing the four fundamental rules or the shortcut rules, prove that\n\n$$\n\\P\\bigl(\\cat{has} \\| (\\cat{has} \\lor \\cat{have}) \\and \\yI) =\n\\frac{\n\\P(\\cat{has} \\| \\yI)\n}{\n\\P(\\cat{has} \\| \\yI) + \\P(\\cat{have} \\| \\yI)\n}\n$$\n\ntaking into account that $\\cat{has}$ and $\\cat{have}$ are mutually exclusive. (In our case $\\yI \\defd \\cat{everyone} \\land D$, where $D$ is the learning and background information.)\n:::\n\n\n----\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\ngeneratetext(\n    agent = opmSLM,\n    prompt = c('ARTICLE', 'n'),\n    stopat = 30,\n    online = FALSE\n)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ARTICLE n EVERYONE HAS THE RIGHT TO LIFE, LIBERTY AND THE RIGHT TO LEAVE ANY COUNTRY, INCLUDING HIS OWN, AND THE FREE AND FULL CONSENT OF THE...\n```\n\n\n:::\n:::\n\n\n\\\n\n[*(To be continued)*]{.grey}\n",
    "supporting": [
      "slm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}