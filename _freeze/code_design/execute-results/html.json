{
  "hash": "d75d21d07671deb0f5525d96651b546a",
  "result": {
    "engine": "knitr",
    "markdown": "# [Implementing an OPM]{.red} {#sec-code-design}\n::: {.hidden}\n<!-- $$\\require{mathtools}$$ -->\n\n\\providecommand{\\ul}{\\uline}\n\\providecommand{\\and}{\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}}\n\\renewcommand*{\\|}[1][]{\\nonscript\\:#1\\vert\\nonscript\\:\\mathopen{}}\n\\providecommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\renewcommand*{\\pr}[1]{\\textsf{\\small`#1'}}\n\\providecommand*{\\prq}[1]{\\textsf{\\small #1}}\n\\providecommand*{\\se}[1]{\\mathsfit{#1}}\n\\renewcommand{\\se}[1]{\\mathsfit{#1}}\n\\providecommand*{\\sei}[1]{\\mathsfit{\\small #1}}\n<!-- \\providecommand{\\cat}[1]{\\texttt{\\small #1}} -->\n\\providecommand{\\cat}[1]{{\\small\\verb;#1;}}\n\\providecommand{\\vec}[1]{\\boldsymbol{#1}}\n\\providecommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\p}{\\mathrm{p}}\n\\renewcommand{\\P}{\\mathrm{P}}\n\\definecolor{quarto-callout-note-color}{HTML}{4477AA}\n\\definecolor{quarto-callout-note-color-frame}{HTML}{4477AA}\n\\definecolor{quarto-callout-important-color}{HTML}{AA3377}\n\\definecolor{quarto-callout-important-color-frame}{HTML}{AA3377}\n\\definecolor{quarto-callout-warning-color}{HTML}{EE6677}\n\\definecolor{quarto-callout-warning-color-frame}{HTML}{EE6677}\n\\definecolor{quarto-callout-tip-color}{HTML}{228833}\n\\definecolor{quarto-callout-tip-color-frame}{HTML}{228833}\n\\definecolor{quarto-callout-caution-color}{HTML}{CCBB44}\n\\definecolor{quarto-callout-caution-color-frame}{HTML}{CCBB44}\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\nonscript\\mkern-3mu\\textrm{\\small#1}\\nonscript\\mkern-3mu}} -->\n\\providecommand*{\\mo}[1][=]{\\mathclose{}\\mathord{\\nonscript\\mkern0mu\\textrm{\\small#1}\\nonscript\\mkern0mu}\\mathopen{}}\n\\providecommand*{\\yX}{\\se{X}}\n\\providecommand*{\\yY}{\\se{Y}}\n\\providecommand*{\\yI}{\\se{I}}\n\\providecommand*{\\yi}[1][]{\\se{I}_{\\text{#1}}}\n\\providecommand{\\di}{\\mathrm{d}}\n\\providecommand{\\defd}{\\coloneqq}\n\\providecommand{\\blue}{\\color[RGB]{68,119,170}}\n\\providecommand{\\red}{\\color[RGB]{238,102,119}}\n\\providecommand{\\purple}{\\color[RGB]{170,51,119}}\n\\providecommand{\\green}{\\color[RGB]{34,136,51}}\n\\providecommand{\\yellow}{\\color[RGB]{204,187,68}}\n\\providecommand{\\lblue}{\\color[RGB]{102,204,238}}\n\\providecommand{\\grey}{\\color[RGB]{187,187,187}}\n\\providecommand{\\midgrey}{\\color[RGB]{119,119,119}}\n\\providecommand{\\black}{\\color[RGB]{0,0,0}}\n\\providecommand*{\\e}{\\mathrm{e}}\n\\providecommand*{\\pu}{\\text{π}}\n\\providecommand*{\\RR}{\\mathbf{R}}\n\n$\\DeclarePairedDelimiter{\\set}{\\{}{\\}}$\n$\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}$\n\\providecommand*{\\argmax}{\\operatorname{argmax}\\limits}\n<!-- \\DeclareMathOperator*{\\argmax}{argmax} -->\n\n<!-- \\renewcommand*{\\prq}[1]{\\textsf{\\small #1}} -->\n<!-- \\definecolor{lightblue}{HTML}{66CCEE} -->\n<!-- \\sethlcolor{lightblue} -->\n<!-- \\providecommand*{\\moo}[1][=]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n<!-- \\providecommand*{\\mo}[1][=]{\\mathrel{\\mkern-4mu#1\\mkern-4mu}} -->\n<!-- \\providecommand*{\\mo}[1][\\textrm{\\small=}]{\\mathord{\\mkern1.5mu#1\\mkern1.5mu}} -->\n\n:::\n\n::: {.hidden}\n\\providecommand*{\\yon}{{\\green\\cat{on}}}\n\\providecommand*{\\yof}{{\\red\\cat{off}}}\n\\providecommand*{\\yy}{{\\lblue\\cat{Y}}}\n\\providecommand*{\\yn}{{\\yellow\\cat{N}}}\n\\providecommand{\\ypl}{{\\green\\cat{+}}}\n\\providecommand{\\ymi}{{\\red\\cat{-}}}\n\\providecommand{\\ypa}{{\\green\\cat{pass}}}\n\\providecommand{\\yfa}{{\\red\\cat{fail}}}\n<!-- \\providecommand{\\ypl}{\\mathord{\\green\\boldsymbol{+}}} -->\n<!-- \\providecommand{\\ymi}{\\mathord{\\red\\boldsymbol{-}}} -->\n\\providecommand{\\hi}{{\\green\\cat{high}}}\n\\providecommand{\\me}{{\\yellow\\cat{medium}}}\n\\providecommand{\\lo}{{\\red\\cat{low}}}\n\\providecommand*{\\yJ}{\\se{J}}\n\\providecommand{\\yva}{{\\lblue-1}}\n\\providecommand{\\yvb}{{\\midgrey0}}\n\\providecommand{\\yvc}{{\\yellow1}}\n\\providecommand*{\\yK}{\\se{K}}\n\\providecommand*{\\yL}{\\se{L}}\n\n\\providecommand*{\\yR}{R}\n\n\\providecommand*{\\bZ}{{\\blue Z}}\n\\providecommand*{\\bz}{{\\blue z}}\n\\providecommand*{\\rY}{{\\red Y}}\n\\providecommand*{\\bY}{{\\blue Y}}\n\\providecommand*{\\ry}{{\\red y}}\n\\providecommand*{\\gX}{{\\green X}}\n\\providecommand*{\\bX}{{\\blue X}}\n\\providecommand*{\\gx}{{\\green x}}\n\\providecommand*{\\vf}{\\vec{f}}\n<!-- \\providecommand*{\\if}{\\se{F}} -->\n\\providecommand*{\\yut}{\\se{K}_{\\textsf{3}}}\n\\providecommand*{\\yul}{\\se{K}}\n\n\\providecommand*{\\bA}{{\\blue A}}\n\\providecommand*{\\bB}{{\\blue B}}\n\\providecommand*{\\bC}{{\\blue C}}\n\n\n\\providecommand*{\\vfa}{\\vf'}\n\\providecommand*{\\vfb}{\\vf''}\n\n:::\n\n\n::: {.hidden}\n\n\\providecommand*{\\data}{\\se{\\green data}}\n\\providecommand*{\\yD}{\\se{I}_{\\textrm{d}}}\n\\providecommand*{\\ya}{k}\n\\providecommand*{\\yb}{l}\n\\providecommand*{\\amin}{\\ya_{\\text{mi}}}\n\\providecommand*{\\amax}{\\ya_{\\text{ma}}}\n\\providecommand*{\\aux}{\\operatorname{aux}}\n\n:::\n\n\nWe now try to build up a real prototype AI agent from basic principles, using the formulae summarized in [ch. @sec-summary-formulae] and in the previous chapter. By design, this agent is as close to optimal as theoretically possible; so let's call it an\n\n![](optimal_predictor_machine.png){width=75%}\n\nor *OPM* for short.\n\nBefore starting, let's agree on some terminology so as not to get confused in the discussion below.\n\n- We shall call [*task*]{.blue} a *repetitive* inference problem with a specified set of units and variates. For instance, a task could be the consecutive prediction of the urgency of several incoming patients, given their mean of transportation.\n- We shall call [*application*]{.blue} or [*instance*]{.blue} of the task a *single* inference about a new unit, for example one new incoming patient.\n\n\\\n\n## Desired characteristics of the OPM {#sec-characteristics-opm}\n\nWe design our Optimal Predictor Machine with the following specific characteristics:\n\n- It handles variates of *nominal* type ([§@sec-basic-types]).\n\n- It handles inferences and decisions about approximately infinite populations, and its beliefs about the population are *exchangeable* ([ch. @sec-exchangeable-beliefs]).\n\n- Its initial beliefs about the population frequencies are represented by a *Dirichlet-mixture distribution* ([ch. @sec-dirichlet-mix]).\n\n- Before deployment, it learns from a set of $N$ units.\n\\\n\n### Example of what kind of agent we want\n\nLet's give an example of what we want our agent to be able to do. Suppose we have a population having three nominal variates\\ \\ $Z = (Y \\and X \\and W)$\\ (keep in mind that $X$, $Y$, $W$ could each be a joint variate). Abbreviate the set of $N$ training data as\n\n$\\data \\defd\n( Z_{N}\\mo z_{N} \\and \\dotsb \\and\nZ_{2}\\mo z_{2} \\and\nZ_{1}\\mo z_{1} )$    \n\n:::{.column-margin}\nRecall that $Z$ denotes all (nominal) variates of the population\n:::\n\nwhere $z_N, \\dotsc, z_2, z_1$ are specific values, stored in some training dataset. To simplify things, we assume that no values are missing.\n\nWe want an agent that can draw inferences like the following ones, as often as required:\n\n- $P(X\\mo\\dotso\\|\\data \\and \\yD)$,\\  $P(Y\\mo\\dotso\\|\\data \\and \\yD)$,\\ etc.: inference about a predictand variate, without knowledge of any predictors.\n\n- $P(Y\\mo\\dotso \\and W\\mo\\dotso\\|\\data \\and \\yD)$: same but for any two predictand variates.\n\n- $P(Y\\mo\\dotso \\and X\\mo\\dotso \\and W\\mo\\dotso\\|\\data \\and \\yD)$: same but for all three variates.\n\n- $P(X\\mo\\dotso\\|Y\\mo\\dotso \\and \\data \\and \\yD)$: inference about any one predictand variate, given information about one predictor variate.\n\n- $P(X\\mo\\dotso\\| Y\\mo\\dotso \\and W\\mo\\dotso \\and\\data \\and  \\yD)$: same, but given information about any pair of predictors.\n\n- $P(Y\\mo\\dotso \\and W\\mo\\dotso\\|X\\mo\\dotso \\and \\data \\and \\yD)$: inference about any two predictand variates, given information about one predictor.\n\nNote that *we are not fixing beforehand which variates are predictands and which are predictors*. Once the agent has learnt from the training data, we want to be able to change *on the fly, at each new application*, what the predictands are, and what the predictors are (if any).\n\nPause for a second and *ponder about the flexibility that we are requesting from our prototype agent!* Consider that virtually all present-day machine-learning algorithms only work one way: a machine-learning algorithm designed to guess a label from some features cannot guess features from a label. Will we really manage to build an agent with the amazing versatility illustrated above?\n\n\n## Computations needed and computational challenges {#sec-code-computations}\n\nThe examples above of requested inferences show that the OPM agent must essentially use formulae (@eq-dmagent-y)--(@eq-dmagent-aux) from [§@sec-formulae-dirmix], which we repeat here:\n\n::::{.callout-note}\n##  \n\n$\\P(Y\\mo y \\| \\data, \\yD)\n=\n\\frac{\n\\sum_{\\ya = \\amin}^{\\amax}\n\\Bigl(\\tfrac{2^\\ya}{M_Y} + \\#y\\Bigr)\n\\cdot \\aux(\\ya)\n}{\n\\sum_{y}\\sum_{\\ya = \\amin}^{\\amax}\n\\Bigl(\\tfrac{2^\\ya}{M_Y} + \\#y\\Bigr)\n\\cdot \\aux(\\ya)\n}$   (@eq-dmagent-y)\n\n$\\P(Y\\mo y \\| X\\mo x \\and \\data, \\yD)\n=\n\\frac{\n\\sum_{\\ya = \\amin}^{\\amax}\n\\Bigl(\\tfrac{2^\\ya}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\aux(\\ya)\n}{\n\\sum_{y}\\sum_{\\ya = \\amin}^{\\amax}\n\\Bigl(\\tfrac{2^\\ya}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\aux(\\ya)\n}$   (@eq-dmagent-yx)\n\nwith\\ \\ $\\aux(\\ya)\n\\defd\n\\frac{\n\\prod_{x,y,w} \\Bigl(\\frac{2^{\\ya}}{M} + \\#(x,y,w) - 1\\Bigr)!\n}{\n\\bigl(2^{\\ya} + N\\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{\\ya} -1 \\bigr)!\n}{\n{\\Bigl(\\frac{2^{\\ya}}{M} - 1\\Bigr)!}^M\n}$   (@eq-dmagent-aux)\n::::\n\nThe values of $\\aux(\\ya)$ can be calculated just once, when the OPM agent is built, and stored. Subsequently the agent will draw inferences by using (@eq-dmagent-y) or (@eq-dmagent-yx) as needed. To use those formulae, the agent needs to store the counts $\\#(x,y,w,\\dotsc)$, which it found in the training data, for all combinations of values $x,y,w,\\dotsc$.\n\\\n\nThis kind of storage and computation could be implemented in a straightforward way if we had unlimited storage and computation precision. But in a real implementation we must face several difficulties. Here are the main difficulties and their solutions:\n\n[{{< fa circle-exclamation >}} Finite precision]{.red}\n: Owing to finite precision, the operations in the formulae may easily lead to overflow or underflow: large numbers are treated as `infinity`, and small non-zero numbers as `0`. For instance this is what happens if we directly compute something like $(2^{10})! / (2^{10})!$, obviously equal to $1$:\n\n::: {.cell}\n\n```{.r .cell-code}\nfactorial(2^10) / factorial(2^10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NaN\n```\n\n\n:::\n:::\n\nOne way to bypass this problem is by rewriting the formulae in ways that are mathematically equivalent but less prone to over- and under-flow. For example we can use identities like\n\n$$\nx / y = \\exp\\bigl(\\ln x - \\ln y\\bigr)\\ ,\\quad x, y > 0 \\ .\n$$\n\nNow indeed it works; note that `lfactorial()` is `log(factorial())` in R:\n\n::: {.cell}\n\n```{.r .cell-code}\nexp( lfactorial(2^10) - lfactorial(2^10) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\nAnother useful identity that avoids over- and under-flow, if $\\pmb{x}$ is a vector of positive numbers, is the following:\n\n$$\n\\frac{\\pmb{x}}{\\operatorname{\\texttt{sum}}(\\pmb{x})}\n=\n\\frac{\n\\operatorname{\\texttt{exp}}\\bigl(\\operatorname{\\texttt{log}}(\\pmb{x})\n- \\operatorname{\\texttt{max}}(\\operatorname{\\texttt{log}}(\\pmb{x}))\\bigr)\n}{\\operatorname{\\texttt{sum}}\\bigl(\n\\operatorname{\\texttt{exp}}\\bigl(\\operatorname{\\texttt{log}}(\\pmb{x})\n- \\operatorname{\\texttt{max}}(\\operatorname{\\texttt{log}}(\\pmb{x}))\\bigr)\n\\bigr)}\n$$\n\n\\\n\n[{{< fa circle-exclamation >}} Storage]{.red}\n: With many variates and large domains, we may run out of memory in storing all possible counts $\\#(x,y,w,\\dotsc)$. For instance if we have four variates with 20 possible values each, we would need to store $4^{20}$ integers, which would take more than 4 000 GB:\n\n::: {.cell}\n\n```{.r .cell-code}\ntry( x <- integer(length = 4^20) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError : cannot allocate vector of size 4096.0 Gb\n```\n\n\n:::\n:::\n\nWe can bypass this problem again by using smart mathematical manipulations. In the case of formulae (@eq-dmagent-y)--(@eq-dmagent-aux), the product over all possible values $(x,y,w,\\dotsc)$ can be rewritten in one over all different *values of the counts*, which usually has much fewer terms. For example, if we have $N=10000$ datapoints, and $4^{20} -1$ counts are equal to $9000$, while one count is equal to $1000$, then we only need to store these *four* numbers rather than $4^{20}$ numbers!\n\n\\\n\n[{{< fa circle-exclamation >}} Speed]{.red}\n: The formulae that the agent uses may involve sums over many terms, or repeated computations for many different variate values. Computation speed may therefore become an issue. There are two kinds of solutions to this problem. The first is, again, to use mathematical identities to rewrite formulae in ways that require fewer computations. The second is to exploit computation features of the programming language used to code the agent, such as vectorization or parallel computing. In our case we shall use some R functions that performs computations in a vectorized way, that is, using underlying fast C or C++ implementations.\n\n\\\n\nYou see that mathematical \"tricks\" become very important when we must implement formulae with finite precision and limited memory. Unfortunately getting acquainted with such tricks requires a separate course.\n\n::::{.column-margin}\n::: {.callout-tip}\n## {{< fa rocket >}} For the extra curious\n§6.1 in [*Numerical Recipes*](references.html)\n:::\n::::\n\n\nNote that **the solutions just discussed are *not* approximations**. Even if we use different mathematical formulae, they are still equivalent to the original ones. The internal logic of our OPM agent is therefore still fully correct. In other situations the mathematical or computational solutions above may not be enough, and then we may need to resort to approximations, as it often happens with machine-learning algorithms.\n\n:::{.callout-caution}\nTry to prove some of the mathematical identities above.\n:::\n\n## The code {#sec-code-writing}\n\nWe implement the OPM agent and its inferences through three main functions. These and other helper functions are defined in the script [`OPM_nominal.R`](https://github.com/pglpm/ADA511/blob/code/OPM_nominal.R):\n\n- `buildagent()`\n: creates an \"agent\" object that stores the built-in information and the information learned from the training data. The built-in information consists in the choices of $\\amin$ and $\\amax$ parameters, the $\\aux(k)$ parameters, and the list of variates and of their domains -- the agent knows what are the possible values before seeing any data. The learned information consists in the set of counts $\\#(x,y,\\dotsc)$ for all joint values of the variates.\n    \n    We can used this function to build several agents, which differ in their background information or in the data they have learned.\n    \n    We write this function with an option `savememory` for storing the learned information in a memory-efficient way, if needed.\n\n- `infer()`\n: asks an agent to draw an inference for a new unit, specifying the desired predictands and the known predictors for the new unit, if any are available. It returns the agent's degrees of belief about the predictands, using formulae (@eq-dmagent-y)--(@eq-dmagent-aux).\n    \n    The formulae must be implemented in slightly different ways, depending on whether the learned information is stored in a memory-efficient way. For this reason we actually have two implementations of this function, called `infer.agent()` and `infer.agentcompressed()`.\n    \n    This function can be used as often as we please, and with any agents we please.\n\n- `decide()`\n: asks an agent to make a decision, specifying the list of possible decisions, the predictands and their probabilities, and the utilities for the different decisions and outcomes. We shall discuss this function more in detail in [ch. @sec-example-opm2].\n\n\n:::{.callout-caution}\nOpen the script [`OPM_nominal.R`](https://github.com/pglpm/ADA511/blob/code/OPM_nominal.R) and locate the functions `buildagent()` and `infer.agent()`. In each, identify the lines of code that implement the various calculations discussed above. Note that `alpha` stands for $2^k$, and `counts` stands for the array or list of counts $\\#(y,x,\\dotsc)$.\n:::\n\n\\\n\nBesides the three main functions above, we can write other functions that help us handling the inference task and calculate other quantities of interest:\n\n- `guessmetadata()`\n: takes a dataset and builds a preliminary metadata file, encoding the information about variates and domain *guessed* from the dataset. Typically we have to correct this preliminary file to include values that may be missing from the learning data.\n\n- `rF()`\n: draws one or more possible full-population frequency distribution $\\vf$, according to an agent's degree of belief $\\p(F\\mo\\vf \\| \\data \\and \\yD)$ updated after learning the data.\n\n- `plotFbelief()`\n: plots, as a generalized scatter plot, the possible full-population marginal frequency distributions for a single (not joint) predictand variate. If required it also plots the final probability distribution obtained with `infer()`.\n\n- `mutualinfo()`\n: asks an agent to calculate the mutual information ([§@sec-entropy-mutualinfo]) between any two sets of variates.\n\n\n\\\n\nIn the next chapter we give some more documentation on these functions and on how to use them, and in [ch. @sec-example-opm1] we use them in a concrete task.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}