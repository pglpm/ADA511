# Framework
{{< include macros.qmd >}}

## What does the intro problem tell us?

Let's approach the "accept or discard?" problem of the previous [chapter @sec-intro] in an intuitive way.

:::{.column-margin}
We're jumping the gun here, because we haven't learned the method to solve this problem yet!
:::

First let's say that we `accept` the component. What happens?

We must try to make sense of that $10\%$ probability that the component fails within a year. Different people do this with different imagination tricks. We can imagine, for instance, that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year.

In each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses $10\$$. That's a total loss of $10 \cdot 10\$ = 100\$$. In each of the 90 imaginary repetitions in which the component doesn't fail early, the manufacturer gains $1\$$. That's a total gain of $90\$$. So over all 100 imaginary repetitions the manufacturer gains

$$
10\cdot (-10\$) + 90\cdot 1\$ = {\red -10\$} \ ,
$$

that is, the manufacturer has not gained, but *lost* $10\$$ ! That's an average of $0.1\$$ *lost* per repetition.

Now let's say that we `discard` the component instead. What happens? In this case we don't need to invoke imaginary repetitions, but even if we do, it's clear that the manufacturer doesn't gain or lose anything -- that is, the "gain" is $0\$$ -- in each and all of the repetitions.

The conclusion is that if in a situation like this we [accept]{.red} the component, then we'll [lose $1\$$]{.red} on average; whereas if we [discard]{.green} it, then on average we won't [lose anything or gain anything]{.green}.

Obviously the best, or "least worst", decision to make is to **discard** the component.

:::{.callout-caution}
## {{< fa user-edit >}} Exercises
@. Now that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with
    
    - failure probability `10%` and failure cost `5$`;
    - failure probability `5%` and failure cost `10$`;
    - failure probability `10%`, failure cost `1$`, non-failure gain `2$`.
    
    Feel free to get wild and do plots.

@. Identify the failure probability at which accepting the component doesn't lead to any loss or any gain, so it doesn't matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)

@. Consider the special case with failure probability `0%` and failure cost `10$`. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but **confirm** that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.

@. Consider this completely different problem:

    > A patient is examined by a brand-new medical diagnostics AI system.
    >
    > The AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.
    >
    > Then the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.
    >
    > If the patient is dismissed, then the life expectancy doesn't increase or decrease if the disease is not present, but it decreases by 9 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), and also if the disease is present (because it cures the disease, giving 10 more years to the patient, minus 1 year for the side effects).
    >
    > For this patient, the clinical tests indicate that there is a $10\%$ probability that the patient has the disease.

    Should the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.


:::
\

From the solution of the problem and from the exploring exercises, we gather some instructive points:

- Is it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than $50\%$?

    Obviously not. We found that if the failure probability is $10\%$ then it's best to discard; but if it's $5\%$ then it's best to accept. In both cases the component was less likely to fail than not, but the decisions were different. Moreover, we found that the probability affected the loss if one made the non-optimal decision. Therefore:
	
	[**Knowledge of exact probabilities is absolutely necessary for making the best decision**]{.blue}

- Is it enough if we simply know that failure leads to a cost? that is, that its gain is less than the gain for non-failure?

    Obviously not. The situation is similar to that with the probability. In the exercise we found that if the failure cost is $10\$$ then it's best to discard; but if it's $5\$$ then it's best to accept. It's also best to accept if the failure cost is $10\$$ but the non-failure gain is $2\$$. Therefore:
	
	[**Knowledge of the exact gains and losses is absolutely necessary for making the best decision**]{.blue}
	
- Is this kind of decision situation only relevant to assembly lines and sales?

    By all means not. We found a clinical situation that's exactly analogous: there's uncertainty, there are gains and losses (of time rather than money), and the best decision depends on both.

	<!-- [Decision-making problems under uncertainty occur repeatedly everywhere in engineering and outside engineering, on small and big scales. They all boil down to the same elements: potential decisions, uncertainty in the consequences, gains and risks involved in the consequences.]{.blue} -->
    

## Our focus: decision-making and data science

Every data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects. 

In the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and "not deciding" is not an option -- or it's just another kind choice). Making a particular decision will lead to some consequences, some leading to a desired goal, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of [**decision-making under uncertainty**]{.blue} or **under risk**^[We'll avoid the word "risk" because it has several different technical meanings in the literature, some even contradictory.], or simply a "decision problem" for short.

This problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.

In this course we're going to focus on decision problems and their systematic solution method. We'll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we'll learn a universal set of principles to solve it. This set of principles goes under the name of [**Decision Theory**]{.blue}.
<!-- This set of principles is important because it mathematically guarantees an optimal solution to the problem -- within the goals, means, and data into which we framed the problem. -->

But what do decision-making under uncertainty and Decision Theory have to do with *data* and *data science*? The three are profoundly and tightly related on many different planes:

- We saw that *probability* values are essential in a decision problem. How do we find them? As you can imagine, *data* play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.

- We saw that also the values of *gains and losses* are essential. *Data* play an important part in their calculation as well.

- *Data science* is based on the laws of *Decision Theory*. Here's an analogy: a rocket engineer relies on fundamental physical laws (balance of momentum, energy, and so on) for making a rocket work. Failure to account for those laws leads at best to sub-optimal solutions, at worst to disasters. As we shall see, the same is true for a data scientist and the rules of decision theory.

- *Machine-learning* algorithms, in particular, are realizations or approximations of the rules of *Decision Theory*. This is clear, for instance, considering that the main task of a machine-learning classifier is to decide among possible output labels or classes.

- The rules of *Decision Theory* are also the foundations upon which *artificial-intelligence* agents, which must make optimal inferences and decisions, are built.

These five planes will constitute the major parts of the present course.

\

There are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the *discovery* or the *invention* of new technologies and solutions. These aspects can barely be planned or decided; but their fruits, once available, should be handled and used optimally -- thus leading to a decision problem.

Artificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study.




## Our goal: optimality, not "success"

What should we demand from a systematic method for solving decision problems?

By definition, in a decision problem under uncertainty there is generally no method to *determine* the decision that surely leads to the desired consequence -- if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the *successful* decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty.

Imagine two persons, Henry and Tina, who must bet on "heads" or "tails" under the following conditions (but who otherwise don't get any special thrill from betting):

- If the bet is "heads" and the coin lands "heads", the person wins a *small* amount of money; but if it lands "tails", they lose a *large* amount of money.
- If the bet is "tails" and the coin lands "tails", the person *wins* a small amount of money; if it lands "heads", they lose the same *small* amount of money.

Henry chooses the first bet, on "heads". Tina chooses the second bet, on "tails". The coin comes down "heads". So Henry wins the small amount of money, while Tina loses the same small amount. What would we say about their decisions?

Henry's decision was lucky, and yet *irrational*: he risked losing much more money than in the second bet, without any possibility of at least winning more. Tina's decision was unlucky, and yet *rational*: the possibility and amount of winning was the same in the two bets, and she chose the bet with the least amount of loss. We expect that any person making Henry's decision in similar, future bets will eventually lose more money than any person making Tina's decision.

This example shows two points. First, "success" is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts. Second, even if there is no method to determine which decision is successful, there is a method to determine which decision is rational or [**optimal**]{.blue}, given the particular gains, losses, and uncertainties involved in the decision problem. We had a glimpse of this method in our introductory scenarios.

Let us emphasize, however, that we are not giving up on "success", or trading it for "optimality". Indeed we'll find that [**Decision Theory automatically leads to the *successful* decision**]{.blue} in problems where uncertainty is not present or is irrelevant. It's a win-win. It's important to keep this point in mind:

::::: {.column-page-inset-right}
:::: {.callout-note appearance="default" icon=false}
##  
[Aiming to find the solutions that are *successful* can make us *fail* to find those that are optimal when the successful ones cannot be determined.]{.red style="font-size:120%"}

[Aiming to find the solutions that are *optimal* makes us automatically find those that are *successful* when those can be determined.]{.green style="font-size:120%"}
::::
:::::

We shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms.


## Decision Theory

So far we have mentioned that Decision Theory has the following features:

- [{{< fa check >}} it tells us what's optimal and, when possible, what's successful]{.green}

- [{{< fa check >}} it takes into consideration decisions, consequences, costs and gains]{.green}

- [{{< fa check >}} it is able to deal with uncertainties]{.green}

What other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?

If we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision `discard` could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.

In data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.

Decision Theory turns out to meet these demands too, thanks to the following features:

- [{{< fa check >}} it is susceptible to recursive, sequential, and modular application]{.green}

- [{{< fa check >}} it can be used not only for human decision-makers, but also for automated or AI devices]{.green}

\

Decision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the &minus;300s, and appearing in its present form around 1920--1960. What's remarkable about it is that it is not only *a* framework, but *the* framework we must use. A logico-mathematical theorem shows that [**any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory**]{.blue}. In other words, any "alternative" framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory's. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.

Thus we list one more important characteristic of Decision Theory:

- [{{< fa check >}} it is [**normative**]{.blue}]{.green}

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
- [*Judgment under uncertainty*](references.html)
- [*Heuristics and Biases*](references.html)
- [*Thinking, Fast and Slow*](references.html)
:::
::::
*Normative* contrasts with *descriptive*. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That's exactly what we want to avoid and improve!


## Anatomy of a decision problem

Decision Theory can be roughly divided into two main parts: [**Probability Theory**]{.blue}, which deals with information, data, uncertainty, inference; and [**Utility Theory**]{.blue}, which deals with actions, consequences, gain and loss, decisions. We shall get acquainted with Decision Theory step by step, introducing its main ideas and notions as they become necessary.


An extremely important -- and surprisingly often neglected -- first step in every engineering problem is to define exactly what the problem is. This means, in particular, to specify unambiguously the goals, the available data and information, the available decisions or courses of action, the hypotheses of interest.

Decision theory analyses any problem in terms of nested or sequential basic decision problems, each of which is framed in terms of these elements:

::::{.column-margin}
:::{.callout-tip}
## {{< fa seedling >}}
Remember: What matters is to be able to identify these elements in a concrete engineering problem, understanding their role. Their technical names don't matter.
:::
::::

- [**Agent**]{.blue}, and [**background**]{.blue} or  [**prior information**]{.blue}: the agent is the person or device that has to make an optimal decision. An agent has some specific background information and data that are used and taken for granted in the decision-making process. Since different agents typically have different background information, we shall somehow conflate them.

- [**Decisions**]{.blue} or [**courses of actions**]{.blue}: the choices available to the agent.

- [**Assumptions**]{.blue}: data and information that are known, or temporarily imagined to be known, to the agent. Assumptions are different from background information because the latter is never used for hypothetical or counterfactual reasoning -- a difference that will become clear shortly.

- [**Outcomes**]{.blue}: hypotheses, conjectures, or actual facts that the agent wishes to assess; often they depend on the possible decisions.

- [**Probabilities**]{.blue}: forecasting the outcomes given the assumptions, and the uncertainty of these forecasts.

- [**Utilities**]{.blue}: the gains and losses involved in making each possible decision.

Together with the "assumptions" and "outcomes" it is also useful to keep in mind two more, somewhat different elements:

- [**Knowns**]{.blue}: the data and information that are actually known to the agent.

- [**Unknowns**]{.blue}: hypotheses, conjectures, and situations whose truth or falsity are actually unknown to the agent.

The basic idea is that the agent will [**infer**]{.blue} the outcomes given the assumptions, with some degree of [**uncertainty**]{.blue}. From these inferences it will determine the [**optimal**]{.blue} decision.

A basic decision problem and some of its elements can be graphically represented in a diagram like this:

::::: {.column-page-inset-right}
![](decision_tree.svg){width=100%}
:::

It has one *decision node*, usually represented by a square {{< fa regular square >}}, from which the available decisions depart as lines. Each decision leads to an *uncertainty node*, usually represented by a circle {{< fa regular circle >}}, from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability, which depends on the assumptions common to the problem and, often, on the decision.

{{< fa exclamation-circle >}}\ \ A priori there isn't any particular temporal order among decisions, outcomes, and even assumptions. In some cases an outcome happens after a decision; for instance, you pick an umbrella, and then it rains. In other cases the outcome already happened before the decision; for instance, a clinician decides to treat a patient who might have a disease, but the disease, if present, was already there before the clinician's decision. The difference between decisions and outcomes is not one of time, but of control: decisions are under our control; outcomes are not and are generally uncertain.

:::{.callout-note}
## Example
> A particular kind of electronic component is produced on an assembly line. At the end of the line, an automated inspection device makes some tests on every newly produced component. The tests give an uncertain forecast of whether the component would fail within less than a year of use, or after a year.
>
> Depending on the test results, the automated inspector approves the component, or discards it. An approved component is sold, leading to some net monetary gain if it works for at least a year; but leading to a net loss if it fails within a year, owing to damage and warranty refunds. A discarded component doesn't lead to any gain or loss.
\

A decision problem is involved in this process: one for every new component that arrives at the automated inspector. The elements of the problem can be identified as follows:

- *Agent* and *background information*: the automated inspector (and indirectly the engineer who designed and programmed it), and its builtin knowledge base.
- *Decisions*: approve the new electronic component, or discard it.
- *Assumptions*: the values obtained from the tests.
- *Outcomes*: three possible ones: (1) failure of the component within a year, (2) survival of the component for longer than a year, (3) destruction of the component. Either of the first two outcomes can happen if the component is sold; the third outcome happens if the component is discarded.
- *Probabilities*: the quantified forecast of failure within a year if the component were sold. If the component is discarded, there's nothing to forecast.
- *Utilities*: The gains or losses to be had if the component is sold and doesn't fail within a year, if it's sold and fails, and if it's discarded.
:::
:::{.callout-caution}
## Exercise
Suppose that a new component arrives at the inspector. Tests give a `10%` probability of failure within a year, and `90%` otherwise, if the component is approved. If the component is approved, it will be sold for a net gain of `1$` if it works for a year, but for a net *loss* of `5$` if it fails instead. If the component is discarded, there won't be any net gain or loss.

1. Sketch the diagram for this basic decision problem.

2. Should the automated inspector `approve` or `discard` this new component? Try to make a decision on intuitive grounds, trying to interpret what the probabilities and utilities given above could mean. What kind of background information did you use?
:::

Some of the decision-problem elements listed above may themselves need to be analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.

The elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined. Suppose someone (probably a politician) says: "We must solve the energy crisis by reducing energy consumption or producing more energy". This person has effectively said *nothing whatsoever*. By definition the "energy crisis" is the problem that energy production doesn't meet demand. So this person has only said "we would like the problem to be solved", without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their costs, gains, and probable outcomes would be.

::::{.column-margin}
::: {.callout-tip}
## {{< fa book >}} For the curious
See MacKay's options-vs-costs rational analysis in [Sustainable Energy -- without the hot air](https://www.withouthotair.com)
:::
::::

An advantage of decision theory is that its application *forces* us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.

