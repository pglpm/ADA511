[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511 0.3 Data Science and AI prototyping",
    "section": "",
    "text": "Dear student and aspiring data- & AI-engineer\nIf you can’t join ’em,\nbeat ’em.\n(J. Schwinger)\nThe goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to choose a good prompt for a Large Language Model, or how to do cross-validation in the fastest way, or what is the latest improvement in random-forest algorithms.\nThe goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices of the future. And, as a side effect, you’ll also learn how to concretely improve present-day algorithms, and also how to determine if any of them has already reached its maximal theoretical performance.\nHow can such a goal be achieved?\nThere is a small set of rules and one method that are mathematically guaranteed to output the optimal solution of any inference, prediction, classification, and decision-making problem. You can think of this set as defining an “unbeatable, optimal universal machine”. Or, from an AI point of view, you can think of these rules and method as the “laws of robotics” that should govern any ideal AI designed to draw inferences, give answers, and make decisions.\nThese rules and method are quite easy to grasp and understand. You’ll learn them very quickly, and they’ll be the solid ground on which your data & AI engineering knowledge and skills are built.\nThese rules and method are computationally extremely expensive; the more so, the more data points and data dimensions we need to deal with. Current machine-learning algorithms, from deep networks to large language models, are approximations to this ideal universal method; each one uses a different kind of approximation. The upside of these approximations is that they allow for much faster computations; their downside is that they generally give sub-optimal or non-intelligent results.1\nBut approximations can be improved with new technologies. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a useless approximation 50 years ago, before the introduction of Graphical Processing Units.\nEvery new technological advance (think of possibly forthcoming quantum computers) opens up possibilities for new approximations that get us closer and closer to the ideal optimum. However, in order to see and realize these possibilities, or to judge whether they have already been realized, a data scientist needs at the very least:\nto know the foundation of the maximally optimal method\nto think outside the box\nWithout the first requirement, how do you know what is the target to approximate towards, and how far you are from it? You risk:\n making an approximation that leads to worse results than before;\n evaluating the approximation in the wrong way, so you don’t even realize it’s worse than before;\n trying to improve an approximation that has already attained the theoretical optimum. Think about an engine that has already the maximal efficiency dictated by thermodynamics; and an engineer, ignorant of thermodynamics, who wastes effort in trying to improve it further.\nWithout the second requirement, you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse-carriage wooden wheels, you’ll never conceive a combustion engine. If you keep thinking in terms of how to improve combustion fuel, you’ll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum – so we’re back to the first requirement.\nIf you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of “training sets”, “cross-validation”, “supervised learning”, “overfitting”, “models”, and similar notions. In this course you’ll see for yourself that such notions are anchored to the box of present-day approximations.\nAnd we want to think outside that box.\nThis course will not only prepare you for the future. With the knowledge and insights acquired, you will be able to devise and implement concrete improvements to present-day methods as well, or calculate whether they can’t be improved further.",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "index.html#your-role-in-the-course-bugs-features",
    "href": "index.html#your-role-in-the-course-bugs-features",
    "title": "ADA511 0.3 Data Science and AI prototyping",
    "section": "Your role in the course Bugs & features",
    "text": "Your role in the course Bugs & features\nThis course is still in an experimental, “alpha” version. So you will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students. Thank you for this in advance!\nFor this reason it’s good to clarify some goals and guidelines of this course: \n\n  Undergraduate maths requirements\n\nWe believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations \\(+ - \\times /\\). So this course only requires maths at a beginning first-year undergraduate level.\n\n\n\n  Informal style\n\nThe course notes are written in an informal style; for example they are not developed along “definitions”, “lemmata”, “theorems”. This does not mean that they are inexact. We will warn you about parts that are oversimplified or that only cover special contexts.\n\n\n\n  Names don’t constitute knowledge\n\n\nIn these course notes you’ll often stumble upon terms in blue bold and definitions in blue Italics. This typographic emphasis does not mean that those terms and definitions should be memorized: rather, it means that there are important ideas around there which you must try to understand and use. In fact we don’t care which terminology you adopt. Instead of the term statistical population, feel free to use the term pink apple if you like, as long you explain the terms you use by means of a discussion and examples.2 What’s important is that you know, can recognize, and can correctly use the ideas behind technical terms.\n2 Some standard technical terms are no better. The common term random variable, for instance, often denotes something that is actually not “random” and not variable. Go figure. Using the term green banana would be less misleading!Memorizing terms, definitions, and where to use them, is how large language models (like chatGPT) operate. If your study is just memorization of terms, you’ll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.\n\n\n\n\n  Diverse textbooks\n\nThis course does not have only one textbook: it refers to and merges together parts from several books and articles. As you read these works, you will notice that they adopt quite different terminologies, employ different symbolic notations, give different definitions for similar ideas, and sometimes even contradict each other.\nThese differences and contradictions are a feature, not a bug!\nYou might think that this makes studying more difficult; but it actually helps you to really understand an idea and acquire real knowledge, because it forces you to go beyond words, symbols, and specific points of view and examples. This point connects with the previous point, “names don’t constitute knowledge”. The present course notes will help you build comprehension bridges across those books.\n\n\nThe textbook material is presented in Study reading boxes. There are two kinds:\n\n“Read”: you don’t need to study this as for an exam, but you do have to read it, as if it were an interesting piece of news or your favourite blog.\n“Skim through”: you don’t need to read every word, you can just skim through the text; but you must get an idea of what it’s speaking about and what its main points are.\n\nYou’ll find further details of each item in the final Bibliography.\n\n\n\n  Artificial intelligence\n\nIn order to grasp and use the fundamental laws of inference and decision-making, we shall use notions that are also at the foundations of Artificial Intelligence (and less common in present-day machine learning). So you’ll also get a light introduction to AI for free. Indeed, a textbook that we’ll draw frequently from is Russell & Norvig’s Artificial Intelligence: A Modern Approach (we’ll avoid its part V on machine learning, however, because it’s poorly explained and written).\n\n\n\n\n  Concrete examples\n\nSome students find it easier to grasp an idea by starting from an abstract description and then examining concrete examples; some find it easier the other way around. We try to make both happy by alternating between the two approaches. Ideas and notions are always accompanied by examples that we try to keep simple yet realistic, drawing from scenarios ranging from glass forensics to hotel booking.\n\n\n\n  Code\n\nWe shall perform inferences on concrete datasets, also comparing different methodologies. Most of these can be performed with any specific programming language, so you can use your favourite one – remember that we want to try to think outside the box of present-day technologies, and that includes present-day programming languages. Most examples in class and in exercises will be given in R and sometimes in Python, but are easily translated into other languages.\n\n\n\n  Extra material\n\nThe course has strong connections with many other disciplines, such as formal logic, proof theory, psychology, philosophy, physics. We have tried to provide a lot of extra reading material in “For the extra curious” side boxes, for those who want to deepen their understanding of topics covered or just connected to the present course. Maybe you’ll stumble into a new passion or even into your life call?\n\n\n\n\n\n\n\n\n\n For the extra curious",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "ADA511 0.3 Data Science and AI prototyping",
    "section": "Course structure",
    "text": "Course structure\nThe course structure reflects the way in which the ideal universal decision-making machine works. It can be roughly divided into three or four parts, illustrated as follows (this is just a caricature, don’t take this diagram too literally):\n \n\nData parts (top-left, yellow box) develop the language in which a problem can be fed into the decision-making machine. Here you will also learn about important pitfalls in handling data.\nInference parts (left-centre, green box) develop the “inference engine” of the machine. Here you will learn ideas at the foundation of AI; and you will also meet probability, but from a point of view that may be quite novel to you – and much more fun.\n\nThese two parts will alternate so that their development proceeds almost in parallel.\n\nThe utility part (top-right, light-blue box) develops the “decision engine” of the machine. Here you will meet several ideas that will probably be quite new to you – but also very simple and intuitive.\nThe solution part (bottom, dark-blue box) simply shows how the inference and utility engines combine together to yield the optimal solution to the problem. This part is simple, short, intuitive; it will be a breeze.\n\n\n\nWe shall start with a quick preview of the solution part in chapters 1–4, because it is very simple to understand, and it shows why the inference and the utility parts are necessary.\nThen we shall continue with the inference part in chapters 5–10 and 14–18, alternating it with the data part in chapters 12–13 and 20–23, and with interludes about present-day machine-learning algorithms and their approximations in chapters 4, 11, 19, and 24–26.\nAs soon as the inference and data parts are complete, you will be able to apply the machine to real, albeit not too complex, inference problems. This application will be made in chapters 32–35.\nWe finally round up with the utility part in chapters 36–37, extending our concrete application to it in chapter 38. Final connections with present-day machine learning are made in chapters 39–40.\nYou should be able to see this timeline in the index tab on the side.",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Mechanics and engineers\nScience is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\nWhat is the difference between a car mechanic and an automotive engineer?\nBoth have knowledge about cars, but their knowledge domains are different and focus on different goals.\nA car mechanic can keep your car in top-notch condition; can do different kinds of easy and difficult repairs if problems arise with it; knows whether a particular brand of valve can be used as a replacement for another brand; can recommend the optimal kind of tyres to use in a given season for different brands of cars. But a car mechanic would face difficulties in calculating the theoretical maximal efficiency of an engine; or predicting the temperature increase caused by a new kind of fuel; or exploiting the phase transition of a new kind of foam to design a safer airbag system; or calculating the optimal surface curvature for a spoiler. A car mechanic typically possesses a large amount of case-specific knowledge, and doesn’t need to know in depth the principles of electromechanics and thermochemistry, or the laws of balance of momentum, energy, entropy.\nVice versa, an automotive engineer can assess how to use the electromechanical properties of a new material in order to design a more efficient and environmentally friendly engine; can calculate how a new material-surface handling would affect air drag and speed; and ultimately can research how to exploit new physical phenomena to build completely new means of transportation. Yet, an automotive engineer could be completely incapable of changing a pipe in your car, or tell you whether it can use a particular brand of lubricant oil. An automotive engineer typically possesses knowledge about the principles of electromagnetism, mechanics, or thermochemistry; is acquainted with relevant physical laws; and doesn’t need to have in-depth case-specific kinds of knowledge.\nNote that the differences just sketched do not imply a judgement of value. Both professions, kinds of knowledge, and goals are necessary, interesting, and couldn’t exist without each other. Choice between them is a subjective matter of personal passions and aspirations.\nIn fact there isn’t a clear divide between these two kinds of knowledge, but rather a continuum between two vague extremities. A car mechanic can have knowledge and insight about new technologies, and an automotive engineer can know how to fix a carburettor. The two sketches above are meant to expose and emphasize the existence of such a continuum of knowledge and of goals.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-mechanics-and-data-engineers",
    "href": "preface.html#data-mechanics-and-data-engineers",
    "title": "Preface",
    "section": "Data mechanics and data engineers",
    "text": "Data mechanics and data engineers\nA continuum with two similar extremities can also be drawn in data science.\nSome data scientists have in-depth knowledge on, for instance, how to optimally store and read large amounts data; what kind of machine-learning algorithm to use in a given task; how to fine-tune an algorithm’s parameters, and the currently best software for this purpose. Their particular knowledge is fundamental for the working of today’s technological infrastructure.\nAt the same time, these data scientists typically face difficulties, for instance, in:\n\ncalculating the theoretical maximal accuracy or performance achievable – by any possible algorithm – in a given inference problem\nexplaining how the fundamental rules of inference and decision-making are implemented in a particular machine-learning algorithm\nidentifying which sub-optimal approximations to the fundamental rules are made by popular machine-learning algorithms\nexploiting new technologies to build new algorithms that do calculations closer to the exact theoretical ones, thereby achieving a performance closer to the theoretical optimum\n\nAnd it is also possible that they are not aware of, and maybe would be surprised by, some basic facts of data science. For instance:\n\nthere is an optimal, universal inference & decision algorithm, of which all machine-learning algorithms (from support vector machines and deep networks to random forests and large language models), are an approximation\nthere are only five or six fundamental laws upon which any inference, prediction, classification, regression, decision task is (or ought to be) based upon\nsplittings of data into “training set”, “validation set”, and similar sets, are not part of the exact application of the laws of inference and decision-making; such splittings arise as coarse approximations of the exact method.\ncross-validation and related techniques are not part of the exact method either; they also arise as approximations\noverfitting, underfitting and related notions are not problems that appear in the exact method (which takes care of them automatically); they also arise from approximations\nit is possible to calculate, within probable bounds, the maximal accuracy (or other performance metric) achievable by any classification or regression algorithm for a given application\nsome evaluation metrics, such as precision or the area under the curve of the receiver operating characteristic (AUC), have intrinsic flaws and may attribute higher values to worse-performing algorithms\n\n…because this is a kind of general and principled knowledge that these data scientists don’t need in their jobs. Their knowledge is more case-specific.\nDrawing a parallel with the car example, a data scientist with this kind of case-specific knowledge is like a “data mechanic”.\nA “data engineer”, on the other hand, is the kind of data scientist who has no difficulties with the knowledge and skills implicit in the bullet points above; but at the same time might not know what software to use for tuning parameters of a particular class of deep networks, or the best format to store particular kinds of data.\nJust like in the case of the automotive industry, the difference just sketched does not imply any judgement of value. Both kinds of knowledge and goals are important and can’t exist without each other.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#goals-of-this-course",
    "href": "preface.html#goals-of-this-course",
    "title": "Preface",
    "section": "Goals of this course",
    "text": "Goals of this course\nThere is a plethora of academic courses, in all kinds of format, that target knowledge and goals for the “data mechanic”. Those courses are usually inadequate to cover the knowledge and goals for the “data engineer”. Some courses, misleadingly, even present approximations and recipes that are only valid for particular situations as if they were universal rules or methods instead.\nCourses that target the “data engineer” seem to be more rare. One possible reason is that this kind of knowledge is actually hidden in courses on probability, statistics, and risk analysis, presented with a language which makes only opaque and confusing connections with fields in data science and their goals; or, worse, with a language which emphasizes connections that are actually superficial and misleading.\nWe believe that it is important to teach and keep alive the less “mechanic” and more “engineer” side of data science:\n\nContinuous advances in computational technology – think of quantum computers – will offer completely novel and superior ways to approximate the exact method of inference and decision. Only the data scientist who knows the exact method and theory, and understands how present-day algorithms approximate it, will be able to exploit new technologies.\nEven without looking at the future, several present-day machine-learning algorithms could already be greatly optimized by any data engineer who is acquainted with the basic principles underlying data science.\nThe foundations of data science are the bridge to the sibling discipline of Artificial Intelligence.\n\nThe present course aspires to give an introduction to the “data engineer” side, rather than “data mechanic” one, of data science, but using a point of view more familiar to data scientists than to, say, statisticians.\nMore details about its aims, structure, and features are already given in the Dear student introduction.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\nLet’s start with a question that could arise in an engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line there is an automated inspection device that works as follows on each new component:\n\nThe inspection device first performs some tests on the component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\n\nConsider also the following context. When a new electronic component is sold, the manufacturer has a net gain of 1$. That’s the net gain if the component works for at least a year. But if the component instead fails within a year of use, the manufacturer incurs a net loss of 11$ (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has 0$ net gain.\nNow we have a new electronic component, just come out of the assembly line. The tests of the automated inspection device indicate that there is a 10% probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept the new component? or discard it?\n\nTry to give and motivate an answer:\n\n\n\n\n\n\n Very first exercise!\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nIt doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all. The purpose here is for you to do some introspection about your own reasoning.\nThen examine and discuss the following points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Accept or discard?]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html",
    "href": "framework.html",
    "title": "2  Framework",
    "section": "",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\nFirst, what happens if we accept the component?\nWe must try to make sense of the 10% probability that the component fails within a year. For the moment let’s use an imagination trick: imagine that the present situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year. Later on we’ll approach this in a more rigorous way, where the idea of “imaginary repetitions” is not needed.\nIn each of the 10 imaginary repetitions where the component fails early, the manufacturer loses \\(\\color[RGB]{238,102,119}11\\$\\). That’s a total loss of \\({\\color[RGB]{204,187,68}10} \\cdot {\\color[RGB]{238,102,119}11\\$} = {\\color[RGB]{238,102,119}110\\$}\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(\\color[RGB]{34,136,51}1\\$\\). That’s a total gain of \\({\\color[RGB]{204,187,68}90} \\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{34,136,51}90\\$}\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n{\\color[RGB]{204,187,68}10}\\cdot ({\\color[RGB]{238,102,119}-11\\$}) + {\\color[RGB]{204,187,68}90}\\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{238,102,119}-20\\$}\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\)! That’s an average of \\(0.2\\$\\) lost per repetition.\nNow let’s examine the second choice: what happens if we discard the component instead?\nIn this case it’s clear that the manufacturer doesn’t gain or lose anything. That is, the “gain” is \\(0\\$\\) (this is for sure, so we don’t need to imagine any “repetitions”).\nThe conclusion is this: If in a situation like the present one we accept the component, then we’ll lose \\(0.2\\$\\) on average. Whereas if we discard it, then we’ll lose \\(0\\$\\) on average.\nObviously the best, or “least worst”, decision to make is to discard the component.\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "",
    "text": "We’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and different values of the cost of failure. Is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the probability of failure for which there is no loss or gain, on average, if we accept the component (so it doesn’t matter whether we discard or accept). You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.\nConsider the special case with failure probability 0% and failure cost 10$. That probability means that no new component will ever fail. It’s clear what’s the optimal decision in this limit case, without any calculations or imaginary repetitions. Yet, confirm mathematically that we arrive at this obvious conclusions if we perform a mathematical analysis like before.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nFirst, the AI performs some clinical tests on the patient. The tests give an uncertain forecast on whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then their life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then their life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a 10% probability that they have the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\n\nIs it enough if we simply know that the component is less likely to fail than not? In other words, is it enough to know that the probability of failure is less than 50% without knowing its precise value?\nObviously not. We found that if the failure probability is 10% then it’s best to discard. But we also found that if it’s 5% then it’s best to accept. In either case the probability of failure was less than 50%, but the decision was different.\nOn top of that, we also found that the probability value determines the average amount of loss when the non-optimal decision is made. Therefore:\n Knowledge of precise probabilities is absolutely necessary for making the best decision.\n\n\nIs it enough if we simply know that failure leads to a loss, and non-failure leads to a gain, without knowing the precise amounts of loss and gain?\nObviously not. In the exercise we found that if the cost of failure is 11$, then it’s best to discard. But we also found that if it’s 5$, then it’s best to accept (given the same probability of failure). And we also found that it’s best to accept when the cost of failure is 11$ but the gain from non-failure is 2$. Therefore:\n Knowledge of the precise gains and losses is absolutely necessary for making the best decision.\n\n\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We examined a clinical problem that’s exactly analogous: there’s uncertainty and probability, there are gains and losses (of lifetime rather than money), and the best decision depends on both probabilities and costs.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering problem is unique, with unique difficulties, questions, issues. But there are some general aspects that are common to all engineering problems.\nIn the scenarios that we explored above, we found an extremely important problem-pattern:\n There is a decision or choice to make (and “not deciding” is not an option, or it’s just another kind of choice).\n Making a particular decision will lead to some consequences. Some consequences are desirable, others are undesirable.\n The decision is difficult to make, because its consequences are not known with certainty, even considering the information and data available in the problem: we may lack information and data about past or present details, about future events and responses, and so on.\nThis is what we call a problem of decision-making under uncertainty or under risk1; or simply a “decision problem” for short.\n1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.This problem-pattern appears literally everywhere. Stop for a second, and think about all different situations in which you had to make a decision today. Do they show this pattern?\nBut our exploration of different scenarios also suggests something important: this problem-pattern seems to have a sort of systematic method of solution!\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some general notions that allow us to frame and analyse this kind of problems. And we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly, tightly connected on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nData science is based on the laws of Decision Theory. These laws are similar to what the laws of physics are to a rocket engineer. Failure to account for these fundamental laws leads to sub-optimal solutions – or to disasters.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that a machine-learning classifier is actually choosing among possible output classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents – which must perform optimal inferences and decisions – are built.\nWe saw that probability values are essential to a decision problem. How do we find them? Obviously data play an important part in their calculation. In our introductory example, the failure probability must have come from observations or experiments on previous similar electronic components.\nWe saw that the values of gains and losses are essential. Data play an important part in their calculation as well.\n\nThese five planes will constitute the major parts and motivations of the present course.\n\n\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. Aspects such as these can barely be planned or decided. Their drive and direction, however, rest on a strive for improvement and optimization. But the fundamental laws of Decision Theory tell us what’s optimal and what’s not, so they play some part in these creative aspects as well.\nArtificial intelligence is proving to be a valuable aid in these creative aspects. This kind of use of AI is outside the scope of the present notes. But some aspects of this creativity-assisting use do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-optimality",
    "href": "framework.html#sec-optimality",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence. If such a method existed, the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. Then what should be the goal of such a method?\nImagine two persons, Henry and Tina, who must choose between a “heads-bet” or a “tails-bet” before a coin is tossed. The bets are these:\n\n“heads-bet”: If the coin lands heads, the person wins a small amount of money. But if it lands tails, they lose a large amount of money.\n“tails-bet”: If the coin lands tails, the person wins a small amount of money. If it lands heads, they lose the same small amount of money.\n\n \n\n\n\n\n\n\n Exercise\n\n\n\nWhich bet would you choose? why?\n\n\nNow this happens: Henry chooses the heads-bet. Tina chooses the tails-bet. The coin comes down heads. So Henry wins the small amount of money, while Tina loses the same small amount.\nWhat would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than he could win. Tina’s decision was unlucky, and yet rational: she wasn’t risking to lose more than she could win. Said otherwise, the heads-bet had higher risk of loss than the tails-bet, and not even an higher chance of gain. We expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThe method we’re looking for is therefore one that, in the hypothetical situation above, would lead to the same decision as Tina’s – even if Tina’s decision was unlucky. That’s the decision that we call rational or optimal in such an uncertain situation.\n\n\n\n\n\n\n\n\n\n\nIf you’re thinking “wouldn’t it be best to have a method that works under uncertainty but that leads to Henry’s decision, every time that decision is lucky?” – then let’s repeat: such a method cannot logically exist. If we know which decision is “lucky”, then it means that we have no uncertainty. If we are uncertain, then it means that we don’t know which decision is “lucky”, and so it’s impossible to choose it for sure.\n\n\n\n\nOur discussion and the distinction between “successful” and “optimal” decisions also shows that we cannot evaluate the efficacy of a method for decisions under uncertainty, by checking whether or how often that method leads to the desired, “successful” consequence. This point is also easily illustrated with a variation on Henry and Tina’s example:\nSuppose the general context and the bets are exactly the same. But now imagine Henry and Tina to be the names of two automated decision methods, say two machine-learning algorithms. Also, let’s say that you first toss the coin in secret and see its outcome, then you offer the possible bets to Henry and Tina, who are completely ignorant about the outcome (note that no cheating is involved).\nYou toss the coin and see that it lands heads. Then the choice of bets is offered to Henry and Tina. Henry chooses the heads-bet and Tina the tails-bet.\nNow consider this: you know the “truth”, you know what the successful decision would be. It turns out that Henry made the choice corresponding to the truth. Tina didn’t. Would you then evaluate the Henry algorithm to be better than the Tina algorithm?\nFor exactly the same reasons already discussed, the Tina algorithm is the better one; it made the optimal decision. Yet it didn’t choose the “truth”. You realize that comparing algorithms is not as simple as checking which one yields the truth more often.\n\n\nWe have then arrived at two conclusions:\n\n “Success” or “correspondence to truth” is generally not a good criterion to judge a decision under uncertainty or to evaluate an algorithm that makes such decisions. \n Even if there is no method to determine which decision is successful, there is nevertheless a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem.\n\nWe had a glimpse of this method in our introductory scenarios with electronic components and their variations.\nLet us emphasize, however, that we are not giving up on “success”; nor are we trading “success” for “optimality”. We’ll find out that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. Keep this point firmly in mind:\n\n\n\n\n\n\n\n \n\n\n\nAiming to find the solution that is successful can make us fail to find the solution that is optimal, when the successful one cannot be determined.\nAiming to find the solution that is optimal makes us automatically also find the solution that is successful, when this can be determined.\n\n\n\nWe shall later witness this fact with our own eyes. We will also take it up in the discussion of some misleading techniques to evaluate machine-learning algorithms.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-decision-theory",
    "href": "framework.html#sec-decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n\n It tells us what’s optimal and, when possible, what’s successful.\n It takes into consideration decisions, consequences, costs and gains.\n It is able to deal with uncertainties.\n\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science? Here are two:\n\nIf we find an optimal decision in regards to some problem, it may still happen that this decision leads to new, subsequent decision problems. For example, in the assembly-line scenario the decision discard could be carried out by burning, recycling, and so on. And each of these actions could have uncertain results and costs or gains. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision, at least in specific kinds of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system; or a software company designing an image classifier.\n\nWell, Decision Theory turns out to meet these two demands too, thanks to the following features:\n\n\n It is susceptible to recursive, sequential, and modular application.\n It can be used not only for human decision-makers, but also for AI or automated devices.\n\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s. It appeared in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework: it is the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, an “alternative” framework might use different terminology and apparently different mathematical operations, but it would boil down to the same notions and mathematical operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) your framework would lead to some irrational or illogical consequences; or (b) your framework would lead to results identical to Decision Theory. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n\n It is normative.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid! We want a theory, a norm, that human decision-makers should aspire to. That’s what Decision Theory is.\n\n\n\n\n\n\n Study reading\n\n\n\nWho says that Decision Theory should be normative? – this is a respectable scientific question. If you found yourself wondering and doubting about this, then congratulations: that’s how a scientist should think!\nLater on we’ll examine material and arguments about this point. As a start in your investigations, skim through:\n\nCh. 15, especially §15.1 and § “Bibliographical and Historical Notes” of Artificial Intelligence\nBriggs 2014/2019: Normative Theories of Rational Choice: Expected Utility\nSteele & al. 2015/2020: Decision Theory\nChapters 1–2 of Raiffa: Decision Analysis",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html",
    "href": "basic_decisions.html",
    "title": "3  Basic decision problems",
    "section": "",
    "text": "3.1 Graphical representation and elements\nDecision Theory analyses any decision-making problem in terms of nested or sequential basic or minimal decision problems. The assembly-line scenario of the introduction 1 is an example.\nA basic decision problem can be represented by a diagram like this:\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an inference node,1 usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular gain or loss, depending on the decision. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of the following elements:\nThe relation between the elements above can be depicted as follows – but note that this is just an intuitive illustration:\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "",
    "text": "1 also called chance node or uncertainty node\n\n\n\n\n\n\n \n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\n\n\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities, gains, and losses of the outcomes, together with other available data and information. Different agents typically have different background information.\n\n\n\nAgent means “conductor”, “mover”, and similar (from Latin ago = to move or drive and similar meanings).\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n Decisions available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n\n\n\nDecisions are called courses of action in some literature.\n\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n\n\n\nMany other terms instead of outcome are used in the literature, for instance state or event.\n\n Probabilities for each of the outcomes and for each decision. Their values typically depend also on the background information and the additional data.\n Utilities: the gains or losses associated with each possible outcome and each decision. We shall mainly use the term utility, instead of “gain”, “loss”, and similar, for several reasons:\n\ngain and losses may involve not money, but time, or energy, or health, or emotional value, or other kinds of commodities and things that are important to us; or even a combination of them. The term “utility” is useful as a neutral term that doesn’t mean “money”, but depends on the context\nwe can just use one term instead of two: for example, when the utility is positive it’s a “gain”; when it’s negative it’s a “loss”\n\nThe particular numerical values of the utilities are always context-dependent: they may depend on the background information, the decisions, the outcomes, and the additional data.\nWe shall sometimes use the generic currency sign  ¤  to denote utilities, to make clear that gains and losses do not necessarily involve money, and not to reference any country in particular.\n\n\n\n\n\n\n\n\n\n Don’t over-interpret the decision diagram\n\n\n\n\nThe diagram above doesn’t have any temporal meaning, that is, it doesn’t mean that the decisions happen before the outcomes, or vice versa.\nIn some situations the outcome can be realized after the decision is made; for instance, someone bets on heads or tails, and then a coin is tossed.\nIn other situations, the outcome can be realized before the decision is made; for instance, sometimes a coin is tossed and covered, then one is asked to bet on what the outcome was. Another example is some research decision made by a archaeologist, the unknown being some detail about a dinosaur from millions of years ago.\nIn yet other situations the outcome may have a complex nature, and it may be realized partly before the decision is made, and partly after; for instance, someone can bet on the outcome of two coin tosses; one coin is tossed before the decision is made, and the other after.\nThe diagram above is not something that an agent must use in making decisions. It is not part of the theory. It’s just a very convenient way to visualize and operate with the mathematics underlying the theory.\nIt not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred. In such situations we can still use diagrams like the one above, because the help us doing the calculation, although the actual outcome is already known.\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§1.1.4 in Artificial Intelligence\n\nSkim through:\n\nCh. 15 of Artificial Intelligence. No need to read thoroughly: just quickly glimpse whether there are ideas and notions that look familiar (a little like when you’re in a large crowd and look quickly around to see if there are any familiar faces)\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the decision diagram for the assembly-line decision problem.\n\n\n\n\n\n\n\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#sec-decision-matrices",
    "href": "basic_decisions.html#sec-decision-matrices",
    "title": "3  Basic decision problems",
    "section": "3.2 Setting up a basic decision problem",
    "text": "3.2 Setting up a basic decision problem\nA basic decision problem can be set up along the following steps (which we illustrate afterwards with a couple of examples):\n\n\n\n\n\n\nSetup of a basic decision problem\n\n\n\n\nList all available decisions\nFor each decision, list its possible outcomes\nPool together all outcomes of all decisions, counting the common ones only once\nPrepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\nIn the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\n\n\n\nExample: the assembly-line problem\nLet’s apply the steps above in the assembly-line example of ch.  1:\n1. List all available decisions\nEasy: they are “accept the electronic component” and “discard it”.\n\n\n2. For each decision, list its possible outcomes\nIn general you will notice that some outcomes may be common to all decisions, while other outcomes can happen for some decisions only, or even for just one decision.\nIn the present example, the accept decision has two possible outcomes: “the component works with no faults for at least a year” and “the component fails within a year”.\nThe discard cannot have those outcomes, because the component is discarded. It has indeed only one outcome: “component discarded”.\n\n\n3. Pool together all outcomes of all decisions, counting the common ones only once\nIn total we have three pooled outcomes:\n\nno faults (from the accept decision)\nfails (from the accept decision)\ndiscarded (from the discard decision)\n\n\n\n4. Prepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn the present example each table looks like this:\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\n\n\n\n\ndiscard\n\n\n\n\n\n\n\n\n5. In one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\n\nProbability table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(90\\%\\)\n\\(10\\%\\)\n\\(0\\%\\)\n\n\ndiscard\n\\(0\\%\\)\n\\(0\\%\\)\n\\(100\\%\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\%\\) probability (in grey). This is just a way of saying “this outcome can’t happen, if this decision is made”.\n\n\n6. In the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\nUtility table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(+1\\$\\)\n\\(-11\\$\\)\n\\(0\\$\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\$\\) utility (in grey). We shall see later that it actually doesn’t matter which utilities we give to these impossible outcomes.\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nApply the steps above to the following basic decision problems (you only need to set them up with their probability & utility tables, but feel free to solve them as well, if you like):\n\nThe “heads-bet” vs “tails-bet” example of § 2.3. Assume that the “small amount” of money is \\(10\\$\\), the “large amount” is \\(1000\\$\\), and the two outcomes’ probabilities are \\(50\\%\\) each.\nPeter must reach a particular destination, and is undecided between three alternatives: go by car, or ride a bus, or go on foot.\nIf he goes by car, he could arrive without problems, with a probability of \\(80\\%\\) and a utility of \\(10\\,¤\\), or he could get stuck in a traffic jam and arrive late, with a probability of \\(20\\%\\) and a utility of \\(-10\\,¤\\).\nIf he rides a bus, he could arrive without problems, with a probability of \\(95\\%\\) and a utility of \\(15\\,¤\\), or arrive in time but travelling in a fully-packed bus, with a probability of \\(5\\%\\) and a utility of \\(-10\\,¤\\).\nIf he goes on foot, he could arrive without problems, with a probability of \\(20\\%\\) and a utility of \\(20\\,¤\\), or he could get soaked from rain, with a probability of \\(80\\%\\) and a utility of \\(-5\\,¤\\).\n(We are using the symbol  “\\(¤\\)”  because Peter’s utilities are a combination of money savings, time of arrival, and comfort.)",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#sec-make-decision",
    "href": "basic_decisions.html#sec-make-decision",
    "title": "3  Basic decision problems",
    "section": "3.3 How to make a basic decision?",
    "text": "3.3 How to make a basic decision?\nUp to now we have seen what are the elements of a basic decision problem, and how to arrange them in a diagram and with tables. But how do we determine what’s the optimal decision?\nDecision Theory says that the optimal decision is determined by the “principle of maximal expected utility”.\nWe shall study this principle more in detail toward the end of the course, although you already know its basic idea, because you intuitively used this very principle in solving all decision problems we met so far, starting from the assembly-line one.\nHowever, let’s quickly describe already now the basic procedure for this principle:\n\n\n\n\n\n\nPrinciple of maximal expected utility\n\n\n\n\nFor each decision, multiply the probability and the utility of each of its outcomes, and then sum up these products. This way you obtain the expected utility of the decision.\nChoose the decision that has the largest expected utility; if several decisions are maximal, choose any of them unsystematically.\n\n\n\nThis procedure can also be described in terms of the probability and utility tables introduced in the previous section:\n\nMultiply element-by-element the probability table and the utility table, obtaining a new table with the same number of rows and columns\nSum up the elements of each row of the new table (this sum is the expected utility); remember that every row corresponds to a decision\nChoose the decision corresponding to the largest of the sums above; if there are several maximal ones, choose among them unsystematically\n\n\nExample: the assembly-line problem\nMultiplying the Probability table and the Utility table above, element-by-element, we obtain the following table, where we also indicate the sum of each row:\n\n\nProbability × Utility table\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\nsum\n\n\n\n\naccept\n\\(+0.9\\$\\)\n\\(-1.1\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{-0.2\\$}\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{0\\$}\\)\n\n\n\n\nand, as we already knew, discarding the electronic component is the decision with the maximal expected utility.\n\n\n\n\n\n\n Exercise\n\n\n\nFeel free to sketch some code (in your preferred programming language) that chooses the optimal decision according to the principle above. The code should take two inputs: the table or matrix of probabilities, and the table or matrix of utilities; and should give one output: the row-number of the optimal decision.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#plan-for-the-next-chapters",
    "href": "basic_decisions.html#plan-for-the-next-chapters",
    "title": "3  Basic decision problems",
    "section": "3.4 Plan for the next chapters",
    "text": "3.4 Plan for the next chapters\nThe expected-utility maximization above is intuitive and simple, and is the last stage in a basic decision problem.\nBut there are two stages which occur before, and which are the most difficult:\n\n Inference\n\nis the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\n\n\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n\n Utility assesment\n\nis the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n\n\n\n\nWe shall now explore each of these two stages. We take up inference first because it is the most demanding and probably the one that can be optimized the most by new technologies.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html",
    "href": "connection-1-ML.html",
    "title": "4  Connection with machine learning and AI",
    "section": "",
    "text": "4.1 Inferences with machine-learning algorithms\nSome works in machine learning focus on “guessing the correct answer”, and this focus is reflected in the way their machine-learning algorithms – especially classifiers – are trained and used.\nIn § 2.3 we emphasized that “guessing successfully” can be a misleading goal, however, because it can lead us away from guessing optimally. We shall now see two simple but concrete examples of this.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html#inferences-with-machine-learning-algorithms",
    "href": "connection-1-ML.html#inferences-with-machine-learning-algorithms",
    "title": "4  Connection with machine learning and AI",
    "section": "",
    "text": "A “max-success” classifier vs an optimal classifier\n\n\n\n\n\n\n \n\n\n\nYou find the code for this chapter and exercises also in this JupyterLab notebook for R and (courtesy of Viktor Karl Gravdal!) this JupyterLab notebook for python.\n\n\nWe shall compare the results obtained in some numerical simulations by using\n\na Machine-Learning Classifier trained to do most successful guesses\na prototype “Optimal Predictor Machine” trained to make the optimal decision\n\nFor the moment we treat both as “black boxes”, that is, we don’t study yet how they’re calculating their outputs (although you may already have a good guess at how the Optimal Predictor Machine works).\nTheir operation is implemented in this R script that we now load:\n\nsource('code/mlc_vs_opm.R')\n\nThis script simply defines the function hitsvsgain():\nhitsvsgain(\n    ntrials,\n    chooseAtrueA,\n    chooseAtrueB,\n    chooseBtrueB,\n    chooseBtrueA,\n    probsA\n)\nhaving six arguments:\n\nntrials: how many simulations of guesses to make\nchooseAtrueA: utility gained by guessing A when the successful guess is indeed A\nchooseAtrueB: utility gained by guessing A when the successful guess is B instead\nchooseBtrueB: utility gained by guessing B when the successful guess is indeed B\nchooseBtrueA: utility gained by guessing B when the successful guess is A instead\nprobsA: a tuple of probabilities (between 0 and 1) to be used in the simulations (recycling it if necessary), for the successful guess being A; the corresponding probabilities for B are therefore 1-probsA. If this argument is omitted it defaults to 0.5 (not very interesting)\n\n\n\nExample 1: electronic component\nLet’s apply our two classifiers to the Accept or discard? problem of § 1. We call A the alternative in which the element won’t fail before one year, and should therefore be accepted if this alternative were known at the time of the decision. We call B the alternative in which the element will fail within a year, and should therefore be discarded if this alternative were known at the time of the decision. Remember that the crucial point here is that the classifiers don’t have this information at the moment of making the decision.\nWe simulate this decision for 100 000 components (“trials”), assuming that the probabilities of failure can be 0.05, 0.20, 0.80, 0.95. The values of the arguments should be clear:\n\nhitsvsgain(\n    ntrials = 100000,\n    chooseAtrueA = +1,\n    chooseAtrueB = -11,\n    chooseBtrueB = 0,\n    chooseBtrueA = 0,\n    probsA = c(0.05, 0.20, 0.80, 0.95)\n)\n\n\nTrials: 100000\nMachine-Learning Classifier: successes 87423 ( 87.4 %) | total gain -26897\nOptimal Predictor Machine:   successes 72351 ( 72.4 %) | total gain 9731\n\n\nNote how the machine-learning classifier is the one that makes most successful guesses (around 88%), and yet it leads to a net loss! If the utility were in kroner, this classifier would cause the company producing the components a net loss of more than 20 000 kr.\nThe optimal predictor machine, on the other hand, makes fewer successful guesses overall (around 72%), and yet it leads to a net gain! It would earn the company a net gain of around 10 000 kr.\n\n\n\n\n\n\n Exercise\n\n\n\nHow is this possible? Try to understand what’s happening; feel free to research this by modifying the hitsvsgain() function, so that it prints additional outputs.\n\n\n\n\nExample 2: find Aladdin! (image recognition)\nA typical use of machine-learning classifiers is for image recognition: for instance, the classifier guesses whether a particular subject is present in the image or not.\nIntuitively one may think that “guessing successfully” should be the best goal here. But exceptions to this may be more common than one thinks. Consider the following scenario:\n\nBianca has a computer folder with 10 000 photos. Some of these include her beloved cat Aladdin, who sadly passed away recently. She would like to select all photos that include Aladdin and save them in a separate “Aladdin” folder. Doing this by hand would take too long, if at all possible; so Bianca wants to employ a machine-learning classifier.\nFor Bianca it’s important that no photo with Aladdin goes missing, so she would be very sad if any photo with him weren’t correctly recognized; on the other hand she doesn’t mind if some photos without him end up in the “Aladdin” folder – she can delete them herself afterwards.\n\nLet’s apply and compare our two classifiers to this image-recognition problem, using again the hitsvsgain() function. We call A the case where Aladdin is present in a photo, and B where he isn’t. To reflect Bianca’s preferences, let’s use these “emotional utilities”:\n\nchooseAisA = +2: Aladdin is correctly recognized\nchooseBisA = -2: Aladdin is not recognized and photo goes missing\nchooseBisB = +1: absence of Aladding is correctly recognized\nchooseAisB = -1: photo without Aladding end up in “Aladding” folder\n\nand let’s say that the photos may have probabilities 0.3, 0.4, 0.6, 0.7 of including Aladding:\n\nhitsvsgain(\n    ntrials = 10000,\n    chooseAtrueA = +2,\n    chooseAtrueB = -1,\n    chooseBtrueB = 1,\n    chooseBtrueA = -2,\n    probsA = c(0.3, 0.4, 0.6, 0.7)\n)\n\n\nTrials: 10000\nMachine-Learning Classifier: successes 6557 ( 65.6 %) | total gain 4724\nOptimal Predictor Machine:   successes 6013 ( 60.1 %) | total gain 5576\n\n\nAgain we see that the machine-learning classifier makes more successful guesses than the optimal predictor machine, but the latter yields a higher “emotional utility”.\nYou may sensibly object that this result could depend on the peculiar utilities or probabilities chosen for this example. The next exercise helps answering your objection.\n\n\n\n\n\n\n Exercise\n\n\n\n\nIs there any case in which the optimal predictor machine yields a strictly lower utility than the machine-learning classifier?\n\nTry using different utilities, for instance using ±5 instead of ±2, or whatever other values you please.\nTry using different probabilities as well.\n\nAs in the previous exercise, try to understand what’s happening. Consider this question: how many photos including Aladdin did each classifier miss?\nModify the hitsvsgain() function to output this result.\nDo the comparison using the following utilities: chooseAtrueA = +1, chooseAtrueB = -1, chooseBtrueB = 1, chooseBtrueA = -1. What’s the result? what does this tell you about the relationship between the machine-learning classifier and the optimal predictor machine?",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html#what-is-artificial-intelligence",
    "href": "connection-1-ML.html#what-is-artificial-intelligence",
    "title": "4  Connection with machine learning and AI",
    "section": "4.2 What is “Artificial Intelligence”?",
    "text": "4.2 What is “Artificial Intelligence”?\n\n“AI” as opposed to what?\nThe field of Artificial Intelligence is vast, and its boundaries are not clear-cut. Different books give slightly different definitions of AI. In everyday parlance the term “AI” is moreover used in ways that are not technically correct – a bit like it happens with physics terms such as “energy” or “force”. In this course we want to use AI in a technically more correct way.\nThe discussion of the possible definitions of AI could take several chapters. Let’s try a shorter approach, by examining why the two words “artificial” and “intelligence” are used specifically.\n\nArtificial as opposed to what? As opposed to natural for example. So it denotes something human-made, as opposed to something directly found in nature; say in an orangutan or in a dolphin.\n\nIntelligence as opposed to what? As opposed to stupidity. The definition of “intelligence” itself, even natural intelligence, is still quite open. Generally we mean something that is logical or rational. Thus an agent that breaks some logical procedure, or that does not follow a procedure that it claims to follow, is not “intelligent”.\n\nOf course neither term is fully dichotomous: we can distinguish different degrees of artificiality and of intelligence.\n\n\n“Intelligence” is not “human-likeness”\nWe can distinguish two distinct endeavours in the field of Artificial Intelligence, considered in its most general extension:\n\nachieving human-like behaviour;\nachieving intelligent reasoning, or we could say logical or rational reasoning.\n\nIt’s important to recognize immediately that these two endeavours may not be mutually compatible. We often associate human behaviour with error-making and irrationality. We may say that a person is very irrational, yet we don’t say that because of this the person is inhuman.\nGiven the incompatible character of the two endeavours above, we must be very clear and conscious about which goal we’re trying to achieve; otherwise we won’t achieve any goal at all. And in technical discussions we must be careful to adopt the correct terminology. In particular we should avoid the term “intelligent” when we instead mean “human-like”, and vice versa.\nAn example of such confusion is with present-day large language models (LLMs), and in particular those with a Generative Pre-training Transformer (GPT) architecture. In many media they are referred to as “AI systems”; yet what they achieve is not intelligence, but rather human-like language processing – including non-intelligent processing.\nIf you have access to a large language model, you have surely witnessed examples of stupid output1. You can try a variation of the following experiment:\n1 often euphemistically called “hallucination” because this term may increase sales, whereas “stupid” would risk decreasing sales.\nAsk the LLM to write down a short list of some set, for instance of all Norwegian counties.\nAsk the LLM to select from the list only those item that have one or more letter “r” in their name. See the result.\nAsk the LLM to give you a step-by-step procedure to achieve the selection required in the previous step.\n\nTypically a LLM fails at task 2., even if it can give a completely sound procedure in task 3. Clearly it isn’t internally following the logical procedure.\n\nThis is the reason why in this course we do not categorize LLMs as “artificial intelligence”, but rather as human-mimicking machines. But we shall consider possible ways in which a true intelligence framework could be built into these machines.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\nChapters 1–2 of Artificial Intelligence.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "5  What is an inference?",
    "section": "",
    "text": "5.1 The wide scope and characteristics of inferences\nIn the assembly-line decision problem of § 1, the probability of early failure was very important in determining the optimal decision. If the probability had been \\(5\\%\\) instead of \\(10\\%\\), the optimal decision would have been different. Also, if the probability had been \\(100\\%\\) or \\(0\\%\\), it would have meant that we knew for sure what was the successful decision.\nIn that decision problem, the probabilities of the outcomes were already given. But in real decision problems the probabilities of the outcomes almost always need to be calculated, and their calculation can be the most time- and resource-demanding stage in solving a decision problem.\nWe’ll loosely refer to problems of calculating probabilities as “inference problems”, and to their calculation as “drawing an inference”. Drawing inferences is very often a goal or need in itself, without any underlying decision process.\nOur purpose now is to learn how to draw inferences – that is, how to calculate probabilities. We’ll proceed by facing the following questions, in order:\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "5  What is an inference?",
    "section": "",
    "text": "Looking at the weather, we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects the patient, in order to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nThe computer of a self-driving car needs to assess, from the current set of camera frames, whether a particular patch of colours in the frames is a person, in order to slow down the car and stop if that’s the case.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change, if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a value of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test value could have been \\(100\\,\\mathrm{mV}\\) even if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test value could have been \\(80\\,\\mathrm{mV}\\) if the component had not been defective.\n\n\n\nFrom measurements of the Sun’s energy output, measurements of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and measurements of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done with full certainty (that is, to know which decision is successful), based the data given explicitly and implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only”? That is, has their object or the data on which they are based never happened before and will never happen again?\nAre any of the inferences above based on data and information that come chronologically after the object of the inference?\nAre any of the inferences above about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\n\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say for sure whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information. In other words, inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-inference-origin",
    "href": "inference.html#sec-inference-origin",
    "title": "5  What is an inference?",
    "section": "5.2 Where are inferences drawn from?",
    "text": "5.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics that originated in the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n \n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on.\nProbably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that in order to prove some theorem, we must always start from some axioms. There are “inferences”, called tautologies, that can be drawn without requiring others, but they are all trivial: for example “this component failed early, or it didn’t”. These tautologies are of little use in a real problem, although they have a deep theoretical importance. Useful inferences, on the other hand, must always start from some axioms.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\nIn concrete applications, we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by everyone. In this case the final inference has a somewhat contingent character. We accept it (as well as the solution of any underlying decision problem) as the best available one for the moment. This is partly the origin of the term “model”.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-basic-elements-inference",
    "href": "inference.html#sec-basic-elements-inference",
    "title": "5  What is an inference?",
    "section": "5.3 Basic elements of an inference",
    "text": "5.3 Basic elements of an inference\nLet us introduce some mathematical notation and more precise terminology for inferences.\n\nEvery inference has an “object”: what is to be assessed or guessed. We call proposal the object of the inference.\nEvery inference also has data, information, hypotheses, or hypothetical scenarios on which it is based. We call conditional what the inference is based upon.\nWe separate proposal and conditional with a vertical bar  “ \\(\\pmb{\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}}\\) ”, which can be pronounced “given” or “conditional on”.\nFinally, we put parentheses around this and a “\\(\\mathrm{P}\\)” in front, short for “probability”:\n\n\n\nProposal is Johnson’s (1924) terminology; Keynes (1921) uses “conclusion”; modern textbooks do not seem to use any specialized term. Conditional is modern terminology; other terms used: “evidence”, “premise”, “supposal”. The vertical bar, originally a solidus, was introduced by Keynes (1921).\n\\[\n\\mathrm{P}( \\underbracket[1px]{\\color[RGB]{34,136,51}\\boldsymbol{\\cdots}}_{\\textit{proposal}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\underbracket[1px]{\\color[RGB]{68,119,170}\\boldsymbol{\\cdots}}_{\\textit{conditional}}\n) = {\\color[RGB]{238,102,119}\\boldsymbol{\\cdots}\\%}\n\\]\nthis means “the probability that [proposal], supposing [conditional], is . . . %”. Or also: “supposing [conditional], we can infer [proposal] with . . . % probability”.\nWe have remarked that in order to calculate the probability for an inference, we must use the probabilities of other inferences, which in turn are calculated by using the probabilities of other inferences, and so on, until we arrive at probabilities that are taken for granted. A basic inference process could therefore be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects of an inference. Thereafter we shall study the rules for drawing correct inferences.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html",
    "href": "sentences.html",
    "title": "6  Sentences",
    "section": "",
    "text": "6.1 The central components of knowledge representation\nWe have seen that an inference involves, at the very least, two things: the object of the inference (proposal), and the data, information, or hypotheses on which the inference is based (conditional).\nWe also observed that wildly different “things” can be the object of an inference or the information on which the inference is based: measurement results, decision outcomes, hypotheses, not-real events, assumptions, data and information of all kinds (for example, images). In fact, such variety in some cases can make it difficult to pinpoint what an inference is about or what it is based upon.\nIs there a general, flexible, yet precise way of representing all these kinds of “things”?\nWhen speaking of “data”, what comes to mind to many people is numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of “things” exemplified above? Well, this option turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it.\nInstead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. You can do different things with this piece of information. For instance, if you had decided to send one probe to each official planet, now you know you have to build eight probes. Or maybe you can win at a pub quiz with it.\n“Data” is therefore not just numbers. A number is not “data” unless there’s an additional verbal and non-numeric context accompanying it – even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but is essentially non-numeric data. Sure, in some situations we could represent this data with numbers, say “1” for “recovered” and “0” for “not recovered”. But the opposite or some other convention could also be used: “0” for “recovered” and “1” for “not recovered”, or the numbers “0.3” and “174”. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\nThe examples above, however, actually reveal the answer to our needs! In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “things” that can be the proposal or the conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts – we’ll return to these later on.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "6  Sentences",
    "section": "",
    "text": "1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\n\n\n\n\n\n\n Study reading\n\n\n\nRead §7.1 in Artificial Intelligence.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "6  Sentences",
    "section": "6.2 Identifying and working with sentences",
    "text": "6.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\nA sentence is something for which an agent can determine, at least in principle, whether it is true or false.\nLet’s make this definition clearer with some remarks:\n\n\n A sentence doesn’t have to contain only words. It can contain pictures, sounds, and other non-verbal items. For example, the following:\n“This:  is an animated picture of Saitama.”\nis a sentence, even if it contains animated graphics, because we can say that it is true. Likewise, the following:\n“This link leads to a song by Pink Floyd.”\nis also a sentence, even if it contains links and audio, because we can say that it is false (that’s a song by Monty Python).\n A meaningful phrase may not be a sentence. For instance, a phrase like “Apples are much tastier than pears” may not be a sentence, because it’s a matter of personal taste whether it’s true or false. Moreover, an agent’s opinion about apples and pears might change from time to time.\nThe phrase “Jenny right now finds apples tastier than pears”, on the other hand, could be a sentence; its truth being found by asking Jenny at that very moment.\nIn an engineering context, the phrase “This valve will operate for at least two months” is a sentence, even if its truth is unknown at the moment: one has to wait two months, and then its truth will be unambiguously known.\n\n\n\n An expression involving technical terms may not be a sentence (and not meaningful either). For instance, in a data-science context the phrase “This neural-network algorithm has better performance than that random-forest one” is not a sentence unless we have objectively specified what “better” means (higher accuracy? higher true-positive rate? faster?), for example by adopting a particular comparison metric.\nSome expressions involving technical terms may appear to be sentences at first; but a deeper analysis then reveals that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention. The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n Be particularly careful when reading scientific and engineering papers with a lot of technical terms and phrases. Technical jargon often makes it especially difficult to see whether something true or at least meaningful is being said, or not!\n\n\n\n A sentence can be expressed in different ways by different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\n\n\nThere are many advantages in working with sentences (rather than just numbers), and in keeping in mind that every inference is about sentences:\nFirst, this point of view leads to clarity in engineering problems, and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing a solution, it is important to understand what information or outcomes the end user exactly wants. These “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, this point of view is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe therefore agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can be true or false.\nMany inferences, especially when they concern numerical measurements, involve more than one sentence. For example, an inference about the result of rolling a die actually consists of the probabilities for six separate proposals:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences of this kind. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false sentences. This analysis may reveal contradictions in our inference problem. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of § 5.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "6  Sentences",
    "section": "6.3 Notation and abbreviations",
    "text": "6.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression. Example:\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed. Sometimes we may simply write\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. As mentioned before, in these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\), possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We express such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq\\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means that the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\).\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three sentences:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\]\nThe quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. The difference greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike in the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\\[\n\\mathsfit{S}\n\\]\nSome key word appearing in the sentence:\n\\[\n\\textsf{\\small temperature}\n\\]\nAn equality:\n\\[\nT\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\]\nThe quantity appearing in the sentence:\n\\[\nT\n\\]\nThe value appearing in the sentence:\n\\[\nx\n\\]\n\nGet familiar with these kinds of abbreviations because they are all very common. Some texts may even jump from one abbreviation to another in the same page or paragraph!",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-connecting-sentences",
    "href": "sentences.html#sec-connecting-sentences",
    "title": "6  Sentences",
    "section": "6.4 Connecting sentences",
    "text": "6.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences, out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following composite sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols:\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For example it might become necessary to consider atomic sentences with other values for the reported voltage, such as\n\\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\]\nand so on.\n\n\nConnectives\nHow do we construct composite sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives. You may have already encountered them in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n \n\n\n\n\n\nNot (symbol  \\(\\lnot\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\[1ex]\n\\lnot \\mathsfit{s} &= \\textsf{\\small`The component is broken after the shock test'}\n\\end{aligned}\\]\n\nAnd (symbols  \\(\\land\\)  also  \\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\) )\n\nexample:\n\n\n\\[\n\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\[1ex]\n\\mathsfit{s} \\land \\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\\\\n\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\end{aligned}\n\\]\n\nOr (symbol  \\(\\lor\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\\\[1ex]\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} &= \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\end{aligned}\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly more complex composite sentences.\nThe and connective appears very frequently in probability formulae. Using its standard symbol “\\(\\land\\)” would consume a lot of horizontal space. For this reason a comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is often used as an alternative symbol. So the expressions \\(\\mathsfit{s} \\land \\mathsfit{h}\\) and \\(\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h}\\) are completely equivalent.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. The and connective may for instance correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take\n\\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq\\textsf{\\small`The component is broken'}\n  \\]\nBut in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that the sentences it connects can be both true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this:\n\\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full composite sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through §7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "6  Sentences",
    "section": "6.5 “If… then…”",
    "text": "6.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it is often the starting point of our inferences. We shall discuss this point in more detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#actual-implementation",
    "href": "sentences.html#actual-implementation",
    "title": "6  Sentences",
    "section": "6.6 Actual implementation",
    "text": "6.6 Actual implementation\nAs we said at the beginning, sentences are the central components of knowledge representation in AI agents. But how are sentences and their relationships actually implemented in a concrete AI agent?\nThe notation and symbols we discussed above are tools that we, humans, use to study and discuss about sentences. These symbols and tools are independent of technology; there also lies their usefulness. But we cannot expect a concrete AI agent to work with “letters” or similar symbols internally.\nKnowledge representation is a whole AI field in itself, and unfortunately we don’t have time to delve into its present-day state and concrete implementations. Also because, remember, we’re trying to adopt a view that’s technology-independent, a view that allows us to see potential in new technologies.\nBut it’s good to get a glimpse of present-day implementations knowledge representations. Here are examples from NASA:\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through:\n\nOno & al. 2015: SMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in Ingham 2012: No More Band-Aids: Integrating FM into the Onboard Execution Architecture\n\n\npart IV in Williams & al. 2003: Model-based programming of intelligent embedded systems and robotic space explorers\n\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n\n\n@@ TODO: add connections to large language models (Gödel & Co.).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html",
    "href": "truth_inference.html",
    "title": "7  Truth inference",
    "section": "",
    "text": "7.1 A trivial inference\nSome inferences can be drawn with absolute certainty, that is, we can ascertain for sure the truth or falsity of their proposal. We call this particular “sure” kind of inferences truth inferences. Mathematical inferences are a typical example of this kind. You probably have some acquaintance with rules for drawing truth inferences, so we start from these.\nConsider again the assembly-line scenario of § 1, and suppose that an inspector has the following information about an electric component:\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "7  Truth inference",
    "section": "",
    "text": "This electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the shock test or the heating test. This component passed the shock test.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference-analysis",
    "href": "truth_inference.html#sec-trivial-inference-analysis",
    "title": "7  Truth inference",
    "section": "7.2 Analysis and representation of the problem",
    "text": "7.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with compact symbols.\n\n(1) Atomic sentences\nWe can introduce the following atomic sentences and symbols:\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\n(2) Proposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]\n\n\n\n(3) Starting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the shock test or the heating test. We write this as\n\\[\n\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-truth-inference-rules",
    "href": "truth_inference.html#sec-truth-inference-rules",
    "title": "7  Truth inference",
    "section": "7.3 Truth-inference rules",
    "text": "7.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when sure inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). These systems are all equivalent, of course. The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n \n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{7.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.3}\\]\n\nRule for truth:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{7.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\nprovided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nAgain wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\]\nstarting from the given ones\n\\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n \n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (7.1)–(7.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (7.1)–(7.4) are more complicated than truth-tables, but have two important advantages. First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "7  Truth inference",
    "section": "7.4 Logical AI agents and their limitations",
    "text": "7.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in § 6.1, are programmed according to these rules.\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free to take a look at\n\nHuth & Ryan: Logic in Computer Science\nMordechai: Mathematical Logic for Computer Science\nPelletier & Hazen 2021/2023: Natural Deduction Systems in Logic",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html",
    "href": "probability_inference.html",
    "title": "8  Probability inference",
    "section": "",
    "text": "8.1 When truth isn’t known: probability\nIn most engineering and data-science problems we don’t know the truth or falsity of outcomes and hypotheses that interest us. But this doesn’t mean that nothing can be said or done in such situations. Now we shall finally see how to draw uncertain inferences, that is, how to calculate the probability of something that interests us, given particular data, information, and assumptions.\nSo far we have used the term “probability” somewhat informally and intuitively. It is time to make it more precise and to emphasize some of its most important aspects. Then we’ll dive into the rules of probability-inference.\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent to “probability” are degree of belief, plausibility, credibility1, certainty.\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nAlternatively, if an agent assigns to a sentence a probability 1, it means that the agent is completely certain that the sentence is true. If the agent assigns a probability 0, it means that the agent is completely certain that the sentence is false. If the agent assigns a probability 0.5, it means that the agent is equally uncertain about the truth as about the falsity of the sentence.\nLet’s emphasize and agree on some important facts about probabilities:\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance2 – or even cause deaths.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "8  Probability inference",
    "section": "",
    "text": "1 credibility literally means “believability” (from Latin credo = to believe).\n\n\n\n Probabilities are assigned to sentences. We already discussed this point in § 6.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different. If the engineer is the person who decides how to set that output, and has decided to set it to 100 MW, then the probability is obviously \\(100\\%\\) (or very close to), because the engineer already knows what the output will be. The probability depends not only on a number, but on what it’s being done with that number: measuring, setting, third-party reporting, and so on. Often we write simply “\\(O \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{W}\\)”, provided that the full sentence behind this shorthand is understood.\n Probabilities are agent- and knowledge-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all. If he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider it unreasonable (he didn’t see the outcome, so why exclude \\(\\textsf{\\small`heads'}\\)?). At the same time we would be baffled if Alice assigned only \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she actually saw that the outcome was heads; maybe we would wonder whether she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Diaconis & al. 2007: Dynamical Bias in the Coin Toss. \n\n\n\n\n2 This fact can be mathematically proven.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#the-many-uses-of-the-word-probability",
    "href": "probability_inference.html#the-many-uses-of-the-word-probability",
    "title": "8  Probability inference",
    "section": "8.2  The many uses of the word “probability”",
    "text": "8.2  The many uses of the word “probability”\nIn these notes we shall consistently use the term “probability” in the sense explained above. But beware that this term is used in many different and incompatible senses, depending on whom you’re speaking with or which literature you’re reading.\nSome people use this term in the sense of “frequency”: the number of times something happened in a series of repetitions. But a frequency is an objective, measurable quantity; it doesn’t depend on the knowledge of an agent. So it is not useful in building an AI that can reason about uncertainty, because it doesn’t quantify the belief or certainty of an agent. Suppose a coin is tossed 100 times, and it comes up heads 80 times. The frequency of heads is 80/100. Now suppose that an agent who has no knowledge about the 100 tosses and their outcomes is asked to predict whether a toss of the same coin will be heads or tails. What should the agent’s degree of belief or of certainty be? Obviously it should be 50%/50%. If we were to program the agent so that it has a degree of belief of 80% for heads in situations where nothing is know about previous tosses (because that’s the situation our agent was in), then such an agent would on average make poor inferences and decisions in dealing with new coins.\nBut frequency is data, and if a frequency is known, then obviously an agent should take it into account in quantifying a credibility. If an agent knows that the coin came up heads 80 times in 100, then it is reasonable that the agent’s degree of belief for the next toss should be around 80% for heads. And we shall see that this is indeed what happens.\nSo the distinction between “frequency” and “probability” is crucial. Frequencies do not enable us to quantify an agent’s beliefs in situations where data are missing. But an agent’s beliefs will automatically come close to frequencies – when doing so is reasonable.\n\n\nSome people use “probability” in the sense of the number of a-priori successes over the number of possibilities. For instance, if you roll a die, and the die comes up , then this result was 1 among six possible results. This is fine, but this definition does not allow us to capture the difference between an agent who has seen that the outcome of the roll was , and an agent who has not seen that outcome. We expect the two agents to behave differently, because they have different knowledge. If we programmed the agent to have a degree of belief of 1/6 even after seeing the outcome of a die roll, we would have programmed an agent incapable of using new data. Would you be happy if a clinician made some medical tests, and then ignored the results of the tests, behaving as if they were still unknown?\n\n\nAll these different uses are just a matter of semantics, and in the end it doesn’t matter which word we use, as long as we understand its meaning and as long as we’re adopting the meaning that is useful for our present task.\n\n\n\n\n\n\n Beware of likelihood as a synonym for probability\n\n\n\nIn everyday language, “likelihood” is synonym with “probability”. In technical writings about probability or statistics, however, “likelihood” means something different and is not a synonym of “probability”, as we explain below (§ 8.8.1).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "8  Probability inference",
    "section": "8.3 An unsure inference. Probability notation",
    "text": "8.3 An unsure inference. Probability notation\nConsider now the following variation of the trivial inference problem of § 7.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production either it didn’t pass the heating test or it didn’t pass the shock test. The probability that it passed neither test (that is, both tests failed) is 10%. There’s no reason to believe that the component passed the heating test, more than to believe that it passed the shock test.\n\nAgain the inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot be drawn with certainty in this case. One way to argue this is by presenting two different scenarios that fit the given data but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?\n\n\n\nFor this inference problem we cannot find a true or false final value. The truth-inference rules (7.1)–(7.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents a belief in the truth of the sentence which is as strong as the belief in its falsity.\nSecond, let’s symbolically write in the following way that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\):\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "8  Probability inference",
    "section": "8.4 Inference rules",
    "text": "8.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (7.1)–(7.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than only for \\(0\\) and \\(1\\).\nThis important result was taken more or less for granted at least since Laplace in the 1700s, but was formally proven for the first time in 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n  THE FUNDAMENTAL LAWS OF INFERENCE  \n\n\n\n\n\n\\(\\boldsymbol{\\lnot}\\) “Not” rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 - \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\]\n\n\n\\(\\boldsymbol{\\land}\\) “And” rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n\\(\\boldsymbol{\\lor}\\) “Or” rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nTruth rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\n\nHow to use the rules:\nEach equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – maybe billions of times or more. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is just a study of how to apply these rules to particular kinds of problems.\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\nCh. 2 of Gregory: Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of O’Hagan: Probability\n§§1.0–1.2 of Sivia: Data Analysis\n\nSkim through:\n\nCox 1946: Probability, Frequency and Reasonable Expectation. Try to get the ideas behind the reasoning, even if you can’t follow the mathematical details.\nChs 1–2 of Jaynes: Probability Theory\n\n\n\n\nThe fundamental inference rules are used in the same way as their truth-inference counterpart of [§@truth-inference-rules]: Each equality can be rewritten in different ways according to the usual rules of algebra. The left and right side of the equality thus obtained can replace each other in a proof.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-solution-first-probinference",
    "href": "probability_inference.html#sec-solution-first-probinference",
    "title": "8  Probability inference",
    "section": "8.5 Solution of the uncertain-inference example",
    "text": "8.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual, we first analyse it and represent it in terms of atomic sentences; we find what are its proposal and conditional; and we find which initial inferences are given in the problem.\n\n(1) Atomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\nThe background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\n(2) Proposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\n(3) Starting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass the heating test or the shock test (or neither). This is given as a sure fact. Let’s write it as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\tag{8.1}\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\tag{8.2}\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\tag{8.3}\\]\nNote the interesting situation above: we are not given the numerical values of these two probabilities; we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinding the target inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all paths will lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small starting inferences (8.1–2)}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small starting inference (8.3)}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it:\n\\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then:\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-algorithm-rules",
    "href": "probability_inference.html#sec-algorithm-rules",
    "title": "8  Probability inference",
    "section": "8.6 Use and implementation of the inference rules",
    "text": "8.6 Use and implementation of the inference rules\nIn the step-wise solution above you noticed that the equations of the fundamental rules were not only used to calculate the probability on their left side, given those on their right side. For example, in the very first step, when we went from the probability\n\\[\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\]\nto the sum\n\\[\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n+ \\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) \\,,\n\\]\nwe used the or-rule rewritten, by simple algebra, as follows:\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}{\\color[RGB]{238,102,119}\\lor} \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\,.\n\\]\nThe four rules in fact represent, first of all, constraints of logical consistency (the precise technical term is coherence) among probabilities. For instance, if we have probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z}) &= 0.1\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) &= 0.7\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) &= 0.2 \\,,\n\\end{aligned}\\]\nthen there must be an inconsistency the agent’s degrees of belief, because these values violate the and-rule:\n\\[0.2 \\ne 0.1 \\cdot 0.7 \\,.\\]\nWhen this happens, the inconsistency must be found and solved. (However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.)\nThe fundamental rules also imply more general constraints. For example we must always have\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\n\nTry to prove the two constraints above from the four fundamental rules.\nTranslate the two constraints above verbally. Do they make intuitive sense? (For example, the first constraint says, very roughly speaking, that our belief in two things can’t be stronger than our belief in either one of the two things alone.)\n\n\n\n\n\nIn following the step-wise solution you may have been asking yourself after some steps: “OK, why this peculiar step now, and not some other step?”. This is a very intelligent question. You essentially noticed the lack of an algorithm. Compare this situation with the solution of a basic decision problem: there we had a clear sequence of steps to do: writing down some tables, do some element-wise multiplications and then some sums, and finally find the largest of a list of numbers.\nIs there an algorithmic way of drawing inferences, that is, of calculating some target probabilities given some initial ones? If our inference problem involves a finite number of sentences, then the answer is yes, but with some caveats.\nGiven a set of initial probabilities, and a target probability whose value we want to find, there is an algorithm that yields the minimum and the maximum possible values of the target probability. Specifically it can yield the following results (note that some are particular instances of others):\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe complete proof that inference problems can be solved this way seems to have been given first by Hailperin:\n\n(1965): Best Possible Inequalities for the Probability of a Logical Function of Events,\n(1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications,\n\nalthough particular cases were explored already by Boole.\n\n\n\n   No values\n\nThis means that the initial probabilities have inconsistent values; that is, they violate some of the four rules, as in the “\\(0.2 \\ne 0.1 \\cdot 0.7\\)” example above.\n\n\n\n   \\(0\\) and \\(1\\)\n\nThat is, the target probability is completely unspecified. This means that the initial probabilities are not sufficient to determine the target probability. For example, if we have \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.2\\), and \\(\\mathsfit{Y}\\) is a sentence completely unrelated to \\(\\mathsfit{X}\\) (no atomic sentences in common), then \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\) could have any value.\n\n\n\n   Values \\(v\\), \\(V\\) with \\(v &lt; V\\) and at least one different from 0 or 1\n\nThat is, the target probability cannot have any value whatsoever, but a its value is still unspecified. This means that the initial probabilities constrain the target probability somewhat, but do not determine it. The two constraints discussed above are an example of this: if we have \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.2\\) and \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.3\\), the we can say that \\(0 \\le \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\le 0.2\\), but its precise value is not determined.\n\n\n\n   One value \\(p\\)\n\nThe target probability is completely determined by the initial ones. This occurred in our example inference about the electronic component.\n\n\n\n\nThis algorithm is not mathematically difficult, but it is somewhat involved. It boils down to: (1) writing the sentences in the initial and target probabilities in disjunctive normal form; (2) rewriting the initial and target probabilities as sum of probabilities of basic conjunctions; (3) solving two linear-fractional optimization problems, equivalent to linear optimization oens (for which there are algorithms available).\nYou can find an implementation of the algorithm above in the R function findP(), which requires the lpSolve R package. This function takes as first argument the probability we want to find, and as remaining arguments the values of the given probabilities, or equalities among probabilities. We must use the following notation:\n\n\\(\\lnot\\)  becomes  !\n\\(\\land\\)  becomes  & or equivalently &&\n\\(\\lor\\)  becomes  | (sadly) or equivalently ||\n\\(=\\)  becomes  ==\nconditional bar \\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)  becomes  ~\n\nThe output are the minimum and maximum possible values of the target probability, or NA if the starting probabilities are inconsistent.\nWe can apply this function in the example above about the electronic component:\n\nlibrary('lpSolve')\nsource('code/findP.R') # load the function\n\nfindP(\n    ## target probability:\n    P(!h ~ f & J),\n    ## initial probabilities:\n    P(!h || !s ~ f & J) == 1,\n    P(!h & !s ~ f & J) == 0.1,\n    P(h ~ f & J) == P(s ~ f & J)\n)\n\n min  max \n0.55 0.55 \n\n\nObviously the algorithm becomes more expensive, the larger the number of sentences in the inference problem. In inference problems involving continuous quantities, such as energy, such an algorithm cannot be applied in practice. Also, the way the algorithm above works cannot be represented as a sequence of “logical” steps with consecutive applications of the rules, as shown in the step-wise solution above. Later on in this course we shall focus on specific kinds of inferences for which other, less opaque inference algorithms are available.\n\n\n\n\n\n\n Exercises\n\n\n\n\nPlay with the function findP(): test it in other inference problems, find problems where there is no solution and others where the min and max values are different.\nConsider the following inference:\n\n\nfindP(\n    P(hypothesis ~ evidence1 & evidence2 & I), # target\n    P(hypothesis ~ evidence1 & I) == 0.1,\n    P(hypothesis ~ evidence2 & I) == 0.9\n)\n\nmin max \n  0   1 \n\n\nWhat does this result tells us about “combining evidence” to prove a hypothesis?",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "8  Probability inference",
    "section": "8.7 Consequences of not following the rules",
    "text": "8.7 Consequences of not following the rules\nThe fundamental rules of inference guarantee that the agent’s uncertain reasoning is self-consistent, and that it follows logic when there’s no uncertainty. Breaking the rules means that the resulting inference has some logical or irrational inconsistencies.\nThere are many examples of inconsistencies that appear when the rules are broken. Imagine for instance an agent that gives an 80% probability that it rains3 in the next hour; and it also gives a 90% probability that it rains and that the average wind is above 3⋅m/s in the next hour. This is clearly unreasonable, because the raining scenario alone would be true with wind above 3 m/s and also below 3⋅m/s – therefore it should be more probable than the scenario where the wind is above 3 m/s. And indeed the two given probabilities break the and-rule, showing that they are unreasonable or illogical.\n3 to be precise, let’s say “it rains above 1 mm”\n\n\n\n\n\n Exercise\n\n\n\nProve that the two probabilities in the example above break the and-rule. (Hint: you must use the fact that probabilities are numbers between 0 and 1, and that multiplying a number by something between 0 and 1 can only yield a smaller number.)\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nRead §12.2.3 in Artificial Intelligence\nAs you continue your studies, skim through chapters 4–8 of Hastie & Dawes: Rational Choice in an Uncertain World, just to get the main messages and an overview of curious psychological phenomena.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "8  Probability inference",
    "section": "8.8 Remarks on terminology and notation",
    "text": "8.8 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”, and “likelihood” of “probability”. But in technical writings about probability, inference, and decision-making, “likelihood” has a very different meaning. Beware of this important difference in definitions:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nWe can also say:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\), is \\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\).\nthe likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\), is \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of probability.\n\n\nOmitting background information\nIn the analyses of the inference examples of § 7.1 and § 8.3 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that specific background information.\nIn many concrete inference problems the background information usually stays in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening in books where you see “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as\n\\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\]\nand then proceed as usual. If the background information was not explicitly written, this may lead to confusion and mistakes: there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the degrees of belief of different agents.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper in the quantum-theory literature: Brukner & Zeilinger 2000: Conceptual inadequacy of the Shannon information in quantum measurements arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in the scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, and it has several shortcomings:\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\n\nAs discussed in § 8.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nWe want a general AI agent to be able to deal with uncertainty and probability also in situations that do not involve mathematical sets.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown (simple example: we are uncertain about the mass of a particular block of concrete, so we speak of the probability of some mass value; this doesn’t mean that the mass of the block of concrete is changing).\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word. Strangely enough, texts that use that word never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\nFinally, some texts speak of the probability of an “event”. For all purposes, an “event” is just what’s expressed in a sentence.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html",
    "href": "derived_rules.html",
    "title": "9  Shortcut rules",
    "section": "",
    "text": "9.1 Falsity and truth cannot be altered by additional knowledge\nThe fundamental rules introduced in chapter  8 are all we need, and all an AI needs, in order to draw inferences from other inferences and from initial data.\nFrom them, however, it is possible to derive some “shortcut” rules than can make the inferences shorter and faster. The situation is similar to what happens with some rules in algebra: for instance, we know that whenever we find the expression\n\\[\n(a+b) \\cdot (a-b)\n\\]\nthen we can automatically substitute it with\n\\[\na^2 - b^2\n\\]\nno matter the values of \\(a\\) and \\(b\\). The rule “\\((a+b) \\cdot (a-b) = a^2-b^2\\)” is not a new algebraic rule: it’s simply the result of the application of the rules for addition \\(+\\) and multiplication \\(\\cdot\\), and indeed we could just apply them directly:\n\\[\n\\begin{aligned}\n(a+b) \\cdot (a-b)\n&=a\\cdot a + b\\cdot a - a\\cdot b - b\\cdot b\n\\\\\n&=a^2 + b\\cdot a - b\\cdot a - b^2\n\\\\\n&=a^2 - b^2\n\\end{aligned}\n\\]\nBut if we remember that they always lead to the result \\(a^2-b^2\\), then we can directly use the “shortcut” rule \\((a+b) \\cdot (a-b) = a^2-b^2\\) and save ourselves some time.\nLikewise with the four rules of inference. Some particular sequences of application of the rules occur very often. We can then simply memorize the starting and final steps of these sequences, and use them directly, skipping all the steps in between. These shortcut rules are not only useful for saving time, however. We shall see that they reveal interesting and intuitive inference patterns, which are implicit in the four inference rules.\nIt is possible and legitimate to implement these shortcut rules in an AI agent, besides the four fundamental ones. Such an agent will arrive at the same results and decisions of an identical AI agent that doesn’t use the shortcut rules – but a little faster.\nHere are the shortcut rules we’ll frequently use in the rest of the course:\nSuppose that sentence \\(\\mathsfit{X}\\) is judged to be completely impossible, conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\n\\]\nIt can then be proved, from the fundamental rules, that \\(\\mathsfit{X}\\) is also completely impossible if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 0\n\\]\nWhat if we use \\(\\lnot\\mathsfit{X}\\) for \\(\\mathsfit{Y}\\), that is, what if we acquire knowledge that \\(\\mathsfit{X}\\) is actually true? Then it can be proved that all probability calculations break down. The problem is that \\(\\lnot\\mathsfit{X}\\) and \\(\\mathsfit{Z}\\) turn out to be mutually contradictory, so all inferences are starting from contradictory premises. You probably know that in formal logic if we start from contradictory premises then we can obtain any conclusion whatsoever. The same happens with probability logic.\nNote that this problem does not arise, however, if \\(\\mathsfit{X}\\) is only extremely improbable conditional on \\(\\mathsfit{Z}\\), say with a probability of \\(10^{-100}\\), rather than flat-out impossible. In practical applications we often approximate extremely small probabilities by \\(0\\), or extremely large ones by \\(1\\). If the probability calculations break down, we must then step back and correct the approximation.\nBy using the not-rule it is possible to prove that full certainty about a sentence behaves in a similar manner. If sentence \\(\\mathsfit{X}\\) is judged to be completely certain conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 1\n\\]\nthen, from the fundamental rules, \\(\\mathsfit{X}\\) is also completely certain if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 1\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-truth-stable",
    "href": "derived_rules.html#sec-truth-stable",
    "title": "9  Shortcut rules",
    "section": "",
    "text": "Exercise\n\n\n\nTry to prove this. (Hint: try using the and-rule one or more times.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut rules: permanence of truth and falsity\n\n\n\n\\[\n\\begin{aligned}\n&\\text{if}\\quad \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\\text{ or }1\n\\\\\n&\\text{then}\\quad \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\quad\\text{for any $\\mathsfit{Y}$ not contradicting $\\mathsfit{Z}$}\n\\end{aligned}\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-boolean",
    "href": "derived_rules.html#sec-boolean",
    "title": "9  Shortcut rules",
    "section": "9.2 Boolean algebra",
    "text": "9.2 Boolean algebra\nIt is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules of § 8.4. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nShortcut rules: Boolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\nFor example, if we have the probability\n\\[\\mathrm{P}[\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Y}) \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} (\\lnot\\lnot\\mathsfit{Z}) \\land \\mathsfit{I}]\\]\nwe can directly replace it with\n\\[\\mathrm{P}[\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}\\land \\mathsfit{I}]\\]\n\nThe derivation of the Boolean-algebra rules from the four inference rules is somewhat involved. As as example, a partial proof of the rule \\(\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\), called “and-idempotence” goes as follows:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}| \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small truth-rule}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\nand with a similar procedure it can be shown that \\(\\mathsfit{X}\\land \\mathsfit{X}\\) can be replaced with \\(\\mathsfit{X}\\) no matter where it appears. The above proof shows that the and-idempotence rule is tightly connected with the truth-rule of inference.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-extension-conversation",
    "href": "derived_rules.html#sec-extension-conversation",
    "title": "9  Shortcut rules",
    "section": "9.3 Law of total probability or “extension of the conversation”",
    "text": "9.3 Law of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\set{\\mathsfit{H}_1, \\mathsfit{H}_2, \\dotsc, \\mathsfit{H}_n}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given some background knowledge \\(\\mathsfit{Z}\\):\n\n\n\\[\n    \\mathrm{P}(\\mathsfit{H}_1\\land\\mathsfit{H}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n    \\mathrm{P}(\\mathsfit{H}_1\\land\\mathsfit{H}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n    \\mathrm{P}(\\mathsfit{H}_{n-1}\\land\\mathsfit{H}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n    \\]\n\n\nThey are exhaustive, meaning that the “or” of all of them is true, given the background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{H}_1\\lor \\mathsfit{H}_2 \\lor \\dotsb \\lor \\mathsfit{H}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nIn other words, according to our background knowledge, one of those sentences must be true, but only one.\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{H}_1,\\mathsfit{H}_2,\\dotsc\\):\n\n\n\n\n\n\n\nShortcut rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[2ex]\n&\\quad{}=\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\n\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probability of that sentence conditional on several auxiliary “scenarios” or hypotheses1. The name extension of the conversation for this shortcut rule comes from the fact that we are able to call these additional scenarios or hypotheses into play. This situation occurs very often in concrete applications.\n1 this is why we used the symbol \\(\\mathsfit{H}\\) for these sentences",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-bayes-theorem",
    "href": "derived_rules.html#sec-bayes-theorem",
    "title": "9  Shortcut rules",
    "section": "9.4 Bayes’s theorem",
    "text": "9.4 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability of a proposal \\(\\mathsfit{Y}\\) and a conditional \\(\\mathsfit{X}\\) to the probability where their proposal-conditional roles are exchanged:\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\nShortcut rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\nBayes’s theorem is extremely useful when we want to assess the probability of a hypothesis (the proposal) given some data (the conditional), and it is easy to assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) doesn’t always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) doesn’t always need to be “data”.\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead §8.8 of Hastie & Dawes: Rational Choice in an Uncertain World",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-bayes-extension",
    "href": "derived_rules.html#sec-bayes-extension",
    "title": "9  Shortcut rules",
    "section": "9.5 Bayes’s theorem & extension of the conversation",
    "text": "9.5 Bayes’s theorem & extension of the conversation\nBayes’s theorem is often used with several sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nShortcut rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "href": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "title": "9  Shortcut rules",
    "section": "9.6 The many facets of Bayes’s theorem",
    "text": "9.6 The many facets of Bayes’s theorem\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule for “updating an agent’s beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all the agent’s knowledge. The agent’s degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, the agent gets to know – maybe thanks to an observation or measurement – that the sentence \\(\\mathsfit{X}\\) is true. The agent’s whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). The agent’s degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows us to find the agent’s degree of belief about \\(\\mathsfit{Y}\\) conditional on the new state of knowledge, from the one conditional on the old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change (note that this is different from the rule of truth-permanence of § 9.1).\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§§4.1–4.3 in Sox & al.: Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of O’Hagan: Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-idempotent",
    "href": "derived_rules.html#sec-idempotent",
    "title": "9  Shortcut rules",
    "section": "9.7 Importance of seemingly trivial rules",
    "text": "9.7 Importance of seemingly trivial rules\nSome of the fundamental or shortcut rules may seem obvious or unimportant, but are of extreme importance in data science. For instance, the and-idempotence rule    \\(\\mathsfit{X}\\land\\mathsfit{X}= \\mathsfit{X}\\)   effectively asserts that whenever we draw inferences, redundant information or data is automatically counted only once.\nThis amazing feature saves us from a lot of headaches. Imagine that an AI decision agent at the assembly line has been given the following background information: if an electronic component passes the heating test (\\(\\mathsfit{h}\\)), then its probability of early failure (\\(\\mathsfit{f}\\)) is only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 0.1\\]\nNow let’s say that a new voltage test has also been devised, and if a component passes this test (\\(\\mathsfit{v}\\)) then its probability of early failure is also 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{Z}) = 0.1\\]\nHowever, it is discovered that the voltage test works in exactly the same way as the heating test – they’re basically the same test! \\(\\mathsfit{v}=\\mathsfit{h}\\). This means that if an element passes the heating test then it will automatically pass the voltage test, and vice versa (they’re the same test!):2\n2 We are assuming that a test, if repeated, will always give the same result.\\[\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 1\\]\nor equivalently   \\(\\mathsfit{v}\\land \\mathsfit{h}= \\mathsfit{h}\\land \\mathsfit{h}= \\mathsfit{h}\\).\nNow suppose that inadvertently we give our AI agent the redundant information that an electronic component has passed the heating test and the voltage test. What will the agent say about the probability of early failure, given this duplicate information? will it count the test twice? Let’s calculate:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{h}\\land \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n}\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{1}\n=\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small initial probability}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{h}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small truth cannot be altered}\n\\\\[1ex]\n&\\qquad= 0.1\n&&\\text{\\small initial probability}\n\\end{aligned}\n\\]\nCompare the first and next-to-last lines: in the final probability, the sentence \\(\\mathsfit{v}\\) has been dropped. Thus the AI agent, thanks to the truth-rule or equivalently the and-idempotence rule, correctly detected the redundancy of the sentence \\(\\mathsfit{v}\\) (“the element passed the voltage test”) and automatically discarded it.\n This feature is of paramount importance in machine learning and data-driven engineering: the “features” that we give as an input to a machine-learning classifier could contain redundancies that we don’t recognize, owing to the complexity of the data space. But if the classifier makes inferences according to the four fundamental rules, it will automatically discard any redundant features.\n\n\n\n\n\n\n Exercise\n\n\n\n\nConfirm the result above using the findP() function introduced in §.  8.6.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html",
    "href": "monty.html",
    "title": "10  Monty Hall and related inference problems",
    "section": "",
    "text": "10.1 Motivation: calculation vs intuition\nThe “Monty Hall problem”, inspired by the TV show Let’s make a deal! hosted by Monty Hall, was proposed in the Parade magazine in 1990 (the numbers of the doors are changed here):\nThe web is full of insightful intuitive solutions and of informal probability discussions about this inference problem. Our purpose here is different: we want to solve it mechanically, by applying the fundamental rules of inference (§ 8.4) and the shortcut rules (§ 9) derived from them. No intuitive arguments. Our purpose is different because of two main reasons:\nIt is instructive, however, if you also check what your intuition told you about the problem:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-motivation",
    "href": "monty.html#sec-monty-motivation",
    "title": "10  Monty Hall and related inference problems",
    "section": "",
    "text": "Suppose you are on a game show and given a choice of three doors. Behind one is a car; behind the others are goats. You pick door No. 1, and the host, who knows what is behind them and wouldn’t open the door with the car, opens No. 2, which has a goat. He then asks if you want to pick No. 3. Should you switch?\n\n\n\n\n\n\nWe want to be able to implement or encode the procedure algorithmically in an AI agent.\nWe generally cannot ground inferences on intuition. Intuition is shaky ground, and hopeless in data-science problems involving millions of data with millions of numbers in abstract spaces of millions of dimensions. To solve such complex problems we need to use a more mechanical procedure, a procedure mathematically guaranteed to be self-consistent. That’s the probability calculus. Intuition is only useful for arriving at a method which we can eventually prove, by mathematical and logical means, to be correct; or for approximately explaining a method that we already know, again by mathematical and logical means, to be correct.\n\n\n\n\n\n\n\n Misleading intuition in high dimensions\n\n\n\nAs an example of our intuition can be completely astray in problems involving many data dimensions, consider the following fact.\nTake a one-dimensional Gaussian distribution of probability. You probably know that the probability that a data point is within three standard deviations from the peak is approximately \\(99.73\\%\\). If we take a two-dimensional (symmetric) Gaussian distribution, the probability that a data point (two real numbers) is within three standard deviations from the peak is \\(98.89\\%\\), slightly less than the one-dimensional case. For a three-dimensional Gaussian, the analogous probability is \\(97.07\\%\\), slightly smaller yet.\nNow try to answer this question: for a 100-dimensional Gaussian, what is the probability that a data point is within three standard deviations from the peak? The answer is \\(\\boldsymbol{(1.83 \\cdot 10^{-32})\\%}\\). This probability is so small that you would never observe a data point within three standard deviations from the peak, even if you checked one data point every second for the same duration as the present age of the universe – which is “only” around \\(4\\cdot 10^{17}\\) seconds.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nFor further examples of how our intuition leads us astray in high dimensions see\n\nCounterintuitive Properties of High Dimensional Space\nExercise 2.20 (and its solution) in MacKay: Information Theory, Inference, and Learning Algorithms\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nExamine what your intuition tells you the answer should be, without spending too much time thinking, just as if you were on the game show. Examine which kind of heuristics your intuition uses. If you already know the solution to this puzzle, try to remember what your intuition told you the first time you faced it. Keep your observations in mind for later on.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-agent",
    "href": "monty.html#sec-monty-agent",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.2 Which agent? whose knowledge?",
    "text": "10.2 Which agent? whose knowledge?\nA sentence can be assigned different probabilities by different agents having different background information, although in some cases different background information can still lead to numerically equal probabilities.\nIn the present case, who’s the agent solving the inference problem? And what background information does it have?\nFrom the problem statement it sounds like you (on the show) are the agent. But we can imagine that you have programmed an AI agent having your same background information, and ready to make the decision for you.\nWe must agree on which background information \\(\\mathsfit{K}\\) to give to this agent. Let’s define \\(\\mathsfit{K}\\) as the knowledge you have right before picking door 1. We make this choice so that we can add your door pick as additional information.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-sentences",
    "href": "monty.html#sec-monty-sentences",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.3 Define the atomic sentences relevant to the problem",
    "text": "10.3 Define the atomic sentences relevant to the problem\nThe following sentences seem sufficient:\n\\[\n\\begin{aligned}\n\\mathsfit{K}&\\coloneqq\\text{\\small[the background knowledge discussed in the previous section]}\n\\\\[1ex]\n\\mathsfit{\\small car1} &\\coloneqq\\textsf{\\small`The car is behind door 1'}\n\\\\\n\\mathsfit{\\small you1} &\\coloneqq\\textsf{\\small`You initially pick door 1'}\n\\\\\n\\mathsfit{\\small host2} &\\coloneqq\\textsf{\\small`The host opens door 2'}\n\\\\\n&\\text{\\small and similarly for the other door numbers}\n\\end{aligned}\n\\]\nWe could have used other symbols for the sentences, for instance “\\(C_1\\)” instead of “\\(\\mathsfit{\\small car1}\\)”. The specific symbol choice doesn’t matter. We could also have stated the sentences slightly differently, for instance “You choose door 1 at the beginning of the game”. What’s important is that we understand and agree on the meaning of the atomic sentences above.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-goal",
    "href": "monty.html#sec-monty-goal",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.4 Specify the desired inference",
    "text": "10.4 Specify the desired inference\nWe want the probabilities of the sentences \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\), given the knowledge that you picked door 1 (\\(\\mathsfit{\\small you1}\\)), that the host opened door 2 (\\(\\mathsfit{\\small host2}\\)), and the remaining background knowledge (\\(\\mathsfit{K}\\)). So in symbols we want the values of the following probabilities:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{aligned}\n\\]\nYou may object: “but we already know that there’s no car behind door 2, the one opened by the host; so that probability is 0%”. That’s correct, but how did you arrive at that probability value? Remember our goal: to solve this inference mechanically. Your intuitive probability must therefore either appear as an initial probability, or be derived via the inference rules. No intuitive shortcuts.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-prior",
    "href": "monty.html#sec-monty-prior",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.5 Specify all initial probabilities",
    "text": "10.5 Specify all initial probabilities\nAs discussed in § 5.2, any inference – logical or uncertain – can only be derived from other inferences, or taken for granted as a starting point (“initial probability”, or “axiom” in logic). The only inferences that don’t need any initial probabilities are tautologies. We must explicitly write down the initial probabilities implicit in the present inference problem:\n\nThe car is for sure behind one of the three doors, and cannot be behind more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small car1} \\lor \\mathsfit{\\small car2} \\lor \\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\nRemember from the shortcut rule for the permanence of truth and falsity (§ 9.1) that the \\(1\\) and \\(0\\) probabilities above do not change if we and additional information to \\(\\mathsfit{K}\\).\nThe host cannot open the door you picked or the door with the car. This translates in several initial probabilities. Here are some:\n\\[\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n  \\end{gathered}\n  \\]\nThe host must open one door, and cannot open more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host1} \\lor \\mathsfit{\\small host2} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\n\n\nThe probabilities above are all quite clear from the description of the puzzle. But implicit in that description are some more probabilities that will be needed in our inference. The values of these probabilities can be more open to debate, because the problem, as stated, provides ambiguous information. You shall later explore possible alternative values for these probabilities.\n\nIt is equally probable that the car is behind any of the three doors, and your initial pick doesn’t change this uncertainty:\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n  \\end{aligned}\n  \\]\nRemember that a probability is not a physical property. We aren’t saying that the car should appear behind each door with a given frequency, or something similar. The values 1/3 are simply saying that in the present situation you have no reason to believe the car to be behind one specific door more than behind another.\nIf the host can choose between two doors (because the car is behind the door you picked initially), we are equally uncertain about the choice:\n\\[\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\n  \\]\n\nThis probability could be analysed into further hypotheses. Maybe the host, out of laziness, could more probably open the door that’s closer. But from the problem it isn’t fully clear which one is closer. The host could also more probably open the door that’s further from the one you choose. The host could have a predetermined scheme on which door to open. The hypotheses are endless. We can imagine some hypotheses that make \\(\\mathsfit{\\small host2}\\) more probable, and some that make \\(\\mathsfit{\\small host3}\\) more probable, conditional on \\(\\mathsfit{\\small you1} \\land \\mathsfit{\\small car1} \\land \\mathsfit{K}\\). The probability of 50% seems like a good compromise. You shall later examine the effects of changing this probability.\n\nSome peculiar probabilities\nWe defined the background knowledge \\(\\mathsfit{K}\\) as the one you have right before choosing door 1. In this way the sentence \\(\\mathsfit{\\small you1}\\), expressing your door pick, can be added as additional information: \\(\\mathsfit{\\small you1}\\land \\mathsfit{K}\\).\nIt is legitimate to ask: what is the probability that you pick door 1, given only the background information \\(\\mathsfit{K}\\):\n\\[\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\ ?\\]\nTo answer this question we would need to specify \\(\\mathsfit{K}\\) more in detail. It is possible, for instance, that you planned to pick door 1 already the day before. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\\) or very nearly so. Or you could pick door 1 right on the spot, with no clear conscious thought process behind your choice. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1/3\\) or a similar value.\nLuckily in the present problem these probabilities are not needed. If they are used, their numerical values turn out not to matter: they will “cancel out” of the computation.\n\n\n\n\n\n\n Silly literature\n\n\n\nSome texts on probability say that if you have decided something and therefore know for certain it in advance, then the probability of that something is undefined “because it is not random”. Obviously this is nonsense. If you already know something, then the probability of that something is well-defined and its value is 100% – or something short of this value, if you want to make allowance for the occurrence of unplanned events.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-solution",
    "href": "monty.html#sec-monty-solution",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.6 Solution",
    "text": "10.6 Solution\nLet’s try first to calculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\), that is, the probability that the car is behind the door you picked.\nSeeing that we have several initial probabilities of the “\\(\\mathrm{P}(\\mathsfit{\\small host} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)” form, we can use Bayes’s theorem together with the “extension of the conversation” (§ 9.5) to swap the positions of “\\(\\mathsfit{\\small car}\\)” and “\\(\\mathsfit{\\small host}\\)” sentences between proposal and conditional. In the present case the exhaustive and mutually exclusive sentences are \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\):\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll probabilities in green are initial probabilities discussed in the previous steps. Let’s substitute their values:\n\\[\n\\begin{aligned}\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n{\\color[RGB]{34,136,51}0} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}1/3}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll that’s left is to find \\(\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\). It’s intuitively clear that this probability is 100%, because the host is forced to choose door 2 if you picked door 1 and the car is behind door 3. But our purpose is to make a fully mechanical derivation, starting from the initial probabilities only. We can find this probability by applying the or-rule and the and-rule to the probabilities that the host opens at least one door and cannot open more than one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\lor \\mathsfit{\\small host1} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad= 1 - 0 - 0 + 0 + 0 + 0 - 0 = 1\n\\end{aligned}\n\\]\nas expected.\nFinally, using this probability in our previous calculation we find\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n1 \\cdot\n1/3\n}\n= \\frac{1/6}{3/6} = \\boldsymbol{\\frac{1}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 1/3 probability that the car is behind the door we picked!\n\nWhat about door 3, that is, the probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)? Also in this case we can use Bayes’s theorem with the extension of the conversation. The calculation is immediate, because we have already calculated all the relevant pieces:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n}{\n\\enspace\\left[\\,\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\\n&\\qquad=\\frac{\n1 \\cdot\n1/3\n}{\n\\enspace\\left[\\,\\begin{gathered}\n1/2 \\cdot\n1/3 +{}\\\\\n0 \\cdot\n1/3 +{}\\\\\n1\n\\cdot\n1/3\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{1/3}{1/2} = \\boldsymbol{\\frac{2}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 2/3 probability that the car is behind door 3. If we’d like to win the car, then we should switch doors.\n\n\n\n\n\n\n Exercises\n\n\n\n\nPerform a similar calculation to find \\(\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)\nConfirm the result using the findP() function introduced in §.  8.6.\n\n\n\n\nNote that we found these probabilities, and solved the Monty Hall problem, just by applying the fundamental rules of inference (§ 8.4), specifically the and-rule and or-rule, and the Boolean-algebra shortcut rules (§ 9), starting from given probabilities. Here is a depiction of how the fundamental and the shortcut rules connect the initial probabilities, at the top, to the final ones, at the bottom:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-remarks",
    "href": "monty.html#sec-monty-remarks",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.7 Remarks on the use of Bayes’s theorem",
    "text": "10.7 Remarks on the use of Bayes’s theorem\nYou notice that at several points our calculations could have taken a different path. For instance, in order to find \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) we applied Bayes’s theorem to swap the sentences \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\) in their proposal and conditional positions. Couldn’t we have swapped \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\land \\mathsfit{\\small you1}\\) instead? That is, couldn’t we have made a calculation starting with\n\\[\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\\dotso} \\enspace ?\n\\]\nafter all, this is also a legitimate application of Bayes’s theorem.\nThe answer is: yes, we could have, and the final result would have been the same. The self-consistency of the probability calculus guarantees that there are no “wrong steps”, as long as every step is an application of one of the four fundamental rules (or of their shortcuts). The worst that can happen is that we take a longer route – but to exactly the same result. In fact it’s possible that there’s a shorter calculation route to arrive at the probabilities that we found in the previous section. But it doesn’t matter, because it would lead to the same result that we found.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-sensitivity",
    "href": "monty.html#sec-monty-sensitivity",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.8 Sensitivity analysis",
    "text": "10.8 Sensitivity analysis\nIn § 10.5 we briefly discussed possible interpretations or variations of the Monty Hall problem, for which the probability that the host chooses among the available doors 2 and 3 (if the car is behind the door you picked) is different from 50%.\nWhen we want to know how an initial probability value can affect the final probabilities, we can leave its value as a variable, and check how the final probabilities change as we change this variable. This procedure is often called sensitivity analysis. Try to do a sensitivity analysis for the Monty Hall problem:\n\n\n\n\n\n\n Exercise\n\n\n\nInstead of assuming\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\\]\nassign a generic variable value \\(p\\)\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = p\n\\qquad\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1-p\\]\nwhere \\(p\\) could be any value between \\(0\\) and \\(1\\).\n\nCalculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) as was done in the previous sections, but keeping \\(p\\) as a generic variable. This way you’ll find a probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) that depends numerically on \\(p\\); it could be considered as a function of \\(p\\).\nPlot how the value of \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) depends on \\(p\\), as the latter ranges from \\(0\\) to \\(1\\).\nFor which range of values of \\(p\\) is it convenient to switch door, that is, \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &lt; 1/2\\) ?\nImagine and describe alternative scenarios or background information that would lead to values of \\(p\\) different from \\(0.5\\).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-variations",
    "href": "monty.html#sec-monty-variations",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.9 Variations and further exercises",
    "text": "10.9 Variations and further exercises\n\n\n\n\n\n\n Exercise: other variations\n\n\n\n\nIn § 10.2 we decided that the agent in this inference was you, with the knowledge \\(\\mathsfit{K}\\) right before you picked door 1. Try to change the agent: do you arrive at different probabilities?\n\nConsider a person in the audience, right before you picked door 1, as the agent, and re-solve the problem, adjusting all initial probabilities as needed.\nConsider the host as the agent, right before you picked door 1, and re-solve the problem, adjusting all initial probabilities as needed. Note that the host knows for certain where the car is, so you need to provide this additional, secret information. Consider the cases where the car is behind door 1 and behind door 3.\n\n\n\n\n\nSuppose a friend of yours, backstage, gave you partial information about the location of the car (you cheater!), which makes you believe that the car should be closer to door 1. Assign the probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 + q\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 - q\n  \\end{aligned}\n  \\]\nwith \\(0 \\le q \\le 1/3\\) (this background information is different from the previous one, so we denote it \\(\\mathsfit{K}'\\)). Re-solve the problem keeping the variable \\(q\\), and find if there’s any value for \\(q\\) for which it’s best to keep door 1.\n\n\n\n\n\n\n\n\n\n\n\n Exercise: making decisions\n\n\n\nIn this chapter we only solved the inference problem for the Monty Hall scenario. We calculated the probabilities of various outcomes. But no decision has been made yet.\n\nAssign utilities to winning the car or winning the goat from the point of view of an agent who values the car more. The available decisions are, of course, “keep door 1” vs “switch to door 3”. Then solve the decision-making problem according to the procedure of § 3.3. What’s the optimal decision?\nNow assign utilities from the point of view of an agent who values the goat more than the car. Then solve the decision-making problem according to the usual procedure. What’s the optimal decision?\n\n\n\n\n\n\n\n\n\n\n\n Exercise: the Sleeping Beauty problem\n\n\n\nTake a look at the inference problem presented in this video:\n \nand try to solve it, not using intuition, but using the mechanical procedure and steps as in the Monty Hall solution above.\nNote that the video asks “What do you believe is the probability that the coin came up heads?”. Since probability and degree of belief are the same thing, that is like asking “What do you believe is your belief that the coin came up heads?” which is a redundant or quirky question. Instead, simply answer the question “What is your degree of belief (that is, probability) that the coin came up heads?”.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html",
    "href": "connection-2-ML.html",
    "title": "11  Second connection with machine learning",
    "section": "",
    "text": "11.1 “Learning” and “output” from the point of view of inference & decision\nIn these first chapters we have been developing notions and methods about agents that draw inferences and make decisions, sentences expressing facts and information, and probabilities expressing uncertainty and certainty. Let’s draw some first qualitative connections between these notions and notions typically used in machine learning.\nA machine-learning algorithm is usually presented in textbooks as something that first “learns” from some training data, and thereafter performs some kind of task – typically it yields a response or outcome, for example a label, of some kind. More precisely, the training data are instances or examples of the task that the algorithm is expected to perform. These instances have a special status because their details are fully known, whereas new instances, where the algorithm will be applied, have some uncertain aspects. A new instance typically has an ideal or optimal outcome, for example “choosing the correct label”, but this outcome is unknown beforehand. The response given by the algorithm in new instances depends on the algorithm’s internal architecture and parameters (for brevity we shall just use “architecture” to mean both).\nLet’s try to rephrase this description from the point of view of the previous chapters. A machine-learning algorithm is given known pieces of information (the training data), and then forms some kind of connection with a new piece of information of similar kind (the outcome in a new application) that was not known beforehand. The connection depends on the algorithm’s architecture.\nThe remarks above reveal similarities with what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences \\({\\color[RGB]{34,136,51}\\mathsfit{D}_1}, {\\color[RGB]{34,136,51}\\mathsfit{D}_2}, {\\color[RGB]{34,136,51}\\dots}, {\\color[RGB]{34,136,51}\\mathsfit{D}_N}\\), together with some background or built-in information \\(\\color[RGB]{204,187,68}\\mathsfit{I}\\), in order to calculate the probability of a new piece of information of a similar kind, expressed by a sentence \\(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\):\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{D}_{N} \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1 \\color[RGB]{0,0,0}\\land {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\]\nWe can thus consider a first tentative correspondence:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis correspondence seems convincing for architecture and training data: in both cases we’re speaking about the use of pre-existing or built-in information, combined with additional information.\nBut the correspondence is less convincing with regard to the outcome. The “agents” that we have envisioned find the probabilities for several possible “outcomes” or “outputs”; they don’t yield only one output. This indicates that there must also be some decision involved among the possible outcomes.\nWe’ll return to this tentative connection later.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-outputs",
    "href": "connection-2-ML.html#sec-1stconn-outputs",
    "title": "11  Second connection with machine learning",
    "section": "11.2 Why different outputs?",
    "text": "11.2 Why different outputs?\nIn the previous chapters we have seen, over and over, what was claimed at the beginning of these lecture notes: that an inference & decision problem has only one optimal solution. Once we specify the utilities and the initial probabilities of the problem, the fundamental rules of inference and the principle of maximal expected utility lead to one unique answer (unless, of course, there are several optimal ones with equal expected utilities).\nDifferent machine-learning algorithms, trained with the same training data, often give different answers or outputs to the same problem. Where do these differences come from? From the point of view of decision theory there are three possibilities, which don’t exclude one another:\n\nThe initial probabilities given to the algorithms are different. Since the training data are the same, this means that the background information built into one machine-learning algorithm is different from those built into another.\nIt is therefore important to understand what are the built-in background information and initial probabilities of different machine-learning algorithms. The built-in assumptions of an algorithm must match those of the real problem as closely as possible, in order to avoid sub-optimal or even disastrously wrong answers and outputs.\nThe utilities built into one machine-learning algorithm are different from those built into another.\nIt is therefore also important to understand what are the built-in utilities of different machine-learning algorithms. The built-in utilities must also match those of the real problem as closely as possible.\nThe calculations made by the algorithms are approximate, and different algorithms use different approximations. This means that the algorithms don’t arrive at the unique answer determined by decision theory, but to some other answers which may be approximately close to the optimal one – or not!\nIt is therefore important to understand what are the calculation approximations made by different machine-learning algorithms. Some approximations may be too crude for some real problems, and may again lead to sub-optimal or even disastrously wrong answers and outputs.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-preprocess",
    "href": "connection-2-ML.html#sec-1stconn-preprocess",
    "title": "11  Second connection with machine learning",
    "section": "11.3 Data pre-processing and the data-processing inequality",
    "text": "11.3 Data pre-processing and the data-processing inequality\n“Data pre-processing” is a collective name given to very different operations on data before they are used in some algorithm to solve a decision or inference problem. Some of these operations are often said to be essential for the solution of these problems. This statement in not completely true, and needs qualification.\nWe can divide pre-processing procedures in roughly three categories:\n\nInconsistency checks\n\nProcedures in this category make sure that the data are what they were intended to be. For instance, if data should consist of the power outputs of several engines, but one datapoint is the physical weight of an engine, then that “datapoint” is actually no data at all for the present problem. It’s something included by mistake and should be removed. Such procedures are necessary and useful, but they are just consistency checks and do not change the information contained in the proper data.\n\n\n\n\n\n\n\n\n\n\n\n\nIn later chapters we shall say more about some often erroneous procedures, like “tail trimming”, that actually remove proper data and lead to sub-optimal or completely erroneous solutions.\n\n\n\nFormatting\n\nThese procedures make sure that data are in the correct format to be inputted into the algorithm. They may also include rescaling of numerical values for avoiding numerical overflow or underflow errors during computation. Such procedures are often necessary and useful, but they just change the way data are encoded. They do not actually change the information contained in the data.\n\n“Mutilation” or information-alteration\n\nProcedures of this kind alter the content of data. For instance, such a procedure may replace, in a dataset of temperatures, a datapoint having value 20 °C with one having value 25 °C; this is not just a simple rescaling. Procedures of this kind include “de-noising”, “de-biasing”, “de-trending”, “filtering”, “dimensionality reduction” and similar ones (often having noble-sounding names). We must state, clearly and strongly, that within Decision Theory and Probability Theory, such information-altering pre-processing is not necessary , and is in fact detrimental. This is why we call it “mutilation” here.\n\n\nIt is important that you understand that such data pre-processing is not something that one, in principle, has to do in data science. Quite the opposite, in principle we should not do it, because it is a destructive procedure. Such pre-processing is done in order to correct deficiencies of the algorithms currently in use, as discussed below.\nIf we build an “optimal predictor machine” that fully operates according to the four rules of inference (§ 8.4) and of maximization of expected utility, then the data fed into this machine should not be pre-processed with any information-altering procedures. The reason is that the four fundamental rules automatically take care, in an optimal way, of factors such as noise, bias, systematic errors, redundancy. We briefly discussed this fact in § 9.7 and saw a simple example of how redundancy is accounted for by the four inference rules.\nIf we have information about noise or other factors affecting the data, then we should include this information in the background information provided to the “optimal predictor machine”, rather than altering the data given to it. The reason, in intuitive terms, is that the machine does the adjustments while fully exploring the data themselves; so it can “see” more deeply how to make optimal adjustments given the “inner structure” of the data. In the pre-processing phase – as the prefix “pre-” indicates – we don’t have the full picture about the data, so any adjustment risks to eliminate actually useful information.\nMore formally, this is the content of the data-processing inequality from information theory:\n\n\n\n\n\n\nData-processing inequality\n\n\n\n\n“No clever manipulation of the data can improve the inferences that can be made from the data”\n(Elements of Information Theory §2.8)\nor, from a complementary point of view:\n“Data processing can only destroy information”\n(Information Theory, Inference, and Learning Algorithms exercise 8.9)\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through:\n\n§2.8 of Cover & Thomas: Elements of Information Theory\nExercise 8.9 and its solution in MacKay: Information Theory, Inference, and Learning Algorithms\n\n\n\n\nThere are two main, partially connected reasons why one performs “mutilating” pre-processing of data:\n\nThe algorithm used is non-optimal: it’s only using approximations of the four fundamental rules, and therefore cannot remove noise, bias, redundancies, or similar factors in an optimal way, or at all. In this case, pre-processing is an approximate way of correcting the deficiency of the non-optimal algorithm.\nFull optimal processing is computationally too expensive. In this case we try to simplify the optimal calculation by doing, in advance and in a cruder, faster way, some of the “cleaning” that the full calculation would otherwise spend time doing in an optimal way.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html",
    "href": "quantities_types.html",
    "title": "12  Quantities and data types",
    "section": "",
    "text": "Motivation for the “Data I” part\nIn the “Inference I” part we surveyed the four fundamental rules of inference, which determine how an agent’s degrees of belief should propagate and be self-consistent. We explored some applications and consequences of the four fundamental rules. The rules can be used with any sentences whatsoever, so their application can be further developed and specialized in a wide spectrum of directions, with applications ranging from robotics to psychology. Each of these possible developments would require by itself a full university course!\nWe shall now restrict our attention to applications typical of “data science” and machine learning, like classification, forecast, prognosis, hypothesis testing, in situations that involve quantifiable and measurable phenomena. For this purpose we focus on sentences of particular kinds, which can express such quantification and measurement. In a sense, we develop a specialized “language” for this kind of situations.\nStill, since we’re nonetheless dealing with sentences (Ch.  6), the probability calculus and its inference rules apply without changes of any kind.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "12  Quantities and data types",
    "section": "12.1 Quantities",
    "text": "12.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of things that we can measure. We represent them by mathematical objects of different kinds. These objects have particular mathematical properties and can undergo particular operations.\nThe different mathematical properties of these things reflect the kind of activities that we can do with them. For instance, colours are represented by particular tuples of numbers. These tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation, called “convex combination”, represents the fact that colours can be obtained by mixing other colours in different proportions.\n\n\n25% #FF0000 + 75% #0000FF = #4000C0\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation. Yet it’s convenient if we find one, so we can discuss the general theory without getting bogged down in individual cases. To this purpose we’ll borrow the term quantity from physics and engineering.\n\n\n\n\n\n\n \n\n\n\nThe definition of “quantity” we are using here is similar to the one having the maximum specific level as defined in §1.1 of the International vocabulary of metrology by the Joint Committee for Guides in Metrology.\nUsing the word “quantity” this way is just a convention between us. Other texts and scientists may use other words – for example “variable”, “event”, “state”. When you read a text or listen to a scientist, try to grasp the general idea behind their words.\nAs a general term, we prefer the word “quantity” to a word like “variable”, because the latter may give the idea of something changing in time, and that may very well not be the case (think of the mass of a block of concrete). Same goes with a word like “state”, for the opposite reason.\n\n\n\n\nWe distinguish between a quantity and its value. For instance, a quantity could be:\n“The temperature at the point having GPS coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”;\nand its value could be:\n\\(24\\,\\mathrm{°C}\\). To understand the difference between a quantity and its value, you may think of the quantity as a question, and of the value as the answer to that question:\n(quantity:) “What was the temperature at the point having GPS coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z?”\n(value:) “It was \\(24\\,\\mathrm{°C}\\).”\nThe distinction between a quantity and its value is important and necessary in inference and decision problems, because an agent may not know the value of a particular quantity, while still knowing what the quantity is. In this case the agent can consider every possible value that the quantity could have, and assign a probability to each. The set of possible values is called the domain of the quantity. Think of it as the collection of all meaningful answers that could be given to the question. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nKeep in mind that our definition of quantity is quite general. Here’s another example:\n\nQuantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say 128 × 128 × 3 integers between \\(0\\) and \\(255\\)).\nOne example value is this:  (corresponding to a grid of 128 × 128 × 3 specific numbers). Another example value: .\nDomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values).\n\n\nOther examples of quantities and their domains:\n\nThe distance between two objects in the Solar System at a specific Barycentric Coordinate Time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of a specific online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object at a specific time and place. The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, 0 to 10 000 nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\). Note that the domain in this case is not made of numbers.\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz (that is, 48 000 audio samples per second). The domain could be all possible sequences of 2 880 000 numbers in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket at a specific time and place, with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\nIn these notes we agree that a quantity has one, and only one, actual value.\n\n\n\n\n\n\n Quantity vs variate or variable\n\n\n\nWe can consider something that changes with time, or with space, or from individual to individual, or from unit to unit. Then this “something” is not a quantity, according to our present terminology, but a collection of quantities: one for each time, or space, or individual. Later we shall call this collection a variate, especially when it refers to individuals or unit; or a variable, especially when it refers to time.\nFor instance, your height at this exact moment is a quantity, but your height throughout your life is a variable, and the height (at this moment) across all Norwegian people is a variate.\nThese are just terminological conventions adopted in the present notes. As mentioned before, different scientists often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the two notions that we here call “quantity” and “variate”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\), or \\(U\\), or \\(A\\). The sentences that appear in decision-making and inferences are therefore often of the kind:\n“the quantity \\(X\\) was observed to have value \\(x\\)”,\nwhere “\\(x\\)” stands for a specific value. This kind of sentences are often abbreviated in the form “\\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\)”.\n\n\n\n\n\n\n \n\n\n\n Keep in mind our discussion from § 6.3: we must make clear what “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” means. It could mean “observed”, “set”, “reported”, and so on.\n Note the subtle difference between \\(X\\), in italics, and \\(\\mathsfit{X}\\), in sans-serif. The first denotes a quantity, the second denotes a sentence. Usually we don’t have to worry too much about these symbol differences, because the meaning of the symbol is clear from the context. But just in case, you know the convention.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-basic-types",
    "href": "quantities_types.html#sec-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.2 Basic types of quantities",
    "text": "12.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts, and with various degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that when we speak of a nominal quantity, it does not make sense to say, for instance, that one value is “twice” or “1.5 times” another; or that one value is “larger” or “later” than another. Nor does it make sense to “add” two quantities. In particular, there is no notion of cumulative probability, quantile, median, average, or standard deviation for a nominal quantity; these are notions that we’ll discuss in Ch.  21.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\njust because \\(3-2=2-1\\), or similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that that when we speak of a nominal quantity, it does not make sense to say that one value is “twice” or “1.5 times” another, and we cannot add or subtract two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, or “larger”, and similar. Owing to the ordering property, it does make sense to speak of cumulative probability, quantile, and median of an ordinal quantity; but there is no notion of average or standard deviation for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what a pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers do not make sense. For this reason the use of numbers to represent an ordinal quantity can be misleading. A less misleading possibility is to represent ordered values by alphabet letters.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that for such a quantity we can say, at the very least, whether the interval or “distance” between one pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether one value is larger than another. We can also take weighted sums of values, called convex combinations (keep in mind that simple addition of values may be meaningless for some quantities).\nOwing to these mathematical properties, it does make sense to speak of the cumulative probability, quantile, median, and also average and standard deviation for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous interval quantity.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue. For example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval type. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, then it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other type.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity: it could be measured, say, in seconds, as precisely as we want. But in a specific situation we could simply label its values as slow, medium, fast, thus turning it into an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity.\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nStevens 1946: On the theory of scales of measurement",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.3 Other attributes of basic types",
    "text": "12.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. Ultimately all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. But in practice, in many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes. This can be convenient also because we can then use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and analogously with unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (upper-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\n Exercises\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long a website (or the Earth?) can last, and how many people can live during such an extent of time.\nWhat about the age of a person? What upper bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or an infinite number (at least in theory) of possible values. The domain of a continuous quantity always has an infinite number of values. Note that a domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, because of the way it’s measured. In this case the quantity could be considered discrete rather than continuous.\n\n\n The Iris dataset from its original paper\nFor instance, the famous Iris dataset consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could of course have intermediate values. The age of a person is another frequent example of an in-principle continuous quantity which is often rounded, say to the year or to the month.\nRounding can impact the way we do inferences about such a quantity. In some situations, rounding can lead to quantities with different unrounded values to take on identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository:\n\nRead the description of the quantities listed in the dataset (sometimes in a readme file included with the dataset download)\nAnalyse the values of some of the quantities in the dataset: check if they can be considered continuous, discrete, or rounded; bounded or unbounded; uncensored or censored; and so on.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-true-quantities",
    "href": "quantities_types.html#sec-true-quantities",
    "title": "12  Quantities and data types",
    "section": "12.4 “True” vs “measured” values",
    "text": "12.4 “True” vs “measured” values\nA difference is often drawn, especially in physics and engineering, between the “true” value of a quantity and the value “observed” or “measured” with a particular measuring instrument. What’s the difference? and how is the “true” value defined? There actually are deep philosophical questions and choices underlying this distinction, and it would take a whole university course to do them justice.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nBridgman: The Logic of Modern Physics\n\n\nIntuitively we define the “true” value as the value that would be measured with an instrument that is perfectly calibrated and as precise as theoretically possible. If we make a distinction between such value and the currently measured value then we’re implying that the current measurement is made with a less precise instrument, and that the “true” and “measured” values could be different.\nIn some circumstances this distinction is unimportant and an agent can use the “measured” value without worries, and consider it as the “true” one. Typically this is the case when the possible discrepancy between measured and true value is enough small to have no consequences. In other circumstances the discrepancy is important: slightly different values might lead to quite different consequences. In such circumstances it is then necessary for the agent to try to infer – using the probability calculus – the true value, using the measured one as “data” or “evidence”. Said otherwise, the agent doesn’t use the measured value directly, but only as an intermediate step to guess the true value. The latter, in turn, can be used for further inferences.\nFrom the point of view of inference and decision-making, the distinction between “true” and “measured” value doesn’t lead to anything methodologically new. It just means that an agent has to do a chain of inferences instead of just one, using the four rules of inference as usual. This situation often requires the definition of two distinct quantities, the “true” and the “observed”, which can have slightly different domains. For instance, we could have a voltage \\(V_\\text{obs}\\) measured with rounding to \\(1\\,\\mathrm{V}\\) and therefore with discrete domain \\(\\set{10\\,\\mathrm{V}, 11\\,\\mathrm{V}, 12\\,\\mathrm{V}, \\dotsc}\\); but we need the “true” voltage \\(V_\\text{true}\\) with a precision of at least \\(0.01\\,\\mathrm{V}\\), so this latter quantity could have a continuous domain.\nIn solving data-science and engineering problems it’s important to make clear whether a particular quantity value can be considered “true” and used as-is, or only “observed” with insufficient precision and used as data to infer the true value.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-metadata",
    "href": "quantities_types.html#sec-metadata",
    "title": "12  Quantities and data types",
    "section": "12.5 Importance of metadata for inference and decision",
    "text": "12.5 Importance of metadata for inference and decision\nThe characteristics of quantities and domains that we have discussed so far are examples of metadata. As the name implies, metadata is information that typically cannot be found in the data.\nAs a simple example, consider this collection of numerical data values:\n8   2   6   19   1   5   4   19   1   8   12   3   1   2   17\nand suppose that some machine-learning algorithm has to generate a new number “similar” to the ones above, or guess what a new one could be. Consider the following possible guesses; would they be acceptable?:\n\n13.  Note that this number does not appear among the values above. Could it be that it is impossible for some particular reason? For example, this value is omitted from the seat numbers of some airlines or street addresses, because of triskaidekaphobia.\n\n\n\n\n\n\n\n\n\n21.  This guess would be impossible if the reported values are rolls of a 20-sided die, typical of fantasy roleplay games. But if the values are, say, the ages of some people, then 21 could be an admissible guess.\n3.5.  The reported values are all integers. But they could be rounded values of say, temperature readings or ages. We don’t know whether a more precise, non-rounded guess such as 3.5 could be acceptable.\n-2.  If the reported values are people’s ages or objects’ weights, then a negative value would be impossible. But if the values were temperatures in degrees Celsius, then the guess -2 could be acceptable.\n\nThe collection of values does not allow us to determine which of the possible scenarios above applies.\n\nA simple piece of metadata can actually correspond to an infinite amount of datapoints. Even if we have a collection of one million positive numbers, they still don’t tell us whether negative values are impossible or not. When we are given the information that the domain of the relevant quantity is positive, this effectively corresponds to knowing that all future data – possibly an infinity – will not be negative.\nAn AI agent or machine-learning algorithm will therefore make better guesses and better decisions if it is given full metadata, besides “training” data. For this reason metadata are extremely important for inference and decision-making, and an optimal agent should make use of metadata.\n\n\n\n\n\n\n Exercises\n\n\n\nExamine the adult-income dataset at the UC Irvine Machine Learning Repository. This set contains more than 30 000 datapoints.\nTake a look at the quantity native_country (what type of variate is this?).\n\nDo you see the values Norway or Finland or Sweden listed among these values?\nDoes this mean that in the adult USA population there are no people coming from these three countries?\nShould an AI agent or machine-learning algorithm exclude these three values from its future guesses?\nWhat would you do to give the algorithm this metadata information? (In a later chapter we shall see how to do this in a rigorous way.)",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html",
    "href": "quantities_types_multi.html",
    "title": "13  Joint quantities and complex data types",
    "section": "",
    "text": "13.1 Joint quantities\nQuantities of more complex types can often be viewed and represented as sets (that is, collections) of quantities of basic and possibly different types.\nA simple collection of quantities of basic types, for instance “age, sex, nationality”, usually does not have any new mathematical properties appearing just because we’re considering those quantities together. We shall call such a collection a joint quantity. Note that a “joint quantity” it is still a quantity, but not a quantity of a basic type.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Its domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, {\\small\\verb;F;}, {\\small\\verb;Norwegian;})\\). The domain could be\n\\[\n[0,+\\infty)\\times\n\\set{{\\small\\verb;F;}, {\\small\\verb;M;}} \\times\n\\set{{\\small\\verb;Afghan;}, {\\small\\verb;Albanian;}, \\dotsc, {\\small\\verb;Zimbabwean;}}\n\\]",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-multiv",
    "href": "quantities_types_multi.html#sec-data-multiv",
    "title": "13  Joint quantities and complex data types",
    "section": "",
    "text": "1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\n Exercises\n\n\n\nConsider again the examples of § 12.1.1. Do you find any examples of joint quantities?",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-complex",
    "href": "quantities_types_multi.html#sec-data-complex",
    "title": "13  Joint quantities and complex data types",
    "section": "13.2 Complex quantities",
    "text": "13.2 Complex quantities\nWe shall call “complex quantity” a quantity that is not of a basic type, nor a collection of quantities of basic type, that is, a joint quantity.\nFamiliar examples of complex quantities are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Other examples are images, sounds, videos.\nNote that a complex quantity may be represented as a collection of quantities of basic type. This collection, however, is “more than the sum of its parts”, in the sense that it has new mathematical properties that do not apply or do not make sense for the single components.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nWe can numerically represent these images as the matrices\n\\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nWith this representation, this quantity is made to correspond to 16 binary digits, or in other words 16 binary quantities.\nFrom the point of view of the individual binary quantities, these three values are “equally different” from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is a new mathematical property that does not belong to any of the 16 binary quantities individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. Machine-learning methods are important because they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or why they may classify correctly but for the wrong reason.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html",
    "href": "probability_distributions.html",
    "title": "14  Probability distributions",
    "section": "",
    "text": "Motivation for the “Inference II” part\nIn the “Data I” part we developed a language, that is, particular kinds of sentences, to approach inferences and probability calculations typical of data-science and engineering problems.\nIn the present part we focus on probability calculations that often occur with this kind of sentences and data. We also focus on how to visually represent such probabilities in useful ways.\nAlways keep in mind that at bottom we’re just using the four fundamental rules of inference over and over again – nothing more than that!",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-distribute-prob",
    "href": "probability_distributions.html#sec-distribute-prob",
    "title": "14  Probability distributions",
    "section": "14.1 Distribution of probability among values",
    "text": "14.1 Distribution of probability among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases – all the possible values that could be the true one.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting the temperature with \\(T\\), these sentences can be abbreviated as\n\\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these five sentences, conditional on the agent’s state of knowledge, which we may denote by the letter \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\). These probabilities could be, for instance, \n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\n\\]\nNote that they sum up to one:\n\\[\n\\begin{aligned}\n&\\quad\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\\\[1ex]\n&=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40}\n\\\\[1ex]\n&=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\nThis collection of probabilities is called a probability distribution, because we are distributing the probability among the possible alternatives.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to the agent.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\]\nProve, using the fundamental rules of inferences and any derived rules from § 8, that we must then have\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in § 12.\nWe start with probability distributions over discrete domains.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-discr-prob-distr",
    "href": "probability_distributions.html#sec-discr-prob-distr",
    "title": "14  Probability distributions",
    "section": "14.2 Discrete probability distributions",
    "text": "14.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\):\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, or discrete-interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. The rectangles might be contiguous or not. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents an area equal to 1. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations, for instance when we want to have a clearer idea of how the probability varies across values for ordinal or interval quantities; or when we want to compare several different probability distributions over the same values.\nIn these cases we can use standard line plots, or variations thereof. Compare the following example.\nA technician wonders which component of a laptop failed first (only one can fail at a time), with seven possible alternatives: \\(\\set{{\\small\\verb;hard-drive;}, {\\small\\verb;motherboard;}, {\\small\\verb;CPU;}, {\\small\\verb;keyboard;}, {\\small\\verb;screen;}, {\\small\\verb;graphics-card;}, {\\small\\verb;PCI;}}\\). This is a nominal quantity.\nBefore examining the laptop, the technician’s belief about which component failed first is distributed among the seven alternatives as shown by the blue histogram with solid borders. After a first inspection of the laptop, the technician’s belief has a new distribution, shown by the red histogram with dashed borders:\n\nIt requires some concentration to tell the two probability distributions apart, for example to understand where their peaks are. Let us represent them by two line plots instead: solid blue with circles for the pre-inspection belief distribution, and dashed red with squares for the post-inspection one:\n\nthis line plot displays more cleanly the differences between the two distributions. We see that at first the technician most strongly believed the \\({\\small\\verb;keyboard;}\\) to be the faulty candidate, the second strongest belief being for the \\({\\small\\verb;PCI;}\\). After the preliminary inspection, the technician most strongly believes the \\({\\small\\verb;PCI;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;graphics card;}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-prob-densities",
    "href": "probability_distributions.html#sec-prob-densities",
    "title": "14  Probability distributions",
    "section": "14.3 Probability densities",
    "text": "14.3 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often using linguistic expressions that make only sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that this quantity has some value  \\(x\\)  we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small, because we never have infinite precision. For example, for double-precision values stored in a computer, the width1 must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) . You can check indeed that your computer might not distinguish between two numbers that differ in their 16th decimal digit:\n1 more precisely the relative width#### R code\n## difference in 15th decimal digit\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n## difference in 16th decimal digit\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nThe value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nWhen we consider a distribution of probability for a continuous quantity, the probabilities are therefore distributed among such small ranges, not among single values.\nSince these ranges are very small, they are also very numerous. But the total probability assigned to all of them must still sum up to \\(1\\). This means that each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real-valued quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.000 000 000 000 000 08\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around the value \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is\n\\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\]\nwhich is a more convenient number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still\n\\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942 \\ .\n\\]\nAs you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation:\n\\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\]\nThis definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nIf \\(X\\) is a continuous quantity, the expression “\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\)”. The probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\) exactly is, if anything, zero.\nThat expression instead means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probabilities and probability densities. We shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as\n\\[\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}X\\]\nwhere “\\(\\mathrm{d}X\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity.\n\nPhysical dimensions and units\nIn the International System of Units (SI), “Degree of belief” is considered to be a dimensionless quantity, or more precisely a quantity of dimension “1”. This is why we don’t write units such as “metres” (\\(\\mathrm{m}\\)), “kilograms” (\\(\\mathrm{kg}\\)) or similar together with a probability value.2\n2 See also the material at the International Bureau of Weights and Measures (BIPM)A probability density, however, is defined as the ratio of a probability amount and an interval \\(\\epsilon\\) of some quantity. This latter quantity might well have physical dimensions, say “metres” \\(\\mathrm{m}\\). Then the ratio, which is the probability density, has dimensions \\(1/\\mathrm{m}\\). So probability densities in general have physical dimensions.\nAs another example, suppose that an agent with background knowledge \\(\\mathsfit{I}\\) assigns a degree of belief \\(0.00012\\) to an interval of temperature of width \\(0.0001\\,\\mathrm{°C}\\), around the temperature \\(T = 20\\,\\mathrm{°C}\\). Then the probability density at \\(20\\,\\mathrm{°C}\\) is equal to\n\\[\n\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\frac{0.00012}{0.0001\\,\\mathrm{°C}} = 1.2\\,\\mathrm{°C^{-1}}\n\\]\nIt is an error to report probability densities without their correct physical units. In fact, keeping track of these units is often useful for consistency checks and finding errors in calculations, just like in other engineering or physics calculations.\nOn the other hand, if we write probability densities as previously suggested, in this case as “\\(\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}T\\)”, then the density written this way does not need any units: the units “\\(\\mathrm{°C^{-1}}\\)” disappear because multiplied by \\(\\mathrm{d}T\\), which has the inverse units “\\(\\mathrm{°C}\\)”.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-represent-dens",
    "href": "probability_distributions.html#sec-represent-dens",
    "title": "14  Probability distributions",
    "section": "14.4 Representation of probability densities",
    "text": "14.4 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the example plots shown in the margin.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath the curve, then we just have a line-based, or rather curve-based, representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\nKeep in mind that the curve representing the probability density is not quite a function. In fact it’s best to call it a “density” or a “density function”. There are important reasons for keeping this distinction, which have also consequences for probability calculations, but we shall not delve into them for the moment.\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0.1\\), by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range containing 10 lines has twice the probability of a range containing 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one of the examples above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain. For example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n@@ TODO Behaviour of representations under transformations of data.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§5.3.0–5.3.1 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "14  Probability distributions",
    "section": "14.5 Combined probabilities",
    "text": "14.5 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from Ch.  1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if it’s sold), and represent it by the quantity \\(T\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”:\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\]\nwhich we can shorten to  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24\\); note the slightly different meaning of the last value.\n\n\n\n\n\n\n Exercise\n\n\n\nWhat is the basic type of the quantity \\(T\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(T\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nTheir values are stored in the file failure_probability.csv and plotted in the histogram on the side.\n\n\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(T\\):\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\[1ex]\n&\\qquad\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad\\qquad{}\\equiv\n(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1) \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2) \\lor \\dotsb \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12)\n\\end{aligned}\n\\]\nThe probability needed by the agent is therefore\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhich can be calculated using the or-rule, considering that the sentences involved are mutually exclusive:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{t=1}^{12} \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing your favourite programming language:\n\nLoad the file failure_probability.csv containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months of use, or after a year of use”.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html",
    "href": "joint_probability.html",
    "title": "15  Joint probability distributions",
    "section": "",
    "text": "15.1 Joint probability distributions\nSo far we have considered probability distributions for quantities of a basic (binary, nominal, ordinal, interval) type. These distributions have a sort of one-dimensional character and can be represented by ordinary histograms, line plots, and scatter plots. We now consider probability distributions for the kind of joint quantities that were discussed in § 13.1.\nA joint quantity is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the person's sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values:\n\\[\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}31\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{m}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\dotsc\n\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s state of knowledge, and the probabilities sum up to 1. We call each of these probabilities a joint probability, and their collection a joint probability distribution. Usually these probabilities are written in a much abbreviated form. A comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is typically used instead of “\\(\\land\\)” (§ 6.4). You can commonly encounter the following notation:\n\\[\n\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nor even just\n\\[\n\\mathrm{P}(25, \\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-prob",
    "href": "joint_probability.html#sec-repr-joint-prob",
    "title": "15  Joint probability distributions",
    "section": "15.2 Representation of joint probability distributions",
    "text": "15.2 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest of the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a joint quantity consists of two, discrete and finite component quantities, the joint probabilities can be reported as a table, sometimes called a contingency table1.\n1 this term is most often used for joint distributions of frequencies rather than probabilityExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means; and the possibility that the patient will need \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care. We can represent these possibilities by two quantities \\(T\\) (nominal) and \\(U\\) (binary). Now suppose that an agent has the following joint probability distribution, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\n\nTable 15.1: Joint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\n\n\nFrom the table we see that the most probable possibility is that the next patient will arrive by other transportation means than ambulance and helicopter, and will not require urgent care:\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.64\\]\n\n\n Probability distribution over the 27 × 27 possible bigrams \\(xy\\) in an English language document. Probabilities are represented by the areas of white squares. From MacKay’s Information Theory, Inference, and Learning Algorithms\nIn this kind of tables it is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n\n\n\n\n\n\n Exercise – never forget the agent!\n\n\n\nWho could be the agent whose degrees of belief are represented in the table above? What could be the background information leading to such beliefs?\n\n\n\n\nScatter plots and similar\nWe saw in § 14.2 that probability distributions for nominal, ordinal, or discrete-interval quantities can be represented by histograms or line plots. Histograms could be generalized to quantities consisting of two joint discrete quantities: a probability could be represented by a cuboid or rectangular prism, or cylinder, or similar. This representation, even if it can look flamboyant, is often inconvenient because some of the three-dimensional objects can be hidden from view, as in the example in the margin illustration.\n\n\n\n\n Examples of a density histogram and a generalized histogram (from Mathematica)\nAlternatively, one can replace the numerical values of the probabilities in the tabular representation of the previous section with some graphical encoding. An example is a colour scheme with white for probability \\(0\\), black for probability \\(1\\), and grey levels for intermediate probabilities. This is sometimes called a “density histogram”; see the example in the margin figure. This representation can be useful for qualitative or semi-quantitative assessments, for example for seeing which joint values have highest probabilities.\n\nAnother representation, similar to the scatter plot (§ 14.4.2), is to encode the probability values with a proportional number of points or other shapes, as illustrated here for the probabilities of table 15.1:\n\n\n\n\n\n\nFigure 15.1: Scatter plot for the urgency-transportation joint probability distribution\n\n\n\nthe points do not need to be scattered in regular fashion as long as it’s clear which quantity value they are associated with. The scatter plot above has 100 points, and therefore we can see for instance that \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small urgent} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.03\\), since the corresponding region has 3 points out of 100.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-joint-prob-densities",
    "href": "joint_probability.html#sec-joint-prob-densities",
    "title": "15  Joint probability distributions",
    "section": "15.3 Joint probability densities",
    "text": "15.3 Joint probability densities\nIf a joint quantity consists in several continuous interval quantities, then its joint probability distribution is usually represented by a joint probability density, which generalizes the one-dimensional discussion of § 14.3 to several dimensions.\nFor instance, if \\(X\\) and \\(Y\\) are two continuous interval quantities, then the notation\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.001\n\\]\nmeans that the joint sentence “\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\), and \\(Y\\) between \\(y-\\delta/2\\) and \\(y+\\delta/2\\)”, or in symbols\n\\[\n\\bigl(x-\\tfrac{\\epsilon}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} X \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} x+\\tfrac{\\epsilon}{2}\\bigr)\n\\land\n\\bigl(y-\\tfrac{\\delta}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} Y \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} y+\\tfrac{\\delta}{2}\\bigr)\n\\]\nin being given a degree of belief \\(0.001\\cdot\\epsilon\\cdot\\delta\\), conditional on the background knowledge \\(\\mathsfit{I}\\). Visually, the rectangular region of values around \\((x,y)\\) with sides of lengths \\(\\epsilon\\) and \\(\\delta\\) is assigned a probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\).\nRemember that a density typically has physical units, as in the one-dimensional case (§ 14.3). For instance, if \\(X\\) above is a temperature measured in kelvin (\\(\\mathrm{K}\\)) and \\(Y\\) a resistance measured in ohm (\\(\\Omega\\)), then we should write\n\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\frac{0.001}{\\mathrm{K}\\,\\Omega}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-dens",
    "href": "joint_probability.html#sec-repr-joint-dens",
    "title": "15  Joint probability distributions",
    "section": "15.4 Representation of joint probability densities",
    "text": "15.4 Representation of joint probability densities\nFor one-dimensional densities we discussed line-based representations and scatter plots (§ 14.4). The first of these representations can be generalized to two-dimensional densities, leading to a surface plot. Below you see the surface density plot for the probability density given by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\n\nThis kind of representation can be neat, but it has three drawbacks: 1. It sometimes hides from view some features of the density (in the plot above, can you exclude that there’s a small peak right behind the main one?). 2. It cannot be extended to three-dimensional densities. 3. Sometimes the analytical expression for the probability density (like the formula above) is not available.\nThe scatter plot overcomes the three drawbacks above. It does not hides features; it can also be used for three-dimensional densities; it can be generated in cases where the analytical formula of a probability distribution is not available or too complicated, but we can still obtain “representative” points from it. The representation of a scatter plot is, however, quantitatively more imprecise. Here is a scatter plot, using 10 000 points, for the probability density given above:\n\n\n\n\n\n\nFigure 15.2: Scatter-plot representation of the joint probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) above\n\n\n\nThe probability of a small region is proportional to the density of points in that region. If we had a joint density for three continuous quantities, its scatter plot would consist of three-dimensional clouds of points instead.\nClearly both kinds of representation have advantages and disadvantages. The choice between them depends on the problem, on the probability density, and on what we wish to visually emphasize. It is also possible to use both, of course.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-joint-mix-distr",
    "href": "joint_probability.html#sec-joint-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.5 Joint mixed discrete-continuous probability distributions",
    "text": "15.5 Joint mixed discrete-continuous probability distributions\nFrequently occurring in engineering and data-science problems are joint quantities composed by some discrete and some continuous quantities. Their joint probability distribution is a density with respect to the continuous component quantity.\nSuppose for instance that \\(Z\\) is a binary quantity with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;high;}}\\), and \\(X\\) a real-valued continuous quantity with domain \\(\\mathbf{R}\\). Together they form the joint quantity \\((Z,X) \\in \\set{{\\small\\verb;low;}, {\\small\\verb;high;}} \\times \\mathbf{R}\\). Then the probability expression\n\\[\n\\mathrm{p}(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.07\n\\]\nmeans that the agent with background information  \\(\\mathsfit{I}\\)  has a degree of belief equal to   \\(0.07\\cdot \\epsilon\\)   in the joint sentence “\\(Z\\) has value \\({\\small\\verb;low;}\\) and \\(X\\) has value between \\(3-\\epsilon/2\\) and \\(3+\\epsilon/2\\)”.  As usual, this is only valid for any small \\(\\epsilon\\), and if \\(X\\) has physical dimensions, say metres \\(\\mathrm{m}\\), then the probability density above has value  \\(0.07\\,\\mathrm{m^{-1}}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-mix-distr",
    "href": "joint_probability.html#sec-repr-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.6 Representation of mixed probability distributions",
    "text": "15.6 Representation of mixed probability distributions\nMixed discrete-continuous probability distributions can be somewhat tricky to represent graphically. Here we consider line-based representations and scatter plots. We take as example the probability that the next patient who arrives at a particular hospital has a given age (positive continuous quantity) and arrives by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means (table 15.1).\n\nMulti-line plots\nA line plot can be used to represent the probability density for the continuous quantity and each specific value of the discrete quantity:\n\n\n\n\n\n\nFigure 15.3: Line plot for the age-transportation joint probability distribution (table 15.1)\n\n\n\nWith the plot above it’s important to keep in mind that the three curves are three pieces of the same probability density, not three different densities. This is also clear from the fact that the three areas under them (which partly overlap) cannot each be equal to 1, as would instead be required for a probability density. The probability density is separated into three curves owing to the presence of the discrete quantity, which has three possible values.\nThe area under the solid blue curve is equal to \\(0.55\\), the area under the dashed red curve is \\(0.25\\), and the area under the dotted green curve is \\(0.20\\) . The total area under the three curves (counting also the overlapping regions) is equal to \\(1\\), as it should.\nA possible disadvantage of this kind of plots is that some details, such as peaks, of the densities for some values of the discrete quantity, may be barely discernible.\n\n\nScatter plots\nAs discussed before, in a scatter plot we represent the probability density by a cloud of “representative” objects, such as points, obtained from it. The density of these objects is approximately proportional to the density of probability.\nHere is an example of scatter plot for the probability density of table 15.1:\n\nIn the plot above, the probability density is reflected by the density of vertical lines. Using points instead of vertical lines, the density would have been difficult to discern, since the points would all lie on three lines.\nWe can use points if we give some variation, usually called jitter, to their vertical coordinate; but we must keep in mind that such vertical variation has no meaning. The idea is similar to the one of fig.  15.1. In our current example of table 15.1 we obtain a plot like this:\n\n\n\n\n\n\nFigure 15.4: Point-scatter plot for the age-transportation joint probability\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nCompare the line plot of fig.  15.3 and the point-scatter plot of fig.  15.4, which represent the same joint probability density. Do some introspection, and analyse the contrasting impressions that the two kinds of representations may give you. For instance, does the line plot give you a wrong intuition about the sharpness of the peaks in the density?\nCompare with what you did in the exercise of § 14.4.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-general-distr",
    "href": "joint_probability.html#sec-repr-general-distr",
    "title": "15  Joint probability distributions",
    "section": "15.7 Representation of more general probability distributions and densities",
    "text": "15.7 Representation of more general probability distributions and densities\nProbability distributions for complex types of quantity can be quite tricky to visualize and represent in an informative way. They typically require a case-by-case approach.\nOften the idea behind the scatter plot works also in these complex cases: the probability distribution or density is represented by a “representative” sample of objects. The objects can even depict the quantity itself.\n\n\n A voltage-current converter\nFor instance, imagine the complex quantity \\(L\\) defined as “the linear relationship between input voltage and output current of a specific electronic component”. The possible values of this quantity are straight lines, that is, functions of the form “\\(y=m\\,x + q\\)”, where \\(x\\) is the input voltage and \\(y\\) the output current. These possible values – straight lines – can differ in their angular coefficient \\(m\\) or in their intercept \\(q\\). One possible value could be the straight line\n\\(y= (2\\,\\mathrm{A/V})\\, x - 3\\,\\mathrm{A}\\)\nanother possible value could be the straight line\n\\(y= (-1\\,\\mathrm{A/V})\\, x + 5\\,\\mathrm{A}\\)\nand so on. The quantity \\(L\\) so defined is a continuous quantity, but it isn’t a quantity of a basic type.\nAn agent may be uncertain about the actual value of \\(L\\), that is, about what is the straight line that correctly expresses the voltage-current relationship of this particular electronic component. The agent therefore assignes a probability density over all possible values: over all possible straight lines. How to visually represent such a “probability density over lines”?\nOne way is to use a scatter plot. The probability distribution is represented by a collection of straight lines, whose density is approximately proportional to the probability density. Here is an example using 360 representative straight lines:\n\n\n\n\n\n\nFigure 15.5: Scatter plot for a probability density over the voltage-current relationship\n\n\n\nFrom this plot we can read some important semi-quantitative information about the agent’s degrees of belief. For instance:\n\nIt’s most probable that the voltage-current relationship has a positive angular coefficient \\(m\\) with value around \\(0.5\\,\\mathrm{A/V}\\), and an intercept \\(q\\) around \\(3\\,\\mathrm{A}\\).\nIt is improbable, but not impossible, that the voltage-current relationship has a negative angular coefficient (that is, the output current decreases as the input voltage is increased).\nIt’s practically impossible that the voltage-current relationship is almost vertical (say, changes in current larger than \\(\\sim 5\\,\\mathrm{A}\\) with changes in voltage smaller than \\(\\sim 0.2\\,\\mathrm{V}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository, for example\n\nThe adult-income dataset\nThe heart-disease dataset\n\nAssume that the data given are representative “points” of a probability distribution or density (of which we don’t know the analytic formula). Plot the probability distributions and probability densities as scatter plots using some of these representative points.\nLook around for analytic formulae of some probability distributions and densities of simple and joint quantities, and plot them using different representations.\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§5.3.2 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§12.2.2 of Artificial Intelligence",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html",
    "href": "marginal_probability.html",
    "title": "16  Marginal probability distributions",
    "section": "",
    "text": "16.1 Marginal probability: neglecting some quantities\nIn some situations an agent has a joint distribution of degrees of belief for the possible values of a joint quantity, but it needs to consider its belief in the value of one component quantity alone, irrespective of what the values for the other components quantities might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table  15.1 from § 15.2, with joint quantity \\((U,T)\\). We may be interested in the probability that the next patient will need \\({\\small\\verb;urgent;}\\) care, independently of how the patient is transported to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from § 8.4.\nThe sentence of interest is “The next patient will require urgent care”, or in symbols\n\\[U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\]\nThis sentence is equivalent to “The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means”, or in symbols\n\\[\nU \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land\n(\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\n)\n\\]\nUsing the derived rules of Boolean algebra of § 9.2 we can rewrite this sentence in yet another way:\n\\[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\]\nThis last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule, with the and terms being zero (we shall now use the comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” for and):\nWe have found that the probability for a value of the urgency quantity \\(U\\), independently of the value of the transportation quantity \\(T\\), can be calculated by summing all joint probabilities with all possible \\(T\\) values. Using the \\(\\sum\\)-notation we can write this compactly:\n\\[\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{t}\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nwhere it’s understood that the sum index \\(t\\) runs over the values \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\).\nThis is called a marginal probability.\nConsidering now a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), the probability for a specific value of \\(\\color[RGB]{34,136,51}X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(\\color[RGB]{238,102,119}Y\\) might be, is given by\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from § 14.5. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the sentence \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-probs",
    "href": "marginal_probability.html#sec-marginal-probs",
    "title": "16  Marginal probability distributions",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[1ex]\n&\\quad{}=\n\\mathrm{P}\\bigl[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\quad{}=\\begin{aligned}[t]\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the values from table 15.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise: test your understanding\n\n\n\nUsing again the values from table 15.1, calculate the probability that the next patient will need urgent care and will be transported either by ambulance or by helicopter.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-dens",
    "href": "marginal_probability.html#sec-marginal-dens",
    "title": "16  Marginal probability distributions",
    "section": "16.2 Marginal density distributions",
    "text": "16.2 Marginal density distributions\nIn the example of the previous section, suppose now that the quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous. Then the joint probability is expressed by a density:\n\\[\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwith the usual meaning. The marginal probability density for \\(\\color[RGB]{34,136,51}X\\) is still given by a sum, but this sum occurs over intervals of values of \\(\\color[RGB]{238,102,119}Y\\), intervals with very small widths. As a consequence the sum will have a very large number of terms. To remind ourselves of this fact, which can be very important in some situations, we use a different notation in terms of integrals:\n\\[\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}y}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) represents the domain of the quantity \\(\\color[RGB]{238,102,119}Y\\).\nThis is called a marginal probability density.\nThe appearance of integrals is sometimes extremely useful, because it allows us to use the theory of integration to calculate marginal probabilities quickly and precisely, instead of having to compute sums of a large numbers of small terms – a procedure that can be computationally expensive and lead to numerical errors owing to underflow or similar computation problems.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-scatter",
    "href": "marginal_probability.html#sec-marginal-scatter",
    "title": "16  Marginal probability distributions",
    "section": "16.3 Marginal probabilities and scatter plots",
    "text": "16.3 Marginal probabilities and scatter plots\nIn the previous chapters we have often discussed scatter plots (§ 14.4.2, § 15.6.2) for representing probability distributions of various kinds: discrete, continuous, joint, mixed, and so on.\nOne more advantage of representing a joint distribution with a scatter plot is that it can be quickly modified to represent any marginal distribution, again with a scatter plot. Whereas the use of a surface plot would require analytical calculations or approximations thereof.\nConsider for instance the joint probability density from § 15.4, represented by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\nand suppose we would like to visualize the marginal probability density for \\(X\\):\n\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\nIn order to represent this marginal probability density with a line plot, we would first need to calculate the integral of the formula above over \\(Y\\):\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\int_{-\\infty}^{\\infty}\n\\Bigl[\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\Bigr]\n\\, \\mathrm{d}y\n\\]\n\nNow instead suppose that we have stored the points used to represent the joint probability density  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  as a scatter plot, as in fig.  15.2. Each of these points is a pair of coordinates \\((x, y)\\), representing an \\(X\\)-value and a \\(Y\\)-value. It turns out that these same points can be used to make a scatter-plot of the marginal density for \\(X\\), simply by considering their \\(x\\)-coordinates only, that is, by discarding their \\(y\\)-coordinates. Often we use a subsample (unsystematically chosen) of them, so that the resulting one-dimensional scatter plot doesn’t become too congested and difficult to read.\nAs an example, here is a scatter plot for the marginal probability density  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  above, obtained by selecting a subset of 400 points from the scatter plot (fig.  15.2) for the joint distribution. The points are replaced by vertical lines for better visibility:\n\n\n\n\n\n\nFigure 16.1: Scatter-plot representation of the marginal probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nThe points for the scatter plot of fig.  15.2 (§ 15.4) are saved in the file scatterXY_samples.csv. Use them to represent the marginal probability density \\(\\mathrm{p}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), for the other quantity \\(Y\\), as a scatter plot.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-use-pitfall-marginal",
    "href": "marginal_probability.html#sec-use-pitfall-marginal",
    "title": "16  Marginal probability distributions",
    "section": "16.4 Uses and pitfalls of marginal probability distributions",
    "text": "16.4 Uses and pitfalls of marginal probability distributions\nAn agent’s distribution of degrees of belief for a multi-dimensional joint quantity is not easily – or at all – visualizable. This shortcoming is especially bad because, as discussed in § 10.1, our intuition often fails us horribly in multi-dimensional problems.\nMarginal probability distributions for one or two of the component quantities are useful because they offer us a little glimpse of the multi-dimensional “monster” distribution. In concrete engineering and data-science problem, when we need to discuss a multi-dimensional distribution it is good practice to visually report at least its one-dimensional marginal distributions.\nIn the machine-learning literature, this low-dimensional glimpse is often used to qualitatively assess whether two multi-dimensional distributions are similar. Their one-dimensional marginals are visually compared and, if they overlap, one hopes (but some works in the literature even erroneously conclude) that the multi-dimensional distributions are somewhat similar as well.\nKeep in mind that this may very well not be the case. Marginal distributions can also be quite deceiving:\n\n\n\n\n\n\n Exercise\n\n\n\nHere are three different joint probability densities for the joint quantity \\((X,Y)\\), each density represented by a scatter plot with 200 points. the files containing the coordinates of the scatter-plot points are also given:\nA. File scatterXY_A.csv:\n\nB. File scatterXY_B.csv:\n\nC. File scatterXY_C.csv:\n\n\n\n\nReproduce the three scatter plots above using the points from the three files, just to confirm that they are correct.\nFor each density, plot the marginal density for the quantity \\(X\\) as a scatter plot. Use the method described in § 16.3; do not subsample the points.\nWhat can you say about the three marginal densities you obtain?\n\nDo the same, but for the marginal densities for \\(Y\\).\nWhat can you say about the three marginal densities you obtain?\nIf two joint probability distributions have the same marginals, can we conclude that they are identical, or at least similar?\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§5.3.2–5.3.3 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§12.3 of Artificial Intelligence\n§§5.1–5.5 of O’Hagan: Probability",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html",
    "href": "conditional_probability.html",
    "title": "17  Conditional probability and learning",
    "section": "",
    "text": "17.1 The meaning of the term “conditional probability”\nWhen we introduced the notion of degree of belief – a.k.a. probability – in chapter  8, we emphasized that every probability is conditional on some state of knowledge or information. So the term “conditional probability” sounds like a pleonasm, just like saying “round circle”.\nThis term must be understood in a way analogous to “marginal probability”: it applies in situations where we have two or more sentences of interest. We speak of a “conditional probability” when we want to emphasize that additional sentences appear in the conditional (right side of “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”) of that probability. For instance, in a scenario with these two probabilities:\n\\[\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\qquad\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwe call the first conditional probability of \\(\\mathsfit{A}\\) (given \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\)) to emphasize or point out that its conditional includes the additional sentence \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\), whereas the conditional of the second probability doesn’t include this sentence.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-prob_learning",
    "href": "conditional_probability.html#sec-conditional-prob_learning",
    "title": "17  Conditional probability and learning",
    "section": "17.2 The relation between learning and conditional probability",
    "text": "17.2 The relation between learning and conditional probability\nWhy do we need to emphasize that a particular degree of belief is conditional on an additional sentence? Because the additional sentence usually represents new information that the agent has learned.\nRemember that the conditional of a probability usually contains all factual information known to the agent1. Therefore if an agent acquires new data or a new piece of information expressed by a sentence \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), it should draw inferences and make decisions using probabilities that include \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\) in their conditional. In other words, the agent before was drawing inferences and making decisions using some probabilities\n1 Exceptions are, for instance, when the agent does counterfactual or hypothetical reasoning, as we discussed in § 5.1.\\[\n\\mathrm{P}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\]\nwhere \\(\\mathsfit{K}\\) is the agent’s knowledge until then. Now that the agent has acquired information or data \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), it will draw inferences and make decisions using probabilities\n\\[\n\\mathrm{P}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\]\nVice versa, if we see that an agent is calculating new probabilities conditional on an additional sentence \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), then it means2 that the agent has acquired that information or data \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\).\n2 But keep again in mind exceptions like counterfactual reasoning; see the previous side note.Therefore conditional probabilities represent an agent’s learning and should be used when an agent has learned something.\nThis learning can be of many different kinds. Let’s examine two particular kinds by means of some examples.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-dis",
    "href": "conditional_probability.html#sec-conditional-joint-dis",
    "title": "17  Conditional probability and learning",
    "section": "17.3 Learning about a quantity from a different quantity",
    "text": "17.3 Learning about a quantity from a different quantity\nConsider once more the next-patient arrival scenario of § 15.2, with joint quantity \\((U,T)\\) and an agent’s joint probability distribution as in table  15.1, reproduced here:\n\nJoint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\nSuppose that the agent must forecast whether the next patient will require \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care, so it needs to calculate the probability distribution for \\(U\\) (that is, the probabilities for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)).\nIn the first exercise of § 16.1 you found that the marginal probability that the next patient will need urgent care is\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) = 18\\%\\]\nthis is the agent’s degree of belief if it has nothing more and nothing less than the knowledge encoded in the sentence \\(\\mathsfit{I}_{\\text{H}}\\).\nBut now let’s imagine that the agent receives a new piece of information: it is told that the next patient is being transported by helicopter. In other words, the agent has learned that the sentence  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\)  is true. The agent’s complete knowledge is therefore now encoded in the anded sentence\n\\[T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\ \\land\\ \\mathsfit{I}_{\\text{H}}\\]\nand this composite sentence should appear in the conditional. The agent’s belief that the next patient requires urgent care, given the new information, is therefore\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\\]\nCalculation of this probability can be done by just one application of the and-rule, leading to a formula connected with Bayes’s theorem (§ 9.4):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\cdot\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[3ex]\n&\\quad\\implies\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\n\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\end{aligned}\n\\]\n\nLet’s see how to calculate this. The agent already has the joint probability for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) that appears in the numerator of the fraction above. The probability in the denominator is just a marginal probability for \\(T\\), and we know how to calculate that too from § 16.1. So we find\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\sum_u\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\]\nwhere it’s understood that the sum index \\(u\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\).\nThis is called a conditional probability; in this case, the conditional probability of  \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\).\nThe collection of probabilities for all possible values of the quantity \\(U\\), given a specific value of the quantity \\(T\\), say \\({\\small\\verb;helicopter;}\\):\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\qquad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\]\nis called the conditional probability distribution for \\(U\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\). It is indeed a probability distribution because the two probabilities sum up to 1.\n\n\n\n\n\n\n\n\n\n\nNote that the collection of probabilities for, say, \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), but for different values of the conditional quantity \\(T\\), that is:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\]\nis not a probability distribution. Calculate the three probabilities above and check that in fact they do not sum up to 1.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nUsing the values from table 15.1 and the formula for marginal probabilities, calculate:\n\nThe conditional probability that the next patient needs urgent care, given that the patient is being transported by helicopter.\nThe conditional probability that the next patient is being transported by helicopter, given that the patient needs urgent care.\n\nNow discuss and find an intuitive explanation for these comparisons:\n\nThe two probabilities you obtained above. Are they equal? why or why not?\nThe marginal probability that the next patient will be transported by helicopter, with the conditional probability that the patient will be transported by helicopter given that it’s urgent. Are they equal? if not, which is higher, and why?",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-sim",
    "href": "conditional_probability.html#sec-conditional-joint-sim",
    "title": "17  Conditional probability and learning",
    "section": "17.4 Learning about a quantity from instances of similar quantities",
    "text": "17.4 Learning about a quantity from instances of similar quantities\nIn the previous section we examined how learning about one quantity can change an agent’s degree of belief about a different quantity, for example knowledge about “transportation” affects beliefs about “urgency”, or vice versa. The agent’s learning and ensuing belief change are reflected in the value of the corresponding conditional probability.\nThis kind of change can also occur with “similar” quantities, that is, quantities that represent the same kind of phenomenon and have the same domain. The maths and calculations are identical to the ones we explored above, but the interpretation and application can be somewhat different.\nAs an example, imagine a scenario similar to the next-patient arrival above, but now consider the next three patients to arrive and their urgency. Define the following three quantities:\n\\(U_1\\) : urgency of the next patient\n\\(U_2\\) : urgency of the second future patient from now\n\\(U_3\\) : urgency of the third future patient from now\n\nEach of these quantities has the same domain: \\(\\set{{\\small\\verb;urgent;},{\\small\\verb;non-urgent;}}\\).\nThe joint quantity \\((U_1, U_2, U_3)\\) has a domain with \\(2^3 = 8\\) possible values:\n\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n. . .\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n\n\nSuppose that an agent, with background information \\(\\mathsfit{I}\\), has a particular joint belief distribution for the joint quantity \\((U_1, U_2, U_3)\\). For example consider the joint distribution implicitly given as follows: \n\nIf \\({\\small\\verb;urgent;}\\) appears in the probability 0 times out of 3:  probability = \\(53.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 1 times out of 3:  probability = \\(11.4\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 2 times out of 3:  probability = \\(3.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 3 times out of 3:  probability = \\(1.4\\%\\)\n\nHere are some examples of how the probability values are determined by the description above:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036 \\quad&&\\text{\\small(${\\small\\verb;urgent;}$ appears twice)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.114 &&\\text{\\small(${\\small\\verb;urgent;}$ appears once)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036 &&\\text{\\small(${\\small\\verb;urgent;}$ appears twice)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.536 &&\\text{\\small(${\\small\\verb;urgent;}$ doesn't appear)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nCheck that the joint probability distribution as defined above indeed sums up to \\(1\\).\nCalculate the marginal probability for \\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), that is,  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\). \nCalculate the marginal probability that the second and third patients are non-urgent cases, that is\n\n\\[\\mathrm{P}(U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\ .\\] \n\n\nFrom this joint probability distribution the agent can calculate, among other things, its degree of belief that the third patient will require urgent care, regardless of the urgency of the preceding two patients. It’s the marginal probability\n\\[\n\\begin{aligned}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})  &=\n\\sum_{u_1}\\sum_{u_2}\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]\n&= 0.114 + 0.036 + 0.036 + 0.014\n\\\\[1ex]\n&= \\boldsymbol{20.0\\%}\n\\end{aligned}\n\\]\nwhere each index \\(u_1\\) and \\(u_2\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\). This double sum therefore involves four terms. The first term in the sum corresponds to “\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)” and therefore has probability \\(0.014\\) . The second term corresponds to “\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)” and therefore has probability \\(0.036\\). And so on.\nTherefore the agent, with its current knowledge, has a \\(20\\%\\) degree of belief that the third patient will require urgent care.\n\n\nNow fast-forward in time, after two patients have arrived and have been taken good care of; or maybe they haven’t arrived yet, but their urgency conditions have been ascertained and communicated to the agent. Suppose that both patients were or are non-urgent cases. The agent now knows this fact. The agent needs to forecast whether the third patient will require urgent care.\nThe relevant degree of belief is obviously not  \\(\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\),  calculated above, because this belief represents an agent knowing only \\(\\mathsfit{I}\\). Now, instead, the agent has additional information about the first two patients, encoded in this anded sentence:\n\\[\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\n\\]\nThe relevant degree of belief is therefore the conditional probability\n\\[\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nWhich we can calculate with the same procedure as in the previous section:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.114}{0.65}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{17.5\\%}\n\\end{aligned}\n\\]\nThis conditional probability \\(17.5\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\) is lower than \\(20.0\\%\\) calculated previously, which was based only on knowledge \\(\\mathsfit{I}\\). Learning about the two first patients has thus affected the agent’s degree of belief about the third.\n\nLet’s also check how the agent’s belief changes in the case where the first two patients are both urgent instead. The calculation is completely analogous:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.030}{0.107}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{28.0\\%}\n\\end{aligned}\n\\]\nIn this case the conditional probability \\(28.0\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) is higher than the \\(20.0\\%\\), which was based only on knowledge \\(\\mathsfit{I}\\).\nOne possible intuitive explanation of these probability changes, in the present scenario, is that observation of two non-urgent cases makes the agent slightly more confident that “this is a day with few urgent cases”. Whereas observation of two urgent cases makes the agent more confident that “this is a day with many urgent cases”.\n\n\n\n\n\n\n The diversity of inference scenarios\n\n\n\nIn general we cannot say that the probability of a particular value (such as \\({\\small\\verb;urgent;}\\) in the scenario above) will decrease or increase as similar or dissimilar values are observed. Nor can we say how much the increase or decrease will be.\nIn a different situation the probability of \\({\\small\\verb;urgent;}\\) could actually increase as more and more \\({\\small\\verb;non-urgent;}\\) cases are observed. Imagine, for instance, a scenario where the agent initially knows that there are 10 urgent and 90 non-urgent cases ahead (maybe these 100 patients have already been gathered in a room). Having observed 90 non-urgent cases, the agent will give a much higher, in fact 100%, probability that the next case will be an urgent one. Can you see intuitively why this conditional degree of belief must be 100%?\nThe differences among scenarios are reflected in differences in joint probabilities, from which the conditional probabilities are calculated. One particular joint probability can correspond to a scenario where observation of a value increases the degree of belief in subsequent instances of that value. Another particular joint probability can instead correspond to a scenario where observation of a value decreases the degree of belief in subsequent instances of that value.\nAll these situations are, in any case, correctly handled with the four fundamental rules of inference and the formula for conditional probability derived from them!\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nUsing the same joint distribution above, calculate\n\\[\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nthat is, the probability that the first patient will require urgent care given that the agent knows the second and third patients will not require urgent care.\n\nWhy is the value you obtained different from  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) ?\nDescribe a scenario in which the conditional probability above makes sense, and patients 2 and 3 still arrive after patient 1. That is, a scenario where the agent learns that patients 2 and 3 are non-urgent, but still doesn’t know the condition of patient 1.\n\n\n\n\n\nDo an analysis completely analogous to the one above, but with different background information \\(\\mathsfit{J}\\) corresponding to the following joint probability distribution for \\((U_1, U_2, U_3)\\):\n• If \\({\\small\\verb;urgent;}\\) appears 0 times out of 3:  probability = \\(0\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 1 times out of 3:  probability = \\(24.5\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 2 times out of 3:  probability = \\(7.8\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 3 times out of 3:  probability = \\(3.1\\%\\)\n\nCalculate\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\] \nand\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\] and compare them.\nFind a scenario for which this particular change in degree of belief makes sense.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-general",
    "href": "conditional_probability.html#sec-conditional-joint-general",
    "title": "17  Conditional probability and learning",
    "section": "17.5 Learning in the general case",
    "text": "17.5 Learning in the general case\nTake the time to review the two sections above, focusing on the application and meaning of the two scenarios and calculations, and noting the similarities and differences:\n\n The calculations were completely analogous. In particular, the conditional probability was obtained as the quotient of a joint probability and a marginal one.\n In the first (urgency & transportation) scenario, information about one aspect of the situation changed the agent’s belief about another aspect. The two aspects were different (transportation and urgency). Whereas in the second (three-patient) scenario, information about analogous occurrences of an aspect of the situation changed the agent’s belief about a further occurrence.\n\n\nA third scenario is also possible, which combines the two above. Consider the case with three patients, where each patient can require \\({\\small\\verb;urgent;}\\) care or not, and can be transported by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) means. To describe this situation, introduce three pairs of quantities, which together form the joint quantity\n\\[\n(U_1, T_1, \\ U_2, T_2, \\ U_3, T_3)\n\\]\nwhose symbols should be obvious. This joint quantity has \\((2\\cdot 3)^3 = 216\\) possible values, corresponding to all urgency & transportation combinations for the three patients.\nGiven the joint probability distribution for this joint quantity, it is possible to calculate all kinds of conditional probabilities, and therefore consider all the possible ways the agent may learn new information. For instance, suppose the agent learns this:\n\nthe first two patients have not required urgent care\nthe first patient was transported by ambulance\nthe second patient was transported by other means\nthe third patient is arriving by ambulance\n\nand with this learned knowledge, the agent needs to infer whether the third patient will require urgent care. The required conditional probability is\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n}{\n\\mathrm{P}(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\nand is calculated in a way completely analogous to the ones already seen.\n\n\nAll three kinds of inference scenarios that we have discussed occur in data science and engineering. In machine learning, the second scenario is connected to “unsupervised learning”; the third, mixed scenario to “supervised learning”. As you just saw, the probability calculus “sees” all of these scenarios as analogous: information about something changes the agent’s belief about something else. And the handling of all three cases is perfectly covered by the four fundamental rules of inference.\nSo let’s write down the general formula for all these cases of learning.\nLet’s consider a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). Their joint probability distribution is given. Each of these two quantities could be a complicated joint quantity by itself.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), given that the agent has learned that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.1}\\]\nwhere the index \\(\\color[RGB]{238,102,119}\\upsilon\\) runs over all possible values in the domain of \\(\\color[RGB]{238,102,119}Y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional",
    "href": "conditional_probability.html#sec-conditional-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.6 Conditional probabilities as initial information",
    "text": "17.6 Conditional probabilities as initial information\nUp to now we have calculated conditional probabilities, using the derived formula (17.1), starting from the joint probability distribution, which we considered to be given. In some situations, however, an agent may initially possess not a joint probability distribution but conditional probabilities together with marginal probabilities.\nAs an example let’s consider a variation of our next-patient scenario one more time. The agent has background information \\(\\mathsfit{I}_{\\text{S}}\\) that provides the following set of probabilities:\n\nTwo conditional probability distributions  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) for transportation \\(T\\) given urgency \\(U\\), as reported in the following table:\n\n\n\n\nTable 17.1: Probability distributions for transportation given urgency\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\)\n\ntransportation at arrival  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\ngiven urgency  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U\\)\nurgent\n0.61\n0.22\n0.17\n\n\nnon-urgent\n0.21\n0.01\n0.78\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table has two probability distributions: on the first row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\); on the second row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\). Check that the probabilities on each row indeed sum up to 1.\n\n\n\n\n\nMarginal probability distribution  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\\) for urgency \\(U\\):\n\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.18 \\ ,\n\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.82\n\\tag{17.2}\\]\n\n\nWith this background information, the agent can also compute all joint probabilities simply using the and-rule. For instance, the joint probability for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) is\n\\[\n\\begin{aligned}\n&P(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}= 0.22 \\cdot 0.18 = \\boldsymbol{3.96\\%}\n\\end{aligned}\n\\]\nAnd from the joint probabilities, the marginal ones for transportation \\(T\\) can also be calculated. For instance\n\\[\n\\begin{aligned}\n&P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n0.22 \\cdot 0.18 +\n0.01 \\cdot 0.82\n\\\\[1ex]\n&\\quad{}= \\boldsymbol{4.78\\%}\n\\end{aligned}\n\\]\n\nNow suppose that the agent learns that the next patient is being transported by \\({\\small\\verb;helicopter;}\\), and needs to forecast whether \\({\\small\\verb;urgent;}\\) care will be needed. This inference is the conditional probability  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\), which can also be rewritten in terms of the conditional probabilities given initially:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\\\[2ex]\n&\\quad{}=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}{\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{0.0396}{0.0478}\n\\\\[2ex]\n&\\quad{}=\\boldsymbol{82.8\\%}\n\\end{aligned}\n\\]\nThis calculation was slightly more involved than the one in § 17.3, because in the present case the joint probabilities were not directly available. Our calculation involved the steps  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U \\enspace\\longrightarrow\\enspace T\\land U \\enspace\\longrightarrow\\enspace U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}T\\) .\n\nIn this same scenario, note that if the agent were instead interested, say, in forecasting the transportation means knowing that the next patient requires urgent care, then the relevant degree of belief  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) would be immediately available and no calculations would be needed.\n\n\nLet’s find the general formula for this case, where the agent’s background information is represented by conditional probabilities instead of joint probabilities.\nConsider a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). The conditional probabilities  \\(\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  and  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  are encoded in the agent from the start.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), given that the agent has learned that \\(\\color[RGB]{34,136,51}X \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.3}\\]\nIn the above formula we recognize Bayes’s theorem from § 9.4.\nThis formula is often exaggeratedly emphasized in the literature; some texts even present it as an “axiom” to be used in situations such as the present one. But we see that this formula is simply a by-product of the four fundamental rules of inference in a specific situation. An AI agent who knows the four fundamental inference rules, and doesn’t know what “Bayes’s theorem” is, will nevertheless arrive at this very formula.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-dens",
    "href": "conditional_probability.html#sec-conditional-dens",
    "title": "17  Conditional probability and learning",
    "section": "17.7 Conditional densities",
    "text": "17.7 Conditional densities\nThe discussion so far about conditional probabilities extends to conditional probability densities, in the usual way explained in §§15.3 and 16.2.\nIf \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous quantities, the notation\n\\[\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = {\\color[RGB]{68,119,170}q}\n\\]\nmeans that, given background information \\(\\mathsfit{I}\\) and given the sentence “\\(\\color[RGB]{34,136,51}X\\) has value between \\(\\color[RGB]{34,136,51}x-\\delta/2\\) and \\(\\color[RGB]{34,136,51}x+\\delta/2\\)”, the sentence “\\(\\color[RGB]{238,102,119}Y\\) has value between \\(\\color[RGB]{238,102,119}y-\\epsilon/2\\) and \\(\\color[RGB]{238,102,119}y+\\epsilon/2\\)” has probability \\({\\color[RGB]{68,119,170}q}\\cdot{\\color[RGB]{238,102,119}\\epsilon}\\), as long as \\(\\color[RGB]{34,136,51}\\delta\\) and \\(\\color[RGB]{238,102,119}\\epsilon\\) are small enough. Note that the small interval \\(\\color[RGB]{34,136,51}\\delta\\) for \\(\\color[RGB]{34,136,51}X\\) is not multiplied by the density \\(\\color[RGB]{68,119,170}q\\).\nThe relation between a conditional density and a joint density or a different conditional density is given by\n\\[\n\\begin{aligned}\n&\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon}\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\end{aligned}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) is the domain of \\(\\color[RGB]{238,102,119}Y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-repr-conditional",
    "href": "conditional_probability.html#sec-repr-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.8 Graphical representation of conditional probability distributions and densities",
    "text": "17.8 Graphical representation of conditional probability distributions and densities\nConditional probability distributions and densities can be plotted in all the ways discussed in chapters 15 and 16. If we have two quantities \\(A\\) and \\(B\\), often we want to compare the different conditional probability distributions for \\(A\\), conditional on different values of \\(B\\):\n\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;one-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;another-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\dotsc\\)\n\nand so on. This can be achieved by representing them by overlapping line plots, or side-by-side scatter plots, or similar ways.\n\nIn § 16.3 we saw that if we have the scatter plot for a joint probability density, then from its points we can often obtain a scatter plot for its marginal densities. Unfortunately no similar advantage exists for the conditional densities that can be obtained from a joint density. In theory, a conditional density for \\(Y\\), given that a quantity \\(X\\) has value in some small interval \\(\\delta\\) around \\(x\\), could be obtained by only considering scatter-plot points having \\(X\\) coordinate in a small interval between \\(x-\\delta/2\\) and \\(x+\\delta/2\\). But the number of such points is usually too small and the resulting scatter plot could be very misleading.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§5.4 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§§12.2.1, 12.3, and 12.5 of Artificial Intelligence\n§§4.1–4.3 in Sox & al.: Medical Decision Making\n§§5.1–5.5 of O’Hagan: Probability – yes, once more!",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_summary.html",
    "href": "conditional_summary.html",
    "title": "Learning and conditional probability: a summary",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\n\n\nThe previous chapter 17  Conditional probability and learning discussed many concepts that are important for what follows, and for artificial intelligence and machine learning in general. So let’s stop for a moment to emphasize and point out some things to keep in mind.\n\n\n\n What does it means that an agent has “learned”? It means that the agent has acquired new information, knowledge, or data. But this acquisition is not just some passive memory storage. As a result of this acquisition, the agent modifies its degrees of belief about any inferences it needs to draw, and consequently may make different decisions ([ch.@sec-basic-decisions]).\nThis change is an important aspect of learning. Think of when a person receives useful information; but the person, in actions or word, doesn’t seem to make use of it. We typically say “that person hasn’t learned anything”.\n\n\n\n\n The relation between the acquired knowledge and the change in beliefs is perfectly represented and quantified by conditional probabilities. These probabilities take account of the acquired information in their conditionals (the right side of the bar “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”). And the probability calculus automatically determines how to calculate the modified belief based on this new conditional.\nIn other words, the probability calculus already has everything we need to deal with and calculate “learning”. This is therefore the optimal, self-consistent way to deal with learning. We may use approximate versions of it in some situations, for instance when the computations would be too expensive. But we must keep in mind that such approximations are also deviations from optimality and self-consistency.\n\n\n\n\n The formula for conditional probability – that is, for the belief change corresponding to learning – involves and requires a joint distribution over several possibilities ([ch.@sec-prob-joint]). Therefore such distribution must be somehow built into the agent from the beginning, for the agent to be able to learn.\n\n\n\n\n The beliefs and behaviour arising from learning can be very different, depending on the context. For example, in some situations frequently observing a phenomenon may increase an agent’s belief in observing that phenomenon again; but in other situations such frequent observation may decrease an agent’s belief instead. Both kinds of behaviour can make sense in their specific circumstances.\nThese differences in behaviour are also encoded in the joint distribution built into the agent.\n\n\n\n\n The formula for belief change arising from learning is amazingly flexible and universal: it holds whether the agent is learning about different kinds of quantities or about past instances of similar quantities.\nFrom a machine-learning point of view, this must therefore be the formula underlying the use of “features” by a classifier, as well as its “training”.\nThis formula is moreover extremely simple in principle: it only involves addition and division! Computational difficulties arise from the huge amount of terms that may need to be added in specific data-science problems, not because of complicated mathematics. (A data engineer should keep this in mind, in case new hardware technologies may make it possible to deal with larger number of terms.)",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Learning and conditional probability: a summary]{.green}"
    ]
  },
  {
    "objectID": "information.html",
    "href": "information.html",
    "title": "18  Information, relevance, independence, association",
    "section": "",
    "text": "18.1 Independence of sentences\nIn an ordinary situation represented by background information \\(\\mathsfit{I}\\), if you have to infer whether a coin will land heads, then knowing that it is raining outside has no impact on your inference. The information about rain is irrelevant for your inference. In other words, your degree of belief about the coin remains the same if you include the information about rain in the conditional.\nIn probability notation, representing “The coin lands heads” with \\(\\mathsfit{H}\\), and “It rains outside” with \\(\\mathsfit{R}\\), this irrelevance is expressed by this equality:\n\\[\n\\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{R} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nMore generally two sentences \\(\\mathsfit{A}\\), \\(\\mathsfit{B}\\) are said to be mutually irrelevant or informationally independent given knowledge \\(\\mathsfit{I}\\) if any one of these three conditions holds:\nThese three conditions turn out to be equivalent to one another. In the first condition, \\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is undefined if \\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})=0\\), but in this case independence still holds; analogously in the second condition.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-indep-sentences",
    "href": "information.html#sec-indep-sentences",
    "title": "18  Information, relevance, independence, association",
    "section": "",
    "text": "“independEnt” is written with an E, not with an A.\n\n\\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n Irrelevance is not absolute and is not a physical notion\n\n\n\n\nIrrelevance or independence is not an absolute notion, but relative to some background knowledge. Two sentences may be independent given some background information, and not independent given another.\nIndependence as defined above is an informational or logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the sentences \\(\\mathsfit{A}\\) and \\(\\mathsfit{B}\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-indep-quantities",
    "href": "information.html#sec-indep-quantities",
    "title": "18  Information, relevance, independence, association",
    "section": "18.2 Independence of quantities",
    "text": "18.2 Independence of quantities\nThe notion of irrelevance of two sentences can be generalized to quantities. Take two quantities \\(X\\) and \\(Y\\). They are said to be mutually irrelevant or informationally independent given knowledge \\(\\mathsfit{I}\\) if any one of these three equivalent conditions holds for all possible values \\(x\\) of \\(X\\) and \\(y\\) of \\(Y\\):\n\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\n\nNote the difference between independence of two sentences and independence of two quantities. The latter independence involves not just two, but many sentences: as many as the combinations of values of \\(X\\) and \\(Y\\).\nIn fact it may happen that for some particular values \\(x^*\\) of \\(X\\) and \\(y^*\\) \\(Y\\) the probabilities become independent:\n\\[\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y^* \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nwhile at the same time this equality does not occur for other values. In this case the quantities \\(X\\) and \\(Y\\) are not independent given information \\(\\mathsfit{I}\\). The general idea is that two quantities are independent if knowledge about one of them cannot change an agent’s beliefs about the other, no matter what their values might be.\n\n\n\n\n\n\n Irrelevance is not absolute and not physical\n\n\n\n\nAlso in this case, irrelevance or independence is not an absolute notion, but relative to some background knowledge. Two quantities may be independent given some background information, and not independent given another.\nAlso in this case, independence is an informational or logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the quantities \\(X\\) and \\(Y\\). It’s simply stating that information about one quantity does not affect an agent’s beliefs about the other quantity.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider our familiar next-patient inference problem with quantities urgency \\(U\\) and transportation \\(T\\). Assume a different background information \\(\\mathsfit{J}\\) that leads to the following joint probability distribution:\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.15\n0.08\n0.02\n\n\nnon-urgent\n0.45\n0.04\n0.26\n\n\n\n\nCalculate the marginal probability distribution \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\) and the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare them. Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\) relevant for inferences about \\(U\\)? \nCalculate the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare it with the marginal \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\). Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) relevant for inferences about \\(U\\)? \nAre the quantities \\(U\\) and \\(T\\) independent, given the background knowledge \\(\\mathsfit{J}\\)?",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-info-uncertainty",
    "href": "information.html#sec-info-uncertainty",
    "title": "18  Information, relevance, independence, association",
    "section": "18.3 Information and uncertainty",
    "text": "18.3 Information and uncertainty\nThe definition of irrelevance given above appears to be very “black or white”: either two sentences or quantities are independent, or they aren’t. But in reality there is no such dichotomy. We can envisage some scenario \\(\\mathsfit{I}\\) where for instance the probabilities \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) and \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) are extremely close in value, although not exactly equal:\n\\[\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n= \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\delta(x,y)\n\\]\nwith \\(\\delta(x,y)\\) very small. This would mean that knowledge about \\(X\\) modifies an agent’s belief just a little. And depending on the situation such modification could be unimportant. In this situation the two quantities would be “independent” for all practical purposes. Therefore there really are degrees of relevance, rather than a dichotomy “relevant vs irrelevant”.\nThis suggests that we try to quantify such degrees. This quantification would also give a measure of how “important” a quantity can be for inferences about another quantity.\nThis is the domain of Information Theory, which would require a course by itself to be properly explored. In this chapter we shall just get an overview of the main ideas and notions of this theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nElements of Information Theory\n\n\n\n\nThe notion of “degree of relevance” is important in data science and machine learning, because it rigorously quantifies two related, intuitive ideas often occurring in these fields:\n\n“Correlation” or “association”: in its general meaning, it’s the idea that if an agent’s beliefs about some quantity change, then beliefs about another quantity may change as well.\n“Feature importance”: it’s the idea that knowledge about some aspects of a given problem may lead to improved inferences about other aspects.\n\nIn the next section we explore, through examples, some tricky aspects and peculiarities of these ideas. These examples also tell us which kind of properties a quantitative measure for “relevance” or “importance” or “informativeness” should possess.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-importance-scenarios",
    "href": "information.html#sec-importance-scenarios",
    "title": "18  Information, relevance, independence, association",
    "section": "18.4 Exploring “importance”: some scenarios",
    "text": "18.4 Exploring “importance”: some scenarios\nThe following examples are only meant to give an intuitive motivation of the notions presented later.\n\nFirst two required properties: a lottery scenario\nA lottery comprises 1 000 000 tickets numbered from 000000 to 999999. One of these tickets is the winner. Its number is already known, but unknown to you. You are allowed to choose any ticket you like (you can see the ticket numbers), before the winning number is announced.\nBefore you choose, you have the possibility of getting for free some clues about the winning number. The clues are these:\n\nClue A:       \n\nThe first four digits of the winning number.\n\nClue B:       \n\nThe 1st, 2nd, 3rd, and 5th digits of the winning number.\n\nClue C:       \n\nThe last three digits of the winning number.\n\n\nNow consider the following three “clue scenarios”.\n\nScenario 1: choose one clue\nYou have the possibility of choosing one of the three clues above. Which would you choose, in order of importance?\nObviously A or B are the most important, and equally important, because they increase your probability of winning from 1/1 000 000 to 1/100. C is the least important because it increases your probability of winning to 1/1000.\n\n\nScenario 2: discard one clue\nAll three clues are put in front of you (but you can’t see their digits). If you could keep all three, then you’d win for sure because they would give you all digits of the winning number.\nYou are instead asked to discard one of the three clues, keeping the remaining too. Which would you discard, in order of least importance?\nIf you discarded A, then B and C together would give you all digits of the winning number; so you would still win for sure. Analogously if you discarded B. If you discarded C, then A and B together would not give you the last digit; so you’d have a 1/10 probability of winning.\nObviously C is the most important clue to keep, and A and B are the least important.\n\n\nScenario 3: discard one more clue\nIn the previous Scenario 2, we saw that discarding A or B would not alter your 100% probability of winning. Either clue could therefore be said to have “importance = 0”.\nIf you had to discard both A and B, however, your situation would suddenly become worse, with only a 1/1000 probability of winning. Clues A and B together can therefore be said to have high “importance &gt; 0”.\n\nLet’s draw some conclusions by comparing the scenarios above.\n\nIn Scenario 1 we found that the “importance ranking” of the clues is\nA = B &gt; C\nwhereas in Scenario 2 we found the completely opposite ranking\nC &gt; A = B\nWe conclude:\n\n\n\n\n\n\nImportance is context-dependent\n\n\n\n\nIt doesn’t make sense to ask which aspect or feature is “most important” if we don’t specify the context of its use. Important if used alone? Important if used with others? and which others?\nDepending on the context, an importance ranking could be completely reversed. A quantitative measure of “importance” must therefore take the context into account.\n\n\n\n\nIn Scenario 3 we found that two clues may be completely unimportant if considered individually, but extremely important if considered jointly.\nWe conclude:\n\n\n\n\n\n\nImportance is non-additive\n\n\n\n\nA quantitative measure of importance cannot be additive, that is, it cannot quantify the importance of two or more features as the sum of their individual importance.\n\n\n\n\n\n\nThird required property: A two-quantity scenario\nSuppose we have a discrete quantity \\(X\\) with domain \\(\\set{1,2,3,4,5,6}\\) and another discrete quantity \\(Y\\) with domain \\(\\set{1,2,3,4}\\). We want to infer the value of \\(Y\\) after we are told the value of \\(X\\).\nThe conditional probabilities for \\(Y\\) given different values of \\(X\\) are as follows (each column sums up to \\(1\\))\n\n\n\nTable 18.1: Example conditional distribution for two discrete quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)\n\n  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\)\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n  \\(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.50\n\n\n2\n0.00\n1.00\n0.00\n0.00\n0.50\n0.00\n\n\n3\n0.00\n0.00\n1.00\n0.50\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.50\n0.50\n0.50\n\n\n\n\n\n\nLet’s see what kind of inferences could occur.\nIf we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\), then we know for sure that \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\). Similarly if we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2\\) or that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3\\). These three values of \\(X\\) are therefore “most informative” for inference about \\(Y\\). If we instead learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}4\\), then our uncertainty about \\(Y\\) is split between two of its values. Similarly if we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5\\) or that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}6\\). These three values of \\(X\\) are therefore “least informative” for inference about \\(Y\\).\nBut what if we want to quantify the importance of a quantity or feature like \\(X\\), and not just one specific value? What is the “overall importance” of \\(X\\)?\n\nConsider again three scenarios.\n\nIn the first, we have 33% probability each of learning one of the values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 99%. And 0.33% probability of learning any one of the remaining values, for a total of 1%. (Grand total 100%.)\nIn this scenario we expect to make an almost exact inference about \\(Y\\) after learning \\(X\\). The quantity \\(X\\) has therefore large “overall importance”.\nIn the second scenario the reverse occurs. We have 0.33% probability each of learning one of the values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 1%. And 33% probability of learning any one of the remaining values, for a total of 99%.\nIn this scenario we expect to be roughly equally uncertain between two values of \\(Y\\) after we learn \\(X\\). The quantity \\(X\\) has therefore lower “overall importance”.\nIn the third scenario, we have around 16.7% probability each of observing any one of the values of \\(X\\).\nThis scenario is in between the first two. We expect to make an exact inference about \\(Y\\) half of the time, and to be equally undecided between two values of \\(Y\\) half of the time. The quantity \\(X\\) has therefore some “overall importance”: not as low as in the second scenario, not as high as in the first scenario.\n\nWhat determines the “overall importance” of the quantity or feature \\(X\\) is therefore its probability distribution.\nWe conclude:\n\n\n\n\n\n\nThe importance of a quantity depends on its probability distribution\n\n\n\n\nThe importance of a quantity is not only determined by the relation between its possible values and what we need to infer, but also by the probability with which its values can occur.\nA quantitative measure of “importance” of a quantity must therefore take the probability distribution for that quantity into account.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-entropy-mutualinfo",
    "href": "information.html#sec-entropy-mutualinfo",
    "title": "18  Information, relevance, independence, association",
    "section": "18.5 Entropies and mutual information",
    "text": "18.5 Entropies and mutual information\nThe thought-experiments above suggest that a quantitative measure of the importance of a quantity must have at least these three properties:\n\nContext-dependence: take the context somehow into account.\nNon-additivity: do not calculate the importance of two quantities as the sum of their importance.\nProbability-awareness: take the probability distribution for the quantity into account.\n\nDo measures with such properties exist? They do. Indeed they are regularly used in Communication Theory and Information Theory, owing to the properties above. They even have international standards on their definitions and measurement units.\nBefore presenting them, let’s briefly present the mother of them all:\n\nShannon entropy\nConsider an agent with background knowledge \\(\\mathsfit{I}\\) and a belief distribution \\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) about the finite quantity \\(Y\\). The agent’s uncertainty about the value of \\(Y\\) can be quantified by the Shannon entropy:\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq-\\sum_y \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\;\\mathrm{Sh}\n\\]\nwhose unit is the shannon (symbol \\(\\mathrm{Sh}\\)) when the logarithm is in base 2, as above.1\n1 With the logarithm is in base 10, the unit is the hartley (\\(\\mathrm{Hart}\\)); with the natural logarithm, the unit is the natural unit of information (\\(\\mathrm{nat}\\)). \\(1\\,\\mathrm{Sh} \\approx 0.301\\,\\mathrm{Hart} \\approx 0.693\\,\\mathrm{nat}\\).Shannon entropy lies at the foundation of the whole fields of Information Theory and Communication Theory, and would require a lengthy discussion. Let’s just mention some of its properties and meanings:\n\nIt also quantifies the information that would be gained by the agent, if it learned the value of \\(Y\\).\nIt is always positive or zero.\nIt is zero if, and only if, the agent knows the value of \\(Y\\), that is, if the probability distribution for \\(Y\\) gives 100% to one value and 0% to all others.\nIts maximum possible value is \\(\\log_2 N\\;\\mathrm{Sh}\\), where \\(N\\) is the number of possible values of \\(Y\\). This maximum is attained by the uniform belief distribution about \\(Y\\).\nThe value in shannons of the Shannon entropy can be interpreted as the number of binary digits that we lack for correctly identifying the value of \\(Y\\), if the possible values were listed as integers in binary format. Alternatively, a Shannon entropy equal to  \\(h\\,\\mathrm{Sh}\\)  is equivalent to being equally uncertain among \\(2^h\\) possible alternatives.\n\nA Shannon entropy of 1 Sh quantifies the uncertainty of an agent that gives 50%/50% probability to two possibilities. An entropy of 3 Sh quantifies an equal 12.5% uncertainty among eight possibilities.\n\n\n Plot of the Shannon entropy for a binary quantity \\(Y\\in\\set{1,2}\\), for different distributions \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nAs a curiosity, an entropy of 0.5 Sh quantifies the uncertainty of an agent giving 89% probability to one possibility and 11% to another. So we can say that an 89%/11% belief distribution is half as uncertain as a 50%/50% one.\n\nHere are two useful measures of “informativeness” or “relevance” or “importance” of a quantity about another quantity. Both are based on the Shannon entropy:\n\n\nConditional entropy\nThe conditional entropy2 of a quantity \\(Y\\) conditional on a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\), is defined as\n2 or “equivocation” according to ISO standard.\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\coloneqq\n-\\sum_x \\sum_y\n\\mathrm{P}( X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\,\n\\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\;\\mathrm{Sh}\n\\]\n\nIt satisfies the three requirements above:\n\nContext-dependence\n\nDifferent background knowledge \\(\\mathsfit{I}\\), corresponding to different contexts, leads to different probabilities and therefore to different values of the conditional entropy.\n\nNon-additivity\n\nIf the quantity \\(X\\) can be split into two component quantities, then the conditional entropy conditional on them jointly is more than the sum of the conditional entropies conditional on them individually.\n\nProbability-awareness\n\nThe probability distribution for \\(X\\) appears explicitly in the definition of the conditional entropy.\n\n\nThe conditional entropy has additional, remarkable properties:\n\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is zero. Vice versa, if the conditional entropy is zero, then \\(Y\\) is a function of \\(X\\).\nIf knowledge of \\(X\\) is irrelevant, in the sense of § 18.2, to knowledge of \\(Y\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) takes on its maximal value, determined by the marginal probability for \\(Y\\). Vice versa, if the conditional entropy takes on its maximal value, then \\(X\\) is irrelevant to \\(Y\\).\nThe value in shannons of the conditional entropy has the same meaning as for the Shannon entropy: if the conditional entropy amounts to \\(h\\,\\mathrm{Sh}\\), then after learning \\(X\\) an agent’s uncertainty about \\(Y\\) is the same as if the agent were equally uncertain among \\(2^h\\) possible alternatives.\n\nFor instance, in the case of an agent with belief distribution of table  18.1, the conditional entropy has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_1) = 0.01\\,\\mathrm{Sh}\\) , almost zero. Indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case.\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_2) = 0.99\\,\\mathrm{Sh}\\) , almost 1. Indeed in this case the agent is approximately uncertain between two values of \\(Y\\).\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_3) = 0.5\\,\\mathrm{Sh}\\) . Indeed this case is intermediate between the previous two.\n\n\n\nMutual information\nSuppose that, according to background knowledge \\(\\mathsfit{I}\\), for any value of \\(X\\) there’s a 100% probability that \\(Y\\) has one and the same value, say \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\). The conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is then zero. In this case it is true that \\(Y\\) is formally a function of \\(X\\). But it is also true that we could perfectly predict \\(Y\\) without any knowledge of \\(X\\). Learning the value of \\(X\\) doesn’t really help an agent in forecasting \\(Y\\). In other words, \\(X\\) is not relevant for inference about \\(Y\\).3\n3 There’s no contradiction with the second remarkable property previously discussed: in this case the maximal value that the conditional entropy can take is zero.If we are interested in quantifying how much learning \\(X\\) “helped” in inferring \\(Y\\), we can subtract the conditional entropy for \\(Y\\) conditional on \\(X\\) from the maximum value it would have if \\(X\\) were not learned.\nThis is the definition of mutual information4 between a quantity \\(Y\\) and a quantity \\(X\\), given background knowledge \\(\\mathsfit{I}\\). It is defined as\n4 or “mean transinformation content” according to ISO standard.\n\\[\n\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\sum_x \\sum_y\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\n\\log_2 \\frac{\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})}{\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n} \\;\\mathrm{Sh}\n\\]\n\nIt also satisfies the three requirements – context-dependece, non-additivity, probability-awareness – for a measure of “informativeness” or “importance”. Its properties are somehow complementary to those of the conditional entropy:\n\nIf \\(Y\\) and \\(X\\) are informationally independent, in the sense of § 18.2, then their mutual information is zero. Vice versa, if their mutual information is zero, then these quantities are informationally independent.\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then their mutual information attains its maximal value (which could be zero). Vice versa, if their mutual information attains its maximal value, then \\(Y\\) is a function of \\(X\\).\nIf the mutual information between \\(Y\\) and \\(X\\) amounts to \\(h\\,\\mathrm{Sh}\\), then learning \\(X\\) reduces, on average, \\(2^h\\)-fold times the possibilities regarding the value of \\(Y\\).\nMutual information is symmetric in the roles of \\(X\\) and \\(Y\\), that is, \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{H}(X : Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\n\nIn the case of an agent with belief distribution as in table  18.1, the mutual information has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_1) = 1.61\\,\\mathrm{Sh}\\) , almost equal to the maximal value achievable in this scenario (\\(1.62\\,\\mathrm{Sh}\\)). Indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case. Since \\(2^{1.61}\\approx 2.1\\), learning \\(X\\) roughly halves the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_2) = 0.81\\,\\mathrm{Sh}\\) ; this means that learning \\(X\\) reduces by \\(2^{0.81}\\approx 1.8\\) or almost 2 times the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_3) = 1.5\\,\\mathrm{Sh}\\) ; learning \\(X\\) reduces by \\(2^{1.5} \\approx 2.8\\) or almost 3 times the number of possible values of \\(Y\\).\n\n\n\nUses\nLet’s emphasize that Shannon entropy, conditional entropy, and mutual information are not just fancy theoretical ways of quantifying uncertainty and informativeness. Their numerical values have concrete technological importance; for instance they determine the maximal communication speed of a communication channel. See references on the margin for concrete applications.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nProbability and Information Theory, with Applications to Radar\n\n\n\n\nWhether to use the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) or the mutual information \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) depends on the question we are asking.\nConditional entropy is the right choice if we want to quantify to what degree \\(Y\\) can be considered a function of \\(X\\) – including the special case of a constant function. It is also the right choice if we want to know how many binary-search iterations it would take to find \\(Y\\), on average, once \\(X\\) is learned.\nMutual information is the right choice if we want to quantify how much learning \\(X\\) helps, on average, for inferring \\(Y\\). Or equivalently how many additional binary-search iterations it would take to find \\(Y\\), if \\(X\\) were not known. Mutual information is therefore useful for quantifying “correlation” or “association” of two quantities.\nIf we simply want to rank the relative importance of alternative quantities \\(X_1\\), \\(X_2\\), etc. in inferring \\(Y\\), then conditional entropy and mutual information are equivalent in the sense that they yield the same ranking, since they basically differ by a zero point that would be constant in this scenario.\n\n\n\n\n\n\n Mutual information is superior to the correlation coefficient\n\n\n\nThe Pearson correlation coefficient is actually a very poor measure of correlation or association. It is more a measure of “linearity” than correlation. It can be very dangerous to rely on in data-science problems, where we can expect non-linearity and peculiar associations in large-dimensional data. The Pearson correlation coefficient is widely used not because it’s good, but because of (1) computational easiness, (2) intellectual inertia.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nChapter 8 of MacKay: Information Theory, Inference, and Learning Algorithms\n§12.4 of Artificial Intelligence",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-utility-importance",
    "href": "information.html#sec-utility-importance",
    "title": "18  Information, relevance, independence, association",
    "section": "18.6 Utility Theory to quantify relevance and importance",
    "text": "18.6 Utility Theory to quantify relevance and importance\nThe entropy-based measures discussed in the previous section originate from, and have deep connections with, the problem of repeated communication or signal transmission. They do not require anything else beside joint probabilities.\nIn a general decision problem – where an agent has probabilities and utilities – another approach may be required, however.\nConsider questions like “What happens if I discard quantity \\(X\\) in this inference?” or “If I have to choose between learning either quantity \\(U\\) or quantity \\(V\\), which one should I choose?“.  Such questions are decision-making problems. They must therefore be solved using Decision Theory (this is an example of the recursive capabilities of Decision Theory, discussed in § 2.4). The application of decision theory in these situations if often intuitively understandable. For example, if we need to rank the importance of quantities \\(U\\) and \\(V\\), we can calculate how much the expected utility would decrease if we discarded the one or the other.\nWe’ll come back to these questions in chapters 39 and 40.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "connection-3-ML.html",
    "href": "connection-3-ML.html",
    "title": "19  Third connection with machine learning",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\n\n\nIn chapter  11 we made a second tentative connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some built-in background information (corresponding to the algorithm’s architecture), has received pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance), and is assessing a not-previously known piece of information (other partial aspects of a new task instance):\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThe correspondence about training data and architecture seems somewhat convincing, the one about outcome needs more exploration.\nHaving introduced the notion of quantity in the latest chapters 12 and 13, we recognize that “training data” are just quantities, the values of which the agent has learned. So a datum \\(\\mathsfit{D}_i\\) can be expressed by a sentence like \\(Z_i\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_i\\), where\n\n\\(i\\) is the instance: \\(1,2,\\dotsc,N, N+1\\).\n\\(Z_i\\), a quantity, describes the type of data at instance \\(i\\), for example “128 × 128 image with 24-bit colour depth, with a character label”.\n\\(z_i\\) is the value of the quantity \\(Z_i\\) at instance \\(i\\), for example the specific image & label displayed here:\n\n\n\n\n\n\nlabel = “Saitama”\n\n\nWe can therefore rewrite the correspondence above as follows:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{ Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_2 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis is the kind of inference that we explored in the “next-three-patients” scenario of § 17.4 and in some of the subsequent sections. In chapter  27, after a review of conventional machine-learning methods and terminology, we shall discuss with more care what these inferences are about, what kind of information they use, and how they can be concretely calculated.\n\nIn the last sections we have often been speaking about “instances”, “instances of similar quantities”, “task instances”, and similar expression. What do with mean with “instance”, more exactly? It is time that we make this and related notions more precise: the whole idea of “learning from examples” hinges on them. In the next few chapters we shall therefore make these ideas more rigorous and quantifiable. Statistics is the theory that deals with these ideas. As a bonus we shall find out that a rigorous analysis of the notion of “instances” also leads to concrete formulae for calculating the kind of probabilities discussed in the present chapter.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>[Third connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html",
    "href": "populations_variates.html",
    "title": "20  Populations and variates",
    "section": "",
    "text": "20.1 Collections of similar quantities: motivation\nIn the latest chapters we gradually narrowed our focus on a particular kind of inferences: inferences that involve collections of similar quantities, each of which can be simple, joint, or complex. “Similar” means that all these quantities have a similar meaning and measurement procedure, and therefore have the same domain. For instance, each quantity might have possible values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\); or possible values between \\(0\\,\\mathrm{\\textcelsius}\\) and \\(100\\,\\mathrm{\\textcelsius}\\). These quantities can be considered different “instances” of the same quantity, so to speak. We saw an example in the three-patient hospital scenario of § 17.4, with the three “urgency” quantities \\(U_1\\), \\(U_2\\), \\(U_3\\), corresponding to the urgency of three consecutive patients. Here are other examples:\nIt is easy to think of many other and very diverse examples, with even more complex variates, such as images or words. We shall now try to abstract and generalize this similarity.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-collections",
    "href": "populations_variates.html#sec-collections",
    "title": "20  Populations and variates",
    "section": "",
    "text": "Stock exchange\n\nWe are interested in the daily change in closing price of a stock, during 1000 days. Each day the change can be positive (or zero), or negative.\n\n\nThe daily change on any day can be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}}, {\\color[RGB]{204,187,68}{\\small\\verb;-;}}}\\). The daily changes in 1000 days are a set of 1000 binary quantities with exactly the same domain; but note that each one can have a different value.\n\n\n\n\n\n\n\nMars prospecting\n\nSome robot examines 1000 similar-sized rocks in a large crater on Mars. Each rock either contains haematite, or it doesn’t.\n\n\nThe haematite-content of any rock can be considered as a binary quantity, say with domain \\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;Y;}}, {\\color[RGB]{238,102,119}{\\small\\verb;N;}}}\\). The haematite contents of the 1000 rocks are a set of 1000 binary quantities with exactly the same domain; note again that each one can have a different value.\n\n\n\n\n\n\n\nGlass forensics\n\nA criminal forensics department has 215 glass fragments collected from many different crime scenes. Each fragment is characterized by a refractive index (between \\(1\\) and \\(\\infty\\)), a percentage of Calcium (between \\(0\\%\\) and \\(100\\%\\)), a percentage of Silicon (ditto), and a type of origin (for example “from window of building”, “from window of car”, and similar).\n\n\nThe refractive index, Calcium percentage, Silicon percentage, and type of origin of one fragment constitute a joint quantity, having a joint domain. The refractive index, Calcium percentage, Silicon percentage, and type of origin of the 215 fragments are a set of 215 joint quantities, having identical domains.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-variates-populations",
    "href": "populations_variates.html#sec-variates-populations",
    "title": "20  Populations and variates",
    "section": "20.2 Units, variates, statistical populations",
    "text": "20.2 Units, variates, statistical populations\nConsider a large collection of entities that are somehow similar to one another, as in the preceding examples. We shall call these entities units. Units could be, for instance:\n\nphysical objects such as cars, windmills, planets, or rocks from a particular place;\ncreatures such as animals of a particular species, or human beings, maybe with something in common such as geographical region; or plants of a particular kind;\nautomatons having a particular application;\nsoftware objects such as images;\nabstract objects such as functions or graphs;\nthe rolls of a particular die or the tosses of a particular coin;\nthe weather conditions on several different days.\n\nThese units are similar to one another in that they have some set of attributes1 common to all. These attributes can present themselves in a specific number of mutually-exclusive guises. For instance, the attributes could be:\n1 The term features is frequently used in machine learning\n“colour”, each unit being, say, green, blue, or yellow;\n“mass”, each unit having a mass between \\(0.1\\,\\mathrm{kg}\\) and \\(10\\,\\mathrm{kg}\\);\n“health condition”, each unit (an animal or human in this case) being healthy or ill; or maybe being affected by one of a specific set of diseases;\ncontaining something, for instance a particular chemical substance;\n“having a label”, each unit having one of the labels A, B, C;\na complex combination of several simpler attributes like the ones above.\n\nThe units may also have additional attributes which we simply don’t consider or can’t measure.\n\nFrom the definition above it’s clear that the attributes of each unit are a quantity, as defined in § 12.1.1; often a joint quantity. Once the units and their attributes are specified, we have a set of as many quantities as there are units. All these quantities have identical domains.\nWe call variate the collection of all similar quantities of all the units. When we speak about a “variate”, it is understood that there is some set of units, each having a similar quantity.\nNote the difference between a variate and a quantity. For example, suppose we have three patients A, B, C, and we consider their health condition, which can be healthy or ill. Then “health condition” is a variate, while “the health condition of patient B” is a quantity. There’s a difference because the sentence “the health condition is ill” cannot be said to be true or false, while the sentence “the health condition of patient B is ill” can. If I ask you “is the health condition healthy? or ill?”, you’ll ask me “the health condition of which patient?”.\n\nWe call a collection of units characterized by a variate, as discussed above, a statistical population, or just population when there’s no ambiguity. The number of units is called the size of the population.\nThe notion of statistical population is extremely general. Many different things and collections can be thought of as a statistical population. When we speak of “data”, what we often mean, more precisely, is a particular statistical population.\nThe specification of a population requires precision, especially when it is used to draw inferences, as we shall see later. A statistical population has not been properly specified until two things are precisely specified:\n\nA way to determine whether something is a unit or not: inclusion and exclusion criteria, means of collection, and so on.\nA definition of the variate considered, its possible values, and how it is measured.\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWhich of the following descriptions does properly define a statistical population? explain why it does or does not.\n\nPeople.\nElectronic components produced in a specific assembly line, since the line became operational until its discontinuation, and measured for their electric resistance, with possible values in \\([0\\,\\mathrm{\\Omega}, \\infty\\,\\mathrm{\\Omega}]\\), and for their result on a shock test, with possible values \\(\\set{{\\small\\verb;pass;}, {\\small\\verb;fail;}}\\).\nPeople born in Norway between 1st January 1990 and 31st December 2010.\nThe words contained in all websites of the internet.\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater (as defined by contours on a map), and tested to contain haematite, with possible values \\(\\set{{\\small\\verb;Y;}, {\\small\\verb;N;}}\\).\n\nBrowse some datasets at the UC Irvine Machine Learning repository. Each dataset is a statistical population. The variate in most of these populations is a joint variate (to be discussed below), that is, a collection of several variates.\nExamine and discuss the specification of some of those datasets:\n\nIs it well-specified what constitutes a “unit”? Are the criteria for including or excluding datapoints, their origin, and so on, well explained?\nAre the variates well-defined? Is it explained what they mean, how they were measured, what is their domain, and so on?\n\n\n\n\n\n\n\n\n\n\n\n\n Subtleties in the notion of statistical population\n\n\n\n\nA statistical population is only a conceptual device for simplifying and facing some decision or inference problem. There is no objectively-defined population “out there”.\nAny entity, object, person, and so on has some characteristics that makes it completely unique (say, its space-time coordinates). Otherwise we wouldn’t be able to distinguish it from other entities. From this point of view any entity is just a one-member population in itself. If we consider two or more entities as being “similar” and belonging to the same population, it’s because we have decided to disregard some characteristics of these entities, and only focus on some other characteristics. This decision is arbitrary, a matter of convention, and depends on the specific inference and decision problem.\nTo test whether an entity belongs to a given population, we have to check whether that entity satisfies the agreed-upon definition of that population.\nAny physical entity, object, person, etc. can be a “unit” in very different and even statistical populations. For instance, a 100 cm³ rock found in the Schiaparelli crater on Mars could be a unit in these populations:\nA. Rocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and tested for haematite\nB. Rocks, of volume between 10 cm³ and 200 cm³, found in the Schiaparelli crater and tested for haematite\nC. Rocks, of volume between 10 cm³ and 200 m³, found in any crater on any planet of the solar system, and tested for haematite\nD. Rocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and measured for the magnitude of their magnetic field.\nNote the following differences. Populations A, B, C above have the same variate but differ in their definition of “unit”. Populations A and D have the same definition of unit but different variates. Population B is a subset of population A: they have the same variate, and any unit in B is also a unit in A; but not every unit in A is also a unit in B. Populations A and C have some overlap: they have the same variate, and some units of A are also units of C, and vice versa.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-joint-variates",
    "href": "populations_variates.html#sec-joint-variates",
    "title": "20  Populations and variates",
    "section": "20.3 Populations with joint variates",
    "text": "20.3 Populations with joint variates\nThe quantity associated with each unit of a statistical population can be of arbitrary complexity. In particular it could be a joint quantity (§ 13.1), that is, a collection of quantities of a simpler type.\nWe saw an example at the beginning of this chapter, with a population relevant for glass forensics. The statistical population was defined as follows:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of four variates of a simple kind:\n\n\\(\\mathit{R}\\)efractive \\(\\mathit{I}\\)ndex of the glass fragment (interval continuous variate), with domain from \\(1\\) (included) to \\(+\\infty\\)\nweight percent of \\(\\mathit{Ca}\\)lcium in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\nweight percent of \\(\\mathit{Si}\\)licon in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed, vehicle_windows_non_float_processed, containers, tableware, headlamps\n\n\nHere is a table with the values of the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) for ten units:\n\n\n\nTable 20.1: Glass fragments\n\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n\n\n\nThe variate value for unit 4, for instance, is\n\\[\n\\mathit{RI}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1.52247 \\land\n\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}9.76 \\land\n\\mathit{Si}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}70.26 \\land\n\\mathit{Type}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemember the difference between variate and quantity, discussed previously. Consider the population of glass fragments introduced above, and suppose I say “\\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8.1\\)”. Can you check if what I said is true? No, because you don’t know which unit I’m referring to.\nThe variate for a specific unit is a quantity instead. We can indicate this by appending the unit label to the variate symbol, as we did with “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\)” above. If I tell you “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8\\)”, you can check that what i said is false; therefore \\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\) is a quantity.\nThe units’ IDs don’t need to be consecutive numbers; in fact they don’t even need to be numbers: any label that completely distinguishes all units will do.\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload the dataset2 income_data_nominal_nomissing.csv (4 MB):\n\nHow many variates does this population have?\nWhat types of variate (binary, nominal, etc.) do they seem to be?\nWhat are their domains?\n\nExplore datasets from the UC Irvine Machine Learning Repository, answering the three questions above.\n\n\n\n\n\n2 This is an adapted version of the UCI “adult-income” dataset",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "21  Statistics",
    "section": "",
    "text": "21.1 What’s the difference between Probability Theory and Statistics?\n“Probability theory” and “statistics” are often mentioned together. We shall soon see why, and what are the relationship between them. But first let’s try to define them more precisely:\nThere are clear and crucial differences between the two:\nMany texts do not clearly distinguish between probability and statistics. The distinction is important for us because we will have to solve problems involving the uncertainty about particular statistics, so the two must be kept clearly separate. This distinction was observed by James Clerk Maxwell who used it to develop the theories of statistical mechanics and kinetic theory.\nIn many concrete problems, however, probability theory and statistics do go hand in hand and interact. This happens mainly in two ways:\nLet’s now discuss some important statistics.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-diff-prob-stat",
    "href": "statistics.html#sec-diff-prob-stat",
    "title": "21  Statistics",
    "section": "",
    "text": "Probability theory\n\nis the theory that describes and norms the quantification and propagation of uncertainty, as we saw in § 8.1.\n\nStatistics\n\nis the study of collective properties of the variates of populations or, more generally, of collections of data.\n\n\n\n\nThe fact that we are uncertain about something doesn’t mean that there are populations or replicas involved. We can apply probability theory in situations that don’t involve any statistics.\nIf we have full information about a population – the value of each variate for each unit – then we can calculate summaries and other properties of the variates. And there’s no uncertainty involved: at all times we can exactly calculate any information we like about any variates. So we do statistics, but probability theory plays no role.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMaxwell explains the statistical method and its use in the molecular description of matter:\n\nIntroductory Lecture on Experimental Physics\nMolecules\n\n\n\n\n\nThe statistics of a population give information that can be used in the conditional of an inference.\nWe want to draw inferences about some statistics of a population, whose values we don’t know.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-freq-distr",
    "href": "statistics.html#sec-freq-distr",
    "title": "21  Statistics",
    "section": "21.2 Frequencies and frequency distributions",
    "text": "21.2 Frequencies and frequency distributions\nConsider a statistical population of \\(N\\) units, with a variate \\(X\\) having a finite set of \\(K\\) values as domain. To keep things simple let’s just say these values are \\(\\set{1, 2, \\dotsc, K}\\) (without any ordering implied). Our discussion applies for any finite set. The variate \\(X\\) could be of any non-continuous type: nominal, ordinal, interval, binary (§ 12.2), or of a joint or complex type (§ 13). Let’s denote the variate associated with unit \\(i\\) by \\(X_i\\). For instance, we express that unit #3 has \\(X\\)-variate value \\(5\\) and unit #7 has \\(X\\)-variate value \\(1\\) by writing\n\\[\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\land X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\quad\\text{\\small or equivalently}\\quad\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\]\nFor each value \\(a\\) in the domain of the variate \\(X\\), we count how many units have that particular value. Let’s call the number we find \\(n_a\\). This is the absolute frequency of the value \\(a\\) in this population. Obviously \\(n_a\\) must be an integer between \\(0\\) (included) and \\(K\\) (included). The set of absolute frequencies of all values is called the absolute frequency distribution of the variate in the population. We must have\n\\[\\sum_{a=1}^K n_a = N \\ .\\]\nIt is often useful to give the fraction of counts with respect to the population size, which we denote by \\(f_a\\):\n\\[f_a \\coloneqq n_a/N\\]\nThis is called the relative frequency of the value \\(a\\). Obviously \\(0 \\le f_a \\le 1\\). The collection of relative frequencies for all values, \\(\\set{f_1, f_2, \\dotsc, f_K}\\), satisfies\n\\[\\sum_{a=1}^K f_a = 1 \\ .\\]\nWe call this collection of relative frequencies the relative frequency distribution. We shall denote it with the boldface symbol \\(\\boldsymbol{f}\\) (boldface indicates that it is a tuple of numbers):\n\\(\\boldsymbol{f} \\coloneqq(f_1, f_2, \\dotsc, f_K)\\)\nwith an analogous convention if other letters are used instead of “\\(f\\)”.\n\n\n\n\n\n\n \n\n\n\nIn the following we shall call relative frequencies simply “frequencies”, and explicitly use the word “absolute” when we speak about absolute frequencies.\n\n\n\nThe frequency distribution of values in a population does not give us full information about the population, because it doesn’t tell which unit has which value. In many situations, however, the frequencies are all we need to know, or all we can hope to know.\nFrequencies and frequency distributions are quantities in the technical sense of § 12.1.1. In fact we can say, for instance, “The frequency of the value C is 0.3”, or “The frequency distribution for the values A, B, C is \\((0.2, 0.7, 0.1)\\)”. We shall denote the quantity, as separate from its value, by the corresponding capital letter, for example \\(F_1\\), so that we can write sentences about frequencies in our usual abbreviated form. For instance\n\\[\nF_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}f_3\n\\]\nmeans “The frequency of the variate value \\(3\\) is equal to \\(f_3\\)”, where \\(f_3\\) must be a specific number.\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the statistical population defined as follows:\n\nunits: the bookings at a specific hotel during a specific time period\nvariate: the market segment of the booking\nvariate domain: the set of five values \\(\\set{{\\small\\verb;Aviation;}, {\\small\\verb;Complementary;},  {\\small\\verb;Corporate;}, {\\small\\verb;Offline;},  {\\small\\verb;Online;}}\\)\n\nThe population data is stored in the file hotel_bookings-market.csv. Each row of the file corresponds to a unit, and lists the unit id (this is not a variate in the present population) and the market segment.\nUse any method you like (a script in your favourite programming language, counting by hand, or whatever) to answer these questions:\n\nWhat is the size of the population?\nWhat are the absolute frequencies of the five values?\nWhat are their relative frequencies?\nWhich units have the value \\({\\small\\verb;Corporate;}\\)?\n\n\n\n\nDifferences between frequencies and probabilities\nThe fact that frequencies are non-negative and sum up to 1 makes them somewhat similar to probabilities, from a purely numerical point of view. The two notions, however, are completely different and have different uses. Here is a list of some important differences:\n\n\nNot few works in machine learning tend to call “probabilities” any set of positive numbers that sum up to one. Be careful when reading them. Mentally replace probability with degree of belief and see if the text mentioning “probabilities” still makes sense.\n\n\nA probability expresses a degree of belief.\nA frequency is the count of how many times something occurs.\n\n\n\n\nThe probability of a sentence depends on an agent’s state of knowledge and background information. Two agents can assign different probabilities to the same sentence.\nThe frequency of a value in a population is an objective physical quantity. All agents agree on the frequency (if they have the possibility of counting the occurrences).\n\n\n\n\nProbabilities refer to sentences.\nFrequencies refer to values in a population, not to sentences. \n\n\n\n\nA probability can refer to a specific unit in a population. An agent can consider, for instance, the probability that a variate for unit #7 has value 3.\nA frequency cannot refer to a specific unit in a population. It is meaningless to “count how many times the value 3 appears in unit #7”.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-joint-freq",
    "href": "statistics.html#sec-joint-freq",
    "title": "21  Statistics",
    "section": "21.3 Joint frequencies",
    "text": "21.3 Joint frequencies\nConsider the following population consisting of ten units with joint variate \\((\\mathit{age}, \\mathit{race}, \\mathit{sex}, \\mathit{income})\\), whose component variates have the following properties:\n\n\\(\\mathit{age}\\):   interval discrete with domain \\(\\set{17, 18, \\dotsc, 90+}\\)\n\\(\\mathit{race}\\):   nominal with domain \\(\\set{{\\small\\verb;Amer-Indian-Eskimo;}, {\\small\\verb;Asian-Pac-Islander;} , {\\small\\verb;Black;}, {\\small\\verb;Other;}, {\\small\\verb;White;}}\\)\n\\(\\mathit{sex}\\):   binary with domain \\(\\set{{\\small\\verb;F;}, {\\small\\verb;M;}}\\)\n\\(\\mathit{income}\\):   binary with domain \\(\\set{{\\small\\verb;`&lt;=50K';}, {\\small\\verb;`&gt;50K';}}\\)\n\n\n\n\nTable 21.1: Income\n\n\n\n\n\n\\(\\mathit{age}\\)\n\\(\\mathit{race}\\)\n\\(\\mathit{sex}\\)\n\\(\\mathit{income}\\)\n\n\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n26\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;Amer-Indian-Eskimo;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n\n\n\n\nThe joint frequency distribution for the joint variate of the population above gives the frequencies of all possible joint variate values, for instance the value\n\\(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\\)\nIn this population, most joint values appear each only once, and the remaining values never appear; this is because of the population’s small size and the large number of possible variate values. A couple of joint values appear twice. We have for example\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;M;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{2}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Amer-Indian-Eskimo;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = 0\n\\end{aligned}\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to write a function that takes as input a dataset with a small number of variates and outputs the joint frequency distribution for all combinations of variate values. The best output format is a multidimensional array having one dimension per variate, and for each dimension a length equal to the number of possible values of that variate. The value of the array in each cell is the corresponding frequency.\nFor instance, consider the case of the income dataset above but without the age variate. The output of the function would then be an array with \\(5 \\times 2 \\times 2\\) dimensions",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-marginal-freq",
    "href": "statistics.html#sec-marginal-freq",
    "title": "21  Statistics",
    "section": "21.4 Marginal frequencies",
    "text": "21.4 Marginal frequencies\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the population of the example above, for instance, we might be interested only in the \\(\\mathit{age}\\) and \\(\\mathit{income}\\) variates. These two variates together are then called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes.\n\n\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\), we define the marginal frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\) as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the marginal population with the variate \\({\\color[RGB]{238,102,119}Y}\\) alone. This frequency is simply written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\n\\]\nA conditional frequency can be calculated as the sum of the joint frequencies for all values \\({\\color[RGB]{34,136,51}x}\\), in a way analogous to marginal probabilities (§ 16.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}) = \\sum_{\\color[RGB]{34,136,51}x} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nFor example, if from the population of table  21.1 we consider the marginal population with variates \\((\\mathit{age}, \\mathit{income})\\), some of the marginal frequencies are\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{3}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}26 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{3}{10}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDownload again the dataset income_data_nominal_nomissing.csv:\n\nCalculate the marginal frequencies of some of its variates.\nDoes any variate have a value appearing with marginal absolute frequency equal to 1?",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-summary-stat",
    "href": "statistics.html#sec-summary-stat",
    "title": "21  Statistics",
    "section": "21.5 Summary statistics",
    "text": "21.5 Summary statistics\nIn communicating statistics about a population it is always best to report and, when possible, visually show (for instance as marginal distributions) the full joint frequency distribution of the population’s variates.\nSometimes one wants to share some sort of “summary” of the frequency distribution, emphasizing particular aspects of it; because these are also aspects of the population. Different kinds of aspects can be chosen; some of them are only defined for specific types of variates. They are often called “summary statistics” or “descriptive statistics”. Below we give a brief description of some common ones, emphasizing when they are appropriate and when they are not. These summaries can also be used for probability distributions.\n\nMode\nThe mode is the value having the highest frequency (or probability, if we’re speaking about an agent’s beliefs rather than a population). There can be more than one mode.\nThe mode is defined for any distribution over discrete values, also for nominal quantities.\n\n\n\n\n\n\n\n\n\n\nBe careful in relying too much on the “mode” for a continuous quantity. Continuous quantities can be transformed in a one-to-one way into other, equivalent ones; and such a transformation also give the equivalent frequency or probability density for the new quantity. There is no general relationship between the modes of the densities for the two equivalent quantities. In fact, the density for one quantity can have one mode, whereas the density for the equivalent quantity can have no mode, or many modes. This is true for all kinds of distributions represented by densities, for example a continuous distribution of energy.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSome paradoxes, errors, and resolutions concerning the spectral optimization of human vision\n\n\n\n\nMedian and quartiles\nRecall (§ 12.2) that an ordinal or an interval quantity or variate have values that can be ranked in a specific order. If there is a value for which the sum of the frequencies of all values of rank lower than that value equals the sum of the frequencies of all values of rank higher than that value, then that value is called the median of the distribution. See the following histogram as an example:\n The value \\({\\small\\verb;e;}\\) is the median of this frequency distribution, because \\(f({\\small\\verb;c;})+f({\\small\\verb;d;}) = f({\\small\\verb;f;})+f({\\small\\verb;g;})+f({\\small\\verb;h;})+f({\\small\\verb;i;})= 47.6\\%\\)\nIf there is no such separating value, then sightly different definitions of median exist in the literature; but the approximate idea is the same: a value that somehow divides the domain into two parts of roughly equal (50%) total frequency. This idea can be also applied to continuous distributions represented by densities.\nThe notion of median can be generalized to that of a value that separates the domain into a lower-rank part with total frequency 1/4, and a higher-rank part with total frequency 3/4; and also to that of a value separating into a 3/4 vs 1/4 proportion instead. These values are called the first quartile and third quartile. The two quartiles and the median (also called second quartile) divide the domain into four parts of roughly equal 25% frequencies.\nIf the variate or quantity under consideration is of interval type, then it’s possible to take the difference between the third and first quartile, called the interquartile range.\n\n\nMean and standard deviation\nFor an interval quantity \\(X\\) with values \\(\\set{x_1, x_2, \\dotsc}\\) for which it makes sense to take the sum, it is possible to define the mean and standard deviation:\n\\[\n\\bar{X} \\coloneqq\\sum_i x_i\\cdot f(x_i)\n\\qquad\n\\sigma(X) \\coloneqq\\sqrt{\\sum_i (x_i-\\bar{X})^2\\cdot f(x_i)}\n\\]\nwe assume that their meaning is more or less familiar to you.\n\n\nUses and pitfalls\nFor a nominal variate or quantity it doesn’t make sense to speak of median, quartiles, mean, standard deviation, because its possible values cannot be ranked or added.\nFor an ordinal variate or quantity it doesn’t make sense to speak of mean or standard deviation, because its possible values cannot be added.\n\n\nThe mean and standard deviation can make sense and can be useful in some circumstances. But note that even if the values of a quantity can be summed, their mean (and standard deviation) may not quite make sense.\nConsider the number of patients visiting a hospital in 100 consecutive days. It is possible to consider the mean number of patients per day. This number has a meaning: if this number of patients visited the hospital every day for 100 days, then the total number of visits would be equal to the actual total. The same reasoning can be made for the number of nurses working in the hospital every day for 100 days, and their mean.\nNow consider the daily ratios of patients to nurses, for those 100 days. These ratios are numbers, so we can take their mean. But what does such a mean represent? if we multiply it by 100, we don’t obtain the total of anything. Also, if we consider the total number of patients and total number of nurses in 100 days, their ratio will not be equal to the “mean ratio” we calculated.\nThe example above is not meant to say that a mean of ratios never makes sense, but to point out that mean and standard deviation are often overused. In chapter  23 we will discuss other problems that may arise in using mean and standard deviation.\nIn general, when in doubt, we recommend to use median and quartiles or median and interquartile range, which are more generally meaningful and enjoy several other properties (for example so-called “robustness”) useful in doing statistics.\n\n\nNote, in any case, that the present discussion regards the question of how to provide summary information besides the full frequency (or probability) distribution. If our problem is to choose one value out of the possible ones, then that’s a decision-making problem, which must be solved by specifying utilities and maximizing the expected utility, as preliminary discussed in chapter  2 and as will be discussed more in detail towards the final chapters.\n\n\n\n\n\n\n Study reading\n\n\n\n\n§2.6 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§ “The median estimate” of Cox & O’Hagan 2022: Meaningful expression of uncertainty in measurement\nGould 1985/2013: The Median Isn’t the Message",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-outliers",
    "href": "statistics.html#sec-outliers",
    "title": "21  Statistics",
    "section": "21.6 Outliers vs out-of-population units",
    "text": "21.6 Outliers vs out-of-population units\nThe term “outlier” frequently appears in problems related to statistics and probability, often in conjunction with some summary statistics described above. Unfortunately the definitions of this term can be confusing or misleading. With the notion of outlier often there also comes a barrage of “methods” or rules meant to “deal” with outliers. Some such rules, for instance the rule of discarding any datapoints lying at more than three standard deviations from the mean, are often mindless and dangerous.\nSo let’s avoid the term “outlier” for the moment, and let’s take a different perspective.\n\n\nOne reason why we consider a population of units is that we are interested in making inferences about some units in this population, for which we lack the values of some variates. As we shall see in the forthcoming chapters, such inferences can be made if we first try to infer the full joint frequency distribution for the variates of the population of interest.\nThis kind of inference becomes more difficult if we have reckoned into the population some units that actually don’t belong there.\nSuppose for instance that a hospital is interested in the age of female patients admitted in a year. In collecting data, some male patients are counted in. Then obviously the age frequencies obtained from the collected data will not reflect the age frequencies among females. The problem is that some out-of-population units have been counted in by mistake.\nThe way out-of-population units affect and distort the frequency count can be different from problem to problem.\nIn our example, suppose that the wast majority of female patients could have age between 45–55 years, and that the male patients erroneously counted in also have age in the same range. Then the bulk of the frequency distribution will appear more inflated than it should be. Or suppose instead that the male patients erroneously reckoned have age between 80–90 years. In this case the old-age tail of the distribution will appear more inflated. As you see we can’t a priori point to any “tail” or “bulk” as a problem.\nLow frequencies are relatively affected by out-of-population units more than high frequencies. Suppose 10 female patients out of 100 have age 52; frequency 10%. If one 52-year-old male patient is now included by mistake, the frequency becomes 11/101 ≈ 10.1%, or a 1% relative error. But if one female patient out of 100 has age 96 (1% frequency), and a male patient of the same age is now included by mistake, the frequency becomes 2/101 ≈ 1.98%, with a 98% relative error. This is the reason some people focus on distribution tails and “outliers”, defined as data having with low-frequency values. (Note that this reasoning would concern any regions of low frequency, for example among two modes; not just tails.)\nYet we cannot mindlessly attack low-frequency regions and data just because they could be more affected by out-of-population units. In many problems of data science, engineering, medicine, low-frequency cases are the most important ones (think of rare diseases, rare mineral elements, rare astronomical events, and so on). So if we alter or eliminate low-frequency data only because they might be out-of-population units, then we have dangerously affected all our inferences about such rare events.\nMoreover, how could we judge what the “correct” frequency should be? Many outlier methods assume that the true frequency of the population has a Gaussian shape, and alter or cut the tails based on this assumption. But how can we know if such an assumption is correct? It turns out that the tails of a distribution are important for checking such assumption. Then you see the full circularity behind such mindless methods.\n\n\n\n\n\n\n Study reading\n\n\n\n§2.1 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n\n\nWhich method should one use to face this problem, then? – The answer is that there’s no universal method. The approach depends on the specific problem. A data scientist must carefully examine all possible sources of out-of-population units, make inferences about them, and integrate these inferences in the general inference about the population of interest.\nThere is literature discussing first-principle approaches of this kind for different scenarios, but we cannot discuss them in the present course.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 21 of Probability Theory",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html",
    "href": "subpopulations.html",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "",
    "text": "22.1 Subpopulations\nWhen we have a statistical population with a joint variate, it is often of interest to focus on a subset of units that share the same value of a particular variate.\nConsider for instance the following population, related to the glass-forensics example we encountered before:\nLet’s say we are interested only in units that have the \\(\\mathit{Type}\\) variate equal to \\({\\small\\verb;tableware;}\\). Discarding all others we obtain a new, smaller population with four units:\nwere a bar “\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\)” indicates the variate used for the selection.\nAs another example, we could be interested instead in those units that have both \\(\\mathit{Ca}\\) and \\(\\mathit{Si}\\) variates equal to \\({\\small\\verb;medium;}\\). We obtain a smaller population with five units:\nPopulations formed in this way are called subpopulations of the original one. They are statistical populations in their own right. The notion of “subpopulation” is a relative notion. Any population can often be considered as a subpopulation of some larger population having additional variates.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-subpopulations",
    "href": "subpopulations.html#sec-subpopulations",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "",
    "text": "units: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of three simple variates:\n\nweight fraction of \\(\\mathit{Ca}\\)lcium in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\nweight fraction of \\(\\mathit{Si}\\)licon in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values \\(\\{{\\small\\verb;building_windows_float_processed;}\\), \\({\\small\\verb;building_windows_non_float_processed;}\\), \\({\\small\\verb;containers;}\\), \\({\\small\\verb;tableware;}\\), \\({\\small\\verb;headlamps;}\\}\\)\n\n\n\n\n\nTable 22.1: Simplified glass-fragment population data\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\nTable 22.2: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\nTable 22.3: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\) and \\(\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nFrom the population of table  22.1:\n\nConstruct the marginal population with variate \\(\\mathit{Ca}\\)\nReport the frequency distribution for the marginal population above (remember that \\(\\mathit{Ca}\\) has three possible values)\nConstruct the subpopulation with variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;high;}\\)\nConstruct the subpopulation with variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) and the variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;medium;}\\)\n\n\nCheck your understanding of the reasoning behind the notions of marginal population and subpopulation with this exercise:\n\nFrom the population of table  22.1, construct the subpopulation with variate \\(\\mathit{Type}\\) equal to either \\({\\small\\verb;headlamps;}\\) or \\({\\small\\verb;tableware;}\\).",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-conditional-freqs",
    "href": "subpopulations.html#sec-conditional-freqs",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.2 Conditional frequencies",
    "text": "22.2 Conditional frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\) (and possibly others), we define the conditional frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\), given or “conditional on” the value \\({\\color[RGB]{34,136,51}x}\\) of \\({\\color[RGB]{34,136,51}X}\\), as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the subpopulation selected by \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). This frequency is usually written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nwhere \\(f\\) is the symbol for the joint frequency of the population.\nConsider for instance the glass-fragment population of table  22.1. The conditional frequency of \\({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}}\\) given \\({\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}\\) is the (marginal) frequency of \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\) in the subpopulation of table  22.2, from which we find\n\\[\nf({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\]\nThe collection of these conditional frequencies for all values of \\({\\color[RGB]{238,102,119}Y}\\) constitutes the conditional frequency distribution of \\({\\color[RGB]{238,102,119}Y}\\) conditional on \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). In our example this distribution has three conditional frequencies:\n\\[\\begin{aligned}\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{3}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = 0\n\\end{aligned}\\]\nwhich sum up to \\(1\\) as they should.\n\n\n\n\n\n\n Conditional on a value of a variate\n\n\n\nIt doesn’t make sense to speak of the conditional frequency distribution of \\(Y\\) “conditional on \\(X\\)”. Conditional frequencies and frequency distributions are always conditional on some value of a variate. If we consider all possible values of \\(Y\\) and of \\(X\\) we obtain a collection of frequencies that is not a distribution.\n\n\nA conditional frequency can be calculated as the ratio of a joint and a marginal frequencies, in a way analogous to conditional probabilities (§ 17.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}) =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{f({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})} =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{\n\\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}\n\\]\n\n\n\n\n\n\n Exercises\n\n\n\nCalculate the conditional frequency distributions corresponding to the subpopulations of tables 22.2 and 22.3. For example, for table  22.2 this means calculating\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&\\dotsc\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\n\\end{aligned}\\]",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-association",
    "href": "subpopulations.html#sec-association",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.3 Associations",
    "text": "22.3 Associations\nThe analysis of subpopulations and conditional frequencies is important because it often reveals peculiar associations1 among different variates and groups of variates. Let’s illustrate what we mean by “association” with an example.\n1 In everyday language this is the same as “correlation”. The term “association” is used in statistics to avoid confusion with the Pearson correlation coefficient (see § 18.5)Extract the subpopulation having variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) from the population of table  22.1:\n\n\n\nTable 22.4: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\nwe notice that all units have variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\). In terms of conditional frequencies, this means\n\\[\n\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\end{aligned}\n\\]\nIt is therefore impossible to observe other values of \\(\\mathit{Ca}\\) in this new population.2\n2 We are not claiming that this fact will be true if new units are considered; this important question will be discussed later.On the other hand, if we extract the subpopulation having variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) we obtain\n\n\n\nTable 22.5: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\nwith conditional frequencies such as\n\\[\n\\begin{aligned}\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\\\[1ex]\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{1}{5}\n\\end{aligned}\n\\]\nand so on. The reverse is therefore not true: if \\(\\mathit{Ca}\\) is equal to \\({\\small\\verb;low;}\\), that does not mean that it’s impossible to observe other \\(\\mathit{Type}\\) values besides \\({\\small\\verb;headlamps;}\\). Note especially how these frequencies differ:\n\\[\n\\begin{gathered}\nf(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\[1ex]\nf(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\end{gathered}\n\\]\nIn the original population we have, figuratively speaking, the following interesting association:\n\\[\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\ \\mathrel{\\color[RGB]{34,136,51}\\Rightarrow}\\\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\n\\qquad\\text{\\small but}\\qquad\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\  \\mathrel{\\color[RGB]{238,102,119}\\nRightarrow}\\\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\nThis kind of associations is often useful. Suppose for instance that you are asked to pick a unit with \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) in the original population; but it’s difficult to measure a unit’s \\(\\mathit{Ca}\\) value, while it’s easy to measure its \\(\\mathit{Type}\\) value. Then you could instead search for a unit having \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) (easier search), and you would be sure that the unit you found also has \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\).\n\n\nThe example above, where some values of a variate completely exclude some values of another, is a special one. More often we find that there are small or large changes in the frequency distribution of some variate, depending on the subpopulation considered.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the (marginal) frequency distribution for the \\(\\mathit{Ca}\\) variate for the glass-fragment population of table  22.1. Is the value \\({\\small\\verb;low;}\\) more frequent than \\({\\small\\verb;medium;}\\)? or vice versa?\nCalculate the frequency distribution for \\(\\mathit{Ca}\\), conditional on \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\) (see table  22.2). How does this frequency distribution differ from the one you calculated above? Come up with possible ways to exploit this difference in concrete applications.\n\n\n\n\nAssociations can be very counter-intuitive\nIt is usually best to assess associations by explicitly calculating all relevant conditional frequencies, rather than jumping to intuitive conclusions after having examined just a few. Here’s an example.\n\n\nConsider the statistical population defined as follows:\n\n\n\n\nunits: all reparations done by a repair company on a particular kind of electronic components, which is extremely delicate and usually very difficult to repair. The population has 26 units (every unit actually represents a batch 100 reparations, so the population really refers to 2600 reparations).\na joint variate, consisting in three binary ones:\n\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\) of the repair procedure, with values \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) and \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\);\nrepair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), with values \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\), representing a traditional reparation method and one introduced more recently;\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) of the repair procedure, with values \\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\) and \\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\).\n\n\n\n\n\nTable 22.6: Reparations (each row is one unit, representing 100 reparations).\nfile repair_data.csv\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\)\n\\(\\mathit{\\color[RGB]{34,136,51}Method}\\)\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\)\n\n\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\n\n\n\nThe repair company claims that, in this population, the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\). Can you back up their claims?:\n\n\n\n\n\n\n Exercise (one of the most fun of the course!)\n\n\n\nUse the population data above. The calculations can be done with any tools you like.\n\nExamine the whole population first:\n\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) (note that we are disregarding the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\)).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\).\nCompare the two conditional frequency distributions above. Which of the two repair methods seems more effective?\nAre the claims of the repair company justified?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\):\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for onsite reparations?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in step 3.?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)ly:\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for reparations done remotely?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in steps 3. and 7.?\n\nSummarize and explain all your findings.\nCan the repair company claim that the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is better than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)?\n\nSuppose you need to send an electronic component for repair to this company.\n\nIf you could choose both the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\) and the \\(\\mathit{\\color[RGB]{34,136,51}Method}\\) of the repair, which would you choose? why?\nIf you could only choose the repair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), but have no control over the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\), which method would you choose? why?\n\nIs there other information, missing from the description of the population, that should be known before answering the questions above?\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§2.2–2.4 and 2.7–2.10 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\nLindley & Novick 1981: The role of exchangeability in inference This can be a difficult reading. Try to get the main message.\nMalinas & Bigelow 2004/2016: Simpson’s paradox",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html",
    "href": "samples.html",
    "title": "23  Infinite populations and samples",
    "section": "",
    "text": "23.1 Infinite populations\nThe examples of populations that we explored so far comprised a small number of units, and all their data were exactly and fully known. In concrete inference and decision problems of the kind we have been focusing on in chapters 17 and 19, we usually deal with populations that are much larger or potentially infinite; and data are known only for a small collection of their units.\nIn the glass-forensic example (table  20.1), for instance, many more glass fragments could be examined beyond the 10 units reported there, with no clear bound on the total number. We could even extend that population considering glass fragments from past and future crime scenes:\nthe imaginary example above also shows that the values of some variates for some units might be unknown; this is a situation we shall discuss in depth later.\nWe shall henceforth focus on statistical populations with a number of units that is in principle infinite, or so large that it can be considered practically infinite. “Practically” means that the number of units we’ll use as data or draw inferences about is a very small fraction, say less than 0.1%, of the total population size.\nThis is often the case. Consider for example (as in § 12.1.1) the collection of all possible 128 × 128 images with 24-bit colour depth. This collection has \\(2^{24 \\times 128 \\times 128} \\approx 10^{118 370}\\) units. Even if we used 100 billions of such images as data, and wanted to draw inferences on another 100 billions, these would constitute only \\(10^{-118 357}\\,\\%\\) of the whole collection. This collection is practically infinite.\nNote that we can’t say whether a population, per se, is “practically infinite” or not. It could be practically infinite for a particular inference problem, but not for another.\nWhen we use the term “population” it will often be understood that we’re speaking about a statistical population that is practically infinite with respect to the inference or decision problem under consideration.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-infinite-populations",
    "href": "samples.html#sec-infinite-populations",
    "title": "23  Infinite populations and samples",
    "section": "",
    "text": "Table 23.1: Glass fragments, extended\n\n\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\nnotes\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\n\\({\\small\\verb;vehicle_windows_float_processed;}\\)\n\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\n\\({\\small\\verb;containers;}\\)\n\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n351\n\\(1.52101\\)\n\\(8.75\\)\n\\(71.78\\)\n?\nfrom unsolved-crime scene in 1963\n\n\n…\n…\n…\n…\n…\n…\n\n\n1027\n\\(1.51761\\)\n\\(7.83\\)\n\\(72.73\\)\n?\ncrime scene in 2063\n\n\n…\n…\n…\n…\n…\n…",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-limit-freqs",
    "href": "samples.html#sec-limit-freqs",
    "title": "23  Infinite populations and samples",
    "section": "23.2 Limit frequencies",
    "text": "23.2 Limit frequencies\nIn § 21.2 we defined relative frequencies. Relative frequencies are ratios of two integers, the denominator being the population size \\(N\\). So a frequency \\(f\\) can only take on \\(N+1\\) rational values \\(0/N, \\dotsc, N/N\\) between \\(0\\) and \\(1\\). As the population size increases, the number of distinct, possible frequencies increases and eventually can be considered practically continuous. Frequencies in this case are sometimes called limit frequencies and they are treated as real numbers between \\(0\\) and \\(1\\).",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-samples",
    "href": "samples.html#sec-samples",
    "title": "23  Infinite populations and samples",
    "section": "23.3 Samples",
    "text": "23.3 Samples\n\nLearning from samples\nIn chapters 17 and 19 we considered an agent that must draw an inference about some units from a population. The agent’s degrees of belief in that inference relied (that is, were conditional on) units already observed in the population, the “learning” or “training” data. We saw that the agent’s degrees of belief changed, often becoming sharper, thanks to the information about the observed units.\nUnits for which we have full (or almost full) information, and that an agent can use to update its beliefs, are called a population sample or “sample” for short. Almost all data considered in engineering and data-science problems can be considered to be population samples.\nIt is extremely important to specify how a sample is extracted or collected from a population. For instance, if we consider table  20.1 to be a full population, we could extract a sample in such a way that \\(\\mathit{Type}\\) only has value \\({\\small\\verb;headlamps;}\\) (similarly to when we construct a subpopulation, § 22.1, but for a subpopulation we would select all units having that variate value). The marginal frequency of the value \\({\\small\\verb;headlamps;}\\) in the sample would then be \\(1\\), whereas in the original population it is \\(3/10 \\approx 0.333\\) – two very different frequencies.\n\n\n“Representative” and biased samples\nIf samples from a population are used as conditional information to calculate probabilities about other units, then they should of course be “relevant”, in some sense (not the technical sense of chapter  18), for the inference. The very definition of statistical population (§ 20.2) is meant to have such a relevance built-in: the “similarity” of the units makes each of them relevant for inferences about any other.\nStill, the procedure with which samples are selected from a population may lead to quirky and unreasonable inferences. For instance suppose we are interested in prognosing a disease for a person from a particular population, having observed a sample of people from the same population. If the sample was chosen to consist only of people having the disease, then it is obviously meaningless for our inference.\nThe specific problem in this example is that our inference is based on guessing a frequency distribution in the full population (as we’ll see more in detail in later chapters), but the sample, owing to the way it was chosen, cannot show a frequency distribution similar to the full-population frequency distribution.\n\n\nA sampling procedure may generate a sample that is pointless for some inferences, but still useful for others.\nIn the inference and decision problems under our focus we would like to use a sample for which particular frequencies – most often the full joint frequency – don’t differ very much from those in the full population. We’ll informally call this a “representative sample”. This is a difficult notion; the International Organization for Standardization for instance warns (item 3.1.14):\n\nThe notion of representative sample is fraught with controversy, with some survey practitioners rejecting the term altogether.\n\n\n\nIn many cases it is impossible for a sample of given size to be fully “representative”:\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the following population of 16 units, with four binary variates \\(W,X,Y,Z\\), each with values \\(0\\) and \\(1\\):\n\n\n\n\n\nTable 23.2: Four-bit population\n\n\n\n\n\n\\(W\\)\n\\(X\\)\n\\(Y\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n1\n0\n1\n\n\n0\n0\n1\n1\n\n\n1\n0\n1\n1\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nThe joint variate \\((W,X,Y,Z)\\) has 16 possible values, from \\((0,0,0,0)\\) to \\((1,1,1,1)\\). Each of these values appear exactly once in the population, so it has frequency \\(1/16\\). The marginal frequency distribution for each binary variate is also uniform, with frequencies of 50% for both \\(0\\) and \\(1\\).\n\nExtract a representative sample of size four units. In particular, the marginal frequency distributions of the four variates should be as close to 50%/50% as possible.\n\n\n\nLuckily, the probability calculus allows an agent to draw inferences also when the sample is too small to correctly reflect full-population frequencies, if appropriate background information is provided.\n\n\nObviously we cannot expect a population sample to exactly reflect all frequency distributions – joint, marginal, conditional – of the original population; some discrepancy is to be expected. How much discrepancy should be allowed? And what is the minimal size for a sample not to exceed such discrepancy?\nInformation Theory, briefly mentioned in chapter  18, can give reasonable answers to these questions. Let us summarize some examples here.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapters 1–10 of Information Theory, Inference, and Learning Algorithms\nVideo lectures 1–9 from the Course on Information Theory, Pattern Recognition, and Neural Networks\n\n\n\nFirst we need to introduce the Shannon entropy of a discrete frequency distribution. It is defined in a way analogous to the Shannon entropy for a discrete probability distribution, discussed in § 18.5. Lets say the distribution is \\(\\boldsymbol{f} \\coloneqq(f_1,f_2, \\dotsc)\\). Its Shannon entropy \\(\\mathrm{H}(\\boldsymbol{f})\\) is\n\\[\n\\mathrm{H}(\\boldsymbol{f}) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n\\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nand is measured in shannons when the base of the logarithm is 2.\nIf we have a population with joint frequency distribution \\(\\boldsymbol{f}\\), then a representative sample from it must have at least size\n\\[\n2^{\\mathrm{H}(\\boldsymbol{f})} \\equiv\n\\frac{1}{{f_1}^{f_1}\\cdot {f_2}^{f_2}\\cdot {f_3}^{f_3}\\cdot \\dotsb}\n\\] \nThis particular number has important practical consequences; for example it is related to the maximum rate at which a communication channel can send symbols (which can be considered as values of a variate) with an error as low as we please.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the Shannon entropy of the joint frequency distribution for the four-bit population of table  23.2.\nCalculate the minimum representative-sample size according to the Shannon-entropy formula. Is the result intuitive?\n\n\n\n\n\nIf we are only interested in a smaller number of variates of a population, then the representative sample can be smaller as well: its size would be given by the entropy of the corresponding marginal frequency distribution of the variates of interest. In the example of table  23.2, if we are only interested in the variate \\(X\\), then any sample consisting of two units, one having \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0\\) and the other having \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\), would be a representative sample of the marginal frequency distribution \\(f(X)\\).\n\n\n\n\n\n\n Study reading\n\n\n\nKruskal & al. 1979: Representative Sampling, I\n\n\nA sample that presents some aspects, such as frequency distributions, which are at variance with the original population, is sometimes called biased. This term is used in many different ways by different authors. Unfortunately, most samples are “biased” in this sense.\nThe only way to counteract the misleading information given by a biased sample is to specify appropriate background information, which comes not from data samples but from a general meta-analysis, often based on physical, medical, and similar principles, of the problem and population.\n\n\n\n\nQuirks of samples for mean and standard deviation\nFor some populations, the mean and standard deviation calculated in a sample can be wildly different from those of the full population – even when the sample comprises half of the full population! This does not happen with the median and quartiles. Here is a demonstration in R. Try it out in your favourite programming language.\n\n\n\n\n\n\n Guided exercise\n\n\n\nWe imagine to have a population of 1 000 000 units. These units have continuous interval variates \\(X\\) and \\(Y\\), each with an approximately standard Gaussian frequency distribution. These variates are not actual part of the population definition, however. Rather, an agent only has access to, or maybe it’s only interested in, the ratio of these two variates \\(Z \\coloneqq X/Y\\).\nThe agent is in particular interested in the mean of the variate \\(Z\\) in the full population, but has only access to the values of \\(Z\\) in a sample. How does the mean calculated from a sample of increasing size compare with the actual mean of the full population? For comparison, we also study the median of the full population and of the samples.\nFirst let’s create the values of the variates \\(X\\) and \\(Y\\), and construct \\(Z\\) from them:\n\n## Load custom plot functions\nsource('code/tplotfunctions.R')\nset.seed(1000) # to reproduce results\n\nN &lt;- 1000000 # population size\n\nX &lt;- rnorm(N) # variate invisible to agent\nY &lt;- rnorm(N) # variate invisible to agent\n\nZ &lt;- X / Y # variate considered by agent\n\n## mean and median of Z in the full population\npopmean &lt;- mean(Z)\ncat('\\nThe full-population mean is', popmean, '(unknown to the agent)\\n')\n\n\nThe full-population mean is -8.04481 (unknown to the agent)\n\npopmedian &lt;- median(Z)\ncat('\\nThe full-population median is', popmedian, '(unknown to the agent)\\n')\n\n\nThe full-population median is 0.00188701 (unknown to the agent)\n\n\nNow we imagine that the agent accumulates samples from the population, starting from 100, increasing by 100 units, until half of the population has been sampled. At each sample increase the agent calculates the sample mean. We plot how the sample mean changes with the sample size. We also plot indicate the full-population mean, which the agent doesn’t know and is trying to guess:\n\n## sizes of successive samples\nsamplesizes &lt;- seq(from = 100, to = N / 2, by = 100)\n\n## empty vectors to contain the means and medians of the increasing samples\nsamplemeans &lt;- numeric(length(samplesizes))\nsamplemedians &lt;- numeric(length(samplesizes))\n\n## loop through the increasing samples, calculate mean for each\nfor(sample in seq_along(samplesizes)){\n    samplemeans[sample] &lt;- mean(Z[1:samplesizes[sample]])\n    samplemedians[sample] &lt;- median(Z[1:samplesizes[sample]])\n\n}\n\n## plot how sample means change with sample size, and the actual population mean\ncommonmax &lt;- 1.01 * max(abs(c(popmean, popmedian, samplemeans, samplemedians)))\nmyflexiplot(x = samplesizes, y = samplemeans,\n      xlab = 'sample size', ylab = 'sample mean',\n      col = 2, lwd = 4,\n      ylim = c(-commonmax, commonmax))\nabline(h = popmean, lty = 2, lwd = 3, col = 7)\ntext(y = popmean, x = max(samplesizes),\n    labels = 'population mean (unknown to agent)',\n    adj = c(1, 1), col = 7, cex = 1.2)\n\n\n\n\n\n\n\n## plot how sample medians change with sample size, and the actual population median\nmyflexiplot(x = samplesizes, y = samplemedians,\n      xlab = 'sample size', ylab = 'sample median',\n      col = 3, lwd = 4,\n      ylim = c(-commonmax, commonmax))\nabline(h = popmedian, lty = 2, lwd = 3, col = 7)\ntext(y = popmedian, x = max(samplesizes),\n    labels = 'population median (unknown to agent)',\n    adj = c(1, 1), col = 7, cex = 1.2)\n\n\n\n\n\n\n\n\nTest this again with several pseudorandom seeds.\n\n\n\nNow try a similar exercise but for the standard deviation of \\(Z\\)",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html",
    "href": "machine_learning_overview.html",
    "title": "24  Introduction to machine learning",
    "section": "",
    "text": "24.1 Hyperparameters and model complexity\nSo far we have learned about concepts of doing inference, and about various properties of data, and probability distributions. Now it is time to combine them into a simplified mathematical model, that can use data from the past to make predictions about the unknown. Note that for the chapters in this part we are not building an optimal predictor machine anymore, but rather a approximate one, which serves the common operational constraint of being fast to compute.\nMachine learning methods can be wildly different both in terms of operation and complexity. But they all behave as functions that take in a data point \\(x\\), a set of values \\(\\mathbf{w}\\) which we call parameters, and returns the prediction \\(\\hat{y}\\):\n\\[\n    \\hat{y} = f(x, \\mathbf{w}).\n\\tag{24.1}\\]\nThe prediction \\(\\hat{y}\\) can be any quantity discussed in section 12.2 – most commonly we want to classify the data (and we say we are doing classification), or we want to estimate some value (in which case we call it regression).\nIt has been pointed out already – and we will see it also in the coming chapters – that often there is actually not a functional relationship between the inputs \\(\\mathbf{x}\\), and the property we want to predict. This leaves us with two possibilities for a solution:\nBecause of the second point above, when we describe equation 24.1, we will use the word “function” in the wide sense where we allow it to be non-deterministic. An example is ChatGPT, which will answer you differently if you ask it the same exact question twice. If you prefer more mathematical rigour, we could call \\(f\\) a process instead, which is a more loose term.\nLet us break down equation 24.1. As engineers, when faced with the task of using data to model the behaviour of some system, our job is twofold. First, we need to select a suitable method to serve as the function \\(f\\), and second, we need to find the optimal values for the parameters \\(\\mathbf{w}\\). In ADA501 we will see how to choose an analytical \\(f\\) that corresponds to certain physical systems, while in our course, we will look at methods where \\(f\\) can be practically anything.\nFinding the parameters \\(\\mathbf{w}\\) is what takes us from a general method, to the specific solution to a problem. As we will see, the way of finding them differs between the various machine learning methods, but the principle is always the same: We need to choose a loss function, and then iteratively adjust \\(\\mathbf{w}\\) so that the loss, when computed on known data, becomes as small as possible. The loss function should represent the difference between what the model outputs, and the correct answer – the better the model, the smaller the difference. The choice of this loss function depends on what kind of problem we wish to solve, and we will look at the common ones shortly. But at this point we can already define the three main types of machine learning:\nHaving decided on a method to represent \\(f\\) and found a set of parameters \\(\\mathbf{w}\\), we say that these combined now constitute our model. The model should have internalised the important correlations in the data and thereby allows us to make predictions, i.e. do modelling. If we do decision-making based on the output of the model as well, we typically call in an agent, since there is now some level of autonomy involved. In this chapter, however, we will stick to modelling problems in a supervised fashion.\nYou may have heard the quote by statistician George Box:\nAlthough coming off as a somewhat negative view on things, the quote still captures an important point about statistical modelling – our goal is not to make a complete, 100% accurate description of reality, but rather a simplified description that is meaningful in the context of our task at hand. The “correct” level of simplicity, in other words the optimal number of parameters \\(\\mathbf{w}\\), can be hard to find. Often it will be influenced by practical considerations such as time and computing power, but it is always governed by the amount of data we have available. We will look at the theory of model selection later, but let us first consider a visual example, from which we can define some important concepts.\nImagine that you don’t have a thermometer that shows the outside temperature. Never knowing whether you should wear a jacket or not when leaving the house, you get a great idea: If you can construct a mathematical model for the outside temperature as a function of the time of day, then a look at the clock yould be enough to decide for or against bringing a jacket. You manage to borrow a thermometer for a day, and make ten measurements at different times, which will form the basis for the model:\nThe next step is to choose a function \\(f\\). For one-dimensional data like this, we could for instance select among the group of polynomials, of the form\n\\[\n    f (x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M = \\sum_{j=0}^{M} w_j x^j \\,,\n\\]\nwhere \\(M\\) is the order of the polynomial. Recall that a zero’th order polymonial is just a constant value, so such a model would be represented with one single parameter. A first-order polynomial is a linear function (two parameters), and the higher in order (and number of parameters) we go, the more “curves” the function can have. This already presents us with a problem. Which order polynomial (i.e. which value of \\(M\\)) should we choose? Let us try different ones, and for each case, fit the parameters, meaning we find the parameter values that yield the smallest difference from the measured data points:\nObviously, the constant function does a poor job of describing our data, and likewise for the linear function. A fourth-order polynomial, on the other hand, looks very reasonable. Now consider the ninth-order polynomial: it matches the measurements perfectly, but surely, this does not match our expectation for what the temperature should look like.\nWe say that the first and second model underfit the data. This can happen for two reasons: Either the model has too little complexity to be able to describe the data (which is the case in this example), or, potentially, the optimal parameter values have not been found. The opposite case is overfitting, as shown for the last model, where the complexity is too high and the model adapts to artifacts in the data.\nThis concept is also called the bias-variance tradeoff. We will not go into too much detail on this yet, but qualitatively, we can say that bias (used in this setting) is the difference between the predicted value and the true value, when averaging over different data sets. Variance (again when used in this setting) indicates how big the change in parameters, and hence in model predictions, we get from fitting to different data sets. Let us say you measure the temperature on ten different days, and for each day, you fit a model, like before. These may be the results:\nThe blue dots are our “original” data points, plotted for reference, while the red lines corresponds to models fitted for each day’s measurements. Due to fluctuations in the measurements, they are different, but note how the difference is related to the model complexity. Our linear models (\\(M=1\\)) are all quite similar, but neither capture the pattern in the data very well, so they all have high bias. The overly complex models (\\(M=9\\)) have zero bias on their repective dataset, but high variance. The optimal choice is likely somewhere in-between (hence the tradeoff), as for the \\(M=4\\) models, which perform well without being overly sensitive to fluctuations in data. Since the value of \\(M\\) is chosen by us, we call it a hyperparameter, to separate it from the regular parameters which are optimised by minimising the loss function.",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "href": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "title": "24  Introduction to machine learning",
    "section": "",
    "text": "All models are wrong, but some are useful.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.1: Temperature measurements over the course of 24 hours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zeroth-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n(b) First-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Fourth-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n(d) Ninth-order polymonial\n\n\n\n\n\n\n\nFigure 24.2: Fitting polymonials (red lines) of different orders to the set of measurements (blue circles).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Large bias, low variance\n\n\n\n\n\n\n\n\n\n\n\n(b) Resonably low bias and variance\n\n\n\n\n\n\n\n\n\n\n\n(c) Low bias, large variance\n\n\n\n\n\n\n\nFigure 24.3: Red lines: Different models fitted to separate datasets, where each dataset is drawn from the same distribution as the original one. The original dataset shown in blue circles.\n\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nOur simple temperature model relies on a series of assumptions, some which might be good, and some which might be (very) bad. State the ones you can think of, and evaluate if they are sensible. Hints: are polynomials a good choice for \\(f\\)? Is the data representative?\nFor the different models in figure 24.2 we started by choosing polymonial degree \\(M\\), and then computed the parameters that minimized the difference between the data points and the model. Then we had a look at the results, compared them to our expectations, and decided that \\(M=0\\) and \\(M=9\\) were both unlikely. Can you think of a way to incorporate our initial expectations into the computation?",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#model-selection-in-practice",
    "href": "machine_learning_overview.html#model-selection-in-practice",
    "title": "24  Introduction to machine learning",
    "section": "24.2 Model selection (in practice)",
    "text": "24.2 Model selection (in practice)\nAs alluded to in the exercises above, there are ways of including both the data and our prior expectations when building a model, but it is, in fact, not very common to do so. In this section we will have a look at the typical data science approach, which relies on splitting the data in different sets. Starting from all our available data on a given problem that we wish to model, we divide it into\n\nthe training set, which is the data we will use to determine the model parameters \\(\\mathbf{w}\\),\nthe validation set, which is used to evaluate the model complexity (i.e. finding the optimal bias-variance tradeoff), and\nthe test set, which is used to evaluate the final performance of the model.\n\nThe benefit of this approach is that it is very simple to do. The downside, on the other hand, is that each set is necessarily smaller than the total, which subjects us to increased statistical uncertainty. The final parameters will be slightly less optimal than they could have been, and the measurement of the performance will be slightly less accurate. Still, it is common practice, and for the “standard” machine learning methods there is no direct way of simultaneously optimising for the model complexity.",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#machine-learning-taxonomy",
    "href": "machine_learning_overview.html#machine-learning-taxonomy",
    "title": "24  Introduction to machine learning",
    "section": "24.3 Machine learning taxonomy",
    "text": "24.3 Machine learning taxonomy\nMachine learning has been around for many decades, and the list of methods that fit under our simple definition of looking like equation 24.1 and learning their parameters from data, is very long. For a (non-exhaustive) systematic list, have a look at the methods that are implemented in scikit-learn, a Python library dedicated to data analysis.\nIn this course we will not try to go through all of them, but rather focus on the fundamentals of what they all have in common. With this fundamental understanding, learning about new methods is like learning a new programming language – each has their specific syntax and specific uses, but the underlying mechanism is the same. We will look at two important methods, that are inherently very different, but still accomplish the same end result. The first is decision trees (and the extension into random forests), while the second is neural networks.",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "25  Neural networks",
    "section": "",
    "text": "25.1 Training neural networks\nNeural networks are performing extremely well on complex tasks such as language modelling and realistic image generation, although the principle behind how they work, is quite simple.\nThe basic building block is a node, which receives some values as input, and computes a single output:\nThe output is computed from the inputs \\(x_0, x_1, \\dots, x_N\\), each of which is multiplied by a weight \\(w_1, w_2, \\dots w_N\\), and summed together along with an additional parameter \\(b\\), which is typically called a bias. You can probably identify this step as good old linear regression:\n\\[\na = b + \\sum_{i=0}^N w_i x_i \\,.\n\\]\nA key property to neural networks, however, is to introduce nonlinear relationships. This is done by evaluating the output from each node by an activation function \\(h\\). The final output \\(z\\) from the node is then\n\\[\nz = h(a) = h \\left( b + \\sum_{i=0}^N w_i x_i \\right)\n\\]\nThe activation function is typically rather simple – the most popular is the rectified linear unit (ReLU), which propagates positive values but sets all negative values to zero:\n\\[\n\\text{ReLU}(x) = \\max(0, x) \\,.\n\\]\nVarious other possibilities for choice of activation function exists, as we will explore in the exercises later.\nArmed with our simple node, let us assemble several of them into a network. Starting with, say, four nodes, all the input data will be used by each of them: (todo) mention layers\nNeural networks are great for several reasons. They can be arranged to work with practically any type of data, including unstructured data such as images or text, which is not neatly organised into a table of explicit feature values. Granted, this flexibility does not appear by (FIGURE) alone, but is due to clever additions to the network structure that is beyond the scope of our lectures. A more theoretical argument for neural networks comes from the universal approximation theorem, which states that\nThis is a very powerful property, which is explained in understandable terms here (the proof (CITE) is rather technical). But, as we know already, this only helps if we are trying to model something which has a functional relationship.\nFinding the optimal values of the model’s parameters \\(\\mathbf{w}\\) is usually called to train the model. When we looked at polynomials in the machine learning introduction (Chapter 24) we did not talk about this yet, partly because polynomial models have a closed-form solution for the best parameters, meaning they can be computed directly. Since neural networks are nonlinear by design, we need a different approach, which it to start with random parameter values and iteratively try to improve them.",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#training-neural-networks",
    "href": "neural_networks.html#training-neural-networks",
    "title": "25  Neural networks",
    "section": "",
    "text": "Loss functions\nThe first step towards improving the parameters, is to define what improvement is. Ultimately, our goal is to make predictions about the data that equal the true values, which is to say, we want to minimise the difference between predictions \\(f(\\mathbf{x}, \\mathbf{w}\\) and true values \\(y\\). This difference can be formulated in several different ways, but in the case of regression, the most common is the sum of squared errors:\n\\[\nL(\\mathbf{w}) = \\sum_{\\text{data points } i} (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2\n\\]\n\\(L\\) is called a loss function, alternatively a cost function or an error function. With this in place, the training process becomes a minimisation problem:\n\\[\n\\underset{\\mathbf{w}}{\\mathrm{arg\\,min}}\\, L(\\mathbf{w}) \\,,\n\\tag{25.1}\\]\nTo minimise the loss function we still need to apply it to some data \\(\\mathbf{x}\\), but we have not made this dependence explicit, since the data are “unchanged” throughout the minimisation process.\nAs for any bounded function, the minimum can be found either where the gradient \\(\\nabla L(\\mathbf{w})\\) is zero, or where it does not exist. Solving this analytically is usually impossible, so we resort to a numerical solution – iteratively taking steps in the direction of smaller loss. The crucial point in neural network training is that the loss function is differentiable with respect to the network parameters, meaning we can compute \\(\\nabla L(\\mathbf{w})\\) and take steps in the negative (downwards) direction:\n\\[\n    \\mathbf{w}^{n+1} = \\mathbf{w}^{n} - \\eta \\nabla L(\\mathbf{w}^{n}) \\,.\n\\tag{25.2}\\]\nThis is the method of gradient descent. Here we have introduced a new hyperparameter \\(\\eta\\) called the learning rate, which controls how large each step will be.\nThe process of actually adjusting the parameters in the correct direction is called backpropagation, and involves first computing the value of \\(f(\\mathbf{x}, \\mathbf{w})\\), and then stepping backwards through each layer of the network, recursively updating the parameters by using the derivative. This sounds very tedious, but can be done efficiently by automatic differentiation, i.e. letting a computer do it. Modern frameworks for neural network models require only to know the layout of the network, and will, as we shall see in the exercises, figure out the rest automatically.\n\n\nGradient descent\nWhile we did not get into it earlier, the concept of defining a loss function and doing gradient descent, is in fact how the majority of machine learning algorithms are trained. Even for decision trees, which had their dedicated algorith, we ended up with a loss minimisation task once we introduced boosting.\nStraight-forward gradient descent as shown in equation {25.2} can work fine for relatively simple models, but will stop at the first minimum it encounters. For a reasonably complicated network, the loss function landscape can be expected to have several local minima or saddle points, causing the method to get stuck in places with suboptimal parameters. Several improved algorithms aim to tackle this.\n\nStochastic gradient descent updates the parameters through equation {25.2} for only a subset of data at a time. This is more computationally efficient, and the stochastic element helps against getting stuck in a local minimum, since a local minimum for some subset of data might not be a minimum for a different subset.\nAdaptive gradient methods use different learning rates per parameter, which is updated for each iteration.\nMomentum methods remember the previous gradients and keeps moving in the same direction even through flat or uphill parts, like a massive rolling ball.\n\nAll of the above can be combined, and the most common method of doing so is the Adaptive Moment Estimation (Adam) algorithm.",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "llms.html",
    "href": "llms.html",
    "title": "26  Large Language Models",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\nTo be written",
    "crumbs": [
      "[**Machine learning**]{.midgrey}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "beyond-ML.html",
    "href": "beyond-ML.html",
    "title": "27  Beyond machine learning",
    "section": "",
    "text": "27.1 Machine learning from a bird’s-eye view\nThe last few chapters gave a brief introduction to and overview of popular machine-learning methods, their terminology, and the points of view that they typically adopt. Now let’s try to look at them keeping in mind our main goal in this course: exploring new inference methods, understanding their foundations, and thinking out of the box.\nIn this and the next few chapters we shall focus on the following question: to what purpose do we use machine-learning algorithms?. After answering this question and clarifying what the purpose is, we shall try to achieve it in an optimal way, according to the methods and concepts we studied in the initial part of the course. Remember that they are guaranteed to give the optimal solution (chapter  2). But we shall keep an eye open to see where our optimal methods seem to be similar or dissimilar to machine-learning methods.\nThereafetr, in the last chapters, we shall examine where the optimal solution and machine-learning methods converge and diverge, try to understand what machine-learning methods do from the point of view of our optimal solution, and think of ways to improve them.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Beyond machine learning]{.green}</span>"
    ]
  },
  {
    "objectID": "beyond-ML.html#sec-cat-problems",
    "href": "beyond-ML.html#sec-cat-problems",
    "title": "27  Beyond machine learning",
    "section": "27.2 A task-oriented categorization of some machine-learning problems",
    "text": "27.2 A task-oriented categorization of some machine-learning problems\nFor our goal, the common machine-learning categorization and terminology discussed in chapter  24 are somewhat inadequate. Distinctions such as “supervised learning” vs “unsupervised learning” are of secondary importance to a data engineer (as opposed to a “data mechanic”) for several reasons:\n\n  They group together some types of tasks that are actually quite different from an inferential or decision-making viewpoint; and conversely they separate types of tasks that are quite similar.\n  They focus on procedures rather than on purposes.\n\nThe important questions for us, in fact, are: What do we wish to infer or choose? and From which kind of information? These questions define the problem we want to solve. \nLet’s introduce a different categorization of the kind of tasks that we want to accomplish; a categorization that tries to focus on the purpose or task, and on the types of desired information and of available information, rather than on the procedure.\nThe categorization below is informal. It only provides a starting point from which to examine a new kind of task we may face. Many tasks will fall in between categories: every data-engineering or data-science problem is unique.\n\nWe exclude from the start all tasks that require an agent to continuously and actively interact with its environment for acquiring information, making choices, getting feedback, and so on. Clearly these tasks are the domain of Decision Theory in its most complex form, with ramified decisions, strategies, and possibly the interaction with other decision-making agents. To explore and analyse this kind of tasks is beyond the purpose of this course.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nDecision Analysis\nChapters 16–18 in Artificial Intelligence\nGames and Decisions\n\n\n\n\nWe focus on tasks where multiple “instances” with similar characteristics are involved, and the agent has some question related to a “new instance”. According to the conceptual framework developed in part Data II, we can view these “instances” as units of a practically infinite population. The “characteristics” that the agent has observed or must guess are variates common to all these units.\n\n\nRemember that you can adopt any terminology you like. If you prefer “instance” and “characteristics” or some other words to “unit” and “variate”, then use them. What’s important is that you understand the ideas and methods behind these words\n\nNew unit: given vs generated\nA first distinction can be made between\n\n   Tasks where an agent must itself generate a new unit\n   Tasks where a new unit is given to an agent, who must guess some of its variates\n\nAn example of the first type of task is image generation: an algorithm is given a collection of images and is asked to generate a new image based on them.\nWe shall see that these two types of task are actually quite close to each other, from the point of view of Decision Theory and Probability Theory.\n\nThe terms “discriminative” and “generative” are sometimes associated in machine learning with the two types of task. This association, however, is quite loose, because some tasks typically called “generative” actually belong to the first type. We shall therefore avoid these terms. It’s enough to keep in mind the distinction between the two types of task above.\n\n\n\nGuessing variates: all or some\nFocusing on the second type of task (a new unit is given to the agent), we can further divide it into two subtypes:\n\n  The agent must guess all variates of the new unit\n  The agent must guess some variates of the new unit, but can observe other variates of the new unit\n\nAn example of the first subtype of task is the “urgent vs non-urgent” problem of § 17.4: having observed incoming patients, some of which where urgent and some non-urgent, the agent must guess whether the next incoming patient will be urgent or not. No other kinds of information (transport, patient characteristics, or others) are available.\nWe shall call predictands1 the variates that the agent must guess in a new unit, and predictors those that the agent can observe.2 The first subtype above can be viewed as a special case of the second where all variates are predictands, and there are no predictors.\n1 literally “what has to be predicted”2 In machine learning and other fields, the terms “dependent variable”, “class” or “label” (for nominal variates) are often used for “predictand”; and the terms “independent variable” or “features” are often used for “predictor”.\nThe terms “unsupervised learning” and “supervised learning” are sometimes associated in machine learning with these two subtypes of task. But the association is loose and can be misleading. “Clustering” tasks, for example, are usually called “unsupervised” but they are examples of the second subtype above, where the agent has some predictors.\n\n\n\nInformation available in previous units\nFinally we can further divide the second subtype above into two or three subsubtypes, depending on the information available to the agent about previous units:\n\n   All predictors and predictands of previous units are known to the agent\n   All predictors of previous units, but not the predictands, are known to the agent\n   All predictands of previous units, but not the predictors, are known to the agent\n\n\n\nAn example of the first subsubtype of task is image classification. The agent is for example given the following 128 × 128-pixel images and character-labels from the One Punch Man series:\n\nand is then given one new 128 × 128-pixel image:\n\n\n\n\n\nof which it must guess the character-label.\nIn the example just given, the image is the predictor, the character-label is the predictand.\n\n\nA slight modification of the example above gives us a task of the second subsubtype. A different agent is given the images above, but without labels:\n\nand must then guess some kind of “label” or “group” for the new image above; and possibly also for the images already given. The kind of “group” requested depends on the specific problem.\nIn this example the image is the predictor, and the label or group is the predictand. The difference from the previous example is that the agent doesn’t have the predictand values of previous units.\n\nThe term “supervised learning” typically refer to the first subsubtype above.\nThe term “unsupervised learning” can refer to the second subsubtype, for instance in “clustering” tasks. In a clustering task, the agent tries to guess which group or “cluster” a unit belong to, given a collection of similar units, whose groups are not known either. The cluster effectively is the predictand variate. In some cases the agent may want to guess the cluster not only of a new unit, but also of all previous units.\nThe third subsubtype is very rarely considered in machine learning, yet it is not an unrealistic task.\n\nThe types, subtypes, subsubtypes above are obviously not mutually exclusive or comprehensive. We can easily imagine scenarios where an agent has some predictors & predictands available about some previous units, but only predictors or only predictands available for other previous units. This scenario falls in between the three subsubtypes above. In machine learning, hybrid situations like these are categorized as “missing data” or “imputation”.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Beyond machine learning]{.green}</span>"
    ]
  },
  {
    "objectID": "beyond-ML.html#sec-categ-probtheory",
    "href": "beyond-ML.html#sec-categ-probtheory",
    "title": "27  Beyond machine learning",
    "section": "27.3 Flexible categorization using probability theory",
    "text": "27.3 Flexible categorization using probability theory\nWe have been speaking about the agent’s guessing the values of some variates. “Guessing” means that there’s a state of uncertainty: the agent can’t simply say something like “the value of the label is Saitama”, because that could be false. Uncertainty means that the most honest thing that the agent can do is to express degrees of belief about each of the possible values. Probability theory enters the scene.\nIn fact it turns out that the categorization above into subtypes and subsubtypes of tasks can be presented in a more straightforward and flexible way using probability-theory notation.\n\nNotation\nFirst let’s introduce some symbol conventions to be used in the next chapters.\n\nWe shall denote with \\({\\color[RGB]{68,119,170}Z}\\) all variates that are of interest to the agent: those to be guessed as well as those that may be already known.\nThe variates to be guessed in a new unit (the predictands) will be collectively denoted with \\({\\color[RGB]{68,119,170}Y}\\).\nThe variates that can be observed in a new unit (the predictors) will be collectively denoted with \\({\\color[RGB]{68,119,170}X}\\). In cases where there are no predictors, \\({\\color[RGB]{68,119,170}X}\\) is empty.\n\nTherefore we have \\({\\color[RGB]{68,119,170}Z}= ({\\color[RGB]{68,119,170}Y}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}X})\\). In cases where there are no predictors we have \\({\\color[RGB]{68,119,170}Z}= {\\color[RGB]{68,119,170}Y}\\).\n\n\\({\\color[RGB]{68,119,170}Z}_i\\)  denote all variates for unit #\\(i\\).\n\\({\\color[RGB]{68,119,170}Y}_i\\)  denote all predictands for unit #\\(i\\).\n\\({\\color[RGB]{68,119,170}X}_i\\)  denote all predictors for unit #\\(i\\).\n\nAs usual we number from  \\(i=1\\)  to  \\(i=N\\)  the units that serve for learning, and  \\(i=N+1\\)  is the new unit of interest to the agent.\nRecall (§ 5.3) that in probability notation\n\\[\\mathrm{P}(\\text{\\color[RGB]{238,102,119}\\small[proposal]}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\text{\\color[RGB]{34,136,51}\\small[conditional]} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nthe proposal contains what the agent’s belief is about, and the conditional contains what’s supposed to be known to the agent, together with the background information \\(\\mathsfit{I}\\).\n\nFinally let’s see how to express different typologies of tasks in probability notation.\n\n\n\n\n  The agent must guess all variates of the new unit\nThis kinds of guess is represented by the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible values \\(\\color[RGB]{238,102,119}z\\) in the domain of \\({\\color[RGB]{68,119,170}Z}\\). The specific values \\(\\color[RGB]{34,136,51}z_N, \\dotsc, z_1\\) of the variate \\({\\color[RGB]{68,119,170}Z}\\) for the previous units are known to the agent.\n\n\n\n\n  The agent must guess some variates of the new unit, having observed other variates of the new unit\nThis kind of guess is represented by the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\dotsb \\,\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible values \\(\\color[RGB]{238,102,119}y\\) in the domain of the predictands \\({\\color[RGB]{68,119,170}Y}\\). The value \\(\\color[RGB]{34,136,51}x\\) of the predictors \\({\\color[RGB]{68,119,170}X}\\) for the new unit is known to the agent.\nThe remaining information “\\(\\dotsb\\)” contained in the conditional depends on the subsubtype of task:\n\n\n\n   All predictors and predictands of previous units are known to the agent\nThis corresponds to the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible \\(\\color[RGB]{238,102,119}y\\). All information about predictands \\({\\color[RGB]{68,119,170}Y}\\) and predictors \\({\\color[RGB]{68,119,170}X}\\) for previous units appears in the conditional.\nIn the example with image classification, a pictorial representation of this probability would be\n\nwhere \\({\\color[RGB]{238,102,119}y} \\in \\set{\\color[RGB]{238,102,119}{\\small\\verb;Saitama;}, {\\small\\verb;Fubuki;}, {\\small\\verb;Genos;}, {\\small\\verb;MetalBat;}, \\dotsc \\color[RGB]{0,0,0}}\\).\n\n\n\n\n   All predictors of previous units, but not their predictands, are known to the agent\nThis corresponds to the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nfor all possible \\(\\color[RGB]{238,102,119}y\\). All information about predictors \\({\\color[RGB]{68,119,170}X}\\) for the previous units, but not that about their predictands \\({\\color[RGB]{68,119,170}Y}\\), appears in the conditional.\n\n\n\n\n\nMore general and hybrid tasks\nConsider a task that doesn’t fit into any of the types discussed above: The agent wants to guess the predictands for a new unit, say #3, after observing that its predictors have value \\(\\color[RGB]{34,136,51}x\\). Of two previous units, the agent knows the predictor value \\(\\color[RGB]{34,136,51}x_1\\) of the first, and the predictand value \\(\\color[RGB]{34,136,51}y_2\\) of the second. This task is expressed by the probability\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWrite down the general probability expression for the task of subsubtype “all predictands of previous units, but not their predictors, are known to the agent”.\nWhat kind of task does the following probability express?:\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nWhat kind of task could it represent in machine-learning terminology?\n\n\n\n\n\n\n   Tasks where an agent must itself generate a new unit\nOur very first categorization included the task of generating a new unit, given previous examples. In this kind of task there are possible alternatives that the agent could generate. How should one alternative be chosen? A moment’s thought shows that the probabilities for the possible alternatives should enter the choice.\nSuppose, as a very simple example, that a generative agent has been shown, in an unsystematic order, 30 copies of the symbol  and 10 copies of the symbol , and is asked to generate a new symbol out of these examples. Intuitively we expect that it will generate , but we cannot and don’t want to exclude the possibility that it will generate . These two generation possibilities should simply have different probabilities and, in the long run, appear with different frequencies.\nAlso in this kind of task, therefore, we have the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nthe difference from before is that the sentence \\(\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\) represents not the hypothesis that a given new unit has value \\(\\color[RGB]{238,102,119}z\\), but the possibility of generating a new unit with that value. In other words, the symbol “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” here means “must be set to…” rather than “would be observed to be…”. Remember the discussion and warnings in § 6.3?\n\n\nOur general conclusion is this:\n\n\n\n\n\n\n \n\n\n\n\nProbability distribution such as those discussed above should intrinsically enter all types of machine-learning algorithms.\n\n\n\nThis is the condition for machine-learning algorithms to be optimal and self-consistent. The less an algorithm satisfies that condition, the less optimal and less consistent it is.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Beyond machine learning]{.green}</span>"
    ]
  },
  {
    "objectID": "beyond-ML.html#sec-underlying-distribution",
    "href": "beyond-ML.html#sec-underlying-distribution",
    "title": "27  Beyond machine learning",
    "section": "27.4 The underlying distribution",
    "text": "27.4 The underlying distribution\nA remarkable feature of all the probabilities discussed in the above task categorization is that they can all be calculated from one and the same probability distribution. We briefly discussed and used this feature in chapter  17.\nA conditional probability such as \\(\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) can always be written, by the and-rule, as the ratio of two probabilities:\n\\[\n\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}B} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\\mathsfit{\\color[RGB]{34,136,51}B} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nTherefore we have, for the probabilities of some of the tasks above,\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[2em]\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[2em]\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nWe also know the marginalization rule (chapter  16.1): any quantity \\(\\color[RGB]{204,187,68}C\\) with values \\(\\color[RGB]{204,187,68}c\\) can be introduced into the proposal of a probability via the or-rule:\n\\[\n\\mathrm{P}( {\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\sum_{\\color[RGB]{204,187,68}c}\\mathrm{P}({\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nUsing the marginalization rule we find these final expressions for the probabilities of some machine-learning tasks discussed so far:\n\n\n\n\n\n\n\n \n\n\n\n\n Guess all variates:\n\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}\nZ_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\n\n\n  All previous predictors and predictands known:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}y}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n  Previous predictors known, previous predictands unknown:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\quad{}=\n\\frac{\n\\sum_{\\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{{\\color[RGB]{170,51,119}y}, \\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nAll these formulae, even for hybrid tasks, involve sums and ratios of only one distribution:\n\\[\\boldsymbol{\n\\mathrm{P}(\\color[RGB]{68,119,170}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nStop for a moment and contemplate some of the consequences of this remarkable fact:\n\n  An agent that can perform one of the tasks above can, in principle, also perform all other tasks.\nThis is why a perfect agent, working with probability, in principle does not have to worry about “supervised”, “unsupervised”, “missing data”, “imputation”, and similar situations. This also shows what was briefly mentioned before: all these task typologies are much closer to one another than it might look like from the perspective of current machine-learning methods.\n\n\n\nThe acronym OPM  can stand for Optimal Predictor Machine or Omni-Predictor Machine\n\n  The probability distribution above encodes the agent’s background knowledge and assumptions; different agents differ only in the values of that distribution.\nIf two agents yield different probability values in the same task, with the same variates and same training data, the difference must come from the joint probability distribution above. And, since the data given to the two agents are exactly the same, the difference must lie in the agents’ background information \\(\\mathsfit{I}\\).\n  Data cannot “speak for themselves”\nGiven some data, we can choose two different joint distributions for these data, and therefore get different results in our inferences and tasks. This means that the data alone cannot determine the result: specific background information and assumptions, whether acknowledged or not, always affect the result.\n\nThe qualification “in principle” in the first consequence is important. Some of the sums that enter the formulae above are computationally extremely expensive and, with current technologies and maths techniques, cannot be performed within a reasonable time. But new technologies and new maths discoveries could make these calculations possible. This is why a data engineer cannot simply brush them aside and forget them.\nAs regards the third consequence, we shall see that there are different states of knowledge which can converge to the same results, as the number of training data increases.\n\n\n\n\n\n\n Exercise\n\n\n\nIn a previous example of “hybrid” task we had the probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nRewrite it in terms of the underlying joint distribution.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Beyond machine learning]{.green}</span>"
    ]
  },
  {
    "objectID": "beyond-ML.html#plan-for-the-next-few-chapters",
    "href": "beyond-ML.html#plan-for-the-next-few-chapters",
    "title": "27  Beyond machine learning",
    "section": "27.5 Plan for the next few chapters",
    "text": "27.5 Plan for the next few chapters\nOur goal in building an “Optimal Predictor Machine” is now clear: we must find a way to\n\n\n\n\nassign the joint probability distribution above, in such a way that it reflects some reasonable background information\nencode the distribution in a computationally useful way\n\nThe “encode” goal sounds quite challenging, because the number \\(N\\) of units can in principle be infinite; we have an infinite probability distribution.\nIn the next Inference III part we shall see that partially solving the “assign” goal actually makes the “encode” goal feasible.\n\nOne question arises if we now look at machine-learning methods from our Probability Theory perspective. Some machine-learning methods, including many popular ones, don’t give us probabilities about values. They return one definite value. How do we reconcile this with the probabilistic point of view above? We shall answer this question in full in the last chapters; but a short, intuitive answer can already be given now.\nIf there are several possible correct answers to a given guess, but a machine-learning algorithm gives us only one answer, then the algorithm must have internally chosen one of them. In other words, the machine-learning algorithm is internally doing decision-making. We know from chapters Chapter 2 and Chapter 3 that this process should obey Decision Theory and therefore must involve:\n\n  the probabilities of the possible correct answers\n  the utilities of the possible answer choices\n\nNon-probabilistic machine-learning algorithms must therefore be approximations of an Optimal Predictor Machine that, after computing probabilities, selects one particular answer by using utilities.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Beyond machine learning]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html",
    "href": "exchangeable_probabilities.html",
    "title": "28  Exchangeable beliefs",
    "section": "",
    "text": "28.1 Recap\nIn the chapters of part Inference I we had an overview of how an agent can draw inferences and make predictions of the most general kind, expressed by general sentences, using the four fundamental rules of inference.\nThen, in part Inference II, we successively narrowed our focus on more and more specialized kinds of inference, typical of engineering and data-science problems and of machine-learning algorithms. First we considered inferences about measurements and observations, then inferences about multiple instances of similar measurements and observations. The idea was that an agent can arrive at sharper degrees of belief – that is, it can learn – by using information about “similar instances”.\nFor these purposes we introduced a specialized language about quantities and data types in part Data I, and about “populations” of similar “units” in part Data II.\nIn the Machine learning part we took an overview of current machine-learning methods, and then focused on several types of tasks that popular machine-learning algorithms, such as deep networks and random forests, purport to solve. We found a remarkable result: a perfect agent – one that operates according to Probability Theory – can in principle perform any and all of those tasks by using the joint probability distribution\n\\[\n\\mathrm{P}(\\color[RGB]{68,119,170}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nThe agent only needs to do some calculations with this joint distribution, involving sums and divisions. This distribution must be specified for all possible values of \\(\\color[RGB]{68,119,170}x_{1}, \\dotsc, x_{N+1}\\), \\(\\color[RGB]{68,119,170}y_{1}, \\dotsc, y_{N+1}\\), and \\(N\\).\nTake “supervised learning” for example, that is, the task of predicting some variates (predictands) for a new unit, from knowledge of other variates (predictors) for the same unit and of all variates for \\(N\\) other units. Solving this task corresponds to calculating\nTo build an AI agent that deals with these kinds of task we must: (1) choose a joint distribution according to reasonable assumptions and background information, (2) encode it in a computationally feasible way.\nIn order to reach these two goals we shall now narrow our focus further, upon inferences satisfying a condition that greatly simplifies the calculations, and that is also reasonable in many real inference problems – and it is moreover typical of many “supervised” and “unsupervised” machine-learning applications.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "href": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "title": "28  Exchangeable beliefs",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\n    \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n    \\color[RGB]{0,0,0}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "href": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "title": "28  Exchangeable beliefs",
    "section": "28.2 States of knowledge with symmetries",
    "text": "28.2 States of knowledge with symmetries\nAn agent’s degrees of belief about a particular population may satisfy a special symmetry called exchangeability. This symmetry can be understood from different points of view. Let’s start from one of these viewpoints, and then make connections with alternative ones.\nTake again two populations briefly mentioned in § 20.1:\n\n\nStock exchange\n\nThe daily change in closing price of a stock during 1000 consecutive days. Each day the change can be positive or zero: \\({\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\), or negative: \\({\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\).\n\n\n\n\n\n\n\n\nMars prospecting\n\nA collection of 1000 similar-sized rocks gathered from a specific, large crater on Mars. Each rock either contains haematite: \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), or it doesn’t: \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\n\n\n\n\n\n\n\nSuppose that, in each of these populations, you (the agent) don’t know the variate value for unit #735, and for some reason would like to infer it. You are given the variate values for 100 other units, which you can use to improve your inference. Now consider this question:\n\nHow much does the relative order of the 100 known units and the unknown unit matter to you, for drawing your inference?\n\nWe know, from information theory, that it never hurts having extra information, such as the units’ order. But you probably judge the units’ order to be much more important for your inference in the stock-exchange case than in the Mars-prospecting one. In the stock-exchange case it would be more informative to have data from units temporally close to unit #735; for example units #635–#734, or #736–#835, or #685–#734 & #736–#785, or similar ranges. But in the Mars-prospecting case you might find it acceptable if the 100 known units were picked up in some unsystematic way from the catalogue of remaining 999 units. There are reasons, boiling down to physics, behind this kind of judgement.\nThe question above could also be replaced by others, slightly different but still connected to the same issue. For example:\n\nHow strongly would you like to be able to choose which 100 units you can have data from, in order to draw your inference?\n\nor\n\nHow much would you be upset if the original order of the population units were destroyed by accidental shuffling?\n\nor\n\nWould it be acceptable to you if only the frequencies of the values (\\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}},{\\color[RGB]{238,102,119}{\\small\\verb;-;}}}\\) in one case, \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\) in the other) for the 100 known units were given to you?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nFind examples of populations where the units have some kind of ordering that you think would be very important for drawing inferences about some units, given other units. Examine why you judge such ordering to be important. (The ordering doesn’t need to be one-dimensional. For instance, the pixel intensities of an image also have a two-dimensional relative order or position: is that important if you want to draw inferences about the intensities of some pixels from those of other pixels?)\nFind examples of populations where any potential ordering of the units would not be very important for drawing inferences about some units, given other units. Or, put it otherwise, you wouldn’t be excessively upset or worried if such order were lost owing to accidental shuffling of the units.\n\n\n\nMany kinds of inference considered in data science and engineering, and all inferences done with “supervised” or “unsupervised” machine-learning algorithms, are examples where any ordering of the data used for learning is deemed irrelevant and is, in fact, often lost. This irrelevance is clear from the data-shuffling involved in many procedures that accompany these algorithms.\nWe shall thus restrict our attention to situations and kinds of background information where this judgement of irrelevance is considered appropriate. In reality this is not a black-or-white situation: it is possible that some kind of ordering information would improve our inferences; what we are assuming here is that this improvement is so small that it can be neglected altogether.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "href": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "title": "28  Exchangeable beliefs",
    "section": "28.3 Exchangeable probability distributions",
    "text": "28.3 Exchangeable probability distributions\nLet’s take the Mars-prospecting problem as a concrete example. Denote by \\(H\\) the variate expressing haematite presence, with domain \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}, {\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\).\nIf an agent’s background information or assumption \\(\\mathsfit{I}\\) says that the relative order of units – rocks in this case – is irrelevant for inferences about other units, then it means that a probability such as\n\\[\n\\mathrm{P}(R_{735}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{734}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{733}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{732}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{731}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{730}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nmust be equal to the probability\n\\[\n\\mathrm{P}(R_{735}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{87}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{16}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{52}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{988}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nand in fact to any probability like\n\\[\n\\mathrm{P}(R_{i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{j}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{k}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{l}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{m}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{n}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nfor instance\n\\[\n\\mathrm{P}(R_{356}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{952}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{69}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{740}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{679}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nwhere \\(i\\), \\(j\\), and so on are different but otherwise arbitrary indices.\nIn other words, the probability depends on whether we are inferring \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) or \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\), and on how many \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appear in the conditional; in the example above, three \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and two \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\). This property should also apply if the agent makes inferences about more than one unit, conditional on any number of units. It can be proven that this property is equivalent, in our present example, to this general requirement:\n\nThe value of a joint probability such as\n\\[\\mathrm{P}(R_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\n\\dotso\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\ \\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n\\]\ndepends only on the total number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) values and total number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) values that appear in it, or, equivalently, on the absolute frequencies of the values that appear in it.\n\n\n\nLeaving the Mars-specific example and generalizing, we can define the following property, called exchangeability:\n\n\n\n\n\n\n \n\n\n\n\nA joint probability distribution is called exchangeable if the probabilities for any number of units depend only on the absolute frequencies of the values appearing in them.\n\n\n\nLet’s see a couple more examples.\n\n\nDon’t forget that\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nand\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nmean exactly the same, because and (symbol “\\(,\\)”) is commutative!\n\nConsider an infinite population with variate \\(Y\\) having domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;low;}},{\\color[RGB]{204,187,68}{\\small\\verb;medium;}},{\\color[RGB]{34,136,51}{\\small\\verb;high;}}}\\). If the background information \\(\\mathsfit{J}\\) guarantees exchangeability, then these three joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{283}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{91}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{72}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1838}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause they all have one \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), one \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), two \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\). The same is true for these two probabilities:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3024}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{26}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{611}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{78}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause both have zero \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), two \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), one \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\).\n\n\n\nConsider an infinite population with variates \\((U,V)\\) having joint domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}} \\times \\set{{\\color[RGB]{102,204,238}-1}, {\\color[RGB]{119,119,119}0}, {\\color[RGB]{204,187,68}1}}\\)  (six possible joint values). If the background information \\(\\mathsfit{K}\\) guarantees exchangeability, then these two joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[3ex]\n&=\\mathrm{P}(\nU_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\end{aligned}\n\\]\nbecause both have: one \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{102,204,238}-1})\\),  two \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{119,119,119}0})\\),  zero \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{204,187,68}1})\\),  three \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{102,204,238}-1})\\),  zero \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{119,119,119}0})\\),  one \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{204,187,68}1})\\). From this example, note that it’s important to count the occurrences of the joint values, not of the values of the single variates independently.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nFirst let’s check that you haven’t forgotten the basics about connectives (§ 6.4), Boolean algebra § 9.2), and the four fundamental rules of inference (§ 8.4):\n\nHow much is  \\(\\mathrm{P}(Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\) ?\nSimplify the probability\n\n\\[\\mathrm{P}(X_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwhat are the absolute frequencies of the values \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) among the units in the probability above?\n\n\n\n\nFor each collection of probabilities below (the sentences \\(\\mathsfit{I'}, \\mathsfit{I''}, \\mathsfit{J'}\\dotsc\\) indicate different states of knowledge), say whether they cannot come from an exchangeable probability distribution, or if they might (to guarantee exchangeability, one has to check an infinite number of inequalities, so we can’t be sure about it unless they give us a general formula for the joint probabilities):\n\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 31.6\\%\n  \\\\\n  &\\mathrm{P}(C_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 24.8\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(Z_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{53}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%\n  \\\\\n  &\\mathrm{P}(Z_{3904}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{29}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(A_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{87}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 6.2\\%\n  \\\\\n  &\\mathrm{P}(A_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{10}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{13}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 8.9\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(W_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{300}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 6.2\\%\n  \\\\\n  &\\mathrm{P}(W_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{86}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{107}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 8.9\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(B_{1190}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{1152}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{233}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 7.5\\%\n  \\\\\n  &\\mathrm{P}(B_{1185}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 12.3\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(S_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 5.0\\%\n  \\\\\n  &\\mathrm{P}(S_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 2.9\\%\n  \\end{aligned}\\)\n\n\n\n\n\nFurther constraints\nThe exchangeability property greatly reduces the number of probabilities that an agent needs to specify. For a population with a binary variate, a joint probability distribution for 1000 units would require the specification of around \\(2^{1000} \\approx 10^{300}\\) probabilities. But if this distribution is exchangeable, only \\(1000\\) probabilities need to be specified (the absolute frequency of one of the two values, ranging between 0 and 1000; minus one because of normalization).1\n1 For the general case of a variate with \\(n\\) values, and \\(k\\) units, the number of independent probabilities is \\(\\binom{n+k-1}{k}\\).Moreover, the exchangeable joint distributions for different numbers of units satisfy additional restrictions, owing to the fact that each of them is the marginal distribution of all distributions with a larger number of units. In the Mars-prospecting case, for instance, if \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is the degree of belief that rock #1 contains haematite, we must also have\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n&=\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\\n&= \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nfor any two different units #\\(a\\) and #\\(b\\). Therefore, if the agent has specified \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\), then three of these four probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\\\[1ex]\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\n\\]\nare completely determined if we specify just one of them.\n\n\n\n\n\n\n Exercise\n\n\n\nAssume that the state of knowledge \\(\\mathsfit{I}\\) implies exchangeability, and a population has binary variate \\(R\\in \\set{{\\color[RGB]{204,187,68}{\\small\\verb;N;}},{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}\\).\nIf\n\\[\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.75 \\qquad \\mathrm{P}(R_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.60\\]\nThen how much are the probabilities\n\\[\\mathrm{P}(R_{15}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?} \\qquad\n\\mathrm{P}(R_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{11}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?}\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html",
    "href": "inference_from_freqs.html",
    "title": "29  Inferences from frequencies",
    "section": "",
    "text": "29.1 If the population frequencies were known\nLet’s now see how the exchangeability of an agent’s degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where the agent knows the joint frequency distribution (§§21.2, 21.3, 23.2) for the full population. Second, in the more general case where the agent lacks this population-frequency information.\nWhen the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical “drawing balls from an urn”. We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned “by intuition”, but actually fully determined by the two basic pieces of knowledge or assumptions: exchangeability and known population frequencies. Some simple proof sketches of this will also be given.\nWe consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. In this and the following chapters we shall often use the simplified income dataset (file income_data_nominal_nomissing.csv and its underlying population as an example. This population has nine nominal variates. The variates, their domain sizes, and their possible values are listed at this link.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-pop-freq-known",
    "href": "inference_from_freqs.html#sec-pop-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "",
    "text": "Notation recap\nWe shall mainly use the notation introduced in § 27.3:\nAll population variates, jointly, are denoted \\({\\color[RGB]{68,119,170}Z}\\). In the case of the income dataset, for instance, the variate \\({\\color[RGB]{68,119,170}Z}\\) stands for the joint variate with nine components:\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}Z}&\\coloneqq(\\color[RGB]{68,119,170}\n\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{marital\\_status} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{}\\\\ &\\qquad\n\\color[RGB]{68,119,170}\\mathit{relationship} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{native\\_country} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\n\\color[RGB]{0,0,0})\n\\end{aligned}\n\\]\nWhen we write \\(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\), the symbol \\(\\color[RGB]{68,119,170}z\\) stands for some definite joint values, for instance \\(({\\color[RGB]{68,119,170}{\\small\\verb;Without-pay;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Doctorate;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Ireland;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&gt;50K;}})\\).\nIn applications where the agent wants to infer the values of some predictand variates, given the observation of predictor variates, the former are denoted \\({\\color[RGB]{68,119,170}Y}\\), the latter \\({\\color[RGB]{68,119,170}X}\\). In the income problem, for instance, the agent (some USA census agency) would like to infer the \\(\\color[RGB]{68,119,170}\\mathit{income}\\) variate of a person from the other eight demographic characteristics \\(\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\) of that person. So in this inference problem we define\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}Y}&\\coloneqq{\\color[RGB]{68,119,170}\\mathit{income}}\n\\\\[1ex]\n{\\color[RGB]{68,119,170}X}&\\coloneqq({\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{native\\_country}})\n\\end{aligned}\n\\]\nWe shall, however, also consider slightly different inference problems, for example with \\(({\\color[RGB]{68,119,170}\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}})\\) as predictand and the remaining seven variates \\(({\\color[RGB]{68,119,170}\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}})\\) as predictors.\nOften we shall use red for quantities that are not known in the problem, and green for quantities that are known.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-know-freq",
    "href": "inference_from_freqs.html#sec-know-freq",
    "title": "29  Inferences from frequencies",
    "section": "29.2 Knowing the full-population frequency distribution",
    "text": "29.2 Knowing the full-population frequency distribution\nNow suppose that the agent knows the full-population joint frequency distribution. Let’s make clearer what this means. In the income problem, for instance, consider these two different joint values for the joint variate \\({\\color[RGB]{68,119,170}Z}\\):\n\\[\n\\begin{aligned}\n{\\color[RGB]{68,119,170}z^{*}}&\\coloneqq(\n{\\small\\verb;Private;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Machine-op-inspct;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\\\[2ex]\n{\\color[RGB]{68,119,170}z^{**}}&\\coloneqq(\n{\\small\\verb;Self-emp-not-inc;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad {\\small\\verb;Farming-fishing;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\end{aligned}\n\\]\nThe agent knows that the value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency \\(0.860 369\\%\\); it also knows that the value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) occurs with a relative frequency \\(0.260 058\\%\\). We write this as follows:\n\\[\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}}) = 0.860 369\\% \\ ,\n\\qquad\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}}) = 0.260 058\\%\n\\]\nThe agent knows not only the frequencies of the two particular joint values \\(\\color[RGB]{68,119,170}z^{*}\\), \\(\\color[RGB]{68,119,170}z^{**}\\), but for all possible joint values, that is, for all possible combinations of values from the single variate. In the income example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for \\({\\color[RGB]{68,119,170}Z}\\), which we denote collectively with \\(\\boldsymbol{f}\\) (note the boldface). Let’s introduce the quantity \\(F\\), denoting the full-population frequency distribution. Knowledge that the frequencies are \\(\\boldsymbol{f}\\) is then expressed by the sentence \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\).\n\n\n\n\n\n\n Don’t confuse the full population with a sample from it\n\n\n\nNote that the frequencies reported above are not the ones found in the income_data_nominal_nomissing.csv dataset, because that dataset is only a sample from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).\n\n\nIn other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-1unit-freq-known",
    "href": "inference_from_freqs.html#sec-1unit-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "29.3 Inference about a single unit",
    "text": "29.3 Inference about a single unit\nNow imagine that the agent, given the information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies and some background information \\(\\mathsfit{I}\\), must infer all \\({\\color[RGB]{68,119,170}Z}\\) variates for a specific unit \\(u\\). In the income case, it would be an inference about a specific USA citizen. This unit \\(u\\) could have any particular combination of variate values; in the income case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.1 Which probability values should it assign?\n1 Remember that this is what we mean when we say “drawing an inference”! (See chap.  5 and § 14.1)Intuitively we would say that the probability for a particular value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\) should be equal to the frequency of that value in the full population:\n\n\n\n\n\n\n \n\n\n\n\nif \\(\\mathsfit{I}\\) leads to an exchangeable probability distribution, then\n\\[\n\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z})\n\\]\nfor any unit \\(u\\).\n\n\n\nFor instance, the probabilities that unit \\(u\\) has the values \\(\\color[RGB]{68,119,170}z^{*}\\) or \\(\\color[RGB]{68,119,170}z^{**}\\) above is\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}}) = 0.860 369\\%\n\\\\[1ex]\n&\\mathrm{P}({\\color[RGB]{68,119,170}Z}_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}) = 0.260 058\\%\n\\end{aligned}\n\\]\nThis intuition is the same as in drawing balls, which may have different sets of labels, from a collection, given that we know the proportion of balls with each possible label set.\nBut the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let’s examine a very simple case to get an idea of how this proof works.\n\nExact calculation of the probabilities in a simple case\nSuppose we have three rocks from our Mars-prospecting collection. They are marked #1, #2, #3. They look alike, but we know that two of them have haematite, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) for them, and one doesn’t, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) for that rock. This background information – let’s call it \\(\\mathsfit{K}_{\\textsf{3}}\\) – is a simple case of a finite population with three units and a binary variate \\(R\\). We know that the frequency distribution for this population is\n\n\n\n\\[f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = 2/3 \\qquad f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) = 1/3\\]\nOur information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies corresponds to the following composite sentence:\n\n\\[\nF\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\ \\Longleftrightarrow\\\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nGiven \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\), we know that \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) is true: \\(\\mathrm{P}( F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})=1\\), which means\n\n\\[\n\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}, \\mathsfit{K}_{\\textsf{3}}\\bigr] = 1\n\\]\n\nNow use the or-rule, considering that the three ored sentences are mutually exclusive:\n\n\\[\n\\begin{aligned}\n1&=\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}\\bigr]\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\end{aligned}\n\\]\n\nAccording to our background information \\(\\mathsfit{K}_{\\textsf{3}}\\), our degrees of belief are exchangeable. This means that the three probabilities summed up above must all have the same value, because in each of them \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) once. But if we are summing the same value thrice, and the sum is \\(1\\), that that value must be \\(1/3\\). Hence we find that\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\[1ex]\n&\\text{all other probabilities are zero}\n\\end{aligned}\n\\]\nNow let’s find the probability that a rock, say #1, has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)), given that we haven’t observed any other rocks: \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\). This is a marginal probability (§ 16.1), so it’s given by the sum\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) &=\n\\sum_{i={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\sum_{j={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&= 0 + 1/3 + 1/3 + 0\n\\\\[1ex]\n&= 2/3\n\\end{aligned}\n\\]\nwhich is indeed equal to \\(f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\).\n\n\nThis simple example gives you an idea why our intuition for equating – in specific circumstances – probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) exchangeability.\n\n\n\n\n\n\n Exercises\n\n\n\n\nCalculate \\(\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), that is, the probability that rock #2 has haematite, given that we don’t know the haematite content of any other rock. Is it different from \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), or not? Why?\nBuild a similar proof for a slightly different case; for example four rocks; or two units from a population with a variate having three possible values (instead of just the two \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\)).\nConsider the same calculation we did above, but in the case of background knowledge \\(\\mathsfit{K}_{\\text{NE}}\\) where our degrees of belief are \\(\\text{N}\\)ot \\(\\text{E}\\)xchangeable. For instance, give three different values to the probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\end{gathered}\n\\]\nin such a way that they still sum up to \\(1\\). Then find by marginalization the probability that rock #1 contains haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). Is this probability still equal to the relative frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)?",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-moreunit-freq-known",
    "href": "inference_from_freqs.html#sec-moreunit-freq-known",
    "title": "29  Inferences from frequencies",
    "section": "29.4 Inference about several units",
    "text": "29.4 Inference about several units\nLet’s continue with the Mars-prospecting example of the previous section, with just three rocks. We found that the probability that rock #1 has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)) was \\(2/3\\), given that we haven’t observed any other rocks. This probability was equal to the frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks in the urn.\nNow suppose that we observe rock #2, and it turns out to have haematite (\\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). What is the probability that rock #1 has haematite?\nThe probability we are asking about is \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), and it can be calculated with the usual rules. The result is again the same as the frequency of the \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks, but with respect to the new situation: there are now two rocks left in front of us, and one must contain haematite, while the other doesn’t. The probability is therefore \\(1/2\\), a value different from that we found before, \\(2/3\\):\n\\[\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 1/2\n\\]\nThis situation is quite general: in a collection of many rocks, the probabilities for new observations change accordingly to information about previous observations (and also subsequent ones, if already known).\n\n\nBut consider now the case \\(\\mathsfit{K}\\) of a large collection of 3 000 000 rocks, 2 000 000 of which have haematite and the rest doesn’t.2 The population’s relative frequencies are exactly as in the case with three rocks, and for the probability that rock #1 contains haematite we still have\n2 Note how this scenario becomes very similar to that of coin tosses.\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = \\frac{2 000 000}{3 000 000} = 2/3\n\\]\nNow suppose we examine rock #2 and find haematite. What is the probability that rock #1 also contains haematite? In this case we find\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\frac{1 999 999}{2 999 999} \\approx 2/3\n\\]\nwith an absolute error of only \\(0.000 000 1\\). That is, the probability and frequency are almost the same as before examining rock #2. The reason is clear: the number of rocks is so large that observing some of them doesn’t practically change the content and proportions of the whole collection.\nThe joint probability that rock #2 contains haematite and rock #1 doesn’t is therefore, by the and-rule,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\approx f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\end{aligned}\n\\]\n\nthe approximation being the better, the larger the collection of rocks.\nIt is easy to see that this will hold for more observations, and for different and more complex variate domains, as long as the number of units considered is enough small compared with the population size. For instance\n\n\\[\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\approx\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nwhere \\(\\boldsymbol{f}\\) is the initial frequency distribution for the population.\n\n\nThis situation applies to more general populations: if the full-population frequencies are known, the agent’s beliefs are exchangeable, and the population is practically infinite, then the joint probability that some units have a particular set of values is equal to the product of the frequencies of those values.\n\n\n\n\n\n\n\n \n\n\n\n\nIf an agent:\n\nhas background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nhas full information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the population frequencies \\(\\boldsymbol{f}\\) for the variate \\({\\color[RGB]{68,119,170}Z}\\)\n\nthen\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\approx\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'''}) \\cdot\n\\dotsb\n\\]\nfor any (different) units \\(u', u'', u''', \\dotsc\\) and any (even equal) values \\(\\color[RGB]{68,119,170}z', z'', z''', \\dotsc\\).\n\n\n\n\nThe formula above solves our initial problem: How to calculate and encode the joint probability distribution for the full population?, although it does so only in the case where the full-population frequencies \\(\\boldsymbol{f}\\) are known. In this case this probability is encoded in the \\(\\boldsymbol{f}\\) itself (which can be represented as a multidimensional array), and can be calculated for any desired joint probability distribution just by a multiplication.\nIn the income example from § 29.2, the probability that two units (citizens) #\\(a\\), #\\(c\\) have value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) and one unit #\\(b\\) has value \\(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) is\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{c}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n&\\approx\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{*}}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z^{**}})\n\\\\[1ex]\n&=\n0.260 058\\% \\cdot\n0.860 369\\% \\cdot\n0.260 058\\%\n\\\\\n&= 0.000 005 818 7\\%\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Always check whether the approximate formula above is appropriate\n\n\n\nAs we have seen, the product formula above is strictly speaking only approximate. In situations where the full population has practically infinite size compared to (1) the number of units that the agent uses for learning, and (2) the number of units the agent will draw inferences about, then the formula can be used as if it were exact.\nBut how much is “practically infinite”? No general answer is possible: it depends on the precision required in the specific problem. In some problems, even if learning and inference involve 10% of the units from the full population, the approximation might still be acceptable; but in other problems it might not be. If learning and inference involve 50% or more units from the full population, then the formula above is hardly acceptable.\nThe probability calculus and the four fundamental rules allow us to handle problems with any population size exactly (see the Study reading below), but the exact computation becomes involved and expensive. This is why the approximate product formula above is valuable, when it can be properly used.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-no-learn-freqs",
    "href": "inference_from_freqs.html#sec-no-learn-freqs",
    "title": "29  Inferences from frequencies",
    "section": "29.5 No learning when full-population frequencies are known",
    "text": "29.5 No learning when full-population frequencies are known\nImagine an agent with exchangeable beliefs \\(\\mathsfit{I}\\) and knowledge \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) of the full-population frequencies, who also has observed several units with values (possibly some identical) \\(\\color[RGB]{68,119,170}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\). What is this agent’s degree of belief that a new unit #\\(u\\) has value \\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\)?\nFrom our basic formula for this question,\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\color[RGB]{34,136,51}\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}\n\\\\[2ex]\n&\\qquad{}\\approx\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\nf({\\color[RGB]{68,119,170}{\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}\n\\\\[2ex]\n&\\qquad{}=\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}{\n\\underbracket[0.2ex]{\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z})}_{{}=1} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}\n\\\\[2ex]\n&\\qquad{}=\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z})\n\\\\[3ex]\n&\\qquad{}\\equiv\n\\mathrm{P}(\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\end{aligned}\n\\]\nso the information from the units \\(u'\\), \\(u''\\), and so on is irrelevant to this agent. In other words, this agent’s inferences about some units are not affected by the observation of other units.\nThe reason for this irrelevance is that the agent already knows the full-population frequencies. So the observation of the frequencies of some values provides no new information to the agent.\nObviously this is not what we desire. But it is not a problem: the crucial point is that knowledge of full-population frequencies is only a hypothetical, idealized situation. In the next chapter we shall see that learning occurs when we go beyond this idealization.\n\n\n\n\n\n\n “Learning” about what?\n\n\n\nIn this and the following sections, and sometimes in the following chapters, when we say “the agent is learning” or “the agent is not learning” we mean specifically the change in an agent’s beliefs about observation of variates of some units which had not yet been observed.\nNote that there is always learning about something whenever we put new information in the conditional of a probability. In the Mars-prospecting example above, for example, we have\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 2/3\n\\]\nand the agent has (practically) not learned anything about the sentence \\(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) from the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\nBut we also have\n\\[\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\]\nthe probability for the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) has changed. So the agent has learned something: that rock #2 doesn’t contain haematite (\\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nCh. 3 of Jaynes: Probability Theory. This chapter is extremely instructive in general to understand how probability theory works.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html",
    "href": "inference_about_freqs.html",
    "title": "30  Inference about frequencies",
    "section": "",
    "text": "30.1 Inference when population frequencies aren’t known\nIn chapter  29 we considered an agent that has exchangeable beliefs and that knows the full-population frequencies. The degrees of belief of such an agent have a very simple form: products of frequencies. But for such an agent the observations of units doesn’t give any useful information for drawing inferences about new units: such observations provide frequencies which the agent already knows.\nSituations where we have complete frequency knowledge can be common in engineering problems, where the physical laws underlying the phenomena involved are known and computable. They are far less common in data-science and machine-learning applications: here we must consider agents that do not know the full-population frequencies.\nHow does such an agent calculate probabilities about units? The answer is actually a simple application of the “extension of the conversation” (§ 9.3, which boil down to applications of the and and or rules). A probability given that the frequency distribution is not known is equal to the average of the probabilities given each possible frequency distribution, weighted by the probabilities of the frequency distributions:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\n)\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nBut we saw in § 29.4 that the probability for a sequence of values given a known frequency is just the product of the value’s frequencies. We thus have our long-sought formula:\nThis result is called de Finetti’s representation theorem for exchangeable belief distributions. It must be emphasized that this result is actually independent of any real or imaginary population frequencies. We took a route to it through the idea of population frequencies only to help our intuition. If for any reason you find the idea of a “limit frequency for an infinite population” somewhat suspicious, then don’t worry: the formula above actually does not rely on it. The formula results from the assumption has exchangeable beliefs about a collection of units that can potentially be continued without end.\nLet’s see how this formula works in the simple Mars-prospecting example (with 3 million rocks or more) from § 29.4. Suppose that the agent:\nWhat is the agent’s degree of belief that rock #1 contains haematite? According to the derived rule of extension of the conversation, that is, the main formula written above, we find:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) +\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[1ex]\n&= \\boldsymbol{62.5\\%}\n\\end{aligned}\n\\]\nIn an analogous way we can calculate, for instance, the agent’s belief that rock #1 contains haematite, rock #2 doesn’t, and rock #3 does:\nThis formula generalizes to any population, any variates, and any number of hypotheses about the frequencies.\nMathematical and, even more, computational complications arise when we consider all possible frequency distributions, since there is a practically infinite number of them; they form a continuum in fact. But do not let these practical difficulties affect the intuitive picture behind them, which is simple to grasp once you’ve considered some simple examples.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-freq-not-known",
    "href": "inference_about_freqs.html#sec-freq-not-known",
    "title": "30  Inference about frequencies",
    "section": "",
    "text": "de Finetti’s representation theorem\n\n\n\n\nIf an agent has background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nthen\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}\\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nfor any (different) units \\(u', u'', \\dotsc\\) and any (even equal) values \\(\\color[RGB]{68,119,170}z', z'', \\dotsc\\).\nIn the sum above, \\(\\boldsymbol{f}\\) runs over all possible frequency distributions for the full population.\nProperly speaking the sum is an integral, because \\(F\\) is a continuous quantity. We should write \\(\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n) = \\int\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'})  \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{p}(\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}\\boldsymbol{f}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nForesight: Its logical laws, its subjective sources. This essay gives much insight on our reasoning process in making forecasts and learning from experience.\n\n\n\n\n\n\n\n\nknows that the rock collection consists of:\n\neither a proportion 2/3 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/3 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}'\\)\nor a proportion 1/2 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/2 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}''\\)\n\nassigns a \\(75\\%\\) degree of belief to the first hypothesis, and \\(25\\%\\) to the second (so the sentence \\((F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}') \\lor (F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'')\\) has probability \\(1\\)):\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&\\approx\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) + {}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot {\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot {\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[2ex]\n&\\approx \\boldsymbol{14.236\\%}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider a state of knowledge \\(\\mathsfit{K}'\\) according to which:\n\n\nThe rock collection may have a proportion \\(0/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks (and \\(9/10\\) of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks); call this frequency distribution \\(\\boldsymbol{f}_0\\)\nThe rock collection may have a proportion \\(1/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_1\\)\nand so on… up to\na proportion \\(10/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_{10}\\)\n\n\n\nThe probability of each of these frequency hypotheses is \\(1/11\\), that is:\n\\[\n  \\begin{aligned}\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{0} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\dotso\\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{10} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11\n  \\end{aligned}\n  \\]\n\nCalculate the probabilities\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}')\n\\] \nDo they all have the same value? Try to explain why or why not.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-learning-general",
    "href": "inference_about_freqs.html#sec-learning-general",
    "title": "30  Inference about frequencies",
    "section": "30.2 Learning from observed units",
    "text": "30.2 Learning from observed units\nStaying with the same Mars-prospecting scenario, let’s now ask what’s the agent’s degree of belief that rock #1 contains haematite, given that the agent has found that rock #2 doesn’t contain haematite. In the case of an agent that knows the full-population frequencies we saw § 29.5 that this degree of belief is actually unaffected by other observations. What happens when the population frequencies are not known?\nThe calculation is straightforward:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\frac{\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&\\approx\n\\frac{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&=\n\\frac{\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}{\n{\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}\n\\\\[2ex]\n&\\approx\\frac{\n22.9167\\%\n}{\n37.5000\\%\n}\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\nKnowledge that \\(R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) thus does affect the agent’s belief about \\(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\):\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 62.5\\%\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\approx 61.1\\%\n\\]\nIn particular, the observation of one \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has somewhat decreased the probability of observing a new \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rock.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the minimal number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) observations needed for lowering the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(55\\%\\) or less. \nDoes it seem possible to lower the agent’s belief to less than \\(50\\%\\)? Explain why.\nCalculate the minimal number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) observations needed for increasing the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(65\\%\\) or more. \nDoes it seem possible to increase the agent’s belief to more than \\(2/3\\)? Explain why.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-learn-freqs",
    "href": "inference_about_freqs.html#sec-learn-freqs",
    "title": "30  Inference about frequencies",
    "section": "30.3 How learning works: learning about frequencies",
    "text": "30.3 How learning works: learning about frequencies\nAn agent having full-population frequency information does not learn1 from observation of units, whereas an agent not having such information does learn from observation of units. This fact shows how learning from observed to unobserved units actually works. Crudely speaking, observations do not directly affect the beliefs about unobserved units, but instead affect the beliefs about the population frequencies. And these in turn affect the beliefs about unobserved units. Graphically this could be represented as follows:\n1 remember the warning of § 29.5 about “learning” \nas opposed to this:\n\n\n\n\n \n\n\n\n\n\n\n\n\n Information connections\n\n\n\nThe graphs above represent informational connections, not “causal”. The directed arrows roughly mean “…provides information about…”; they do not mean “…causes…”.\nIn the first graph, the lack of an arrow from observed units to unobserved units means that all information provided by the observed units for the unobserved ones is fully contained in the information about frequencies.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the notion of cause\n\n\n\n\nThe informational relation between observed units, frequencies, and unobserved units becomes clear if we check how the agent’s beliefs about the frequency hypotheses change as observations are made. In the Mars-prospecting example of § 30.1, the agent has initial probabilities\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\nwhere \\(\\boldsymbol{f}'\\) gives frequency \\(2/3\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), and \\(\\boldsymbol{f}''\\) gives frequency \\(1/2\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\). How do these probabilities change, conditional on the agent’s observing that rock #2 doesn’t contain haematite? We just need to use Bayes’s theorem. For the first hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\):\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\frac{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\n\\frac{1}{3} \\cdot\n75\\%\n}{\n\\frac{1}{3} \\cdot\n75\\%\n+\n\\frac{1}{2} \\cdot\n25\\%\n}\n\\\\[2ex]\n&=\n\\boldsymbol{66.667\\%}\n\\end{aligned}\n\\]\n\nand an analogous calculation yields \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})=33.333\\%\\).\nThis result makes sense, because according to the hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) there is a higher proportion of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks than according to \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\), and a \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has been observed. The hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) therefore becomes slightly more plausible, and \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\) slightly less.\nThe updated degree of belief above for the frequencies also gives us an alternative (yet equivalent) way to calculate the conditional probability \\(\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\). Use the derived rule of “extension of the conversation” in a different manner:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n\\text{\\color[RGB]{187,187,187}\\scriptsize(no learning if frequencies are known)}\\enspace\n&=\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}} \\cdot 66.667\\%\n+\n{\\color[RGB]{102,204,238}\\frac{1}{2}} \\cdot 33.333\\%\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\n\nThe result is exactly as in § 30.2 – as it should be: remember from chapter  8 that the four rules of inference are built so as to mathematically guarantee this kind of logical self-consistency.\n\nAn intuitive interpretation of population inference\nThe general expression for the updated belief about frequencies has a very intuitive interpretation. Again using Bayes’s theorem, but omitting the proportionality constant,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&\\propto\n\\mathrm{P}(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&\\propto\n\\overbracket[0.1ex]{f(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\\color[RGB]{0,0,0})  \\cdot \\,\\dotsb\\,\n\\cdot f(\\color[RGB]{34,136,51}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0})}^{\\color[RGB]{119,119,119}\\mathclap{\\text{how well the frequency \"fits\" the data}}}\n\\ \\cdot \\\n\\underbracket[0.1ex]{\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})}_{\\color[RGB]{119,119,119}\\mathclap{\\text{how reasonable the frequency is}}}\n\\end{aligned}\n\\]\n\nThis product can be interpreted as follows.\nTake a hypothetical frequency distribution \\(\\boldsymbol{f}\\). If the data have high frequencies according to it, then the product\n\\[f(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\\color[RGB]{0,0,0})  \\cdot \\,\\dotsb\\,\n\\cdot f(\\color[RGB]{34,136,51}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0})\\]\nhas a large value. Vice versa, if the data have low frequency according to it, that product has a small value. This product therefore expresses how well the hypothetical frequency distribution \\(\\boldsymbol{f}\\) “fits” the observed data.\nOn the other hand, if the factor\n\\[\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\]\nhas a large value, then the hypothetical \\(\\boldsymbol{f}\\) is probable, or “reasonable”, according to the background information \\(\\mathsfit{K}\\). Vice versa, if that factor has a low value, then the hypothetical \\(\\boldsymbol{f}\\) is improbable or “unreasonable”, owing to reasons expressed in the background information \\(\\mathsfit{K}\\).\nThe agent’s belief in the hypothetical \\(\\boldsymbol{f}\\) is a balance between these two factors, the “fit” and the “reasonableness”. This has a very important consequence:\n\n\n\n\n\n\n \n\n\n\n\nProbability inference does not need any “regularization methods” or any procedures against “over-fitting” or “under-fitting”.\nIn fact, the very notions of over- or under-fitting refer to the background information \\(\\mathsfit{K}\\) and the initial belief \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\).\nThink about it: How can we judge that an algorithm is over- or under-fitting, given that we do not know the “ground truth”? (If we knew the ground truth we wouldn’t be making inferences.) Such judgement reveals that we have some preconceived notion of what a reasonable distributions would look like – that’s exactly what \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\) encodes.\n\n\n\nThe agent’s belief about new data is then an average of what the frequency of the new data would be for all possible frequency distributions \\(\\boldsymbol{f}\\):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1} \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{aligned}\n\\]\n\nEach possible \\(\\boldsymbol{f}\\) is weighed by its credibility, which takes into account the fit of the possible frequency to observed data, and its reasonableness against the agent’s background information.\n\n\nOther uses of the belief distribution about frequencies\nThe fact that the agent is actually learning about the full-population frequencies allows it to draw improved inferences not only about units, but also about characteristics intrinsic to the population itself, and also about its own performance in future inferences. For instance, the agent can even forecast the maximal accuracy that can be obtained in future inferences. We shall quickly explore these possibilities in a later chapter.\n\n\n\n\n\n\n Study reading\n\n\n\n\nCh. 4 of Jaynes: Probability Theory\n§§8.1–8.6 of O’Hagan: Probability\nSkim through Heath & Sudderth 1976: De finetti’s theorem on exchangeable variables",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-prob-for-freqs",
    "href": "inference_about_freqs.html#sec-prob-for-freqs",
    "title": "30  Inference about frequencies",
    "section": "30.4 How to assign the probabilities for the frequencies?",
    "text": "30.4 How to assign the probabilities for the frequencies?\nThe general formula we found for the joint probability:\n\\[\n\\mathrm{P}(\n{\\color[RGB]{68,119,170}Z}_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{\\color[RGB]{68,119,170}Z}_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z'}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{68,119,170}z''}\\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nallows us to draw many kinds of predictions about units, which we’ll explore in the next chapter.\nBut how does the agent assign  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) ,  that is, the probability distribution (in fact, a density) over all possible frequency distributions? There is no general answer to this important question, for two main reasons.\nFirst, a proper answer is obviously problem-dependent. In fact  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is the place where the agent encodes any background information relevant to the problem.\nTake the simple example of the tosses of a coin. If you (the agent) examines the coin and the tossing method and they seem ordinary to you, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx \\text{\\small very high}\n\\end{aligned}\n\\]\nBut if you are told that the coin is a magician’s one, with either two heads or two tails, and you don’t know which, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 0\n\\end{aligned}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nAssume the state of knowledge \\(\\mathsfit{I}_{\\text{m}}\\) above and calculate:\n\n\\(\\mathrm{P}(\\textsf{\\small`heads 1st toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}})\\), the probability of heads at the first toss.\n\\(\\mathrm{P}(\\textsf{\\small`heads 2nd toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`heads 1st toss'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{m}})\\), the probability of heads at the second toss, given that heads was observed at the first.\n\nExplain your findings.\n\n\nSecond, for complex situations with many variates of different types it is may be mathematically and computationally difficult to write down and encode  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) .  Moreover, the multidimensional characteristics and quirks of this belief distribution can be difficult to grasp and understand.\nYet it is a result of probability theory (§ 5.2) that we cannot avoid specifying \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). Any “methods” that claim to avoid the specification of that probability distribution are covertly specifying one instead, and hiding it from sight. It is therefore best to have this distribution at least open to inspection rather than hidden.\nLuckily, if  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is “open-minded”, that is, if it doesn’t exclude a priori any frequency distribution \\(\\boldsymbol{f}\\), or in other words if it doesn’t assign strictly zero belief to any \\(\\boldsymbol{f}\\), then with enough data the updated belief distribution   \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{data} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  will actually converge to the true frequency distribution of the full population. The tricky word here is “enough”. In some problems a dozen observed units might be enough; in other problems a million observed units might not be enough yet.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nBayesian statistical inference for psychological research\n\n\n\n\nIn chapter  32 we shall discuss and implement a mathematically concrete belief distribution \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{D})\\) appropriate to task involving nominal variates.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "summary_formulae.html",
    "href": "summary_formulae.html",
    "title": "31  Final inference formulae",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\n\n\nWe finally have all theoretical ingredients and formulae to use the probability calculus for drawing many kinds of inferences about some units in a population, given observations from other units. Keep in mind the minimal assumptions we are making in these formulae – which also underlie all machine-learning algorithms for “supervised” and “unsupervised” learning:\n\nbeliefs about units are exchangeable,\nthe population size is practically infinite.\n\nIn the next part, An Optimal Predictor Machine, we shall computationally implement these formulae and use them in a couple of simple and not-so-simple inference problems.\nHere we collect the main formulae for exchangeable beliefs and tasks about\n\n  forecasting all variates (no predictors)\n  forecasting predictands given predictors; all previous predictors and predictands known\n\nWe still use the general scenario and notation of § 27.3.\n\n\nAll inferences about units of a population rely on the joint probability for any number of units, which is given by the following formula (§ 30.1):\n\n\n\n\n\n\n\nMain formulae for some inference tasks under exchangeable beliefs\n\n\n\n\nde Finetti’s representation\n\n\n\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nor, in terms of predictand \\({\\color[RGB]{68,119,170}Y}\\) and predictors \\({\\color[RGB]{68,119,170}X}\\) variates:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is problem-dependent and must be specified by the agent.\n\n\n\n\n\n Inferences about all variates \\({\\color[RGB]{68,119,170}Z}\\) of a new unit, given observed units\n\n\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\n    \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}\n    =\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}z} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n    \\\\[3ex]\n    &\\qquad{}\n    =\n    \\frac{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n\n\n Inferences about predictands \\({\\color[RGB]{68,119,170}Y}\\) of a new unit, given its predictors \\({\\color[RGB]{68,119,170}X}\\) and given both predictands & predictors of observed units\n\n\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}y} \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n    \\\\[3ex]\n    &\\qquad{}=\n    \\frac{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}) \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>[Final inference formulae]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html",
    "href": "dirichlet-mixture.html",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "",
    "text": "32.1 A belief distribution for frequency distributions over nominal variates\nWe have finally collected all we need to build a real, prototype\nup from ground principles. This will be done in the present and following chapters of this part. In this chapter we specify and discuss a concrete belief distribution representing an agent with exchangeable beliefs about nominal variates. In the next we put it into code. Then we apply it to some real datasets.\nIn this course we sadly shall not examine in depth many mathematical expressions for belief distributions \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) over frequencies. We briefly discuss one, called the Dirichlet-mixture belief distribution. The state of knowledge underlying this distribution will be denoted \\(\\mathsfit{I}_{\\textrm{d}}\\).\nThe Dirichlet-mixture distribution is appropriate for statistical populations with nominal, discrete variates, or joint variates with all nominal components. It is not appropriate to discrete ordinal variates, because it implicitly assumes that there is no natural order to the variate values.\nSuppose we have a simple or joint nominal variate \\({\\color[RGB]{68,119,170}Z}\\) which can assume \\(M\\) different values (these can be joint values, as in the examples of § 29.2). As usual \\(\\boldsymbol{f}\\) denotes a specific frequency distribution for the variate values. For a specific value \\({\\color[RGB]{68,119,170}z}\\), \\(f({\\color[RGB]{68,119,170}z})\\) is the relative frequency with which that value occurs in the full population.\nThe Dirichlet-mixture distribution assigns to \\(\\boldsymbol{f}\\) a probability density given by the following formula:\n\\[\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) =\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Biggl[\\prod_{{\\color[RGB]{68,119,170}z}} f({\\color[RGB]{68,119,170}z})^{\\frac{2^k}{M} -1} \\Biggr]\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\]\nBesides some multiplicative constants, the probability is simply proportional to the product of all frequencies, raised to some powers. The product “\\(\\prod_{{\\color[RGB]{68,119,170}z}}\\)” is over all \\(M\\) possible values of \\({\\color[RGB]{68,119,170}Z}\\). The sum “\\(\\sum_{k}\\)” is over an integer (positive or negative) index \\(k\\) that runs between the minimum value \\(k_{\\text{mi}}\\) and the maximum value \\(k_{\\text{ma}}\\). In the applications of the next chapters these minimum and maximum are chosen as follows:\n\\[\nk_{\\text{mi}}=0\n\\qquad\nk_{\\text{ma}}=20\n\\]\nso the sum \\(\\sum_k\\) runs over 21 terms.\nIn most applications it does not matter if we take a lower \\(k_{\\text{mi}}\\) or a higher \\(k_{\\text{ma}}\\).",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-intro-dirichlet-mix",
    "href": "dirichlet-mixture.html#sec-intro-dirichlet-mix",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "",
    "text": "For the extra curious\n\n\n\nSome of the theoretical basis for the choice of this belief distribution can be found in chapters 4–5 of The Estimation of Probabilities.\n\n\n\n\n\n\n\n\n\n\nMeaning of the \\(k_{\\text{mi}}, k_{\\text{ma}}\\) parameters\nThe parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\) encode, approximately speaking, the agent’s prior belief about how many data are needed to change its initial beliefs. More precisely, \\(2^{k_{\\text{mi}}}\\) and \\(2^{k_{\\text{ma}}}\\) represent a lower and an upper bound on the amount of data necessary to overcome initial beliefs. Values  \\(k_{\\text{mi}}=0\\),  \\(k_{\\text{ma}}=20\\)  represent the belief that such amount could be anywhere between 1 unit and approximately 1 million units. The belief is spread out uniformly across the orders of magnitude in between.\nIf \\(2^{k_{\\text{mi}}}\\) is larger than the amount of training data, the agent will consider these data insufficient, and tend to give uniform probabilities to its inferences, for example a 50%/50% probability to a binary variate.\nIf the amount of data that should be considered “enough” is known, for example from previous studies on similar populations, the parameters \\(k_{\\text{mi}},k_{\\text{ma}}\\) can be set to that order of magnitude (in base 2), minus or plus some magnitude range.\nNote that if such an order of magnitude is not known, then it does not make sense to “estimate” it from training data with other methods, because an agent with a Dirichlet-mixture distribution will already do that internally (and in an optimal way), provided an ample range is given with \\(k_{\\text{mi}}, k_{\\text{ma}}\\).\n\n\n\n\n\n\n Exercise\n\n\n\nIn the calculations and exercises that follow, you’re welcome to do them also with other \\(k_{\\text{mi}}\\) and \\(k_{\\text{ma}}\\) values, and see what happens.\n\n\n\n\nLet’s see how this formula looks like in a concrete, simple example: the Mars-prospecting scenario (which has many analogies with coin tosses).\nThe variate \\(R\\) can assume two values \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\), so \\(M=2\\) in this case. The frequency distribution consists in two frequencies:\n\\[f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\qquad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\\]\nof which only one can be chosen independently, since they must sum up to 1. For instance we could consider \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.5, f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.5\\),  or \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.84, f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.16\\),  and so on.\nOnly for this example we choose\n\\[k_{\\text{mi}}=0 \\qquad k_{\\text{ma}}=2\\]\nso that the sum \\(\\sum_k\\) runs over 3 terms.\nThe agent’s belief distribution for the frequencies is\n\\[\n\\begin{aligned}\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) &=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\Biggl[\\prod_{{\\color[RGB]{68,119,170}z}={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}} f({\\color[RGB]{68,119,170}z})^{\\frac{2^k}{2} -1} \\Biggr]\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[2ex]\n&=\n\\frac{1}{3}\n\\Biggl[\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{0}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{0}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{0} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{0}}{2} - 1\\bigr)!}^2\n}\n+{} \\\\[1ex]&\\qquad\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{1}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{1}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{1} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{1}}{2} - 1\\bigr)!}^2\n}\n+{} \\\\[1ex]&\\qquad\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{2}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{2}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{2} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{2}}{2} - 1\\bigr)!}^2\n}\n\\Biggr]\n\\\\[2ex]\n&=\n\\frac{1}{3}\n\\Biggl[\n\\frac{1}{\\sqrt{f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})}}\\cdot \\frac{1}{\\sqrt{f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})}}\n\\cdot \\frac{1}{\\pi}\n+{} \\\\[1ex]&\\qquad\n1 \\cdot 1 \\cdot \\frac{1}{1}\n+{} \\\\[1ex]&\\qquad f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\cdot \\frac{6}{1}\n\\Biggr]\n\\end{aligned}\n\\]\n\n\nWe can visualize this belief distribution (with \\(k_{\\text{mi}}=0, k_{\\text{ma}}=2\\)) with a generalized scatter plot (§ 15.7) of 100 frequency distributions, each represented by a line histogram (§ 14.2):\n\nAlternatively we can represent the probability density of the frequency \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\):\n\nYou can see some characteristics of this belief:\n\nall possible frequency distributions are taken into account, that is, no frequency is judged impossible and given zero probability\na higher probability is given to frequency distributions that are almost 50%/50%, or that are almost 0%/100% or 100%/0%\n\nThe second characteristic expresses the belief that the agent may more often deal with tasks where frequencies are almost symmetric (think of coin toss), or the opposite: tasks where, once you observe a phenomenon, you’re quite sure you’ll keep observing it. This latter case is typical of some physical phenomena; an example is given by Jaynes:\n\nFor example, in a chemical laboratory we find a jar containing an unknown and unlabeled compound. We are at first completely ignorant as to whether a small sample of this compound will dissolve in water or not. But having observed that one small sample does dissolve, we infer immediately that all samples of this compound are water soluble, and although this conclusion does not carry quite the force of deductive proof, we feel strongly that the inference was justified.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPrior probabilities\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nCalculate the formula above for these three frequency distributions:\n\n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.5\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.5\\)\n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.75\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.25\\)\n\\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.99\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.01\\)",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-joint-prob-dirichlet",
    "href": "dirichlet-mixture.html#sec-joint-prob-dirichlet",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "32.2 Formula for joint probabilities about units with the Dirichlet-mixture distribution",
    "text": "32.2 Formula for joint probabilities about units with the Dirichlet-mixture distribution\nA mathematical advantage of the Dirichlet-mixture belief distribution is that some formulae where it enters can be computed exactly. The most important formula is de Finetti’s representation theorem (§ 30.1), where the sum \\(\\sum_{\\boldsymbol{f}}\\) (really an integral \\(\\int\\,\\mathrm{d}\\boldsymbol{f}\\)) can be calculated analytically in the case of the Dirichlet-mixture:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\n\\color[RGB]{68,119,170}\nZ_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n&=\n\\int\nf(\\color[RGB]{68,119,170}Z_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L} \\color[RGB]{0,0,0}) \\cdot\n\\,\\dotsb\\, \\cdot\nf(\\color[RGB]{68,119,170}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\color[RGB]{0,0,0}) \\cdot\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\,\\mathrm{d}\\boldsymbol{f}\n\\\\[2ex]\n&=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\end{aligned}\n\\]\n\nwhere \\(\\#{\\color[RGB]{68,119,170}z}\\) is the multiplicity with which the specific value \\({\\color[RGB]{68,119,170}z}\\) occurs in the sequence \\(\\color[RGB]{68,119,170}z_1,\\dotsc,z_{L}\\).\n\n\nLet’s see how this formula looks like in an example from the Mars-prospecting scenario.\nTake the sequence1\n1 Remember that the agent has exchangeable beliefs, so the units’ IDs don’t matter (§ 28.3)!\\[\nR_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\n\\]\nin this sequence, value \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears thrice and value \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) once, that is\n\\[L=4 \\qquad \\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3 \\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1\\]\nIn this case we have \\(M=2\\), and we still take \\(k_{\\text{mi}}=0, k_{\\text{ma}}=2\\). The formula above then becomes\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\n\\underbracket[0.1ex]{R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}_{\n\\color[RGB]{187,187,187}L=4\\quad \\#{\\small\\verb;Y;}=3\\quad \\#{\\small\\verb;N;}=1\n}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n\\\\\n&\\qquad{}=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + \\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}- 1\\bigr)!\n\\cdot \\bigl(\\frac{2^{k}}{2} + \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}- 1\\bigr)!\n}{\n\\bigl(2^{k} + 4 -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=-1}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2^{k} + 3 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.048 735 1}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nUsing the formula above, calculate:\n\n\\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\) ; does the result make sense?\n\\(\\mathrm{P}(R_{32}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\)\n\n\n\nTry doing the calculation above on a computer, with \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\). What happens?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-formulae-with-Dirmix",
    "href": "dirichlet-mixture.html#sec-formulae-with-Dirmix",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "32.3 Examples of inference tasks with the Dirichlet-mixture belief distribution",
    "text": "32.3 Examples of inference tasks with the Dirichlet-mixture belief distribution\nWith the explicit formula\n\n\n\n\n\n\n\n \n\n\n\n\\[\n\\mathrm{P}(\n\\color[RGB]{68,119,170}\nZ_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\]\nwhere \\(\\#{\\color[RGB]{68,119,170}z}\\) is the number of times some value \\({\\color[RGB]{68,119,170}z}\\) appears in the sequence\n\n\n\nwe can now solve all the kinds of task involving multiple units that were discussed in § 27.3. Let’s see a simple example of forecasting all variates for a new unit (no predictors), and another example where we guess a predictand given a predictor. We shall solve them step by step.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-dirmix-example1",
    "href": "dirichlet-mixture.html#sec-dirmix-example1",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "32.4 Example 1: Forecast about one variate, given previous observations",
    "text": "32.4 Example 1: Forecast about one variate, given previous observations\nIn this task there are no predictors and only one predictand variate \\(R\\): the presence of haematite. The agent observes the value of this variate in several rocks, and tries to forecast its value in a new rock.\nThe agent has exchangeable beliefs represented by a Dirichlet-mixture distribution. The variate of the population of interest has two possible values, so in the formulae above we have  \\(M=2\\). We still take \\(k_{\\text{mi}}=0\\), \\(k_{\\text{ma}}=2\\).\nThe agent has collected three rocks. Upon examination, two of them contain haematite, one doesn’t. The agent’s data are therefore\n\\[\\text{\\small data:}\\quad R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\]\n(remember that it doesn’t matter how the rocks are labelled, because the agent’s beliefs are exchangeable).\nWhat probability should the agent give to finding haematite in a newly collected rock? That is, what value should it assign to\n\\[\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}}) \\ ?\\]\nOur main formula is\n\n\\[\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{{\\color[RGB]{170,51,119}r}={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}r} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\]\n\n\n\n    The fraction above requires the computation of two joint probabilities:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\end{aligned}\n\\]\nNote how they are associated with the two possible hypotheses about the new rock:\n\\[R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\qquad R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\]\nof which the first one interests us.\n\na   In the first joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears thrice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears once, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3 \\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1 \\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}4}\\]\nThe de Finetti representation formula gives\n\\[\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2^{k} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.048 735 1}\n\\end{aligned}\n\\]\n\n\nb   In the second joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears twice, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 2 \\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 2 \\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}4}\\]\nWe find\n\\[\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}2} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}2} - 1\\bigr)!\n}{\n\\bigl(2^{k} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.033 209 3}\n\\end{aligned}\n\\]\n\n\n\n    The probability for the hypothesis of interest is finally given by the fraction\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{{\\color[RGB]{170,51,119}r}={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}r} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) +\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad{}=\n\\frac{0.048 735 1}{0.048 735 1 + 0.033 209 3}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{59.47\\%}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nThe inference problem above has some analogy with coin tossing: there’s just one, binary, variate. This agent could have been used to make forecasts about coin tosses.\nConsider the result above from the point of view of this analogy. Let’s say that \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) would be “heads”, and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) “tails”. Having observed four coin tosses, with three heads and one tail, the agent is giving a 58% probability for heads at the next toss.\n\nDo you consider this probability reasonable? Why?\nIn which different coin-tossing circumstances would you consider this probability reasonable (given the same previous observation data)?\n\nTry doing the calculation above on a computer, using the values  \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\).\nIf you use the formulae above as they’re given, you’ll probably get just NaNs. The formulae above must be rewritten in a different way in order not to generate overflow. The result would be \\(\\boldsymbol{51.42\\%}\\).",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-dirmix-example2",
    "href": "dirichlet-mixture.html#sec-dirmix-example2",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "32.5 Example 2: Forecast about one predictand, given predictor and previous observations",
    "text": "32.5 Example 2: Forecast about one predictand, given predictor and previous observations\nLet’s go back to the hospital scenario of § 17.5. The units are patients coming into a hospital. The population is characterized by two nominal variates:\n\n\\(T\\): the patient’s means of transportation at arrival, with domain \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\)\n\\(U\\): the patient’s need of urgent care, with domain \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\)\n\nThe combined variate \\((U \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T)\\) has  \\(M = 2\\cdot 3 = 6\\)  possible values. We still use parameters \\(k_{\\text{mi}}=0\\) and \\(k_{\\text{ma}}=2\\) to use the de Finetti formula as-is without causing overflow errors.\nThe agent’s task is to forecast whether the next incoming patient will require urgent care, given information about the patient’s transportation. So \\(U\\) is the predictand variate, \\(T\\) the predictor variate.\nAt the moment the agent has a complete record of two previous patients:\n\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\)\n\\(U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\)\n\nA third patient is incoming by ambulance:\n\n\\(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\)\n\nWhat is the probability that this patient requires urgent care?\n\nInitial belief\nFirst let’s get a glimpse of the agent’s forecast if it were given no information about previous patients. We therefore want to calculate the probability\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\]\n\n\n    We need to calculate the two joint probabilities\n\n\\[\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\qquad\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\]\n\ncorresponding to the two hypotheses of interest, \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\).\n\n\na   In the first joint probability above the value pair \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;})\\) appears once, and the remaining five pairs appear zero times:\n\n\\[\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1} \\qquad\\text{\\small five others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\n\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}1}\\]\n\nThe de Finetti formula gives\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}five factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}1} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{1/6}\n\\end{aligned}\n\\]\n\n\n\nb   In the second joint probability above the value pair \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;})\\) appears once, and the remaining five pairs appear zero times:\n\n\\[\\#({\\small\\verb;non-urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1} \\qquad\\text{\\small five others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\n\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}1}\\]\n\nThe de Finetti formula gives an identical result as before:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}five factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}1} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{1/6}\n\\end{aligned}\n\\]\n\n\n\n    The probability that this incoming patient is urgent is then\n\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{\\color[RGB]{170,51,119}u}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}u} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n+\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad{}=\n\\frac{1/6 }{1/6 + 1/6}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{50\\%}\n\\end{aligned}\n\\]\n\n\n\nThe agent’s background information says that, a priori, urgent and non-urgent patients are equally plausible. This is a characteristic of the particular Dirichlet-mixture belief distribution we are using. It would actually be possible to modify it so as to give a-priori different probabilities for the \\({\\small\\verb;urgent;}\\) and \\({\\small\\verb;non-urgent;}\\) values; but we shall not pursue this possibility here.\n\n\n\n\nForecast after learning\nNow let’s take into account the information about the previous two patients. We therefore want to calculate the probability\n\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\n\\]\n\n\n\n    The hypotheses are again \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\), and we need the joint probabilities\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\end{aligned}\n\\]\n\n\n\na   The first joint probability has the following counts:\n\n\\[\n\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{68,119,170}2} \\quad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;other;}) = {\\color[RGB]{102,204,238}1} \\quad\n\\text{\\small four others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\n\\quad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}3}\\]\n\nand therefore\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{68,119,170}2} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}four factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}3} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.005 915 64}\n\\end{aligned}\n\\]\n\n\n\nb   Counts for the second joint probability:\n\n\\[\n\\begin{gathered}\n\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1} \\qquad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1} \\qquad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;other;}) = {\\color[RGB]{102,204,238}1}\n\\\\[1ex]\n\\text{\\small three others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\n\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}3}\n\\end{gathered}\n\\]\n\nand therefore\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=0}^{2}\n\\frac{\n\\underbracket[0.1ex]{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)!\n\\cdot\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)!\n}_{\\text{\\color[RGB]{187,187,187}three factors}}\n\\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}three factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}3} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.001 594 65}\n\\end{aligned}\n\\]\n\n\n\n    Finally, the probability that the third incoming patient is urgent is\n\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{0.005 915 64}{0.005 915 64 + 0.001 594 65}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{78.77\\%}\n\\end{aligned}\n\\]\n\nThe agent has learned an association between \\({\\small\\verb;ambulance;}\\) and \\({\\small\\verb;urgent;}\\) from the first patient. Note that if we had used the parameters \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\), the result would have been more conservative: \\(\\boldsymbol{54.90\\%}\\).\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nDo the inverse inference, using urgency \\(U\\) as predictor, and transportation \\(T\\) as predictand. That is, calculate the probability\n\\[\\mathrm{P}( T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\\]\nImagine that the urgency variate for the first patient, \\(U_1\\), is not known (missing data). Using the formula for marginalization (see § 27.4), calculate the corresponding probability\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nT_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\\]\nMake similar kinds of inferences, freely trying other combinations of information about the two previous patients.\nDo the same, but with three previous patients instead of two.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-critique-dirmix",
    "href": "dirichlet-mixture.html#sec-critique-dirmix",
    "title": "32  The Dirichlet-mixture belief distribution",
    "section": "32.6 When is the Dirichlet-mixture belief distribution appropriate?",
    "text": "32.6 When is the Dirichlet-mixture belief distribution appropriate?\nThe two examples above reveal some characteristics of an agent based on the Dirichlet-mixture belief distribution:\n\nIn absence of previous data, it assigns uniform probability distributions to any variate.\nIt can be “eager” to learn from previous examples, that is, its probabilities may vary appreciably even with only few observations. The “eagerness” is determined by the parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\). For a general-purpose agent, the values \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\) are more reasonable.\n\nThere are also other subtle characteristics connected to the two above, which we won’t discuss here.\nThese characteristics can be appropriate to some inference tasks, but not to others. It is again a matter of background information about the task one wants to solve.\nThe background information implicit in the Dirichlet-mixture belief distribution can be reasonable in situations where:\n\nThere is very little information about the physics or science behind the (nominal) variates and population, so one is willing to give a lot of weight to observed data. Contrast this with the coin-tossing scenario, where our physics knowledge about coin tosses make us appreciably change our probabilities only after a large number of observations.\n\n\n\n\n\n\n\n\n\n Study reading (again)\n\n\n\nDiaconis & al. 2007: Dynamical Bias in the Coin Toss \n\n\n\nA large number of previous observations is available, “large” relative to the domain size \\(M\\) of the joint variate \\(Z\\).\nThe joint variate \\(Z\\) has a small domain.\n\nIt is possible to modify the Dirichlet-mixture belief distribution in order to alter the characteristics above. Some modifications can assign more a-priori plausibility to some variate values than others, or make the initial belief less affected by observed data.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nThe Dirichlet-tree distribution: discusses a more flexible generalization of the Dirichlet distribution\nMonkeys, kangaroos, and N: is an insightful discussion of how to investigate and represent prior beliefs.\n\n\n\nThese possibilities should remind us about the importance of assessing and specifying appropriate background information. No matter the amount of data, what the data eventually “tell” us acquires meaning only against the background information from which they are observed.\n\nIn the next chapter we discuss the code implementation of the formulae for the Dirichlet-mixture agent.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[The Dirichlet-mixture belief distribution]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html",
    "href": "code_design.html",
    "title": "33  Code design",
    "section": "",
    "text": "33.1 Range of use of the code\nBefore starting, let’s agree on some terminology in order not to get confused in the discussion below.\nThe concrete formulae discussed in the previous chapter  32 can be put into code, for use in different tasks involving only nominal variates. Software of this kind can in principle be written to allow for some or all of the versatility discussed in §§ 27.3–27.4, for example the possibility of taking care (in a first-principled way!) of partially missing training data. But the more versatile we make the software, the more memory, processing power, and computation time it will require.\nRoughly speaking, more versatility corresponds to calculations of the joint probability\nfor more values of the quantities \\(\\color[RGB]{68,119,170}Z_1, Z_2, \\dotsc\\). For instance, if data about unit #4 are missing, then we need to calculate the joint probability above for several (possibly all) values of \\(\\color[RGB]{68,119,170}Z_4\\). If data about two units are missing, then we need to do an analogous calculation for all possible combinations of values; and so on.\nFor our prototype, let’s forgo versatility about units used as training data. From now on we abbreviate the set of training data as\n\\[\n\\mathsfit{\\color[RGB]{34,136,51}data}\\coloneqq\n(\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\land \\dotsb \\land\nZ_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_2 \\land\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0})\n\\]\nwhere \\(\\color[RGB]{68,119,170}z_N, \\dotsc, z_2, z_1\\) are specific values, stored in some training dataset. No values are missing.\nSince the training \\(\\mathsfit{\\color[RGB]{34,136,51}data}\\) are given and fixed in a task, we omit the suffix “\\({}_{N+1}\\)” that we have often used to indicate a “new” unit. So “\\(\\color[RGB]{68,119,170}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\)” simply refers to the variate \\({\\color[RGB]{68,119,170}Z}\\) in a new application of the task.\nWe allow for full versatility in every new instance. This means that we can accommodate, on the spot at each new instance, what the predictand variates are, and what the predictor variates (if any) are. For example, if the population has three variates \\({\\color[RGB]{68,119,170}Z}=({\\color[RGB]{68,119,170}A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}C})\\), our prototype can calculate, at each new application, inferences such as",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Code design]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-code-range",
    "href": "code_design.html#sec-code-range",
    "title": "33  Code design",
    "section": "",
    "text": "\\[\n\\mathrm{P}(\n\\color[RGB]{68,119,170}\nZ_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\quad\n\\tag{33.1}\\]\n\n\n\n\n\n\n\nRecall that \\({\\color[RGB]{68,119,170}Z}\\) denotes all (nominal) variates of the population\n\n\n\n\n\n\\(P({\\color[RGB]{68,119,170}B}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): any one predictand variate, no predictors\n\\(P({\\color[RGB]{68,119,170}A}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}C}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): any two predictand variates, no predictors\n\\(P({\\color[RGB]{68,119,170}A}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}B}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}C}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): all three variates\n\\(P({\\color[RGB]{68,119,170}B}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}A}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): any one predictand variate, any other one predictor\n\\(P({\\color[RGB]{68,119,170}B}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{68,119,170}A}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}C}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): any one predictand variate, any other two predictors\n\\(P({\\color[RGB]{68,119,170}A}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{68,119,170}C}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}B}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): any two predictand variates, any other one predictor",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Code design]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-code-computations",
    "href": "code_design.html#sec-code-computations",
    "title": "33  Code design",
    "section": "33.2 Code design and computations needed",
    "text": "33.2 Code design and computations needed\nTo enjoy the versatility discussed above, the code needs to compute\n\n\n\n\n\n\n\n \n\n\n\n\\[\n\\mathrm{P}(\n\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\\mathsfit{\\color[RGB]{34,136,51}data}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Biggl(\\frac{2^{k}}{M} + {\\color[RGB]{34,136,51}\\#}{\\color[RGB]{68,119,170}z}\\Biggr)\n\\cdot\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + {\\color[RGB]{34,136,51}\\# z} - 1\\bigr)!\n}{\n\\bigl(2^{k} + N \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\tag{33.2}\\]\nfor all possible values \\({\\color[RGB]{68,119,170}z}\\), where \\({\\color[RGB]{34,136,51}\\#}{\\color[RGB]{68,119,170}z}\\) is the number of times value \\({\\color[RGB]{68,119,170}z}\\) appears in the training data, and \\(N = \\sum_{\\color[RGB]{34,136,51}z}{\\color[RGB]{34,136,51}\\# z}\\) is the number of training data\n\n\n\nThis formula is just a rewriting of formula (33.1) for \\(L=N+1\\), simplified by using the property of the factorial\n\\[(a+1)! = (a+1) \\cdot a!\\]\nBut the computation of formula (33.2) (for all values of \\({\\color[RGB]{68,119,170}z}\\)) must be done only once for a given task. For a new application we only need to combine these already-computed probabilities via sums and fractions. For example, in the three-variate case above, if in a new application we need to forecast \\(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a\\) given \\(\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c\\), then we calculate\n\n\n\n\n\n\n\n \n\n\n\n(example with \\(Z \\coloneqq({\\color[RGB]{238,102,119}A}, {\\color[RGB]{68,119,170}B}, {\\color[RGB]{204,187,68}C})\\))\n\\[\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{\\color[RGB]{170,51,119}\\alpha}\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}\\alpha} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\quad\n\\tag{33.3}\\]\n\n\n\nwhere all \\(P(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\) are already computed.\n\n\nOur prototype software must therefore include two main functions, which we can call as follows:\n\n\nbuildagent() (see code)\n\ncomputes \\(\\color[RGB]{34,136,51}\\#{\\color[RGB]{68,119,170}z}\\) for all values \\({\\color[RGB]{68,119,170}z}\\), as well as the multiplicative factors\n\n\n\\[\n  \\frac{\n  \\bigl(2^{k} -1 \\bigr)!\n}{\n\\bigl(2^{k} + N \\bigr)!\n\\cdot\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\]\nfor all \\(k\\), in (33.2). This computation is done once and for all in a given task, using the training \\(\\mathsfit{\\color[RGB]{34,136,51}data}\\) and the metadata \\(\\mathsfit{I}_{\\textrm{d}}\\) provided. The result can be stored in an array or similar object, which we shall call an agent-class object.\n\ninfer() (see code)\n\ncomputes probabilities such as (33.3) at each new instance, using the stored agent-class object as well as the predictor variates and values provided with that instance, and the predictand variates requested at that instance.\n\n\n\n\n\nWe shall also include four additional functions for convenience:\n\n\nguessmetadata()\n\nbuilds a preliminary metadata file, encoding the background information \\(\\mathsfit{I}_{\\textrm{d}}\\), from some dataset.\n\n\n\ndecide()\n\nmakes a decision according to expected-utility maximization (chapter  3), using probabilities calculated with infer() and utilities.\n\n\n\nrF()\n\ndraws one or more possible full-population frequency distribution \\(\\boldsymbol{f}\\), according to the updated degree of belief \\(\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\)\n\n\n\nplotFsamples1D()\n\nplots, as a generalized scatter plot, the possible full-population marginal frequency distributions for a single (not joint) predictand variate. If required it also also the final probability obtained with infer().\n\n\n\nmutualinfo()\n\ncalculates the mutual information (§ 18.5) between any two sets of variates.\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the and-rule, prove (pay attention to the conditional “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)” bar):\n\\[\n\\frac{\n\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{\\color[RGB]{170,51,119}\\alpha}\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}\\alpha} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n=\n\\frac{\n\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}a \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{\\color[RGB]{170,51,119}\\alpha}\\sum_{\\color[RGB]{68,119,170}b}\nP(\\color[RGB]{238,102,119}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}\\alpha} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{68,119,170}B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}b \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n}\n\\]\n\n\nThis exercise shows that instead of\n\\[\\mathrm{P}(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\\mathsfit{\\color[RGB]{34,136,51}data}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\]\nwe could calculate\n\\[\n\\mathrm{P}(\n\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{\\color[RGB]{34,136,51}data}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n\\]\nonce for all possible values \\({\\color[RGB]{68,119,170}z}\\), and use that. Mathematically and logically the two ways are completely equivalent. Numerically they can be different as regards precision or possible overflow errors. Using \\(\\mathrm{P}( \\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}\\mathsfit{\\color[RGB]{34,136,51}data}\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\) would be convenient if our basic formula (33.1) didn’t contain the sum \\(\\sum_k\\) over the \\(k\\) index. Our code shall instead use \\(\\mathrm{P}(\\color[RGB]{68,119,170}Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\\mathsfit{\\color[RGB]{34,136,51}data}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\) because it leads to slightly more precision and speed in some tasks.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Code design]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-code-optim",
    "href": "code_design.html#sec-code-optim",
    "title": "33  Code design",
    "section": "33.3 Code optimization",
    "text": "33.3 Code optimization\nThe formulae of chapter  32, if used as-written, easily lead to two kinds of computation problems. First, they generate overflows and NaN, owing to factorials and their divisions. Second, the products over variates may involve so many terms as to require a long computation time. In the end we would have to wait a long time just to receive a string of NaNs.\nThe first problem is dealt with by rewriting the formulae in terms of logarithms, and renormalizing numerators and denominators of fractions. See for example the lines defining auxalphas in the buildagent() function, and the line that redefines counts one last time in the infer() function.\nThe second problem is dealt with by reorganizing the sums as multiples of identical summands; see the lines working with freqscounts in the buildagent() function.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n§6.1 in Numerical Recipes",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Code design]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html",
    "href": "prototype_code.html",
    "title": "34  Prototype code and workflow",
    "section": "",
    "text": "34.1 Function documentation\nA concise documentation is here given of the prototype R functions designed in chapter  33 and described in § 33.2, together with an example workflow for their use.\nThe functions can be found in\nhttps://github.com/pglpm/ADA511/tree/master/code/OPM-nominal\nOptional arguments are written with =..., which specify their default values. Some additional optional arguments, mainly used for testing, are omitted in this documentation.\n( Further documentation will be added )",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html#sec-opm-docs",
    "href": "prototype_code.html#sec-opm-docs",
    "title": "34  Prototype code and workflow",
    "section": "",
    "text": "guessmetadata(data, file=NULL)\n\n\nArguments:\n\n\ndata: either a string with the file name of a dataset in .csv format (with header line), or a dataset given as a data.table object.\nfile: a string specifying the file name of the metadata file. If no file is given and data is a file name, then file will be the same name as data but with the prefix meta_. If no file is given and data is not a string, then the metadata are output to stdout.\n\n\nOutput:\n\n\neither a .csv file containing the metadata, or a data.table object as stdout.\n\n\n\n\n\n\n\n buildagent(metadata, data=NULL, kmi=0, kma=20)\n\n\nArguments:\n\n\nmetadata: either a string with the name of a metadata file in .csv format, or metadata given as a data.table.\ndata: either a string with the file name of a training dataset in .csv format (with header line), or a training dataset given as a data.table.\nkmi: the \\(k_{\\text{mi}}\\) parameter of formula (33.1).\nkma: the \\(k_{\\text{ma}}\\) parameter of formula (33.1).\n\n\nOutput:\n\n\nan object of class agent, consisting of a list of an array counts and three vectors alphas, auxalphas, palphas.\n\n\n\n\n\n\n\n infer(agent, predictand=NULL, predictor=NULL)\n\n\nArguments:\n\n\nagent: an agent object.\npredictand: a vector of strings with the names of variates.\npredictor: either a list of elements of the form variate=value, or a corresponding one-row data.table.\n\n\nOutput:\n\n\nthe joint probability distribution \\(\\mathrm{P}(\\mathit{predictand} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{predictor}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;values;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\) for all possible values of the predictands.\n\n\nNotes:\n\n\nIf predictors is present, the agent is acting as a “supervised-learning” algorithm. Otherwise it is acting as an “unsupervised-learning” algorithm. The obtained probabilities could be used to generate a new unit similar to the ones observed.\nIf predictand is missing, the predictands are taken to be all variates not listed among the predictors (hence all variates, if no predictors are given).\nThe variate names in the predictand and predictor inputs must match some variate names known to the agent. Unknown variate names are discarded. The function gives an error if predictand and predictor have variates in common.\n\n\n\n\n\n\n\n decide(probs, utils=NULL)\n\n\nArguments:\n\n\nprobs: a probability distribution for one or more variates.\nutils: a named matrix or array of utilities. The rows of the matrix correspond to the available decisions, the columns or remaining array dimensions correspond to the possible values of the predictand variates.\n\n\nOutput:\n\na list of elements EUs and optimal:\n\nEUs is a vector containing the expected utilities of all decisions, sorted from highest to lowest\noptimal is the decision having maximal expected utility, or one of them, if more than one, selected with equal probability\n\n\nNotes:\n\n\nIf utils is missing or NULL, a matrix of the form \\(\\begin{bsmallmatrix}1&0&\\dotso\\\\0&1&\\dotso\\\\\\dotso&\\dotso&\\dotso\\end{bsmallmatrix}\\) is assumed (which corresponds to using accuracy as evaluation metric).\n\n\n\n\n\n\n\n\n rF(n=1, agent, predictand=NULL, predictor=NULL)\n\n(generate population-frequency samples)\n\n rF(n=1, agent, predictand=NULL, predictor=NULL)\n\n(generate population-frequency samples)\n\n mutualinfo(agent, A, B, base=2)\n\n(calculate mutual information)",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html#sec-opm-workflow",
    "href": "prototype_code.html#sec-opm-workflow",
    "title": "34  Prototype code and workflow",
    "section": "34.2 Typical workflow",
    "text": "34.2 Typical workflow\nThe workflow discussed here is just a guideline and reminder of important steps to be taken when applying an optimal agent to a given task. There cannot be more than a guideline, because each data-science and engineering problem is unique. Literally following some predefined, abstract workflow typically leads to sub-optimal results. Sub-optimal results can be acceptable in some unimportant problems, but are unacceptable in important problems, where, say, people’s lives can be involved, such as medical ones.\nWe can roughly identify four main stages:\n\n\n\n\n\n\n\nDefine the task\n\nIn this stage we clarify what the task to be solved is – and why. Asking “why” often reveals the true needs and goals underlying the problem. If possible, the task is formalized. For example, the formal notions introduced in the parts Data I and Data II might be used: a specific statistical population is specified, with well-defined units and variates, and so on.\n\n\nWe often have to get back to his initial stage, as the side arrows in the above flow diagram indicate. New findings in the data or unavoidable limitations in the algorithms used may lead us to re-examine our assumptions and facts, or to re-define our goals (and sometimes to give up!).\n\n\n\n\n\nCollect & prepare background info\n\nBackground and metadata information, as well as auxiliary assumptions, are collected, examined, prepared. Remember that this kind of information is required in order to make sense of the data (§ 27.4). In this stage we ask questions such as “Is our belief about the task exchangeable?”, “Can the statistical population be considered infinite?”, and similar question that make clear which kinds of ready-made methods and approximations are acceptable or not. This stage also helps for correcting possible deficiencies in the training data used in the next stage. For instance, some possible variate values might not appear in the training data, owing to their rarity in the statistical population.\n\n\nIn this stage it is especially important to specify:\n\ndefinition of units (what counts as “unit” and can be used as training data?)\ndefinition of variates and their domains\ninitial probabilities\npossible decisions that may be required in the repeated task applications\nutilities associated with the decisions above\n\n\n\n\n\n\nCollect & prepare training data\n\nUnits similar to the units of our future inferences, but of which we have more complete information, are collected and examined. These are the “training data”. They are used in the next step to make the agent learn from examples. The problematic notion of similarity was discussed in § 20.2: what counts as “similar” is difficult to decide, and often we shall have to revise our decision. Sometimes no units satisfactorily similar to those of interest are available. In this case we must assess which of their informational relationships can be considered similar, which may lead us to use the agent in slightly different ways. We must also check whether training units with partially missing variates can be used by our agent or not.\n\n\n\n\n\n\n\n\n\n How many training data?\n\n\n\n“Data augmentation” is something necessary with particular machine-learning algorithms as a way to compensate or correct their internal background information, but does not apply to an optimal agent.\nThe question “how many training data should we use?” does not make sense for an optimal agent which works according to probability theory and decision theory. The answer is simply “as many as you have available”.\nIf no or few training data are available, then the optimal agent will automatically absorb as much information as possible from them, and combine it with its background information to draw optimal inferences and make optimal decisions.\nGiving artificial training data to the agent, just to increase the number of training data, is pointless and dangerous.\nPointless, because the agent is, in fact, automatically “simulating” artificial data internally as needed, from its background information and the real training data available. This is exactly what the belief distribution \\(\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data},\\mathsfit{I}_{\\textrm{d}})\\) is doing: remember that the agent is internally considering all possible populations of data (chapter  30).\nDangerous, because artificial data may contain incorrect information, leading the agent to arrive at sub-optimal and potentially disastrously deceiving results.\n\n\n\n\n\n\nPrepare OPM agent\n\nThe background information and training data (if any available) are finally fed to the agent.\n\n\n\n\n\n\n\nRepeated application\n\nInferences are drawn, and decision made, for each new application instance. With our prototype agent, the inferences and the decisions can in principle be different from instance to instance.\n\n\n\n\n\nEvery new application can be broken down into several steps:\n\n\n\n\n\n\n\n( Remaining steps to be added soon )",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html",
    "href": "example_opm1.html",
    "title": "35  Example application: adult-income task",
    "section": "",
    "text": "35.1 Define the task\nLet’s illustrate the example workflow described in § 34.2 with a toy, but not too simplistic, example, based on the adult-income dataset.\nAll code functions and data files are in the directory\nhttps://github.com/pglpm/ADA511/tree/master/code/OPM-nominal\nWe start loading the R libraries and functions needed at several stages. You need to have installed1 the packages extraDistr and foreach. Make sure you have saved all source files and data files in the same directory.\nThe main task is to infer whether a USA citizen earns less (≤) or more (&gt;) than USD 50 000/year, given a set of characteristics of that citizen. In view of later workflow stages, let’s note a couple of known and unknown facts to delimit this task in a more precise manner:",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#define-the-task",
    "href": "example_opm1.html#define-the-task",
    "title": "35  Example application: adult-income task",
    "section": "",
    "text": "Given the flexibility of the agent we shall use, we can generalize the task: to infer any subset of the set of characteristics, given any other subset. In other words, we can choose the predictand and predictor variates for any new citizen. Later on we shall also extend the task to making a concrete decision, based on utilities relevant to that citizen.\nThis flexibility is also convenient because no explanation is given as to what purpose the income should be guessed.\nThe training data come from a 1994 census, and our agent will use an exchangeable belief distribution about the population. The value of the USD and the economic situation of the country changes from year to year, as well as the informational relationships between economic and demographic factors. For this reason the agent should be used to draw inferences about at most one or two years around 1994. Beyond such time range the exchangeability assumption is too dubious and risky.\nThe USA population in 1994 was around 260 000 000, and we shall use around 11 000 training data. The population size can therefore be considered approximately infinite.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#collect-prepare-background-info",
    "href": "example_opm1.html#collect-prepare-background-info",
    "title": "35  Example application: adult-income task",
    "section": "35.2 Collect & prepare background info",
    "text": "35.2 Collect & prepare background info\n\nVariates and domains\nThe variates to be used must be of nominal type, because our agent’s background beliefs (represented by the Dirichlet-mixture distribution) are only appropriate for nominal variates. In this toy example we simply discard all original non-nominal variates. These included some, such as age, that would surely be relevant for this task. As a different approach, we could have coarsened each non-nominal variate into three or four range values, so that treating it as nominal would have been an acceptable approximation.\nFirst, create a preliminary metadata file by running the function guessmetadata() on the training data train-income_data_example.csv:\n\nguessmetadata(data = 'train-income_data_example.csv',\n              file = 'preliminary.csv')\n\nInspect the resulting file preliminary.csv and check whether you can alter it to add additional background information.\nAs an example, note that domain of the \\(\\mathit{native\\_country}\\) variate does not include \\({\\small\\verb;Norway;}\\) or \\({\\small\\verb;Sweden;}\\). Yet it’s extremely likely that there were some native Norwegian or Swedish people in the USA in 1994; maybe too few to have been sampled into the training data. Let’s add these two values to the list of domain values, and increase the domain size of \\(\\mathit{native\\_country}\\) from 40 to 42. The resulting, updated metadata file has already been saved as meta_income_data_example.csv.\n\n\nAgent’s parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\)\nHow many data should the agent learn in order to appreciably change its initial beliefs about the variates above, for the USA 1994 population? Let’s put an upper bound at around 1 000 000 (that’s roughly 0.5% of the whole population) with \\(k_{\\text{ma}}= 20\\), and a lower bound at 1 with \\(k_{\\text{mi}}= 0\\); these are the default values. We shall see later what the agent suggests might be a reasonable amount of training data.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#collect-prepare-training-data",
    "href": "example_opm1.html#collect-prepare-training-data",
    "title": "35  Example application: adult-income task",
    "section": "35.3 Collect & prepare training data",
    "text": "35.3 Collect & prepare training data\nThe 11 306 training data have been prepared by including only nominal variates, and discarding datapoints with partially missing data (although the function buildagent() discards such incomplete datapoints automatically). The resulting file is test-income_data_example.csv.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#prepare-opm-agent",
    "href": "example_opm1.html#prepare-opm-agent",
    "title": "35  Example application: adult-income task",
    "section": "35.4 Prepare OPM agent",
    "text": "35.4 Prepare OPM agent\nFor the sake of this example we shall prepare two agents with the same background information:\n\nopm10, trained with 10 training datapoints\nopmall, trained with all 11 306 training datapoints\n\nPrepare and train each with the buildagent() function:\n\n## temporarily load all training data\ntraindata &lt;- read.csv('train-income_data_example.csv', header = TRUE,\n    na.strings = '', stringsAsFactors = FALSE, tryLogical = FALSE)\n\n## feed first 10 datapoints to an agent\nopm10 &lt;- buildagent(metadata = 'meta_income_data_example.csv',\n                    data = traindata[1:10, ])\n\n## delete training data for memory efficiency\nrm(traindata)\n\n\nopmall &lt;- buildagent(metadata = 'meta_income_data_example.csv',\n                     data = 'train-income_data_example.csv')\n\n\n\nWe can peek into the internal structure of these “agent objects” with str()\n\nstr(opmall)\n\nList of 4\n $ counts   : num [1:7, 1:16, 1:7, 1:14, 1:6, 1:5, 1:2, 1:42, 1:2] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 9\n  .. ..$ workclass     : chr [1:7] \"Federal-gov\" \"Local-gov\" \"Private\" \"Self-emp-inc\" ...\n  .. ..$ education     : chr [1:16] \"10th\" \"11th\" \"12th\" \"1st-4th\" ...\n  .. ..$ marital_status: chr [1:7] \"Divorced\" \"Married-AF-spouse\" \"Married-civ-spouse\" \"Married-spouse-absent\" ...\n  .. ..$ occupation    : chr [1:14] \"Adm-clerical\" \"Armed-Forces\" \"Craft-repair\" \"Exec-managerial\" ...\n  .. ..$ relationship  : chr [1:6] \"Husband\" \"Not-in-family\" \"Other-relative\" \"Own-child\" ...\n  .. ..$ race          : chr [1:5] \"Amer-Indian-Eskimo\" \"Asian-Pac-Islander\" \"Black\" \"Other\" ...\n  .. ..$ sex           : chr [1:2] \"Female\" \"Male\"\n  .. ..$ native_country: chr [1:42] \"Cambodia\" \"Canada\" \"China\" \"Columbia\" ...\n  .. ..$ income        : chr [1:2] \"&lt;=50K\" \"&gt;50K\"\n $ alphas   : num [1:21] 1 2 4 8 16 32 64 128 256 512 ...\n $ auxalphas: num [1:21] -160706 -157643 -154588 -151547 -148530 ...\n $ palphas  : num [1:21] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"class\")= chr [1:2] \"agent\" \"list\"\n\n\nthis shows that each agent is encoded as a list of four objects:\n\nthe array counts, containing the counts \\(\\color[RGB]{34,136,51}\\# z\\)\nthe vector alphas, containing the values of \\(2^k\\)\nthe vector auxalphas, containing the (logarithm of) the multiplicative factors (§ 33.2)\nthe vector palphas, containing the updated probabilities about the required amount of training data\n\nThe agent has internally guessed how many training data should be necessary to affect its prior beliefs. We can peek at its guess by plotting the alphas parameters against the palphas probabilities:\n\nmytplot(x = opmall$alphas, y = opmall$palphas, type = 'b',\n      xlim = c(0, 10000), ylim = c(0, NA),\n      xlab = 'required number of training data', ylab = 'probability')\n\n\n\n\n\n\n\n\nThe most probable amount seems to be of the order of magnitude of 2000 units.\nNote that you can see the complete list of variates and their domains by simply calling dimnames(opmall$counts) (or any relevant agent-object name instead of opmall). Here is the beginning of the list:\n\nhead(dimnames(opmall$counts))\n\n$workclass\n[1] \"Federal-gov\"      \"Local-gov\"        \"Private\"          \"Self-emp-inc\"    \n[5] \"Self-emp-not-inc\" \"State-gov\"        \"Without-pay\"     \n\n$education\n [1] \"10th\"         \"11th\"         \"12th\"         \"1st-4th\"      \"5th-6th\"     \n [6] \"7th-8th\"      \"9th\"          \"Assoc-acdm\"   \"Assoc-voc\"    \"Bachelors\"   \n[11] \"Doctorate\"    \"HS-grad\"      \"Masters\"      \"Preschool\"    \"Prof-school\" \n[16] \"Some-college\"\n\n$marital_status\n[1] \"Divorced\"              \"Married-AF-spouse\"     \"Married-civ-spouse\"   \n[4] \"Married-spouse-absent\" \"Never-married\"         \"Separated\"            \n[7] \"Widowed\"              \n\n$occupation\n [1] \"Adm-clerical\"      \"Armed-Forces\"      \"Craft-repair\"      \"Exec-managerial\"  \n [5] \"Farming-fishing\"   \"Handlers-cleaners\" \"Machine-op-inspct\" \"Other-service\"    \n [9] \"Priv-house-serv\"   \"Prof-specialty\"    \"Protective-serv\"   \"Sales\"            \n[13] \"Tech-support\"      \"Transport-moving\" \n\n$relationship\n[1] \"Husband\"        \"Not-in-family\"  \"Other-relative\" \"Own-child\"     \n[5] \"Unmarried\"      \"Wife\"          \n\n$race\n[1] \"Amer-Indian-Eskimo\" \"Asian-Pac-Islander\" \"Black\"              \"Other\"             \n[5] \"White\"",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#application-and-exploration",
    "href": "example_opm1.html#application-and-exploration",
    "title": "35  Example application: adult-income task",
    "section": "35.5 Application and exploration",
    "text": "35.5 Application and exploration\n\nApplication: only predictands\nOur two agents are ready to be applied to new instances.\nBefore applying them, let’s check some of their inferences, and see if we find anything unconvincing about them. If we find something unconvincing, it means that the background information we provided to the agent doesn’t match the one in our intuition. Then there are two or three possibilities: our intuition is misleading us and need correcting; or we need to go back to stage Collect & prepare background info and correct the background information given to the agent; or a combination of these two possibilities.\nWe ask the opm10 agent to forecast the \\(\\mathit{income}\\) of the next unit, using the infer() function:\n\ninfer(agent = opm10, predictand = 'income')\n\nincome\n   &lt;=50K     &gt;50K \n0.506288 0.493712 \n\n\nThis agent gives a slightly larger probability to the \\({\\small\\verb;&lt;=50K;}\\) case. Using the function plotFsamples1D() we can also inspect the opm10-agent’s belief about the frequency distribution of \\(\\mathit{income}\\) for the full population. This belief is represented by a generalized scatter plot of 200 representative frequency distributions, represented as the light-blue lines:\n\nplotFsamples1D(agent = opm10,\n               n = 200, # number of example frequency distributions\n               predictand = 'income',\n               ylim = c(0,1), # y-axis range\n               main = 'opm10') # plot title\n\n\n\n\n\n\n\n\nwhere the black line is the probability distribution previously calculated with the infer() function.\nThis plot expresses the opm10-agent’s belief that future training data might lead to even higher probabilities for \\({\\small\\verb;&lt;=50K;}\\). But note that the agent is not excluding the possibility of lower probabilities.\nLet’s visualize the beliefs of the opmall-agent, trained with the full training dataset:\n\nplotFsamples1D(agent = opmall, n = 200, predictand = 'income',\n               ylim = c(0,1), main = 'opmall')\n\n\n\n\n\n\n\n\nThe probability that the next unit has \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\) is now above 70%. Also note that the opmall-agent doesn’t believe that this probability would change appreciably if more training data were provided.\n\n\nWe can perform a similar exploration for any other variate. Let’s take the \\(\\mathit{race}\\) variate for example:\n\nplotFsamples1D(agent = opm10, n = 200, predictand = 'race',\n               ylim = c(0,1), main = 'opm10',\n               cex.axis = 0.75) # smaller axis-font size\n\n\n\n\n\n\n\n\nNote again how the little-trained opm10-agent has practically uniform beliefs. But it’s also expressing the fact that future training data will probably increase the probability of \\(\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\\).\nThis is corroborated by the fully-trained agent:\n\nplotFsamples1D(agent = opmall, n = 200, predictand = 'race',\n               ylim = c(0,1), main = 'opmall', cex.axis = 0.75)\n\n\n\n\n\n\n\n\n\n\nThese checks are satisfying, but it’s good to examine their agreement or disagreement with our intuition. Examine the last plot for example. The opmall agent has very firm beliefs (no spread in the light-blue lines) about the full-population distribution of \\(\\mathit{race}\\). Do you think its beliefs are too firm, after 11 000 datapoints? would you like the agent to be more “open-minded”? In that case you should go back to the Collect & prepare background info stage, and for example modify the parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\), then re-check. Or you could even try an agent with a different initial belief distribution.\nIn making this kind of considerations it’s important to keep in mind what we learned and observed in previous chapters:\n\n\n\n\n\n\nNote\n\n\n\nOur goal: optimality, not “success”\nRemember (§ 2.3 and § 8.1) that a probability represents the rational degree of belief that an agent should have given the particular information available. We can’t judge a probability from the value it assigns to something we later learn to be true – because according to the information available it could be more rational (and optimal) to consider that something implausible (recall the example in § 8.1 of an object falling from the sky as we cross the street).\nFrom this point of view we should be wary of comparing the probability of something with our a-posteriori knowledge about it.\n\n\nThe data cannot speak for themselves\nWe could build an agent that remains more “open-minded” (more spread in the light-blue lines), having received exactly the same training data. This “open-mindedness” therefore cannot be determined by the training data. Once more this shows that data cannot “speak for themselves” (§ 27.4).\n\n\n\n\n\n\n\n\n\nApplication: specifying predictors\nLet’s now draw inferences by specifying some predictors.\nWe ask the opm10 agent to forecast the \\(\\mathit{income}\\) of a new unit, given that the unit is known to have \\(\\mathit{occupation}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Exec-managerial;}\\) and \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Male;}\\) (two predictor variates). What would you expect?\nThe opm10-agent’s belief about the unit – as well as about the full subpopulation (§ 22.1) of units having those predictors – is shown in the following plot:\n\nplotFsamples1D(agent = opm10, n = 200,\n               predictand = 'income',\n               predictor = list(occupation = 'Exec-managerial',\n                              sex = 'Male'),\n               ylim = c(0,1), main = 'opm10')\n\n\n\n\n\n\n\n\nNote how the opm10-agent still slightly higher probability to \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\), but at the same time it is quite uncertain about the subpopulation frequencies; more than if the predictor had not been specified. That is, according to this little-trained agent there could be large variety of possibilities within this specific subpopulation.\nThe opmall-agent’s beliefs are shown below:\n\nplotFsamples1D(agent = opmall, n = 200,\n               predictand = 'income',\n               predictor = list(occupation = 'Exec-managerial',\n                              sex = 'Male'),\n               ylim = c(0,1), main = 'opmall')\n\n\n\n\n\n\n\n\nit believes with around 55% probability that such a unit would have higher, \\({\\small\\verb;&gt;50K;}\\) income. The representative subpopulation-frequency distributions in light-blue indicate that this belief is unlikely to be changed by new training data.\n\n\nLet’s now see an example of our agent’s versatility by switching predictands and predictors. We tell the opmall-agent that the new unit has \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&gt;50;}\\), and ask it to infer the joint variate \\((\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex})\\); let’s present the results in rounded percentages:\n\nresult &lt;- infer(agent = opmall,\n                predictand = c('occupation', 'sex'),\n                predictor = list(income = '&gt;50K'))\n\nround(result * 100, 1) # round to one decimal\n\n                   sex\noccupation          Female Male\n  Adm-clerical         3.1  4.1\n  Armed-Forces         1.0  1.0\n  Craft-repair         1.1  9.4\n  Exec-managerial      3.6 16.0\n  Farming-fishing      1.0  2.1\n  Handlers-cleaners    1.0  1.6\n  Machine-op-inspct    1.1  3.1\n  Other-service        1.6  1.8\n  Priv-house-serv      1.0  1.0\n  Prof-specialty       4.8 14.7\n  Protective-serv      1.0  3.2\n  Sales                1.8 10.1\n  Tech-support         1.5  3.4\n  Transport-moving     1.1  3.7\n\n\nIt returns a 14 × 2 table of joint probabilities. The most probable combinations are \\(({\\small\\verb;Exec-managerial;}, {\\small\\verb;Male;})\\) and \\(({\\small\\verb;Prof-specialty;}, {\\small\\verb;Male;})\\).\n\n\nThe rF() function\nThis function generates full-population frequency distributions (even for subpopulations) that are probable according to the data. It is used internally by plotFsamples1D(), which plots the generated frequency distributions as light-blue lines.\nLet’s see, as an example, three samples of how the full-population frequency distribution for \\(\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}\\) (jointly) could be:\n\nresult &lt;- rF(n = 3, # number of samples\n             agent = opmall, \n             predictand = c('sex', 'income'))\n\n## name the samples\ndimnames(result)[1] &lt;- list(samples = paste0('#',1:3))\n\n## permute & print so that samples are the last array dimension\nprint(aperm(result) * 100)\n\n, , sample = #1\n\n       sex\nincome  Female    Male\n  &lt;=50K 28.363 43.0849\n  &gt;50K   6.578 21.9741\n\n, , sample = #2\n\n       sex\nincome    Female    Male\n  &lt;=50K 27.81697 43.4817\n  &gt;50K   6.99239 21.7090\n\n, , sample = #3\n\n       sex\nincome    Female    Male\n  &lt;=50K 29.24027 42.3496\n  &gt;50K   6.90106 21.5091\n\n\nThese possible full-population frequency distributions can be used to assess how much the probabilities we find could change, if we collected a much, much larger amount of training data. Here is an example:\nWe generate 1000 frequency distributions for \\((\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex})\\) given \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&gt;50K;}\\), and then take the standard deviations of the samples as a rough measure of how much the probabilities we calculated a couple of cells above could change:\n\nfreqsamples &lt;- rF(n = 1000,\n                  agent = opmall,\n                  predictand = c('occupation', 'sex'),\n                  predictor = list(income = '&gt;50K'))\n\nvariability &lt;- apply(freqsamples,\n                     c('occupation','sex'), # which dimensions to apply\n                     sd) # function to apply to those dimensions\n\nround(variability * 100, 2) # round to two decimals\n\n                   sex\noccupation          Female Male\n  Adm-clerical        0.28 0.33\n  Armed-Forces        0.16 0.16\n  Craft-repair        0.17 0.48\n  Exec-managerial     0.30 0.58\n  Farming-fishing     0.16 0.23\n  Handlers-cleaners   0.16 0.20\n  Machine-op-inspct   0.16 0.27\n  Other-service       0.21 0.21\n  Priv-house-serv     0.16 0.15\n  Prof-specialty      0.34 0.57\n  Protective-serv     0.16 0.29\n  Sales               0.22 0.48\n  Tech-support        0.20 0.29\n  Transport-moving    0.17 0.32\n\n\nthe agent believes (at around 68%) that the current probability wouldn’t change more than about ±0.5%.\n\n\nThe inferences above were partially meant as checks, but we see that we can actually ask our agent a wide variety of questions about the full population, and do all sorts of association studies.\n\n\n\n\n\n\n No “test” or “validation” datasets used or needed\n\n\n\nThe tests and explorations above were done without any “validation” or “test” datasets. This is because our agent is capable of calculating and showing its beliefs about the full population – and therefore about future data.\nThe need for validation or test datasets with common machine-learning algorithms arise from the fact that full-population beliefs are hidden or, more commonly, not computed at all, in order to gain speed. The application of the trained machine-learning algorithm to a validation dataset is an approximate way of extracting such beliefs.\n\n\n\n\n\n\nExploring the population properties: mutual information\nIn § 18.5 we introduced mutual information as the information-theoretic measure of mutual relevance and association of two quantities or variates. For the present task, the opmall-agent can tell us the mutual information between any two sets of variates of our choice, with the function mutualinfo().\nFor instance, let’s calculate the mutual information between \\(\\mathit{occupation}\\) and \\(\\mathit{marital\\_status}\\).\n\nmutualinfo(agent = opmall,\n           A = 'occupation', B = 'marital_status')\n\n[1] 0.0827823\n\n\nIt is a very low association: knowing either variate decreases the effective number of possible values of the other only \\(2^{0.0827823\\,\\mathit{Sh}} \\approx 1.06\\) times.\nNow let’s consider a scenario where, in order to save resources, we can use only one variate to infer the income. Which of the other variates should we prefer? We can calculate the mutual information between each of them, in turn, and \\(\\mathit{income}\\):\n\n## list of all variates\nvariates &lt;- names(dimnames(opmall$counts))\n\n## list of all variates except 'income'\npredictors &lt;- variates[variates != 'income']\n\n## prepare vector to contain the mutual information\nrelevances &lt;- numeric(length(predictors))\nnames(relevances) &lt;- predictors\n\n## calculate, for each variate, the mutual information 'relevance' (in shannons)\n## between 'income' and that variate\nfor(var in predictors){\n    relevances[var] &lt;- mutualinfo(agent = opmall, A = 'income', B = var)\n}\n\n## output the mutual informations in decreasing order\nsort(relevances, decreasing = TRUE)\n\nmarital_status   relationship      education     occupation      workclass \n    0.10074130     0.09046621     0.06332052     0.05506897     0.03002995 \nnative_country            sex           race \n    0.01925227     0.01456655     0.00870089 \n\n\nIf we had to choose only one variate to infer the outcome, on average it would be best to use \\(\\mathit{marital\\_status}\\). Our last choice should be \\(\\mathit{race}\\).\n\n\n\n\n\n\n Exercise\n\n\n\nNow consider the scenario where we must exclude one variate from the eight predictors, or, equivalently, we can only use seven variates as predictors. Which variate should we exclude?\nPrepare a script similar to the one above: it calculates the mutual information between \\(\\mathit{income}\\) and the other predictors but with one omitted, omitting each of the eight in turn.\nWarning: this computation might require 10 or more minutes to complete.\n\nWhich single variate should not be omitted from the predictors? which single variate could be dropped?\nDo you obtain the same relevance ranking as in the “use-one-variate-only” scenario above?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#example-application-to-new-data",
    "href": "example_opm1.html#example-application-to-new-data",
    "title": "35  Example application: adult-income task",
    "section": "35.6 Example application to new data",
    "text": "35.6 Example application to new data\nLet’s apply the opmall-agent to a test dataset with 33 914 new units. For each new unit, the agent:\n\ncalculates the probability of \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\), via the function infer(), using as predictors all variates except \\(\\mathit{income}\\)\nchooses one of the two values \\(\\set{{\\small\\verb;&lt;=50K;}, {\\small\\verb;&gt;50K;}}\\), via the function decide() trying to maximizing utilities corresponding to the accuracy metric\n\nThe function decide() will be described in more detail in chapters 3.3 and 38.\nAt the end we plot a histogram of the probabilities calculated for the new units, to check for instance for how many of the agent was sure (beliefs around 0% or 100%) or unsure (beliefs around 50%). We also report the final utility/accuracy per unit, and the time needed for the computation:\n\n## Load test data\ntestdata &lt;- read.csv('test-income_data_example.csv', header = TRUE,\n    na.strings = '', stringsAsFactors = FALSE, tryLogical = FALSE)\n\nntest &lt;- nrow(testdata) # size of test dataset\n\n## Let's time the calculation\nstopwatch &lt;- Sys.time()\n\ntestprobs &lt;- numeric(ntest) # prepare vector of probabilities\ntesthits &lt;- numeric(ntest) # prepare vector of hits\nfor(i in 1:ntest){\n    \n    ## calculate probabilities given all variates except 'income'\n    probs &lt;- infer(agent = opmall,\n        predictor = testdata[i, colnames(testdata) != 'income'])\n\n    ## store the probability for &lt;=50K\n    testprobs[i] &lt;- probs['&lt;=50K']\n\n    ## decide on one value\n    chosenvalue &lt;- decide(probs = probs)$optimal\n\n    ## check if decision == true_value, and store result\n    testhits[i] &lt;- (chosenvalue == testdata[i, 'income'])\n}\n\n## Print total time required\nprint(Sys.time() - stopwatch)\n\nTime difference of 4.77739 secs\n\n## Histogram and average accuracy (rounded to one decimal)\nmyhist(testprobs, n = seq(0,1,length.out = 10), plot = TRUE,\n      xlab = 'P(income = \"&lt;=50K\")',\n      ylab = 'frequency density in test set',\n      main = paste0('accuracy: ', round(100*mean(testhits), 1), '%'))",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#sec-compare-opm",
    "href": "example_opm1.html#sec-compare-opm",
    "title": "35  Example application: adult-income task",
    "section": "35.7 Comparison",
    "text": "35.7 Comparison\n\n\n\n\n\n\n Exercise\n\n\n\nNow try to use a popular machine-learning algorithm for the same task, using the same training data, and compare it with the prototype optimal predictor machine. Examine the differences. For example:\n\nCan you inform the algorithm that \\(\\mathit{native\\_country}\\) has two additional values \\({\\small\\verb;Norway;}\\), \\({\\small\\verb;Netherlands;}\\) not present in the training data? How?\nCan you flexibly use the algorithm to specify any predictors and any predictands on the fly?\nDoes the algorithm inform you of how the inferences could change if more training data were available?\nWhich accuracy does the algorithm achieve on the test set?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "utilities.html",
    "href": "utilities.html",
    "title": "36  Utilities",
    "section": "",
    "text": "36.1 From inferences to decisions\nAt long last we have seen how an agent can calculate the probabilities of any kind of sentence, given any facts and assumptions available to it. We studied this probability calculation for general problems, and then more in detail for problems enjoying special properties, for instance when the agent’s beliefs are exchangeable (chapter  28). Most important, we saw how previous data and background information can be used to determine, and at the same time affect, these probabilities. We have even built a real, prototype agent that can flexibly calculate probabilities in problems involving nominal variates.\nHaving rational degrees of belief about all possible hypotheses and unknowns is useful, and the first step in scientific research. But often it isn’t enough. Often the agent needs to act, to do something, to make a choice, even if its uncertainty has not disappeared and therefore it can’t be sure of what its choice will lead to. Our very first example of engineering problem (chapter  1) involved the decision on whether to accept or discard an electronic component just produced in an assembly line, not knowing whether it will fail within a year or not.\nIn chapter  3 we met the theory to deal with this kind of decision-making problem: Decision Theory.\nDecision Theory requires the decision-making agent to calculate the probabilities of the unknown outcomes. Now we know how to do that, at least in some kinds of problems! Most of our discussion so far focused on the calculation of those probabilities, which often is the most difficult part of the decision-making task.\nSo let’s face the decision-making problem again at last, and complete the construction of our prototype agent by implementing the final decision-making step.1",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#from-inferences-to-decisions",
    "href": "utilities.html#from-inferences-to-decisions",
    "title": "36  Utilities",
    "section": "",
    "text": "1 Please go back to chapter  3 and review the notions and terminology introduced there.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-decisions-utilities",
    "href": "utilities.html#sec-decisions-utilities",
    "title": "36  Utilities",
    "section": "36.2 Review of a basic decision problem: outcomes, decisions, utilities",
    "text": "36.2 Review of a basic decision problem: outcomes, decisions, utilities\nRecall the structure of a basic decision:\n\nIn order to make a decision, the agent needs:\n\n The set of possible decisions, which we represent as sentences like \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\).\n The set of possible outcomes, whose truth is unknown to the agent. In the kind of decision problems that we are examining, the outcomes correspond to the possible values \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\) of what we have called the predictand variate from § 27.2 onwards.\n The probabilities of the outcomes  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) which are determined by the agent’s background information \\(\\mathsfit{I}\\), by any other available information, such as \\(\\mathsfit{\\color[RGB]{34,136,51}data}\\) about previously observed units and the values of some predictor variates \\(\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\) for the current unit; and possibly also by the agent’s decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\) (see below).\n The utilities of each pair of decision and outcome \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\).\n\nSome texts call the joint pair \\(({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{204,187,68}D})\\), or equivalently \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\), of a decision and an outcome, a consequence. We adopt this terminology from this chapter onward.\nLet’s not forget some important points about the notions above:\n\n\n\n\n\n\n\n\n\n\n\nWe are not assuming any temporal or “causal” relationship between decisions and outcomes: our framework works independently of these relationships. In some decision-making problems the outcomes happen after a decision is made, and may be “influenced” by it or not. In other decision-making problems the outcomes have already happened before a decision is made.\nIn a transportation-choice problem, for instance, the outcome “wet from rain” may happen after we make the decision “go on foot” rather than “go by bus”. In an image-classification problem, on the other hand, the outcome “true label is \\({\\small\\verb;cat;}\\)” was already determined before we made the decision “classify as \\({\\small\\verb;dog;}\\)”.\n\n\n\n\nIn connection with the warning above, in some problems the probabilities of the outcomes may depend on the decision; that is, knowledge about the decision is relevant to the knowledge about the outcome (see again § 18):\n\\[\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nFor instance, in the transportation-choice problem the probability of the outcome “wet from rain” is higher conditional on the decision “go on foot” than on the decision “go by bus”.\nIn many decision-making problems typical of machine learning, such as classification, information about the decision is irrelevant to the information about the outcome (which usually has already happened), and we have\n\\[\n  \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n  = \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n  \\]\n\n\n\nIn what follows we shall consider problems, such as classification, where knowledge of the agent’s decision is irrelevant to the outcome’s probability. We shall nevertheless keep the more general notation  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\).",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-what-utilities",
    "href": "utilities.html#sec-what-utilities",
    "title": "36  Utilities",
    "section": "36.3 What are utilities?",
    "text": "36.3 What are utilities?\nIn most decision-making problems the “gain” or “loss” or “satisfaction” or “desiderability” of the consequences depends on many different aspects. A person purchasing some item may have to choose between something inexpensive but of low quality, or something of high quality but expensive. A clinical patient may have to choose between a treatment that increases life expectancy but worsens the quality of life, or a treatment that improves the quality of life but decreases life expectancy.\nDecision Theory says that whenever an agent makes a decision among alternatives having heterogeneous decision aspects, then it is implicitly using only one real number to summarize and bring together all those aspects. If this weren’t true, the agent would be deciding in an irrational way, which could even be exploited against the agent itself.\nSuch idea is not counter-intuitive in our culture. We are wont, for example, to exchange money for things of wildly different kinds: food, entertainment, health, work, transport, communication, life insurance, knowledge, political power. The monetary value of a human life (“value of statistical life”) for some governments is about USD 10 000 000.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nOn making life and death decisions\n\n\n\n2 With the opposite convention we speak of disutility or loss.Utility is the name we give to the real number that encodes together all heterogeneous desirabilities and gains of a consequence. The convention is that the higher the utility, the more preferable is the consequence.2\n\nNotation\nWe denote the utility of the consequence \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\) as\n\\[\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nThis notation reminds us that the utilities assigned by an agent depend on the agent’s background information \\(\\mathsfit{I}\\).\nThe utilities for all consequences can be encoded in a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\), having one row per decision and one column per outcome:\n\\[\n\\boldsymbol{\\color[RGB]{68,119,170}U}\\coloneqq\n\\begin{bmatrix}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n&\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n& \\dotso\n\\\\\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n&\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n& \\dotso\n\\\\\n\\dotso&\\dotso&\\dotso\n\\end{bmatrix}\n\\]\n\n\nContinuous case\nIn some decision-making problems the set of possible decisions can be considered as continuous.\nA power-plant operator, for example, may have to decide to supply an amount of power between 100 MW and 200 MW to a geographical region in the next hour. The unknown “outcome” \\(\\color[RGB]{238,102,119}Y\\) in this scenario may be the power demand, which could be in the same range. In this case we can represent a decision by a statement \\(\\color[RGB]{204,187,68}D\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}d\\), such as\n\\[\\color[RGB]{204,187,68}D\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}175\\,\\mathrm{MW}\\]\n(where “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” obviously stands for “set to”, not “is observed to be equal to”; recall § 6.3?)\nIn such continuous cases we speak of a utility function\n\\[\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nA typical, extremely abused utility function is the negative squared loss:\n\\[\n\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{sl}})\n= -\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2\n\\]\nstating that the utility decreases as the squared difference between \\(\\color[RGB]{204,187,68}d\\) and \\({\\color[RGB]{238,102,119}y}\\). In concrete problems it is worthwhile to think of more realistic and problem-specific utility functions. In the power-plant scenario, for example, the utility could be worse if the power output is below the power demand, than above. This could be expressed by a function like\n\\[\n\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{plant}})\n=\n\\begin{cases*}\n-2\\,\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2 & if ${\\color[RGB]{204,187,68}d} &lt; {\\color[RGB]{238,102,119}y}$\n\\\\[1ex]\n-\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2 &  if ${\\color[RGB]{204,187,68}d} \\ge {\\color[RGB]{238,102,119}y}$\n\\end{cases*}\n\\]\nor some other asymmetric function.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-whence-utilities",
    "href": "utilities.html#sec-whence-utilities",
    "title": "36  Utilities",
    "section": "36.4 How to determine utilities?",
    "text": "36.4 How to determine utilities?\nIt can be quite difficult to assess the utilities of the decisions and outcomes in a decision-making problems, because of reasons such as heterogeneity or uncertainty, discussed below. Yet, Decision Theory says that any decision is either implicitly using such a number, or is sub-optimal or logically inconsistent. Moreover, the specification, at some level, of utilities not derived by further analysis is simply unavoidable – just like the specification of some initial probabilities.\n\nHeterogeneous factors\nMany heterogeneous factors can enter the determination of utilities. In medical decision-making problems, for example, a clinician must choose one among several possible treatments for a patient, and the utilities of the outcome must take into account factors such as\n\ncost of the treatment\nexpected life length resulting from the treatment\nquality of life resulting from the treatment\npatient’s preferences and attitudes towards life\n\nIt can be very difficult to combine these factors into a single number.\nSeveral fields, such as medicine, have developed and refined several methodologies to arrive at utilities that account for all important factors. Unfortunately we shall not explore any of them in these notes.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nDecisions with Multiple Objectives\nChapter 8 of Sox & al. Medical Decision Making\n\n\n\n\n\nUncertainty\nThe assessment of utilities can also be affected by uncertainties, and therefore become an inference problem in itself. The utility of an outcome may depend on further decisions and further outcomes, whose utilities in turn depend on further decisions and outcomes. Our basic decision framework can in this case be applied repeatedly, as briefly discussed below. In the simplest case, if the agent is uncertain between utility values \\(\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}'\\) with probability \\(p'\\), and utility values \\(\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}''\\) with probability \\(p'' = 1-p'\\), then the utilities to use are the averages\n\\[p'\\,{\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}'} + p''\\,{\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}''}\\]\nBut the specification, at some level, of utilities not derived by further inferences is simply unavoidable – just like the specification of some initial probabilities.\n\n\nApproximate assignments\nAn approximate procedure of assigning utilities goes as follows:\n\n\n\n\n\n\n \n\n\n\n\nWrite down all possible consequences\nSort all consequences from best to worst. Ties are possible, that is, there may be several consequences considered to be equally good or equally bad.\nAssign utility \\(\\color[RGB]{68,119,170}1\\) to the best consequence (or to each of the best, if there’s a tie), and utility \\(\\color[RGB]{68,119,170}0\\) to the worst.\nDetermine the utility \\({\\color[RGB]{68,119,170}u}\\) of each intermediate consequence \\(({\\color[RGB]{238,102,119}y}, \\mathsfit{\\color[RGB]{204,187,68}D})\\) as follows:\n\nConsider a lottery where you can win the best consequence (that with utility \\(\\color[RGB]{68,119,170}1\\)) with some probability \\(p^{+}\\), or the worst consequence with probability \\(1-p^{+}\\).\nChoose one value of \\(p^{+}\\in [0,1]\\) such that it would be OK if you were forced to exchange the lottery with the consequence \\(({\\color[RGB]{238,102,119}y},\\mathsfit{\\color[RGB]{204,187,68}D})\\) under consideration, and vice versa, exchange the consequence under consideration with the lottery.\nThat probability value is the utility \\({\\color[RGB]{68,119,170}u}= p^{+}\\) of the consequence under consideration.\n\nCheck whether the utilities determined through lotteries respect your initial sorting. If they don’t, then you have reasoned inconsistently somewhere. Repeat the steps above thinking more thoroughly about your sorting and lotteries.\nAs an additional check, take any three consequences with utilities \\({\\color[RGB]{68,119,170}u}_1 \\ge {\\color[RGB]{68,119,170}u}_2 \\ge {\\color[RGB]{68,119,170}u}_3\\). Consider again a lottery where you can win the first consequence with probability \\(p\\) or the third with probability \\(1-p\\), and choose \\(p\\) so that it would be OK to be forced to exchange this lottery with the second consequence, and vice versa. Then you should find, at least approximately, that\n\\[{\\color[RGB]{68,119,170}u}_2 = p\\cdot {\\color[RGB]{68,119,170}u}_1 + (1-p)\\cdot {\\color[RGB]{68,119,170}u}_3\\]\nIf you don’t, then there are again inconsistencies or irrational biases in your judgements, and you should review them.\n\n\n\nIn the procedure above, the values \\(\\color[RGB]{68,119,170}1\\) and \\(\\color[RGB]{68,119,170}0\\) for the best and worst consequences are arbitrary: they correspond to setting a zero and a measurement unit of your utility scale. You can choose any other pair of values \\({\\color[RGB]{68,119,170}u_{\\textrm{max}}} &gt; {\\color[RGB]{68,119,170}u_{\\textrm{min}}}\\). The procedure applies in the same way, but the utility corresponding to \\(p^{+}\\) is then given by\n\\[{\\color[RGB]{68,119,170}u}= p^{+}\\cdot{\\color[RGB]{68,119,170}u_{\\textrm{max}}} +\n(1-p^{+})\\cdot {\\color[RGB]{68,119,170}u_{\\textrm{min}}}\n\\]\n\n\nThe assessment of initial utilities constitutes a field under active development (see references below), usually called utility elicitation.\n\n\n\n\n\n\n Study reading\n\n\n\n\n§15.3.1 of Artificial Intelligence\nFinal Summary (pp. 234–235) of chapter 8 in Sox & al. Medical Decision Making\n§§ 9.14–9.17 of Lindley: Making Decisions\n§4.2 of Keeney & al. 1976/1993: Decisions with Multiple Objectives\nSkim through Dawes & al. 1985: Attitude and opinion measurement",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-utilities-metric",
    "href": "utilities.html#sec-utilities-metric",
    "title": "36  Utilities",
    "section": "36.5 Utilities as evaluation metric",
    "text": "36.5 Utilities as evaluation metric\nThe utilities of the consequences not only allow the agent to determine the optimal decision, as we shall see in the next chapter. They also allow us to quantify how much utility an agent yielded in a concrete application or sequence of applications of a specific decision-making task.\nSince the possible gains and losses of a specific problem are encoded in the problem-specific utilities \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\), these utilities quantify by definition how much has been gained or lost in solving the problem.\nSuppose that in the first instance of a decision-making task the agent makes decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}_1\\), and in that instance the outcome \\({\\color[RGB]{238,102,119}Y}_1\\) turns out to be \\({\\color[RGB]{238,102,119}y}_1\\) (this may be discovered a long time after the decision was made). The utility (possibly a loss) gained at that instance is then, by definition,\n\\[\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}1}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nAssuming that the utilities are additive over instances3, then the total utility yield for instances \\(i=1,2,\\dotsc,M\\) is\n3 if they weren’t, the whole decision-making scheme of the next chapter should be changed, but a similar approach would still apply\\[\n\\sum_{i=1}^{M}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nand the average utility yield per instance is\n\\[\n\\frac{1}{M}\n\\sum_{i=1}^{M}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\n\nUse in a test set\nThe utility yield can be calculated in a test run of the agent, to check whether its operation meets its specifications and expectations, or even to compare the performance of different agents.\nFor the test run we need a set of data \\(i=1,2,\\dotsc,M\\) (which should come from the same population underlying the real application, see § 23.3) for which the actual outcomes \\({\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}}\\) are known, and for which any predictors \\({\\color[RGB]{34,136,51}X_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\color[RGB]{0,0,0}i}}\\) are also available, so that they can be used by the agents under evaluation.\nEach agent is then applied to these data: it is given the predictors \\({\\color[RGB]{34,136,51}X_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\color[RGB]{0,0,0}i}}\\), and from these it will determine and possibly execute the optimal decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}_i\\), for all \\(i\\). The total or average utility yield generated by the agent is then given by the formula above.\nThe test utility yield of an agent can be examined to uncover possible design flaws (say, wrong background information, or programming bugs). The yields of different agents can be compared to decide which is most appropriate to the task.\nWe will return to this use of the utility matrix in chapter  39, when we discuss some evaluation metrics typical of present-day machine-learning methodology.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-indep-axiom",
    "href": "utilities.html#sec-indep-axiom",
    "title": "36  Utilities",
    "section": "36.6 Utilities and probabilities must be independent",
    "text": "36.6 Utilities and probabilities must be independent\nThe independence (or “sure-thing”) axiom of Decision Theory says that the utilities cannot be functions of the probabilities. In other words, an agent cannot assign higher or lower utility to some outcome just because its probability is higher or lower (or vice versa) than the probability of another outcome. The converse also holds: the probability of an outcome cannot be judged higher or lower just because the outcome is more or less desirable (or vice versa). Note that there may be some kind of relation between utilities and probabilities, but only because they refer to the same sentences, not because they are determined by each other’s numerical values.\nA dependence of probabilities on utilities we recognize immediately as “wishful thinking”. But some researchers have from time to time objected that the dependence of utilities on probabilities could be rationally justified, and have proposed alleged counterexamples (usually called “paradoxes”) to prove their objection. The most famous are Allais’s and Ellsberg’s paradoxes.\nExamination of these would-be counterexamples show that they actually contain logical inconsistencies of various kind. Here we want to emphasize one particular kind of mistake: they base utilities on particular aspects of the decision-making problem, but then use the probabilities of different aspects. Let’s show this inconsistency with an extreme example that illustrates it clearly.\nSuppose a person is asked to make a decision between two bets or lotteries \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) and \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\):\n\n\\(\\mathsfit{\\color[RGB]{204,187,68}A}\\): 50% probability of winning or losing nothing, and 50% probability of losing 10 000 $ (or an amount that’s high for the person’s economy)\n\\(\\mathsfit{\\color[RGB]{204,187,68}B}\\): 90% probability of winning 100 $, and 10% probability of winning or losing nothing\n\nBefore applying decision theory to this problem we need to assess which factors affect the utilities of this person. It turns out that this person is a gambler: she only cares about the “thrill & risk” of a consequence, and she doesn’t care about losing money.\nA hasty and naive application of decision theory could represent the problem with the following tree:\n\nand the decision with maximal expected utility would be \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\). But the gambler obviously prefers \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\). A critic of Decision Theory would then say that this happens because, contrary to the axiom of independence, we should allow the utilities to depend on the probabilities, which are more uncertain for \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) than for \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\).\nBut the above application is wrong and illogical. In the representation above, the probabilities refer to the monetary outcome; but we said that the gambler doesn’t care about losing money. If “thrill & risk” is the factor that determines the utilities, then the probabilities should be about that same factor.\nFor the gambler, choosing \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\) leads for certain to a situation with little “thrill” (the winning outcome is almost sure) and no risk (no money will be lost in any case). Choosing \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) instead leads for certain to a situation with high “thrill” (completely uncertain outcome) and high risk (huge money loss). The second situation has higher relative utility than the first. The correct representation of the decision problem is therefore like this:\n\nNote that the utilities do not depend on the probabilities, exactly as required by the independence axiom. The principle of maximal expected utility leads to decision \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\), the gambler’s favourite. Also note that the relevant probabilities are not the ones (about money winnings) mentioned in the initial statement of the problem. Just because we read or hear the word “probability” doesn’t mean that that’s the probability we need.\nThis would-be counterexample therefore vindicates Decision Theory. The problem was not in the axiom of independence, but in the fact that the framework was illogically applied. The “need” to break the axiom of independence to recover the intuitively correct solution (basically correcting an error with another error) was actually a warning sign that some illogical reasoning was taking place.\nIn more realistic situations, both utilities and probabilities must refer to a combination of monetary value and other factors, such as emotional ones. What’s important in any case is that they refer to the same factors. So to speak: if you say that you like oranges and don’t care about apples, then you should worry about how many oranges, not apples, there are.\nThis example, even if somewhat exaggerated, reminds us of two caveats that we have repeated several times in these notes:\n\n\nIt is important to enquire what the exact goals and whys of an engineering or data-science problem really are. Otherwise you may end up wasting a lot of time developing the correct solution to the wrong problem.\nDon’t let yourself be deceived by words and technical terms. Try to understand the essence of the problem that lies beyond its verbal description.\n\n\nIn chapter  39 we shall see that some common evaluation metrics in machine learning actually break the independence axiom, and should therefore be avoided.\n\n\n\n\n\n\n Study reading\n\n\n\n\nA theory of human behavior (pp. 30-37) in chapter 2 of Eells: Rational Decision and Causality\nMorrison 1967: On the consistency of preferences in Allais’ paradox\nSkim through §9 (pp. 80–86) in chapter 4 of Raiffa: Decision Analysis",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html",
    "href": "making_decisions.html",
    "title": "37  Making decisions",
    "section": "",
    "text": "37.1 Maximization of expected utility\nIn the previous chapter we associated utilities to consequences, that is, pairs \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\) of decisions and outcomes. We can also associated utilities to the decisions alone – and these are used to determine the optimal decision.\nThe expected utility of a decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\) is calculated as a weighted average over all possible outcomes, the weighs being the outcomes’ probabilities:\n\\[\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n= \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nAccording to Decision Theory the agent’s final decision is determined by the\nIt may happen that there are several decisions which have equal, maximal expected utility. In this case any one of them can be chosen. A useful strategy is to choose one among them with equal probability. Such strategy helps minimizing the loss from possible small errors in the specification of the utilities, or from the presence of an antagonist agent which tries to predict what our agent is doing.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-max-exp-utilities",
    "href": "making_decisions.html#sec-max-exp-utilities",
    "title": "37  Making decisions",
    "section": "",
    "text": "Principle of maximal expected utility\n\n\n\n\nThe optimal decision, which should be made by the agent, is the one having maximal expected utility:\n\\[\n\\mathsfit{\\color[RGB]{204,187,68}D}_{\\text{optimal}} =\n\\operatorname{argmax}\\limits_{\\mathsfit{\\color[RGB]{204,187,68}D}} \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n(where, in some tasks, the probabilities may not depend on \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\))\n\n\n\n\n\nIn the formula above, “\\(\\operatorname{argmax}\\limits_z G(z)\\)” is the value \\(z^*\\) which maximizes the function \\(G(z)\\). Note the difference:  \\(\\max\\limits_z G(z)\\)  is the value of the maximum itself (its \\(y\\)-coordinate, so to speak), whereas  \\(\\operatorname{argmax}\\limits_z G(z)\\)  is the value of the argument that gives the maximum (its \\(x\\)-coordinate). For instance\n\\[\\max\\limits_z (1-z)^2 = 0 \\qquad\\text{\\small but}\\qquad \\operatorname{argmax}\\limits_z (1-z)^2 = 1\\]\n\n\n\n\nNumerical implementation in simple cases\nThe principle of maximal expected utility is straightforward to calculate in many important problems.\nIn § 36.3 we represented the set of utilities by a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\). If the probabilities of the outcomes do not depend on the decisions, we represent them as a column matrix \\(\\boldsymbol{\\color[RGB]{34,136,51}P}\\), having one entry per outcome:\n\\[\n\\boldsymbol{\\color[RGB]{34,136,51}P}\\coloneqq\n\\begin{bmatrix}\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\\n\\dotso\n\\end{bmatrix}\n\\]\nThen the collection of expected utilities is a column matrix, having one entry per decision, given by the matrix product \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\boldsymbol{\\color[RGB]{34,136,51}P}\\).\nAll that’s left is to check which of the entries in this final matrix is maximal.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-max-exp-util-example",
    "href": "making_decisions.html#sec-max-exp-util-example",
    "title": "37  Making decisions",
    "section": "37.2 Concrete example: targeted advertisement",
    "text": "37.2 Concrete example: targeted advertisement\nAs a concrete example application of the principle of maximal expected utility, let’s keep on using the adult-income task from chapter  35, in a typical present-day scenario.\nSome corporation, which offers a particular phone app, wants to bombard its users with advertisements, because advertisement generates much more revenue than making the users pay for the app. For each user the corporation can choose one among three ad-types, let’s call them \\({\\color[RGB]{204,187,68}A}, {\\color[RGB]{204,187,68}B}, {\\color[RGB]{204,187,68}C}\\). The revenue obtained from these ad-types depends on whether the target user’s income is \\({\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}\\) or \\({\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}\\). A separate study run by the corporation has shown that the average revenues (per user per minute) depending on the three ad-types and the income levels are as follows:\n\n\n\n\n\n\n\nTable 37.1: Revenue depending on ad-type and income level\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}income}\\)\n\n\n\\({\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}\\)\n\\({\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}\\)\n\n\nad-type\n\\({\\color[RGB]{204,187,68}A}\\)\n\\(\\color[RGB]{68,119,170}-1\\,\\$\\)\n\\(\\color[RGB]{68,119,170}3\\,\\$\\)\n\n\n\\({\\color[RGB]{204,187,68}B}\\)\n\\(\\color[RGB]{68,119,170}2\\,\\$\\)\n\\(\\color[RGB]{68,119,170}2\\,\\$\\)\n\n\n\\({\\color[RGB]{204,187,68}C}\\)\n\\(\\color[RGB]{68,119,170}3\\,\\$\\)\n\\(\\color[RGB]{68,119,170}-1\\,\\$\\)\n\n\n\n\n\n\n\n\n\n\nAd-type \\({\\color[RGB]{204,187,68}B}\\) is a neutral advertisement type that leads to revenue independently of the target user’s income. Ad-type \\({\\color[RGB]{204,187,68}A}\\) targets high-income users, leading to higher revenue from them; but it leads to a loss if shown to the wrong target (more money spent on making and deploying the ad than what is gained from users’ purchases). Vice versa, ad-type \\({\\color[RGB]{204,187,68}B}\\) targets low-income users, with a reverse effect.\nThe corporation doesn’t have access to its users’ income levels, but it covertly collects, through some other app, all or some of the eight predictor variates \\(\\color[RGB]{34,136,51}\\mathit{workclass}\\), \\(\\color[RGB]{34,136,51}\\mathit{education}\\), \\(\\color[RGB]{34,136,51}\\dotsc\\), \\(\\color[RGB]{34,136,51}\\mathit{sex}\\), \\(\\color[RGB]{34,136,51}\\mathit{native\\_country}\\) from each of its users. The corporation has also access to the adult-income dataset (or let’s say a more recent version of it).\nIn this scenario the corporation would like to use an AI agent that can choose and show the optimal ad-type to each user.\n\n\nOur prototype agent from chapters 33, 34, 35 can be used for such a task. It has already been trained with the dataset, and can use any subset (possibly even empty) of the eight predictors to calculate the probability for the two income levels.\nAll that’s left is to equip our prototype agent with a function that outputs the optimal decision, given the calculated probabilities and the set of utilities. In our code this is done by the function decide() described in chapter  34 and reprinted here:\n\n\n decide(probs, utils=NULL)\n\n\nArguments:\n\n\nprobs: a probability distribution for one or more variates.\nutils: a named matrix or array of utilities. The rows of the matrix correspond to the available decisions, the columns or remaining array dimensions correspond to the possible values of the predictand variates.\n\n\nOutput:\n\na list of elements EUs and optimal:\n\nEUs is a vector containing the expected utilities of all decisions, sorted from highest to lowest\noptimal is the decision having maximal expected utility, or one of them, if more than one, selected with equal probability\n\n\nNotes:\n\n\nIf utils is missing or NULL, a matrix of the form \\(\\begin{bsmallmatrix}1&0&\\dotso\\\\0&1&\\dotso\\\\\\dotso&\\dotso&\\dotso\\end{bsmallmatrix}\\) is assumed (which corresponds to using accuracy as evaluation metric).\n\n\n\n\n\n\n\nExample\nA new user logs in; all eight predictors are available for this user:\n\n\\[\\color[RGB]{34,136,51}\n\\begin{aligned}\n&\\mathit{workclass} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Private;}\n&& \\mathit{education} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Bachelors;}\n\\\\ & \\mathit{marital\\_status} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Never-married;}\n&& \\mathit{occupation} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Prof-specialty;}\n\\\\ & \\mathit{relationship} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Not-in-family;}\n&& \\mathit{race} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\n\\\\ & \\mathit{sex} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Female;}\n&& \\mathit{native\\_country} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;United-States;}\n\\end{aligned}\n\\]\n\nThe agent calculates (using the infer() function) the probabilities for the two income levels, which turn out to be\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{238,102,119}\\mathit{\\color[RGB]{238,102,119}income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}predictor}, \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}) = 83.3\\%\n\\\\&\\mathrm{P}({\\color[RGB]{238,102,119}\\mathit{\\color[RGB]{238,102,119}income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}predictor}, \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}) = 16.7\\%\n\\end{aligned}\n\\]\n\nand can be represented by the column matrix\n\\[\\boldsymbol{\\color[RGB]{34,136,51}P}\\coloneqq\n\\color[RGB]{34,136,51}\\begin{bmatrix}\n0.833\\\\0.167\n\\end{bmatrix}\n\\]\nThe utilities previously given can be represented by the matrix\n\\[\\boldsymbol{\\color[RGB]{68,119,170}U}\\coloneqq\n\\color[RGB]{68,119,170}\\begin{bmatrix}\n-1&3\\\\2&2\\\\3&-1\n\\end{bmatrix}\n\\]\nMultiplying the two matrices above we obtain the expected utilities of the three ad-types for the present user:\n\n\\[\n\\boldsymbol{\\color[RGB]{68,119,170}U}\\boldsymbol{\\color[RGB]{34,136,51}P}=\n\\color[RGB]{68,119,170}\\begin{bmatrix}\n-1&3\\\\2&2\\\\3&-1\n\\end{bmatrix}\n\\,\n\\color[RGB]{34,136,51}\\begin{bmatrix}\n0.833\\\\0.167\n\\end{bmatrix}\\color[RGB]{0,0,0}\n=\n\\begin{bmatrix}\n{\\color[RGB]{68,119,170}-1}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ {\\color[RGB]{68,119,170}3}\\cdot{\\color[RGB]{34,136,51}0.167}\n\\\\\n{\\color[RGB]{68,119,170}2}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ {\\color[RGB]{68,119,170}2}\\cdot{\\color[RGB]{34,136,51}0.167}\n\\\\\n{\\color[RGB]{68,119,170}3}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ ({\\color[RGB]{68,119,170}-1})\\cdot{\\color[RGB]{34,136,51}0.167}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.332\\\\\n2.000\\\\\n\\boldsymbol{2.332}\n\\end{bmatrix}\n\\]\n\nThe highest expected utility is that of ad-type \\({\\color[RGB]{204,187,68}C}\\), which is therefore shown to the user.\n\n\nPowerful flexibility of the optimal predictor machine\nIn the previous chapters we already emphasized and witnessed the flexibility of the optimal predictor machine with regard to the availability of the predictors: it can draw an inference even if some or all predictors are missing.\nNow we can see another powerful kind of flexibility: the optimal predictor machine can in principle use different sets of decisions and different utilities for each new application. The decision criterion is not “hard-coded”; it can be customized on the fly.\nThe possible number of ad-types and the utilities could even be a function of the predictor values. For instance, there could be a set of three ad-types targeting users with \\(\\color[RGB]{34,136,51}\\mathit{education}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Bachelors;}\\), a different set of four ad-types targeting users with \\(\\color[RGB]{34,136,51}\\mathit{education}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Preschool;}\\), and so on.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-DT-generalization",
    "href": "making_decisions.html#sec-DT-generalization",
    "title": "37  Making decisions",
    "section": "37.3 The full extent of Decision Theory",
    "text": "37.3 The full extent of Decision Theory\nThe simple decision-making problems and framework that we have discussed in these notes are only the basic blocks of Decision Theory. This theory covers more complicated decision problems. We only mention some examples:\n\nSequential decisions. Many decision-making problems involve sequences of possible decisions, alternating with sequences of possible outcomes. These sequences can be represented as decision trees. Decision theory allows us to find the optimal decision sequence for instance through the averaging out and folding back” procedure.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\n§15.5 and chapter 16 of Artificial Intelligence\nChapter 2 of Decision Analysis\n\n\n\n\n\n\n\nUncertain utilities. It is possible to recast Decision Theory and the principle of maximum expected utility in terms, not of utility functions \\(\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), but of probability distributions over utility values:\n\\[\\mathrm{P}({\\color[RGB]{68,119,170}U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nFormally the two approaches can be shown to be equivalent.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapter 8 of Rational Descriptions, Decisions and Designs\nBox 11.1 (p. 351) of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\n\n\n\n\n\nAcquiring more information. In many situations the agent has one more possible choice: to gather more information in order to calculate sharper probabilities, rather than deciding immediately. This kind of decision is also accounted for by Decision Theory, and constitutes one of the theoretical bases of “reinforcement learning”.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\n§15.6 of Artificial Intelligence\n§11.5 of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\n\n\n\n\n\nMulti-agent problems. To some extent it is possible to consider situations (such as games) with several agents having different and even opposing utilities. This area of Decision Theory is apparently still under development.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapter 17 of Artificial Intelligence",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html",
    "href": "example_opm2.html",
    "title": "38  The prototype Optimal Predictor Machine makes decisions",
    "section": "",
    "text": "38.1 Initialization and build of OPM agent\nIt is straightforward to implement decision-making in our prototype Optimal Predictor Machine. Let’s continue with the example from chapter  35.\nLoad the necessary libraries and functions, including the decide() function, and train the agent as we did previously:\nlibrary('extraDistr')\nlibrary('foreach')\n\nsource('tplotfunctions.R')\nsource('guessmetadata.R')\nsource('buildagent.R')\nsource('infer.R')\nsource('decide.R')\nsource('mutualinfo.R')\nsource('rF.R')\nsource('plotFsamples1D.R')\n\noptions(repr.plot.width = 6*sqrt(2), repr.plot.height = 6)\n\nopmall &lt;- buildagent(metadata = 'meta_income_data_example.csv',\n                     data = 'train-income_data_example.csv')",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#decision-matrix",
    "href": "example_opm2.html#decision-matrix",
    "title": "38  The prototype Optimal Predictor Machine makes decisions",
    "section": "38.2 Decision matrix",
    "text": "38.2 Decision matrix\nWe use the targeted-advertisement scenario of § 37.2, with the following utility matrix for the three ad-types:\n\nadutilities &lt;- matrix(\n    c(-1, 3,\n        2, 2,\n        3,-1),\n    nrow = 3, byrow = TRUE,\n    dimnames = list(ad_type = c('A','B','C'), income = c('&lt;=50K', '&gt;50K')))\n\nprint(adutilities)\n\n       income\nad_type &lt;=50K &gt;50K\n      A    -1    3\n      B     2    2\n      C     3   -1",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#example-application",
    "href": "example_opm2.html#example-application",
    "title": "38  The prototype Optimal Predictor Machine makes decisions",
    "section": "38.3 Example application",
    "text": "38.3 Example application\nFirst let’s apply the principle of maximal expected utility step-by-step.\nConsider the example from § 37.2. The agent calculates the probabilities for the predictand income from the given predictor values:\n\nuserpredictors &lt;- list(workclass = 'Private', education = 'Bachelors',\n                       marital_status = 'Never-married',\n                       occupation = 'Prof-specialty',\n                       relationship = 'Not-in-family', race = 'White',\n                       sex = 'Female', native_country = 'United-States')\n\nprobs &lt;- infer(agent = opmall, predictand = 'income',\n               predictor = userpredictors)\n\nprint(probs)\n\nincome\n   &lt;=50K     &gt;50K \n0.833333 0.166667 \n\n\nFind the expected utilities of the three possible ad-types by matrix multiplication:\n\nadutilities %*% probs\n\n       \nad_type     [,1]\n      A -0.33333\n      B  2.00000\n      C  2.33333\n\n\nAnd we see that ad-type C is optimal.\n\n\nThe function decide() does the previous calculations. It outputs a list with elements:\n\nEUs: the expected utilities of the decisions, sorted from highest to lowest\noptimal: one decision unsystematically chosen among the optimal ones (if more than one)\n\n\noptimalad &lt;- decide(utils = adutilities, probs = probs)\n\nprint(optimalad)\n\n$EUs\n       C        B        A \n 2.33333  2.00000 -0.33333 \n\n$optimal\n[1] \"C\"",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#performance-on-test-set",
    "href": "example_opm2.html#performance-on-test-set",
    "title": "38  The prototype Optimal Predictor Machine makes decisions",
    "section": "38.4 Performance on test set",
    "text": "38.4 Performance on test set\nFinally let’s apply our prototype agent to a test set, as a demonstration, and see how much utility it yields. This procedure will be discussed in more detail in § 40.\nLoad the test dataset; M is the number of test data:\n\ntestdata &lt;- read.csv('test-income_data_example.csv', header = TRUE,\n    na.strings = '', stringsAsFactors = FALSE, tryLogical = FALSE)\n\nM &lt;- nrow(testdata)\n\nWe build the analogous of a “confusion matrix” (§ 40), telling us how many times the agent chooses the three ad-types for both income levels.\n\nconfusionmatrix &lt;- adutilities * 0L\n\n## Use a for-loop for clarity\nfor(i in 1:M){\n    userpredictors &lt;- testdata[i, colnames(testdata) != 'income']\n    probs &lt;- infer(agent = opmall, predictand = 'income',\n                   predictor = userpredictors)\n    decision &lt;- decide(utils = adutilities, probs = probs)$optimal\n    trueincome &lt;- testdata[i, 'income']\n\n    confusionmatrix[decision, trueincome] &lt;- confusionmatrix[decision, trueincome] + 1L\n}\n\nprint(confusionmatrix)\n\n       income\nad_type &lt;=50K &gt;50K\n      A   769 2149\n      B 11768 5093\n      C 12961 1174\n\n\nThe total utility yield is the total sum of the element-wise product of the confusionmatrix and the adutilities matrix\n\ntotalyield &lt;- sum(adutilities * confusionmatrix)\naverageyield &lt;- totalyield/M\n\ncat('\\nTotal yield =', totalyield, \n'\\nAverage yield =', averageyield, '\\n')\n\n\nTotal yield = 77109 \nAverage yield = 2.27366 \n\n\nNote that:\n\nThis yield is higher than what would be obtained by just choosing the neutral ad-type B for all test units (the average yield would be exactly 2).\nThis yield is also higher than would be obtained by always choosing ad-type C, targeting the majority of units, which have income = '&lt;=50K'. This strategy would yield 2.00737.\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to use some common machine-learning algorithm to perform the same task of choosing between the three ad-types. Is it difficult? why?\nIf you manage to do this, then compare the performances of the machine-learning algorithm and the opmall agent.\nConstruct a scenario where the utility matrix is different depending on the sex predictor variate. Write a script to apply the opmall agent on the test set according to this new scenario.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html",
    "href": "limitations_ML.html",
    "title": "39  Decisions: limitations of present-day machine-learning algorithms",
    "section": "",
    "text": "39.1 The omnipresence of decision-making in data science and machine learning\nMany machine-learning textbooks say that\nSuch statement is misleading, because it suggests that there is a functional relationship from input to output, or from predictor to predictand – a functional relationship that’s only waiting to be discovered and “learned”. But as we discussed in chapter  24, in many important tasks and applications this is actually not true: there isn’t any functional relationship between input and output at all.1 Even if any possible “noise” were removed, there would still not be any functional relationship between the denoised predictor and predictand (here is an example from image classification).\nIn many important tasks there’s only a statistical relationship between predictands and predictors. “Statistical” means that whenever a predictor has value \\(\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\), the predictand value may turn out to be \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y'\\) in some units, but also \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y''\\) in some other units, and so on. As a simple example, consider the Norwegian population in 2022. If our predictand is \\(\\mathit{sex}\\) and we take as predictor that a person’s \\(\\mathit{age}\\) is between \\(85\\)–\\(89\\) years, then a proportion \\(43 542/(43 542 + 28 220) \\approx 61\\%\\) of those persons have \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;female;}\\), and the remaining \\(39\\%\\) proportion has \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;male;}\\). This doesn’t mean that, say, \\({\\small\\verb;female;}\\) is the “true” output, and \\({\\small\\verb;male;}\\) is just the effect of noise, or vice versa. That would be nonsense.\nEven in tasks where there actually is a functional relation from predictors to predictands, the agent typically doesn’t know what is the function’s output for particular predictor values, because no such values have been observed in the training data. It must interpolate or extrapolate what it has learned. Also in this case there are several possibilities to choose from.\nA decision-making step also appears in tasks where the agent must generate a new unit. Obviously there are many candidates for generation, which agree with what was observed in the training data. If the agent generates one unit, then it must internally have chosen among the possible candidates.",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-decision-everywhere",
    "href": "limitations_ML.html#sec-decision-everywhere",
    "title": "39  Decisions: limitations of present-day machine-learning algorithms",
    "section": "",
    "text": "in supervised learning the algorithm learns a functional relationship between some kind of input and some kind of output\n\n\n1 This is one more reason why we use the more general terms “predictor” & “predictand”, rather that “input” & “output”.",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-where-prob-util",
    "href": "limitations_ML.html#sec-where-prob-util",
    "title": "39  Decisions: limitations of present-day machine-learning algorithms",
    "section": "39.2 Where are the probabilities and the utilities? How are they calculated?",
    "text": "39.2 Where are the probabilities and the utilities? How are they calculated?\nThe remarks above have a very important consequence. If a machine-learning algorithm outputs just one predictand value (or generates one unit), among the possible ones that are consistent with the predictor, then it means that the algorithm is internally choosing one of the possibilities.\nSuch a choice is obviously a decision-making problem. We know that the optimal, logically consistent choice must be determined by Decision Theory, and its determination requires:\n\n  Some kind of background knowledge\n  the probabilities of the possible predictand values\n  a list of possible decisions\n  the utilities of the decisions, depending on the predictand’s true value\n\nThe algorithm internally must – at least approximately – be calculating probabilities and maximizing expected utilities. In principle its internal workings should be susceptible to an explanation or interpretation from this point of view.\nFor some, or maybe many, machine-learning algorithms such an interpretation is not readily available, and is or can be a very interesting area of research, leading to improvements or to completely new algorithms.\nAvailable interpretations of machine-learning algorithms are mostly from the point of view of Probability Theory; unfortunately very little from the point of view of Decision Theory. For this reason we now discuss some limitations of present-day algorithms from the decision-theoretic viewpoint.\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe book Machine Learning and Part V of the book Information Theory, Inference, and Learning Algorithms discuss interpretations of several machine-learning algorithms from the point of view of Probability Theory, and in few cases also of Decision Theory.",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-limit-class",
    "href": "limitations_ML.html#sec-limit-class",
    "title": "39  Decisions: limitations of present-day machine-learning algorithms",
    "section": "39.3 Limitations of machine-learning classifiers (and estimators)",
    "text": "39.3 Limitations of machine-learning classifiers (and estimators)\n\nUnknowns vs decisions\nDecision Theory makes a distinction between what is unknown to an agent (“outcomes”), and what the agent has to choose (“decisions”). This distinction is common in everyday problems. We may wonder whether it will rain in the next hour. The point of our wonder, however, is not (except in some situations) the rain phenomenon per se, but its implications about what clothes or shoes we should wear, or about staying indoors or going out, or about going on foot or by car, and so on.\nAn important reason for this distinction is that the set of possible decisions often does not have a correspondence with the set of unknown possibilities. In fact, the two sets often have different numbers of elements. You can think of many situations in which you are unsure whether some event will happen or not, and you have three plans: one if you are almost sure the event will happen, one if you are almost sure the event will not happen, and a third “safe” plan if you are about 50%/50% uncertain. The “safe” plan typically has consequences that are neither too bad or too good, so that losses and gains are kept to a minimum. This example also shows why the probabilities of the outcomes are important.\n\n\nMany present-day machine-learning algorithms are quite limited in this respect:\n\ntheir output is typically one of the unknown values, not one of the decisions\nthey don’t give the probabilities of the unknown values\n\nThis limitation may not be important in some tasks, for instance when you are classifying some images as “cat” or “dog” for the purpose of a photo album. But it is extremely important and impairing in serious applications, such as clinical ones. Let’s illustrate this with a simple example.\n\n\nA typical decision-making problem in medicine\nA clinician may be uncertain about the presence or absence of some medical condition, let’s say a disease. This uncertainty cannot be fully removed. The clinician’s task is not simply to guess about the disease, but to choose among different available treatments. Imagine you may have broken a bone, and the clinician simply tells you: “I guess it’s broken, though I’m not fully sure. Goodbye!”.\nTwo crucial points about treatment choice are these:\n\nthere may be more than two possible treatments, even if the uncertainty is binary (disease present vs absent)\nwhich treatment is optimal depends on the probability that the disease is present, not on a simple “yes/no guess”\n\nNeglecting both points can lead to disastrous consequences.\nFor example, suppose the clinician has three treatments available: \\({\\color[RGB]{204,187,68}A}\\), \\({\\color[RGB]{204,187,68}B}\\), \\({\\color[RGB]{204,187,68}C}\\). They have different efficacies against the disease, if it is present; and different damaging side-effects for the patient, if the disease is not present. Suppose that efficacy and damage can be measured together as decrease in life expectancy for the patient. The effects are as follows:\n\n\n\n\n\n\n\nTable 39.1: Change in life expectancy depending on treatment and medical condition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}disease}\\)\n\n\n\n\n\\({\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\)\n\\({\\color[RGB]{238,102,119}{\\small\\verb;no;}}\\)\n\n\ntreatment\n\\({\\color[RGB]{204,187,68}A}\\)\n\\(\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}0\\,\\mathrm{yr}\\)\n\n\n\\({\\color[RGB]{204,187,68}B}\\)\n\\(\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}\\)\n\n\n\\({\\color[RGB]{204,187,68}C}\\)\n\\(\\color[RGB]{204,187,68}0\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}\\)\n\n\n\n\n\n\n\n\n\n\n\ntreatment \\({\\color[RGB]{204,187,68}A}\\) is mild (it could actually be a no-treatment option): under it, the patient is expected to live four years shorter if the disease is present, but the life expectancy is unaltered if the disease is not present\ntreatment \\({\\color[RGB]{204,187,68}B}\\) is intermediate: under it, the patient is expected to live only one year shorter if the disease is present, but also if the disease is not present, owing to the damage caused by this treatment\ntreatment \\({\\color[RGB]{204,187,68}C}\\) is intensive: under it, the patient is expected not to lose extra years if the disease is present, but will lose four years if the disease is not present, owing to the heavy damage caused by this treatment.\n\nWhich of the treatments above should the clinician choose? We now know how the clinician should make the optimal decision, but let’s explore the possible consequences of not making it, in three different scenarios. In each scenario, the clinician has prescribed several clinical tests (the predictors) for the patient, and obtained their results.\n\nScenario 1: 10%/90%\nGiven the results of the clinical tests, the clinician knows that the patient is typical of a subpopulation of patients where 10% have the disease, and 90% don’t. The present patient could be one among the 10%, or one of among the 90%.\n\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}A}\\) for this subpopulation, including the present patient, then these patients’ lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-40\\,\\mathrm{yr}}\n\\]\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}B}\\) for this subpopulation, including the present patient, then these patients’ lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}C}\\), then the lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-360\\,\\mathrm{yr}}\n\\]\n\nClearly the best decision is treatment \\({\\color[RGB]{204,187,68}A}\\). It is possible that, unfortunately, the present patient’s life will be shortened; but this treatment was the patient’s and clinician’s best bet.\nIn this scenario, a clinician that doesn’t choose the optimal treatment is on average taking away from each patient between 7 months and 3 years of life more than was necessary or unavoidable.\n\n\nScenario 2: 50%/50%\nGiven the results of the clinical tests, the clinician knows that the patient is typical of a subpopulation of patients where 50% have the disease, and 50% don’t. In this case the present patient could be one of the first 50%, or one of the other 50%.\nCalculations similar to those of scenario 1 leads to these results:\n\nTreatment \\({\\color[RGB]{204,187,68}A}\\)\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-200\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}B}\\):\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}C}\\):\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-200\\,\\mathrm{yr}}\n\\]\n\nThe best decision is treatment \\({\\color[RGB]{204,187,68}B}\\).\nIn this scenario, a clinician who chooses treatments \\({\\color[RGB]{204,187,68}A}\\) or \\({\\color[RGB]{204,187,68}C}\\) for patients having the same predictors as the present patient is on average taking away one extra year of life from each patient.\n\n\nScenario 3: 90%/10%\nIn this scenario 90% of patients in the subpopulation with the observed predictors have the disease, and 10% don’t. The present patient could belong to either group.\n\nTreatment \\({\\color[RGB]{204,187,68}A}\\)\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-360\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}B}\\):\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}C}\\):\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-40\\,\\mathrm{yr}}\n\\]\n\nTreatment \\({\\color[RGB]{204,187,68}C}\\) is the best decision in this scenario, for reasons complementary to those of scenario 1.\nIn this scenario, like in the first, a clinician that doesn’t choose the optimal treatment is on average taking away, from each patient, between 7 months and 3 years of life more than was necessary or unavoidable.\n\n\nIn each scenario, note that any decision strategy different from “sticking to the optimal decision” leads to suboptimal results – that is, lives shortened more than what was unavoidable. In scenario 1, for instance, a decision strategy such as “choose treatment \\({\\color[RGB]{204,187,68}A}\\) most of the time, and treatment \\({\\color[RGB]{204,187,68}B}\\) from time to time” leads to an additional life shortening of several months. This is clear from the following graph, which shows the average reduction in life expectancy for each treatment, depending on the percentage of patients with the disease:\n\nConsider the vertical line corresponding to probability 0.1. Any strategy that mixes treatment \\({\\color[RGB]{204,187,68}A}\\) with any of the other two will only reduce the life expectancy. This is true for any other probability values and their corresponding optimal treatments.\nFrom the plot we can see that treatment \\({\\color[RGB]{204,187,68}A}\\) is optimal if the probability that the disease is present is below \\(25\\%\\), treatment \\({\\color[RGB]{204,187,68}C}\\) is optimal if the probability is above \\(75\\%\\), and treatment \\({\\color[RGB]{204,187,68}B}\\) for intermediate probabilities.\n\n\n\n\n\n\n Exercise\n\n\n\nAlthough the intuitive reasoning above has somewhat been phrased in terms of frequency, it’s the probability – the clinician’s degree of belief – that counts. Try to reason about this point through the following exercise:\nConsider a clinician uncertain with 50% probability that the present patient belongs to a 10%/90% frequency subpopulation, and with 50% that the patient belongs to a 90%/10% frequency subpopulation. And unfortunately this uncertainty cannot be removed by further clinical tests. Which treatment should this clinician choose? why?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nChapter 1 of Sox & al.: Medical Decision Making\nSkim through chapter 6 of Sox & al.: Medical Decision Making\n§§ 1.5–1.6 of Lindley: Making Decisions\n\n\n\n\n\n\nSub-optimality of a typical machine-learning algorithm\nThe clinical example above shows that there isn’t any one-to-one connection between decisions and unknowns. We cannot say, for instance, that treatment \\({\\color[RGB]{204,187,68}C}\\) “corresponds” to the presence of the disease, because the best treatment is actually \\({\\color[RGB]{204,187,68}B}\\), not \\({\\color[RGB]{204,187,68}C}\\), if the probability for \\(\\mathit{\\color[RGB]{238,102,119}disease}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) is above 50% but below 75%.\nNow imagine that the clinician inputs the patient’s predictors into a neural network, trained to give an output about the presence or absence of the disease. The neural network outputs yes, but it’s known that the neural network can err. Which treatment should the clinician choose?\n\nDoes the output yes mean that the probability for \\(\\mathit{\\color[RGB]{238,102,119}disease}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) was above 50%? Then the clinician doesn’t know whether it’s above or below 75%, and can’t make the optimal choice between \\({\\color[RGB]{204,187,68}C}\\) and \\({\\color[RGB]{204,187,68}B}\\). At best the clinician can unsystematically alternate between these two treatments, but as we saw above this mixture is sub-optimal.\nIs the output yes produced with a particular probability? But what is this probability? The output could be \\({\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) even if the probability were less than 50% or 25%. Then neither in this case can the clinician make the optimal choice between all three treatments. And a mixture of all three is again sub-optimal.\n\n\n\n\n\n\n\n Exercise\n\n\n\nCould the clinician deduce an approximate probability by looking at the statistics (confusion matrix) of the neural network on some test set?\nExplore and maybe even implement this possibility.\n\n\n\n\nOutput scores or weights are not probabilities!\nSome machine-learning algorithms are capable of giving continuous “scores” or “weights” between 0 and 1, instead of a simple answer such as “yes” or “no”. But unfortunately, typically such scores are not probabilities or frequencies, even if there may be some association between them and the probability or frequency.\nA real-life example of this mismatch is evident in the plot below (see “for the extra curious” below for references). It shows the 0–1 output score given by a random forest in a binary-classification task on a test set, versus the actual frequency of the corresponding class:\n\nClearly the output score is different from the frequency; if they were the same the graphs would be straight lines with ±45° inclination.\nImagine what would happen if the clinician from our example above mistook the output score for a probability:\n\n  Output score in the range \\(0\\)–\\(0.25\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}A}\\). Luckily the upper boundary of this score range approximately corresponds to a frequency of \\(25\\%\\), so in this case the clinician would be choosing the optimal treatment.\n  Output score in the range \\(0.25\\)–\\(0.5\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}B}\\). Luckily the upper boundary of this score range approximately corresponds to a frequency of \\(75\\%\\) (note the mismatch), so in this case the clinician would be choosing the optimal treatment.\n  Output score in the range \\(0.5\\)–\\(0.75\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}B}\\). But the corresponding frequency is approximately between \\(75\\%\\)–\\(90\\%\\), so the optimal treatment is actually \\({\\color[RGB]{204,187,68}C}\\). As calculated in Scenario 3 above, the clinician would be then be shortening the patient’s life by 7 months more than was unavoidable.\n  Output score in the range \\(0.75\\)–\\(1\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}C}\\). Luckily this corresponds to an approximate frequency range \\(90\\%\\)–\\(92\\%\\), where treatment \\({\\color[RGB]{204,187,68}C}\\) is indeed optimal.\n\nNote that we cannot say “the suboptimal treatment is chosen one out of four times”, because in this example we don’t know how many patients end up having a score in the \\(0.5\\)–\\(0.75\\) range. In a concrete case it could be that 90% of the patients end up in this range, which would mean that the misuse of the output score would lead to a suboptimal treatment in 90% of cases.\n\n\nAnother real example of the difference between weights or scores and probabilities is shown in the plot below, obtained by applying a neural network to the same binary-classification task on a test set. The x-axis shows the internal output-layer weight that the neural network assign to one class (more precisely, the diagonal where the two output-layer weights have equal values). The y-axis shows the frequency of that class. Typically a “softmax” or sigmoid function is applied to the output-layer weights, in order to obtain positive, normalized scores that are often miscalled “probabilities”. The softmax in the present case is shown as the dashed grey line.\n\nWe can observe an even worse mismatch that for the random forest. For instance, if the softmax is between \\(0\\)–\\(0.25\\), then the output-layer weight must have been less than approximately \\(-0.5\\), which in turn means that the actual frequency could be anywhere between \\(0\\%\\) and \\(50\\%\\). For a softmax between \\(0.75\\)–\\(1\\), the actual frequency could be anywhere between \\(50\\%\\) and \\(92\\%\\).\nIn both plots above, notice how the probability for each class can never be higher than around \\(92\\%\\), yet the weights or scores go up to \\(1\\).\n\n\n\n\n\n\n Don’t use misleading terminology\n\n\n\nPlease don’t call the score or weight output of a machine-learning algorithm a “probability” or “frequency”, unless you’ve first made sure that it actually is a probability or frequency. Just because some numbers are positive and sum up to one doesn’t mean that they are a probability or frequency distribution.\nUsing this kind of mistaken terminology shows downright scientific incompetence, and its consequences, as you can see from the medical examples above, are borderline scientific malpractice.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDon’t guess what’s true: choose what’s optimal. A probability transducer for machine-learning classifiers",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html",
    "href": "utilities_evaluation.html",
    "title": "40  Evaluation practices and utilities",
    "section": "",
    "text": "40.1 Confusion matrices and evaluation metrics\nMachine-learning methodology uses a disconcerting variety of evaluation metrics to try to quantify, compare, rank the performances of one or more algorithms.\nFor machine-learning classifiers many of these metrics are constructed from the so-called “confusion matrix”. The basic idea behind it is to test the algorithms of interest on a test dataset (the same for all), and then count for how many units the algorithm outputs value \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}y^{*}}\\) when the true value of the unknown is \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), for all combinations of possible \\({\\color[RGB]{238,102,119}y^{*}}\\) and \\({\\color[RGB]{238,102,119}y}\\).\nImagine for instance a binary-classification task with classes \\({\\color[RGB]{238,102,119}\\alpha}\\) and \\({\\color[RGB]{238,102,119}\\beta}\\). It could be the electronic-component scenario we met in the first chapters, with class \\({\\color[RGB]{238,102,119}\\alpha}\\) being “the electronic component will function for at least a year”, and class \\({\\color[RGB]{238,102,119}\\beta}\\) “the electronic component will fail within a year of use”.\nApplication of two algorithms \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) to the same 100 test units results in the following confusion matrices:\nThese matrices can also be normalized, dividing every entry by the total number of test units. Various aspects can be read from the confusion matrices above. Algorithm \\(\\mathcal{B}\\), for example, seems better than \\(\\mathcal{A}\\) at inferring class \\({\\color[RGB]{238,102,119}\\alpha}\\), but slightly worse at inferring class \\({\\color[RGB]{238,102,119}\\beta}\\).\nMost evaluation metrics for classification combine the entries of a confusion matrix in a mathematical formula that yields a single number.\nSuch metrics and formulae can be quite opaque. Their definitions and their motivations are often arbitrary or very case-specific. It’s common to find works where several metrics are used because it’s unclear which single one should be used. And the only hope is that most or all of them will agree at least on the rankings they lead to. However,…",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-confusion",
    "href": "utilities_evaluation.html#sec-confusion",
    "title": "40  Evaluation practices and utilities",
    "section": "",
    "text": "Table 40.1: Confusion matrix for \\(\\mathcal{A}\\)\n\n\n\n\n\n\n\n\n\n\n\ntrue \\({\\color[RGB]{238,102,119}\\alpha}\\)\ntrue \\({\\color[RGB]{238,102,119}\\beta}\\)\n\n\noutput \\({\\color[RGB]{238,102,119}\\alpha}\\)\n27\n15\n\n\noutput \\({\\color[RGB]{238,102,119}\\beta}\\)\n23\n35\n\n\n\n\n\n\n\n\n\n\n\n\nTable 40.2: Confusion matrix for \\(\\mathcal{B}\\)\n\n\n\n\n\n\n\n\n\n\n\ntrue \\({\\color[RGB]{238,102,119}\\alpha}\\)\ntrue \\({\\color[RGB]{238,102,119}\\beta}\\)\n\n\noutput \\({\\color[RGB]{238,102,119}\\alpha}\\)\n43\n18\n\n\noutput \\({\\color[RGB]{238,102,119}\\beta}\\)\n7\n32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMajority votes are not a criterion for correctness\nHere are the scores that some popular metrics assign to algorithms \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), based on their confusion matrices above. The better algorithm according to each metric is indicated in green bold. Almost all these evaluation metrics seem to agree that \\(\\mathcal{B}\\) should be the best of the two:\n\n\n\n\n\n\n\n\n\n\n\n\nmetric\n\\(\\mathcal{A}\\)\n\\(\\mathcal{B}\\)\n\n\nAccuracy\n0.62\n0.75\n\n\nPrecision\n0.64\n0.70\n\n\nBalanced Accuracy\n0.62\n0.75\n\n\n\\(F_1\\) measure\n0.59\n0.77\n\n\nMatthews Correlation Coefficient\n0.24\n0.51\n\n\nFowlkes-Mallows index\n0.59\n0.78\n\n\nTrue-positive rate (recall)\n0.54\n0.86\n\n\nTrue-negative rate (specificity)\n0.70\n0.64\n\n\n\n\n\n\n\nYet, when put to actual use, it turns out that \\(\\mathcal{B}\\) actually leads to a monetary loss of 3.5 $ per unit, whereas \\(\\mathcal{A}\\) leads to a monetary gain of 3.5 $ per unit!",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-dt-util-evaluation",
    "href": "utilities_evaluation.html#sec-dt-util-evaluation",
    "title": "40  Evaluation practices and utilities",
    "section": "40.2 Decision theory and utilities as the basis for evaluation",
    "text": "40.2 Decision theory and utilities as the basis for evaluation\nHow is the surprising result above possible? Decision Theory tells us how, and can even correctly select the best algorithm from the confusion matrices above.\nAs discussed in the preceding sections, each application to a new unit is a decision-making problem. Neither making nor evaluating a decision is possible unless we specify the utilities relevant to the problem. It turns out that the consequences in the present problem have the following monetary utilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\({\\color[RGB]{238,102,119}\\alpha}\\) is true\n\\({\\color[RGB]{238,102,119}\\beta}\\) is true\n\n\n\\({\\color[RGB]{238,102,119}\\alpha}\\) is chosen\n\\(\\color[RGB]{68,119,170}15 \\$\\)\n\\(\\color[RGB]{68,119,170}-335 \\$\\)\n\n\n\\({\\color[RGB]{238,102,119}\\beta}\\) is chosen\n\\(\\color[RGB]{68,119,170}-35 \\$\\)\n\\(\\color[RGB]{68,119,170}165 \\$\\)\n\n\n\n\n\n\n\nThese utility values may be determined by the combination of sale revenue, disposal costs, warranty refunds, and similar factors.\nThe utility matrix above says that for every test unit of true class \\({\\color[RGB]{238,102,119}\\alpha}\\) (the unit will function for at least a year) that the algorithm classifies as \\({\\color[RGB]{238,102,119}\\alpha}\\) (and therefore sends for sale), the production company gains \\(\\color[RGB]{68,119,170}15 \\$\\); for every test unit of class \\({\\color[RGB]{238,102,119}\\alpha}\\) that the algorithm classifies as \\({\\color[RGB]{238,102,119}\\beta}\\) (and therefore discards), the production company gains \\(\\color[RGB]{68,119,170}-35 \\$\\) (so actually a loss); and so on. The total yield from each algorithm on the 100 test units can therefore be calculated by multiplying the four utilities above by the corresponding counts in the confusion matrix, and then taking the total. The average yield per unit is obtained dividing the total by the number of units, or directly using the normalized confusion matrix:\n\n\\[\n\\begin{aligned}\n\\text{$\\mathcal{A}$'s average yield } &=\n\\bigl[27 \\cdot {\\color[RGB]{68,119,170}15 \\$} +\n23 \\cdot ({\\color[RGB]{68,119,170}-35 \\$}) +\n15 \\cdot ({\\color[RGB]{68,119,170}-335 \\$}) +\n35 \\cdot {\\color[RGB]{68,119,170}165 \\$}\\bigr]/100\n\\\\[1ex]\n&= \\boldsymbol{\\color[RGB]{34,136,51}+3.5 \\$}\n\\\\[3ex]\n\\text{$\\mathcal{B}$'s average yield } &=\n\\bigl[43 \\cdot {\\color[RGB]{68,119,170}15 \\$} +\n7 \\cdot ({\\color[RGB]{68,119,170}-35 \\$}) +\n18 \\cdot ({\\color[RGB]{68,119,170}-335 \\$}) +\n32 \\cdot {\\color[RGB]{68,119,170}165 \\$}\\bigr]/100\n\\\\[1ex]\n&= {\\color[RGB]{170,51,119}-3.5 \\$}\n\\end{aligned}\n\\]\n\nThis calculation tells us that \\(\\mathcal{A}\\) is better, and also gives us an idea of the actual utilities that the two algorithms would yield.\n\nUtilities as evaluation metric\nThe calculation above is exactly the one discussed in § 36.5, where we logically motivated the use of utilities as an evaluation metric. In the case of common machine-learning classifiers the logically correct evaluation metric and procedure are is thus straightforward:\n\nfind the utilities relevant to the specific problem under consideration, collect them into a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\)\napply the classifier to a test set and build the confusion matrix \\(\\boldsymbol{\\color[RGB]{238,102,119}C}\\)\nmultiply confusion and utility matrices element-wise and take the total\n\nWhat’s remarkable in this procedure is that it is not only logically well-founded, but also mathematically simple. The formula for the correct evaluation metric is just a linear combination of the confusion-matrix entries. This linearity is actually a subtle consequence of the axiom of independence discussed in § 36.6.\nNote something very important:\n\n\n\n\n\n\n Don’t do class balancing!\n\n\n\nThis procedure requires that the test set be a representative, unsystematic sample of the actual population of interest. In particular, the proportions of classes in the test set should reflect their frequencies in real application. No “class balancing” should be performed.\n\n\nThe evaluation based on decision theory automatically takes into account “class balance” and its interaction with the utilities relevant to the task.\nThere are problems for which the simple strategy “always choose class \\(\\dotso\\)” is optimal, given the predictors available in the problem. So this strategy cannot be beaten by “improving” or finding a “better” algorithm: such endeavour is only a waste of time. The reason is that the maximal information that the predictors can give about the predictand is not enough to sharpen probabilities above the thresholds determined by the utilities. This maximal information is an intrinsic properties of predictors and predictands, so it cannot be improved by fiddling with algorithms. The only way for improvement is to find other predictors.\n“Class balancing” does not solve this problem. It transforms the actual task into a different task, where maybe some algorithm can show improvement, simply because we have changed the population statistics and therefore the mutual information between predictors and predictands. But as soon as we get back to reality, to the actual task, the situation will be as before.\n\n\n\n\n\n\n Study reading\n\n\n\nDrummond & al. 2005: Severe Class Imbalance: Why Better Algorithms Aren’t the Answer",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-good-bad-metrics",
    "href": "utilities_evaluation.html#sec-good-bad-metrics",
    "title": "40  Evaluation practices and utilities",
    "section": "40.3 Popular evaluation metrics: the good and the bad",
    "text": "40.3 Popular evaluation metrics: the good and the bad\nIf an evaluation metric can be rewritten as a linear combination of the confusion-matrix entries, then it can be interpreted as arising from a set of utilities (although they might not be the ones appropriate to the problem).\nThis is the case, for instance, of\n\naccuracy, which turns out to correspond to the utility matrix \\(\\begin{bsmallmatrix}\n1&0\\\\0&1\n\\end{bsmallmatrix}\\) or its non-binary analogues;\ntrue-positive rate, which corresponds to \\(\\begin{bsmallmatrix}\n1&0\\\\0&0\n\\end{bsmallmatrix}\\)\ntrue-negative rate, which corresponds to \\(\\begin{bsmallmatrix}\n0&0\\\\0&1\n\\end{bsmallmatrix}\\)\n\nIf an evaluation metric cannot be rewritten in such a linear form, then it is breaking the axioms of Decision Theory, and is therefore guaranteed to carry some form of cognitive bias. The axiom of independence is specifically broken, because non-linearities imply some functional dependence of utilities on probabilities or vice versa.\nSome quite popular evaluation metrics turn out to break Decision Theory in this way:\n\nprecision\n\\(F_1\\)-measure\nMatthews correlation coefficient\nFowlkes-Mallows index\nbalanced accuracy\n\nThe area under the curve of the receiving operating characteristic (typically denoted “AUC”) is also an evaluation metric that breaks the axioms of Decision Theory, although it is not based on the confusion matrix.\nThis fact is actually funny, because the first papers (in the 1960s–1970s, referenced below) that discussed an evaluation method based on the receiver operating characteristic actually derived it from Decision Theory. The papers gave the correct procedure to use the receiver operating characteristic, and pointed out the “area under the curve” only as a quick but possibly erroneous heuristic procedure.\nIt goes without saying that you should stay away from the cognitive-biased metrics above.\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through Dyrland & al. 2022: Does the evaluation stand up to evaluation?\nSkim through Swets & al. 1961: Decision processes in perception\nSkim through Metz 1978: Basic principles of ROC analysis",
    "crumbs": [
      "[**Further connections with present-day machine-learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "whither.html",
    "href": "whither.html",
    "title": "What next?",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have finally reached the end of this course. Congratulations!\n…Or maybe we should say: *Good luck on your new journey! – Because this is just the beginning.\nWhat we hope you have taken from this course is a big picture of the science underneath data science and data-driven engineering, with a clear idea of its main (and few!) principles. You can now apply these principles to engineering problems similar to those explored in this course, and to other, more challenging problems. The principles you have learned are exactly the same.\n\n\nNow it is up to you in which directions to continue your journey as a data scientist. Maybe you want to…\n\n\n  engineer “optimal predictor machines” that can deal with more complex kind of data\n  improve existing machine-learning algorithms by analysing how they approximate an optimal predictor machine\n  use your understanding of the foundations to interpret and explain how present-day algorithms work\n  look for new technologies that may allow us to do the complicated computations required by an optimal predictor machine\n  disseminate what you have learned here, or explore its foundations, or find ways to make it more understandable\n\n\n…and many other possibilities.\nIt’s important to be aware that most of these directions will require more difficult mathematics, in order to write working code, to face more realistic problems, and to find actual solutions to them. In the part “A prototype Optimal Predictor Machine*” you saw that we needed to bring up Dirichlet distributions, factorials, integrals, and other mathematics in order to build a concrete, working prototype of an optimal agent. And that agent can only work in a limited and somewhat simple class of problems. Solving more complicated problems will, inevitably, require more complicated mathematics. For some this is actually a fun challenge. In any case, don’t forget the ever-positive side: the basic principles are few and intuitively understandable in their essence.\n\nUsing Probability Theory and Decision Theory as thinking and organizational frameworks\nWe hope that you will use basic probability theory, decision theory, and their notation as tools to frame and organize inference, prediction, and decision problems.\nIt does not matter whether the problem can then be solved exactly according to the rules of probability & decision theory, or whether only a crude approximation is available. You have seen that these two theories are extremely useful even just in the beginning stage, when we ask questions like “what do I need to find?”, “why do I need to find it?” “what do I know?”, “what am I assuming?”, “what’s are the gains and costs of success and failure?” – and similar questions.\nFor example, see again how the basic probability notation helped us classify different types of machine-learning algorithms in chapter  27  Beyond machine learning. The notation even suggested at once how to correctly deal with partially missing data (§ Flexible categorization using probability theory).\n\n\nThe basic, universal formula behind all supervised- and unsupervised-learning algorithms\nWe also hope that you will not forget, and actually use as much as possible, the basic formula (chapter  27  Beyond machine learning) that represents how an agent doing any kind of supervised- or unsupervised-learning works. This formula is what a neural network or a random forest are doing under the hood, even if just in an approximate way:\n\n\n\n\n\n\n\n \n\n\n\n\nAll previous predictors and predictands known (supervised learning)\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}y}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n“Guess all variates” (unsupervised learning, generative algorithms):\n\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\n\n\nPrevious predictors known, previous predictands unknown (unsupervised learning, clustering)\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\quad{}=\n\\frac{\n\\sum_{\\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{{\\color[RGB]{170,51,119}y}, \\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\nAll these formulae, even for hybrid tasks, involve sums and ratios of only one distribution:\n\\[\\boldsymbol{\n\\mathrm{P}(\\color[RGB]{68,119,170}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\text{new}}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\text{new}}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nand if the problem is exchangeable, for instance without time dependence or memory effects, the distribution can be calculated in a simpler way:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Y_{\\text{new}} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{\\text{new}} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Y_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Y_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nPossibly there is a final decision about the output (if a single output is required), using some utilities and the principle of maximal expected utility:\n\\[\n\\mathsfit{\\color[RGB]{204,187,68}D}_{\\text{optimal}} =\n\\operatorname{argmax}\\limits_{\\mathsfit{\\color[RGB]{204,187,68}D}} \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\n\n\n\n\n\n\nFurther texts\nIf you are looking for further texts to deepen your understanding of the probability calculus and decision theory, we recommend the following – but it’s a good idea to explore on your own! Try and skim through texts you find, you may stumble onto very interesting good ones.\n\nE. T. Jaynes (1994): Probability Theory: The Logic of Science\nD. J. C. MacKay (1995): Information Theory, Inference, and Learning Algorithms\nJ.-M. Bernardo, A. F. Smith (1994): Bayesian Theory\nH. Raiffa (1968): Decision Analysis: Introductory Lectures on Choices under Uncertainty\nS. J. Russell, P. Norvig (1995): Artificial Intelligence: A Modern Approach Parts I–IV (chapters 1–19)\nP. E. Rossi (2014) Bayesian Non- and Semi-parametric Methods and Applications",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[What next?]{.lightblue}"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Further reading",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\nFoundations of the probability calculus\nThe four fundamental rules of the Probability Calculus have been at least since Laplace in the 1700s, essentially in their present form. Laplace used them to infer properties of planets and their orbit (with results still valid today). Proof of their logical foundation and necessity started to appear in the 1940s, a formal milestone being the proof by R. T. Cox in 1946. They have been tightened and reformulated in different ways since. Here are some old and recent works on the foundations (as opposed to works that simply mention the rules and apply them). Cox’s and Jaynes’s are probably the first ones to be checked:\n\nJ. M. Keynes (1921): A Treatise on Probability.\nW. E. Johnson (1924): Logic. Part III: The Logical Foundations of Science.\nW. E. Johnson (1932): *Probability: The relations of proposal to supposal, Axioms, The deductive and inductive problems.\nH. Jeffreys (1939): Theory of Probability.\nR. T. Cox (1946): Probability, Frequency, and Reasonable Expectation.\nG. Pólya (1949): Preliminary remarks on a logic of plausible inference.\nG. Pólya (1954): Mathematics and Plausible Reasoning. Vol. II: Patterns of Plausible Inference.\nM. Tribus (1969): Rational Descriptions, Decisions and Designs.\nE. T. Jaynes (1994): Probability Theory: The Logic of Science.\nJ. B. Paris (1994): The Uncertain Reasoner’s Companion: A Mathematical Perspective.\nT. Hailperin (1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications.\nP. Snow (1998): On the correctness and reasonableness of Cox’s theorem for finite domains.\nP. Snow (2001): The reasonableness of possibility from the perspective of Cox.\nK. S. Van Horn (2003): Constructing a logic of plausible inference: a guide to Cox’s theorem.\nM. J. Dupré, F. J. Tipler (2009): New axioms for rigorous Bayesian probability.\n\n\n\n\n\nFoundations of Decision Theory\nDecision Theory is much younger than the Probability Calculus, and its foundations probably still needs to be tightened here and there. Here are old and recent works on its foundations:\n\nJ. von Neumann, O. Morgenstern (1953): Theory of Games and Economic Behavior.\nD. Luce, H. Raiffa (1957): Games and Decisions: introduction and critical survey.\nL. J. Savage (1954/1972): The Foundations of Statistics.\nE. Eells (1982/2016): Rational Decision and Causality.\nR. Pettigrew (2011/2019): Epistemic Utility Arguments for Probabilism.\nR. A. Briggs (2014/2019): Normative Theories of Rational Choice: Expected Utility.",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[Further reading]{.lightblue}"
    ]
  },
  {
    "objectID": "thanks.html",
    "href": "thanks.html",
    "title": "Thanks",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\nWe would like to thank, in unsystematic order:\n\nThe master students of this course, for their enthusiasm and interest in the course goal and material, and for their constructive feedback. You rock!\nThe members of the Artificial Intelligence Engineering group at the Western Norway University of Applied Sciences (HVL) for endorsement of the present course and encouragement during its construction.\nSoledad Gonzalo Cogno and Iván Davidovich and their course Concepts in Data Analysis, organized by the Gonzalo Cogno Group at the Kavli Institute for Systems Neuroscience and at the Norwegian University of Science and Technology (NTNU), where some of the present course material was initially designed and tested.\nThe members of the Medical AI group at the Mohn Medical Imaging and Visualization Centre, in particular Ingrid Rye, Alexandra Vik, Marek Kociński, Arvid Lundervold, Astri Lundervold, Alexander Lundervold, for the many group meetings, discussions, and projects where the core of the present course material was tested.\nThe Bulletin of the International Society for Bayesian Analysis and its editor Gregor Kastner for kindly allowing us to share the present course in the “Teaching Highlight” section.\nLars Michael Kristensen for his great work on the Master in Applied Computer Science at HVL, of which this course is part.\nThe developers and maintainers of Quarto for help in the customization of this course’s web pages.\nOur beloved, families, friends for their continuous encouragement and support.\nPGLPM thanks Saitama for being a constant source of awe and inspiration.",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[Thanks]{.lightblue}"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \n\n\nBelieve nothing, O monks, merely because you have been told it, or because it is traditional, or because you yourselves have imagined it. Do not believe what your teacher tells you merely out of respect for the teacher.     (Attributed to Gautama Buddha)\n\n\nBut in the natural sciences, whose conclusions are true and necessary and have nothing to do with human will, one must take care not to place oneself in the defense of error; for here a thousand Demostheneses and a thousand Aristotles would be left in the lurch by every mediocre wit who happened to hit upon the truth for himself.     (Galileo Galilei)\n\nThe notions, ideas, and rules that you have learned in this course have been presented in such a way as to appear plausible and intuitively understandable. In some places we gave sketch of proofs.\nBut that is not enough.\nA “data mechanic” (see preface) might be excused for using incorrect formulae, and might simply say “this is the procedure I was taught”. You instead, as a data engineer and data scientist, have the duty to check the validity of the theory and principles that you use in developing new algorithms, code, solutions.\nIn particular, you cannot accept theories or methods simply because\n\n\n  they are commonly used, or used by the majority of some community;\n  some “authority” or known scientist says they are correct.\n\n\nIn fact, Science began when the two criteria above were discarded as not valid. Galileo’s quote above says this very explicitly. Imagine if Einstein had said “all scientists see no problem with the notion of simultaneity, so it must be correct”, or “great scientists like Maxwell or Poincaré did not see any problem with the notion of simultaneity, so it must be correct”. There is no scientific progress with this kind of reasoning.\nInstead, the only two scientific criteria you have to decide on the validity of a method or theory are\n\n\n  logical proof,\n  experimental corroboration.\n\n\nwhich you must do as much as possible by yourself. The more verification you delegate to others, to majority or “authority”, the less you are doing science.\n\n\nFor this reason you have, at some point, go and check for yourself the validity of what you’ve learned in this course. You might in fact find out that something was not correct! Then you’ll correct it and make science advance. Throughout the course We have given references where many proofs can be found. Here are some final references containing the main proofs of what you have learned here; you should check and validate them at some point.\n\n\n\n\nR. A. Briggs (2014/2019): Normative Theories of Rational Choice: Expected Utility.\nT. M. Cover, J. A. Thomas (1991/2006): Elements of Information Theory.\nR. T. Cox (1946): Probability, Frequency and Reasonable Expectation.\nM. Cox, A. O’Hagan (2022): Meaningful expression of uncertainty in measurement.\nR. M. Dawes, T. L. Smith (1985): Attitude and opinion measurement, pp. 509–566 in G. Lindzey, E. Aronson: Handbook of Social Psychology. Vol. I: Theory and Method.\nP. Diaconis, S. Holmes, R. Montgomery (2007): Dynamical Bias in the Coin Toss.\nC. Drummond, R. C. Holte (2005): Severe Class Imbalance: Why Better Algorithms Aren’t the Answer.\nK. Dyrland, A. S. Lundervold, P.G.L. Porta Mana (2022): Does the evaluation stand up to evaluation?: A first-principle approach to the evaluation of classifiers.\nE. Eells (1982/2016): Rational Decision and Causality.\nN. Fenton, M. Neil (2019): Risk Assessment and Decision Analysis with Bayesian Networks.\nG. Galilei (1632/1967): Dialogue concerning the two chief world systems – Ptolemaic & Copernican\nS. J. Gould (1985/2013): The Median Isn’t the Message.\nP. C. Gregory (2005): Bayesian Logical Data Analysis for the Physical Sciences.\nT. Hailperin (1965): Best Possible Inequalities for the Probability of a Logical Function of Events.\nT. Hailperin (1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications.\nR. Hastie, R. M. Dawes (2001/2010): Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making.\nD. Heath, W. Sudderth (1976): De Finetti’s Theorem on Exchangeable Variables.\nM. Ingham (2012): No More Band-Aids: Integrating FM into the Onboard Execution Architecture.\nE. T. Jaynes (1994/2003): Probability Theory: The Logic of Science.\nR. L. Keeney, H. Raiffa (1976/1993): Decisions with Multiple Objectives: Preferences and Value Tradeoffs.\nW. Kruskal, F. Mosteller (1979): Representative Sampling, I: Non-scientific Literature.\nD. V. Lindley (1971/1988): Making Decisions.\nD. V. Lindley, M. R. Novick (1981): The role of exchangeability in inference.\nD. J. C. MacKay (1995/2005): Information Theory, Inference, and Learning Algorithms.\nG. Malinas, J. Bigelow (2004/2016): Simpson’s paradox.\nC. E. Metz (1978): Basic principles of ROC analysis.\nD. G. Morrison (1967): On the consistency of preferences in Allais’ paradox.\nA. O’Hagan (1988): Probability: Methods and measurement.\nM. Ono, A. Nicholas, F. Alibay, J. Parrish (2015): SMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission.\nH. Raiffa (1968/1970): Decision Analysis: Introductory Lectures on Choices under Uncertainty.\nS. J. Russell, P. Norvig (1995/2022): Artificial Intelligence: A Modern Approach.\nD. S. Sivia (1996/2006): Data Analysis: A Bayesian Tutorial.\nH. C. Sox, M. C. Higgins, D. K. Owens (1988/2013): Medical Decision Making.\nK. Steele, H. O. Stef{'a}nsson (2015/2020): Decision Theory.\nJ. A. Swets, W. P. Tanner, Jr., T. G. Birdsall (1961): Decision processes in perception.\nB. C. Williams, M. D. Ingham, S. H. Chung, P. H. Elliott (2003): Model-based programming of intelligent embedded systems and robotic space explorers.",
    "crumbs": [
      "[References]{.lightblue}"
    ]
  }
]