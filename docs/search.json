[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511 0.4",
    "section": "",
    "text": "Dear student and aspiring data- & AI-engineer\nIf you can’t join ’em,\nbeat ’em.\n(J. Schwinger)\nThe goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to choose a good prompt for a Large Language Model, or how to do cross-validation in the fastest way, or what is the latest improvement in random-forest algorithms.\nThe goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices of the future. And, as a side effect, you’ll also learn how to concretely improve present-day algorithms, and also how to determine if any of them has already reached its maximal theoretical performance.\nHow can such a goal be achieved?\nThere is a small set of rules and one method that are mathematically guaranteed to output the optimal solution of any inference, prediction, classification, and decision-making problem. You can think of this set as defining an “unbeatable, optimal universal machine”. Or, from an AI point of view, you can think of these rules and method as the “laws of robotics” that should govern any ideal AI designed to draw inferences, give answers, and make decisions.\nThese rules and method are quite easy to grasp and understand. You’ll learn them very quickly, and they’ll be the solid ground on which your data & AI engineering knowledge and skills are built.\nThese rules and method are computationally extremely expensive; the more so, the more data points and data dimensions we need to deal with. Current machine-learning algorithms, from deep networks to large language models, are approximations to this ideal universal method; each one uses a different kind of approximation. The upside of these approximations is that they allow for much faster computations; their downside is that they generally give sub-optimal or non-intelligent results.1\nBut approximations can be improved with new technologies. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a useless approximation 50 years ago, before the introduction of Graphical Processing Units.\nEvery new technological advance (think of possibly forthcoming quantum computers) opens up possibilities for new approximations that get us closer and closer to the ideal optimum. However, in order to see and realize these possibilities, or to judge whether they have already been realized, a data scientist needs at the very least:\nto know the foundation of the maximally optimal method\nto think outside the box\nWithout the first requirement, how do you know what is the target to approximate towards, and how far you are from it? You risk:\n making an approximation that leads to worse results than before;\n evaluating the approximation in the wrong way, so you don’t even realize it’s worse than before;\n trying to improve an approximation that has already attained the theoretical optimum. Think about an engine that has already the maximal efficiency dictated by thermodynamics; and an engineer, ignorant of thermodynamics, who wastes effort in trying to improve it further.\nWithout the second requirement, you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse-carriage wooden wheels, you’ll never conceive a combustion engine. If you keep thinking in terms of how to improve combustion fuel, you’ll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum – so we’re back to the first requirement.\nIf you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of “training sets”, “cross-validation”, “supervised learning”, “overfitting”, “models”, and similar notions. In this course you’ll see for yourself that such notions are anchored to the box of present-day approximations.\nAnd we want to think outside that box.\nThis course will not only prepare you for the future. With the knowledge and insights acquired, you will be able to devise and implement concrete improvements to present-day methods as well, or calculate whether they can’t be improved further.",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "index.html#your-role-in-the-course-bugs-features",
    "href": "index.html#your-role-in-the-course-bugs-features",
    "title": "ADA511 0.4",
    "section": "Your role in the course Bugs & features",
    "text": "Your role in the course Bugs & features\nThis course is still in an experimental, “alpha” version. So you will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students. Thank you for this in advance!\nFor this reason it’s good to clarify some goals and guidelines of this course: \n\n  Undergraduate maths requirements\n\nWe believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations \\(+ - \\times /\\). So this course only requires maths at a beginning first-year undergraduate level.\n\n\n\n  Informal style\n\nThe course notes are written in an informal style; for example they are not developed along “definitions”, “lemmata”, “theorems”. This does not mean that they are inexact. We will warn you about parts that are oversimplified or that only cover special contexts.\n\n\n\n  Names don’t constitute knowledge\n\n\nIn these course notes you’ll often stumble upon terms in blue bold and definitions in blue Italics. This typographic emphasis does not mean that those terms and definitions should be memorized: rather, it means that there are important ideas around there which you must try to understand and use. In fact we don’t care which terminology you adopt. Instead of the term statistical population, feel free to use the term pink apple if you like, as long you explain the terms you use by means of a discussion and examples.2 What’s important is that you know, can recognize, and can correctly use the ideas behind technical terms.\n2 Some standard technical terms are no better. The common term random variable, for instance, often denotes something that is actually not “random” and not variable. Go figure. Using the term green banana would be less misleading!Memorizing terms, definitions, and where to use them, is how large language models (like chatGPT) operate. If your study is just memorization of terms, you’ll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.\n\n\n\n\n  Diverse textbooks\n\nThis course does not have only one textbook: it refers to and merges together parts from several books and articles. As you read these works, you will notice that they adopt quite different terminologies, employ different symbolic notations, give different definitions for similar ideas, and sometimes even contradict each other.\nThese differences and contradictions are a feature, not a bug!\nYou might think that this makes studying more difficult; but it actually helps you to really understand an idea and acquire real knowledge, because it forces you to go beyond words, symbols, and specific points of view and examples. This point connects with the previous point, “names don’t constitute knowledge”. The present course notes will help you build comprehension bridges across those books.\n\n\nThe textbook material is presented in Study reading boxes. There are two kinds:\n\n“Read”: you don’t need to study this as for an exam, but you do have to read it, as if it were an interesting piece of news or your favourite blog.\n“Skim through”: you don’t need to read every word, you can just skim through the text; but you must get an idea of what it’s speaking about and what its main points are.\n\nYou’ll find further details of each item in the final Bibliography.\n\n\n\n  Artificial intelligence\n\nIn order to grasp and use the fundamental laws of inference and decision-making, we shall use notions that are also at the foundations of Artificial Intelligence (and less common in present-day machine learning). So you’ll also get a light introduction to AI for free. Indeed, a textbook that we’ll draw frequently from is Russell & Norvig’s Artificial Intelligence: A Modern Approach (we’ll avoid its part V on machine learning, however, because it’s poorly explained and written).\n\n\n\n\n  Concrete examples\n\nSome students find it easier to grasp an idea by starting from an abstract description and then examining concrete examples; some find it easier the other way around. We try to make both happy by alternating between the two approaches. Ideas and notions are always accompanied by examples that we try to keep simple yet realistic, drawing from scenarios ranging from glass forensics to hotel booking.\n\n\n\n  Code\n\nWe shall perform inferences on concrete datasets, also comparing different methodologies. Most of these can be performed with any specific programming language, so you can use your favourite one – remember that we want to try to think outside the box of present-day technologies, and that includes present-day programming languages. Most examples in class and in exercises will be given in R and sometimes in Python, but are easily translated into other languages.\n\n\n\n  Extra material\n\nThe course has strong connections with many other disciplines, such as formal logic, proof theory, psychology, philosophy, physics. We have tried to provide a lot of extra reading material in “For the extra curious” side boxes, for those who want to deepen their understanding of topics covered or just connected to the present course. Maybe you’ll stumble into a new passion or even into your life call?\n\n\n\n\n\n\n\n\n\n For the extra curious",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "ADA511 0.4",
    "section": "Course structure",
    "text": "Course structure\nThe course structure reflects the way in which the ideal universal decision-making machine works. It can be roughly divided into three or four parts, illustrated as follows (this is just a caricature, don’t take this diagram too literally):\n \n\nData parts (top-left, yellow box) develop the language in which a problem can be fed into the decision-making machine. Here you will also learn about important pitfalls in handling data.\nInference parts (left-centre, green box) develop the “inference engine” of the machine. Here you will learn ideas at the foundation of AI; and you will also meet probability, but from a point of view that may be quite novel to you – and much more fun.\n\nThese two parts will alternate so that their development proceeds almost in parallel.\n\nThe utility part (top-right, light-blue box) develops the “decision engine” of the machine. Here you will meet several ideas that will probably be quite new to you – but also very simple and intuitive.\nThe solution part (bottom, dark-blue box) simply shows how the inference and utility engines combine together to yield the optimal solution to the problem. This part is simple, short, intuitive; it will be a breeze.\n\n\n\nWe shall start with a quick preview of the solution part in chapters 1  Accept or discard?–3  Basic decision problems, because it is very simple to understand, and it shows why the inference and the utility parts are necessary.\nThen we shall continue with the inference parts in chapters 5–10, 14–18, 24–29, alternating them with the data parts in chapters 12–13, 20–23, and with interludes about present-day machine-learning in chapters 4, 11, 19. Quick introductions to the R programming language are also given at appropriate places.\nAs soon as the inference and data parts are complete, you will be able to apply the machine to real, albeit not too complex, inference problems. This application will be made in chapters 30–34.\nWe finally round up with the utility part in chapters 35–36, extending our concrete application to it in chapters 37–38. Final connections with present-day machine learning and its limitations are made in chapters 39–43.\nYou should be able to see this timeline in the index tab on the side.",
    "crumbs": [
      "Dear student<br> and aspiring data- & AI-engineer"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Mechanics and engineers\nScience is built up with facts, as a house is with stones. But a collection of facts is no more a science than a heap of stones is a house.     (H. Poincaré)\nWhat is the difference between a car mechanic and an automotive engineer?\nBoth have knowledge about cars, but their knowledge domains are different and focus on different goals.\nA car mechanic can keep your car in top-notch condition; can do different kinds of easy and difficult repairs if problems arise with it; knows whether a particular brand of valve can be used as a replacement for another brand; can recommend the optimal kind of tyres to use in a given season for different brands of cars. But a car mechanic would face difficulties in calculating the theoretical maximal efficiency of an engine; or predicting the temperature increase caused by a new kind of fuel; or exploiting the phase transition of a new kind of foam to design a safer airbag system; or calculating the optimal surface curvature for a spoiler. A car mechanic typically possesses a large amount of case-specific knowledge, and doesn’t need to know in depth the principles of electromechanics and thermochemistry, or the laws of balance of momentum, energy, entropy.\nVice versa, an automotive engineer can assess how to use the electromechanical properties of a new material in order to design a more efficient and environmentally friendly engine; can calculate how a new material-surface handling would affect air drag and speed; and ultimately can research how to exploit new physical phenomena to build completely new means of transportation. Yet, an automotive engineer could be completely incapable of changing a pipe in your car, or tell you whether it can use a particular brand of lubricant oil. An automotive engineer typically possesses knowledge about the principles of electromagnetism, mechanics, or thermochemistry; is acquainted with relevant physical laws; and doesn’t need to have in-depth case-specific kinds of knowledge.\nNote that the differences just sketched do not imply a judgement of value. Both professions, kinds of knowledge, and goals are necessary, interesting, and couldn’t exist without each other. Choice between them is a subjective matter of personal passions and aspirations.\nIn fact there isn’t a clear divide between these two kinds of knowledge, but rather a continuum between two vague extremities. A car mechanic can have knowledge and insight about new technologies, and an automotive engineer can know how to fix a carburettor. The two sketches above are meant to expose and emphasize the existence of such a continuum of knowledge and of goals.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-mechanics-and-data-engineers",
    "href": "preface.html#data-mechanics-and-data-engineers",
    "title": "Preface",
    "section": "Data mechanics and data engineers",
    "text": "Data mechanics and data engineers\nA continuum with two similar extremities can also be drawn in data science.\nSome data scientists have in-depth knowledge on, for instance, how to optimally store and read large amounts data; what kind of machine-learning algorithm to use in a given task; how to fine-tune an algorithm’s parameters, and the currently best software for this purpose. Their particular knowledge is fundamental for the working of today’s technological infrastructure.\nAt the same time, these data scientists typically face difficulties, for instance, in:\n\ncalculating the theoretical maximal accuracy or performance achievable – by any possible algorithm – in a given inference problem\nexplaining how the fundamental rules of inference and decision-making are implemented in a particular machine-learning algorithm\nidentifying which sub-optimal approximations to the fundamental rules are made by popular machine-learning algorithms\nexploiting new technologies to build new algorithms that do calculations closer to the exact theoretical ones, thereby achieving a performance closer to the theoretical optimum\n\nAnd it is also possible that they are not aware of, and maybe would be surprised by, some basic facts of data science. For instance:\n\nthere is an optimal, universal inference & decision algorithm, of which all machine-learning algorithms (from support vector machines and deep networks to random forests and large language models), are an approximation\nthere are only five or six fundamental laws upon which any inference, prediction, classification, regression, decision task is (or ought to be) based upon\nsplittings of data into “training set”, “validation set”, and similar sets, are not part of the exact application of the laws of inference and decision-making; such splittings arise as coarse approximations of the exact method.\ncross-validation and related techniques are not part of the exact method either; they also arise as approximations\noverfitting, underfitting and related notions are not problems that appear in the exact method (which takes care of them automatically); they also arise from approximations\nit is possible to calculate, within probable bounds, the maximal accuracy (or other performance metric) achievable by any classification or regression algorithm for a given application\nsome evaluation metrics, such as precision or the area under the curve of the receiver operating characteristic (AUC), have intrinsic flaws and may attribute higher values to worse-performing algorithms\n\n…because this is a kind of general and principled knowledge that these data scientists don’t need in their jobs. Their knowledge is more case-specific.\nDrawing a parallel with the car example, a data scientist with this kind of case-specific knowledge is like a “data mechanic”.\nA “data engineer”, on the other hand, is the kind of data scientist who has no difficulties with the knowledge and skills implicit in the bullet points above; but at the same time might not know what software to use for tuning parameters of a particular class of deep networks, or the best format to store particular kinds of data.\nJust like in the case of the automotive industry, the difference just sketched does not imply any judgement of value. Both kinds of knowledge and goals are important and can’t exist without each other.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#goals-of-this-course",
    "href": "preface.html#goals-of-this-course",
    "title": "Preface",
    "section": "Goals of this course",
    "text": "Goals of this course\nThere is a plethora of academic courses, in all kinds of format, that target knowledge and goals for the “data mechanic”. Those courses are usually inadequate to cover the knowledge and goals for the “data engineer”. Some courses, misleadingly, even present approximations and recipes that are only valid for particular situations as if they were universal rules or methods instead.\nCourses that target the “data engineer” seem to be more rare. One possible reason is that this kind of knowledge is actually hidden in courses on probability, statistics, and risk analysis, presented with a language which makes only opaque and confusing connections with fields in data science and their goals; or, worse, with a language which emphasizes connections that are actually superficial and misleading.\nWe believe that it is important to teach and keep alive the less “mechanic” and more “engineer” side of data science:\n\nContinuous advances in computational technology – think of quantum computers – will offer completely novel and superior ways to approximate the exact method of inference and decision. Only the data scientist who knows the exact method and theory, and understands how present-day algorithms approximate it, will be able to exploit new technologies.\nEven without looking at the future, several present-day machine-learning algorithms could already be greatly optimized by any data engineer who is acquainted with the basic principles underlying data science.\nThe foundations of data science are the bridge to the sibling discipline of Artificial Intelligence.\n\nThe present course aspires to give an introduction to the “data engineer” side, rather than “data mechanic” one, of data science, but using a point of view more familiar to data scientists than to, say, statisticians.\nMore details about its aims, structure, and features are already given in the Dear student introduction.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\nLet’s start with a question that could arise in an engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line there is an automated inspection device that works as follows on each new component:\n\nThe inspection device first performs some tests on the component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\n\nConsider also the following context. When a new electronic component is sold, the manufacturer has a net gain of 1$. That’s the net gain if the component works for at least a year. But if the component instead fails within a year of use, the manufacturer incurs a net loss of 11$ (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has 0$ net gain.\nNow we have a new electronic component, just come out of the assembly line. The tests of the automated inspection device indicate that there is a 10% probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept the new component? or discard it?\n\nTry to give and motivate an answer:\n\n\n\n\n\n\nExercise 1.1:  Very first exercise!\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nIt doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all. The purpose here is for you to do some introspection about your own reasoning.\nThen examine and discuss the following points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[Accept or discard?]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html",
    "href": "framework.html",
    "title": "2  Framework",
    "section": "",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\nFirst, what happens if we accept the component?\nWe must try to make sense of the 10% probability that the component fails within a year. For the moment let’s use an imagination trick: imagine that the present situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year. Later on we’ll approach this in a more rigorous way, where the idea of “imaginary repetitions” is not needed.\nIn each of the 10 imaginary repetitions where the component fails early, the manufacturer loses \\(11\\$\\). That’s a total loss of \\(10 \\cdot {\\color[RGB]{238,102,119}11\\$} = {\\color[RGB]{238,102,119}110\\$}\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(1\\$\\). That’s a total gain of \\(90 \\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{34,136,51}90\\$}\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n10\\cdot ({\\color[RGB]{238,102,119}-11\\$}) + 90\\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{238,102,119}-20\\$}\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\)! That’s an average of \\(0.2\\$\\) lost per repetition.\nNow let’s examine the second choice: what happens if we discard the component instead?\nIn this case it’s clear that the manufacturer doesn’t gain or lose anything. That is, the “gain” is \\(0\\$\\) (this is for sure, so we don’t need to imagine any “repetitions”).\nThe conclusion is this: If in a situation like the present one we accept the component, then we’ll lose \\(0.2\\$\\) on average. Whereas if we discard it, then we’ll lose \\(0\\$\\) on average.\nObviously the best, or “least worst”, decision to make is to discard the component.\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "",
    "text": "We’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.1\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and different values of the cost of failure. Is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the probability of failure for which there is no loss or gain, on average, if we accept the component (so it doesn’t matter whether we discard or accept). You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.\nConsider the special case with failure probability 0% and failure cost 10$. That probability means that no new component will ever fail. It’s clear what’s the optimal decision in this limit case, without any calculations or imaginary repetitions. Yet, confirm mathematically that we arrive at this obvious conclusions if we perform a mathematical analysis like before.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nFirst, the AI performs some clinical tests on the patient. The tests give an uncertain forecast on whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then their life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then their life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a 10% probability that they have the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\n\nIs it enough if we simply believe that the component is less likely to fail than not? In other words, is it enough if the probability of failure is less than 50% without knowing its precise value?\nObviously not. We found that if the failure probability is 10% then it’s best to discard. But we also found that if it’s 5% then it’s best to accept. In either case the probability of failure was less than 50%, but the decision was different.\nOn top of that, we also found that the probability value determines the average amount of loss when the non-optimal decision is made. Therefore:\n Knowledge of precise probabilities is absolutely necessary for making the best decision.\n\n\nIs it enough if we simply know that failure leads to a loss, and non-failure leads to a gain, without knowing the precise amounts of loss and gain?\nObviously not. In the exercise we found that if the cost of failure is 11$, then it’s best to discard. But we also found that if it’s 5$, then it’s best to accept (given the same probability of failure). And we also found that it’s best to accept when the cost of failure is 11$ but the gain from non-failure is 2$. Therefore:\n Knowledge of the precise gains and losses is absolutely necessary for making the best decision.\n\n\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We examined a clinical problem that’s exactly analogous: there’s uncertainty and probability, there are gains and losses (of lifetime rather than money), and the best decision depends on both probabilities and costs.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-our-focus",
    "href": "framework.html#sec-our-focus",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, data science, AI",
    "text": "2.2 Our focus: decision-making, inference, data science, AI\nEvery data-driven engineering problem and AI task is unique, with unique goals, difficulties, questions, issues. But there are some general aspects that are common to all such problems.\nIn the scenarios that we explored above, we found an extremely important problem-pattern:\n\n There is a decision or choice to make, or act to be executed (and “not deciding” is not an option, or it’s just another kind of choice).\n Making a particular decision will lead to some consequences. Some consequences are desirable, others are undesirable.\n The decision is difficult to make, because its consequences are not known with certainty, even considering the information and data available in the problem: we may lack information and data about past or present details, about future events and responses, and so on.\n\nThis is what we call a problem of decision-making under uncertainty or under risk1; or simply a “decision problem” for short.\n1 We avoid the word “risk” because it has several different technical meanings in the literature, some even incompatible with one another.This problem-pattern appears literally everywhere. Think about all different situations in which you had to make a decision today. Do they show this pattern?\nBut our exploration of different scenarios also suggests something important: this problem-pattern seems to have a systematic method of solution!\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some general notions that allow us to frame and analyse this kind of problems. And we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-meaning-decision",
    "href": "framework.html#sec-meaning-decision",
    "title": "2  Framework",
    "section": "2.3 The broad meaning of decision; universality of decision-making in Data Science and AI",
    "text": "2.3 The broad meaning of decision; universality of decision-making in Data Science and AI\nBut what do decision-making under uncertainty and Decision Theory have to do with Data Science and Artificial Intelligence?\nIn everyday language the word “decision” and the words “action” or “activity” are used in somewhat different ways. If a person has the options to stay at home or to go out, and we see that they stay at home, we can say that a decision about staying at home has been made. If we see a person running on a field, or talking to a friend, we can say that the action of running or of speaking is taking place. Typically the word “decision” is reserved for situations of a more static character and involving a deliberate choice.\nBut also activities like running or speaking involve a continuous decision process, in a more general sense of the word “decision”. At every instant, the runner has the option of moving a leg forward with a particular speed or with another speed, or the option of stopping the movement. At every instant, the speaker can utter one word or another, with one intonation or another, or can suddenly stay silent. The same goes for robots, with their ranges of movement and of other actions, as they interact with their environment.\nIn fact instead of decision we could use the term course of action, which is also commonly used in decision theory, or simply action.\nIt is this more general sense of “decision” and “decision-making” that we study in this course. In this more general sense, decisions permeate every second of any human being and of any agent. Decision-making of all kinds occurs continuously and determines the behaviour of any agent equipped with Artificial Intelligence.\nThus, by studying decisions and decision-making, we’re studying how AI agents should behave.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\nDecision Theory and Data Science are profoundly, tightly connected on many other different planes:\n\nData science is based on the laws of Decision Theory. These laws are similar to what the laws of physics are to a rocket engineer. Failure to account for these fundamental laws leads to sub-optimal solutions – or to disasters.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that a machine-learning classifier is actually choosing among possible output classes.\nWe saw that probability values are essential to a decision problem. How do we find them? Obviously data play an important part in their calculation. In our introductory example, the failure probability must have come from observations or experiments on previous similar electronic components.\nWe saw that the values of gains and losses are essential. Data play an important part in their calculation as well.\n\nThese connections will constitute the major parts and motivations of the present course.\n\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. Aspects such as these can barely be planned or decided. Their drive and direction, however, rest on a strive for improvement and optimization. But the fundamental laws of Decision Theory tell us what’s optimal and what’s not, so they play some part in these creative aspects as well.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-optimality",
    "href": "framework.html#sec-optimality",
    "title": "2  Framework",
    "section": "2.4 Our goal: optimality, not “success”",
    "text": "2.4 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence. If such a method existed, the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. Then what should be the goal of such a method?\nImagine two persons, Henry and Tina, who must choose between a “heads-bet” or a “tails-bet” before a coin is tossed. The bets are these:\n\n“heads-bet”: If the coin lands heads, the person wins a small amount of money. But if it lands tails, they lose a large amount of money.\n“tails-bet”: If the coin lands tails, the person wins a small amount of money. If it lands heads, they lose the same small amount of money.\n\nHere’s a graphical representation of the situation:\n \n\n\n\n\n\n\nExercise 2.2\n\n\n\nWhich bet would you choose? why?\n\n\nNow this happens: Henry chooses the heads-bet. Tina chooses the tails-bet. The coin comes down heads. So Henry wins the small amount of money, while Tina loses the same small amount.\nWhat would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than he could win. Tina’s decision was unlucky, and yet rational: she wasn’t risking to lose more than she could win. Said otherwise, the heads-bet had higher risk of loss than the tails-bet, and not even an higher chance of gain. We expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThe method we’re looking for is therefore one that, in the hypothetical situation above, would lead to the same decision as Tina’s, even if Tina’s decision was unlucky. That’s the decision that we call rational or optimal in such an uncertain situation.\n\n\n\n\n\n\n\n\n\n\nIf you’re thinking “wouldn’t it be best to have a method that works under uncertainty but that leads to Henry’s decision, every time that decision is lucky?” – then let’s repeat: such a method cannot logically exist. If we know which decision is “lucky”, then it means that we have no uncertainty. If we are uncertain, then it means that we don’t know which decision is “lucky”, and so it’s impossible to choose it for sure.\n\n\n\n\nOur discussion and the distinction between “successful” and “optimal” decisions also show that we cannot evaluate the efficacy of a method for decisions under uncertainty, by checking whether or how often that method leads to the desired, “successful” consequence. This point is also easily illustrated with a variation on Henry and Tina’s example:\nSuppose the general context and the bets are exactly the same. But now imagine Henry and Tina to be the names of two automated decision methods, say two machine-learning algorithms. Also, let’s say that you first toss the coin in secret and see its outcome, then you offer the possible bets to Henry and Tina, who are completely ignorant about the outcome (note that no cheating is involved).\nYou toss the coin and see that it lands heads. Then the choice of bets is offered to Henry and Tina. Henry chooses the heads-bet and Tina the tails-bet.\nNow consider this: you know the “truth”, you know what the successful decision would be: heads-bet. It turns out that the Henry algorithm made the choice corresponding to the truth. The Tina algorithm didn’t. Would you then evaluate the Henry algorithm to be better than the Tina algorithm?\nFor exactly the same reasons already discussed, the Tina algorithm is the better one; it made the optimal decision. Yet it didn’t choose the “truth”. You realize that comparing algorithms is not as simple as checking which one yields the truth.\n\nWe have then arrived at two conclusions:\n\n “Success” or “correspondence to truth” is generally not a good criterion to judge a decision under uncertainty or to evaluate an algorithm that makes such decisions. Moreover, in real applications the truth is not known – that’s the whole problem! – so how can we use a criterion based on truth?\n\n\n Even if there is no method to determine which decision is successful, there is nevertheless a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem.\n\nWe had a glimpse of this method in our introductory scenarios with electronic components and their variations.\nLet us emphasize, however, that we are not giving up on “success”; nor are we trading “success” for “optimality”. We’ll find out that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. Keep this point firmly in mind:\n\n\n\n\n\n\n\n \n\n\n\nAiming to find the solution that is successful can make us fail to find the solution that is optimal, when the successful one cannot be determined.\nAiming to find the solution that is optimal makes us automatically find the solution that is successful, when this can be determined.\n\n\n\nWe shall later witness this fact with our own eyes. We will also take it up in the discussion (chapter  43) of some misleading techniques to evaluate machine-learning algorithms.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "framework.html#sec-decision-theory",
    "href": "framework.html#sec-decision-theory",
    "title": "2  Framework",
    "section": "2.5 Decision Theory",
    "text": "2.5 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n\n It tells us what’s optimal and, when possible, what’s successful.\n It takes into consideration decisions, consequences, costs and gains.\n It is able to deal with uncertainties.\n\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science? Here are two:\n\nIf we find an optimal decision in regards to some problem, it may still happen that this decision leads to new, subsequent decision problems. For example, in the assembly-line scenario the decision discard could be carried out by burning, recycling, and so on. And each of these actions could have uncertain results and costs or gains. We thus face a decision after a decision, or a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated AI-based device capable of making an optimal decision, at least in specific kinds of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system; or a software company designing an image classifier.\n\nWell, Decision Theory turns out to meet these two demands too, thanks to the following features:\n\n\n It allows for recursive, sequential, and modular application.\n It can be used not only for human decision-makers, but also for AI or automated devices.\n\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s. It appeared in its present form around 1920–1960, from the contributions of Wald, von Neumann and Morgenstern, Savage, and many others. What’s remarkable about it is that it is not only a framework: it is the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, an “alternative” framework might use different terminology and apparently different mathematical operations, but it would boil down to the same notions and mathematical operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) your framework would lead to some irrational or illogical consequences; or (b) your framework would lead to results identical to Decision Theory. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n\n It is normative.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid! We want a theory, a norm, that human decision-makers should aspire to. That’s what Decision Theory is.\n\n\n\n\n\n\n Study reading\n\n\n\nWho says that Decision Theory should be normative? – this is a respectable scientific question. If you found yourself wondering and doubting about this, then congratulations: that’s how a scientist should think!\nLater on we’ll examine material and arguments about this point. As a start in your investigations, skim through:\n\nCh. 15, especially §15.1 and § “Bibliographical and Historical Notes” of Artificial Intelligence\nBriggs 2014/2019: Normative Theories of Rational Choice: Expected Utility\nSteele & al. 2015/2020: Decision Theory\nChapters 1–2 of Raiffa: Decision Analysis",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>[Framework]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html",
    "href": "basic_decisions.html",
    "title": "3  Basic decision problems",
    "section": "",
    "text": "3.1 Graphical representation and elements\nDecision Theory analyses any decision-making problem in terms of nested or sequential basic or minimal decision problems. The assembly-line scenario of the introduction 1 is an example.\nA basic decision problem can be represented by a diagram like this:\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an inference node,1 usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular gain or loss, depending on the decision. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of the following elements:\nThe relation between the elements above can be depicted as follows – but note that this is just an intuitive illustration:\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#sec-basic-decision-tree",
    "href": "basic_decisions.html#sec-basic-decision-tree",
    "title": "3  Basic decision problems",
    "section": "",
    "text": "1 also called chance node or uncertainty node\n\n\n\n\n\n\n \n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\n\n\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities, gains, and losses of the outcomes, together with other available data and information. Different agents typically have different background information.\n\n\n\nAgent means “conductor”, “mover”, and similar (from Latin ago = to move or drive and similar meanings).\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n Decisions, also called courses of action, available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n\n\n\nMany other terms instead of outcome are used in the literature, for instance state or event.\n\n Probabilities for each of the outcomes and for each decision. Their values typically depend also on the background information and the additional data.\n Utilities: the gains or losses associated with each possible outcome and each decision. We shall mainly use the term utility, instead of “gain”, “loss”, and similar, for several reasons:\n\nGain and losses may involve not money, but time, or energy, or health, or emotional value, or other kinds of commodities and things that are important to us; or even a combination of them. The term “utility” is useful as a neutral term that doesn’t mean “money”, but depends on the context.\nWe can just use one term instead of two: for example, when the utility is positive it’s a “gain”; when it’s negative it’s a “loss”.\n\nThe particular numerical values of the utilities are always context-dependent: they may depend on the background information, the decisions, the outcomes, and the additional data.\n\n\n\n\n\n\n\n\n\n Don’t over-interpret the decision diagram\n\n\n\n\nThe diagram above doesn’t have any temporal meaning, that is, it doesn’t mean that the decisions happen before the outcomes, or vice versa.\nIn some situations the outcome can be realized after the decision is made; for instance, someone bets on heads or tails, and then a coin is tossed.\nIn other situations, the outcome can be realized before the decision is made; for instance, sometimes a coin is tossed and covered, then one is asked to bet on what the outcome was. Another example is some research decision made by a archaeologist, the unknown being some detail about a dinosaur from millions of years ago.\nIn yet other situations the outcome may have a complex nature, and it may be realized partly before the decision is made, and partly after; for instance, someone can bet on the outcome of two coin tosses; one coin is tossed before the decision is made, and the other after.\nThe diagram above is not something that an agent must use in making decisions. It is not part of the theory. It’s just a very convenient way to visualize and operate with the mathematics underlying the theory.\nIt not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred. In such situations we can still use diagrams like the one above, because the help us doing the calculation, although the actual outcome is already known.\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§1.1.4 in Artificial Intelligence\n\nSkim through:\n\nCh. 15 of Artificial Intelligence. No need to read thoroughly: just quickly glimpse whether there are ideas and notions that look familiar (a little like when you’re in a large crowd and look quickly around to see if there are any familiar faces)\n\n\n\n\n\n\n\n\n\nExercise 3.1\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the decision diagram for the assembly-line decision problem.\n\n\n\n\n\n\n\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#sec-decision-matrices",
    "href": "basic_decisions.html#sec-decision-matrices",
    "title": "3  Basic decision problems",
    "section": "3.2 Setting up a basic decision problem",
    "text": "3.2 Setting up a basic decision problem\nA basic decision problem can be set up along the following steps (which we illustrate afterwards with a couple of examples):\n\n\n\n\n\n\nSetup of a basic decision problem\n\n\n\n\nList all available decisions\nFor each decision, list its possible outcomes\nPool together all outcomes of all decisions, counting the common ones only once\nPrepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\nIn the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\n\n\n\nExample: the assembly-line problem\nLet’s apply the steps above in the assembly-line example of ch.  1:\n1. List all available decisions\nEasy: they are “accept the electronic component” and “discard it”.\n\n\n2. For each decision, list its possible outcomes\nIn general you will notice that some outcomes may be common to all decisions, while other outcomes can happen for some decisions only, or even for just one decision.\nIn the present example, the accept decision has two possible outcomes: “the component works with no faults for at least a year” and “the component fails within a year”.\nThe discard cannot have those outcomes, because the component is discarded. It has indeed only one outcome: “component discarded”.\n\n\n3. Pool together all outcomes of all decisions, counting the common ones only once\nIn total we have three pooled outcomes:\n\nno faults (from the accept decision)\nfails (from the accept decision)\ndiscarded (from the discard decision)\n\n\n\n4. Prepare two tables: in each, display the decisions as rows, and the pooled outcomes as columns (or you can do the opposite: decisions as columns and outcomes as rows)\nIn the present example each table looks like this:\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\n\n\n\n\ndiscard\n\n\n\n\n\n\n\n\n5. In one table, report the probabilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\%\\) probability\n\nProbability table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(90\\%\\)\n\\(10\\%\\)\n\\(0\\%\\)\n\n\ndiscard\n\\(0\\%\\)\n\\(0\\%\\)\n\\(100\\%\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\%\\) probability (in grey). This is just a way of saying “this outcome can’t happen, if this decision is made”.\n\n\n6. In the other table, report the utilities for all decision-outcome pairs. If an outcome is not available for that decision, give it a \\(0\\) utility\n\nUtility table\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\n\n\n\n\naccept\n\\(+1\\$\\)\n\\(-11\\$\\)\n\\(0\\$\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\n\n\nNote how the outcomes that do not exist for a particular decision have been given a \\(0\\$\\) utility (in grey). We shall see later that it actually doesn’t matter which utilities we give to these impossible outcomes.\n\n\n\n\n\n\n\n\nExercise 3.2\n\n\n\nApply the steps above to the following basic decision problems (you only need to set them up with their probability & utility tables, but feel free to solve them as well, if you like):\n\nThe “heads-bet” vs “tails-bet” example of § 2.4. Assume that the “small amount” of money is \\(10\\$\\), the “large amount” is \\(1000\\$\\), and the two outcomes’ probabilities are \\(50\\%\\) each.\nPeter must reach a particular destination, and is undecided between three alternatives: go by car, or ride a bus, or go on foot.\nIf he goes by car, he could arrive without problems, with a probability of \\(80\\%\\) and a utility of \\(10\\), or he could get stuck in a traffic jam and arrive late, with a probability of \\(20\\%\\) and a utility of \\(-10\\).\nIf he rides a bus, he could arrive without problems, with a probability of \\(95\\%\\) and a utility of \\(15\\), or arrive in time but travelling in a fully-packed bus, with a probability of \\(5\\%\\) and a utility of \\(-10\\).\nIf he goes on foot, he could arrive without problems, with a probability of \\(20\\%\\) and a utility of \\(20\\), or he could get soaked from rain, with a probability of \\(80\\%\\) and a utility of \\(-5\\).",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#sec-make-decision",
    "href": "basic_decisions.html#sec-make-decision",
    "title": "3  Basic decision problems",
    "section": "3.3 How to make a basic decision?",
    "text": "3.3 How to make a basic decision?\nUp to now we have seen what are the elements of a basic decision problem, and how to arrange them in a diagram and with tables. But how do we determine what’s the optimal decision?\nDecision Theory says that the optimal decision is determined by the “principle of maximal expected utility”.\nWe shall study this principle more in detail toward the end of the course, although you already know its basic idea, because you intuitively used this very principle in solving all decision problems we met so far, starting from the assembly-line one.\nHowever, let’s quickly describe already now the basic procedure for this principle:\n\n\n\n\n\n\nPrinciple of maximal expected utility\n\n\n\n\nFor each decision, multiply the probability and the utility of each of its outcomes, and then sum up these products. This way you obtain the expected utility of the decision.\nChoose the decision that has the largest expected utility; if several decisions are maximal, choose any of them unsystematically.\n\n\n\nThis procedure can also be described in terms of the probability and utility tables introduced in the previous section:\n\nMultiply element-by-element the probability table and the utility table, obtaining a new table with the same number of rows and columns\nSum up the elements of each row of the new table (this sum is the expected utility); remember that every row corresponds to a decision\nChoose the decision corresponding to the largest of the sums above; if there are several maximal ones, choose among them unsystematically\n\n\nExample: the assembly-line problem\nMultiplying the Probability table and the Utility table above, element-by-element, we obtain the following table, where we also indicate the sum of each row:\n\n\nProbability × Utility table\n\n\n\n\n\n\n\n\n\n\nno faults for a year\nfails within a year\ndiscarded\nsum\n\n\n\n\naccept\n\\(+0.9\\$\\)\n\\(-1.1\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{-0.2\\$}\\)\n\n\ndiscard\n\\(0\\$\\)\n\\(0\\$\\)\n\\(0\\$\\)\n\\(\\boldsymbol{0\\$}\\)\n\n\n\n\nand, as we already knew, discarding the electronic component is the decision with the maximal expected utility.\n\n\n\n\n\n\nExercise 3.3\n\n\n\nFeel free to sketch some code (in your preferred programming language) that chooses the optimal decision according to the principle above. The code should take two inputs: the table or matrix of probabilities, and the table or matrix of utilities; and should give one output: the row-number of the optimal decision.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "basic_decisions.html#plan-for-the-next-chapters",
    "href": "basic_decisions.html#plan-for-the-next-chapters",
    "title": "3  Basic decision problems",
    "section": "3.4 Plan for the next chapters",
    "text": "3.4 Plan for the next chapters\nThe expected-utility maximization above is intuitive and simple, and is the last stage in a basic decision problem.\nBut there are two stages which occur before, and which are the most difficult:\n\n Inference\n\nis the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\n\n\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n\n Utility assesment\n\nis the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n\n\n\n\nWe shall now explore each of these two stages. We take up inference first because it is the most demanding and probably the one that can be optimized the most by new technologies.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>[Basic decision problems]{.lightblue}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html",
    "href": "connection-1-ML.html",
    "title": "4  Connection with machine learning and AI",
    "section": "",
    "text": "4.1 Inferences with machine-learning algorithms\nSome works in machine learning focus on “guessing the correct answer”, and this focus is reflected in the way their machine-learning algorithms – especially classifiers – are trained and used.\nIn § 2.4 we emphasized that “guessing successfully” can be a misleading goal, however, because it can lead us away from guessing optimally. We shall now see two simple but concrete examples of this.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html#inferences-with-machine-learning-algorithms",
    "href": "connection-1-ML.html#inferences-with-machine-learning-algorithms",
    "title": "4  Connection with machine learning and AI",
    "section": "",
    "text": "A “max-success” classifier vs an optimal classifier\n\n\n\n\n\n\n \n\n\n\nYou find the code for this chapter and exercises also in this JupyterLab notebook for R and (courtesy of Viktor Karl Gravdal!) this JupyterLab notebook for python.\n\n\nWe shall compare the results obtained in some numerical simulations by using\n\na Machine-Learning Classifier trained to do most successful guesses\na prototype “Optimal Predictor Machine” trained to make the optimal decision\n\nFor the moment we treat both as “black boxes”, that is, we don’t study yet how they’re calculating their outputs (although you may already have a good guess at how the Optimal Predictor Machine works).\nTheir operation is implemented in this R script that we now load:\n\nsource('mlc_vs_opm.R')\n\nThis script simply defines the function hitsvsgain():\nhitsvsgain(\n    ntrials,\n    chooseAtrueA,\n    chooseAtrueB,\n    chooseBtrueB,\n    chooseBtrueA,\n    probsA\n)\nhaving six arguments:\n\nntrials: how many simulations of guesses to make\nchooseAtrueA: utility gained by guessing A when the successful guess is indeed A\nchooseAtrueB: utility gained by guessing A when the successful guess is B instead\nchooseBtrueB: utility gained by guessing B when the successful guess is indeed B\nchooseBtrueA: utility gained by guessing B when the successful guess is A instead\nprobsA: a tuple of probabilities (between 0 and 1) to be used in the simulations (recycling it if necessary), for the successful guess being A; the corresponding probabilities for B are therefore 1-probsA. If this argument is omitted it defaults to 0.5 (not very interesting)\n\n\n\nExample 1: electronic component\nLet’s apply our two classifiers to the Accept or discard? problem of § 1. We call A the alternative in which the element won’t fail before one year, and should therefore be accepted if this alternative were known at the time of the decision. We call B the alternative in which the element will fail within a year, and should therefore be discarded if this alternative were known at the time of the decision. Remember that the crucial point here is that the classifiers don’t have this information at the moment of making the decision.\nWe simulate this decision for 100 000 components (“trials”), assuming that the probabilities of failure can be 0.05, 0.20, 0.80, 0.95. The values of the arguments should be clear:\n\nhitsvsgain(\n    ntrials = 100000,\n    chooseAtrueA = +1,\n    chooseAtrueB = -11,\n    chooseBtrueB = 0,\n    chooseBtrueA = 0,\n    probsA = c(0.05, 0.20, 0.80, 0.95)\n)\n\n\nTrials: 100000\nMachine-Learning Classifier: successes 87447 ( 87.4 %) | total gain -24638\nOptimal Predictor Machine:   successes 72298 ( 72.3 %) | total gain 9943\n\n\nNote how the machine-learning classifier is the one that makes most successful guesses (around 88%), and yet it leads to a net loss! If the utility were in kroner, this classifier would cause the company producing the components a net loss of more than 20 000 kr.\nThe optimal predictor machine, on the other hand, makes fewer successful guesses overall (around 72%), and yet it leads to a net gain! It would earn the company a net gain of around 10 000 kr.\n\n\n\n\n\n\nExercise 4.1\n\n\n\nHow is this possible? Try to understand what’s happening; feel free to research this by modifying the hitsvsgain() function, so that it prints additional outputs.\n\n\n\n\nExample 2: find Aladdin! (image recognition)\nA typical use of machine-learning classifiers is for image recognition: for instance, the classifier guesses whether a particular subject is present in the image or not.\nIntuitively one may think that “guessing successfully” should be the best goal here. But exceptions to this may be more common than one thinks. Consider the following scenario:\n\nBianca has a computer folder with 10 000 photos. Some of these include her beloved cat Aladdin, who sadly passed away recently. She would like to select all photos that include Aladdin and save them in a separate “Aladdin” folder. Doing this by hand would take too long, if at all possible; so Bianca wants to employ a machine-learning classifier.\nFor Bianca it’s important that no photo with Aladdin goes missing, so she would be very sad if any photo with him weren’t correctly recognized; on the other hand she doesn’t mind if some photos without him end up in the “Aladdin” folder – she can delete them herself afterwards.\n\nLet’s apply and compare our two classifiers to this image-recognition problem, using again the hitsvsgain() function. We call A the case where Aladdin is present in a photo, and B where he isn’t. To reflect Bianca’s preferences, let’s use these “emotional utilities”:\n\nchooseAisA = +2: Aladdin is correctly recognized\nchooseBisA = -2: Aladdin is not recognized and photo goes missing\nchooseBisB = +1: absence of Aladdin is correctly recognized\nchooseAisB = -1: photo without Aladdin end up in “Aladdin” folder\n\nand let’s say that the photos may have probabilities 0.3, 0.4, 0.6, 0.7 of including Aladdin:\n\nhitsvsgain(\n    ntrials = 10000,\n    chooseAtrueA = +2,\n    chooseAtrueB = -1,\n    chooseBtrueB = 1,\n    chooseBtrueA = -2,\n    probsA = c(0.3, 0.4, 0.6, 0.7)\n)\n\n\nTrials: 10000\nMachine-Learning Classifier: successes 6496 ( 65 %) | total gain 4529\nOptimal Predictor Machine:   successes 5952 ( 59.5 %) | total gain 5463\n\n\nAgain we see that the machine-learning classifier makes more successful guesses than the optimal predictor machine, but the latter yields a higher “emotional utility”.\nYou may sensibly object that this result could depend on the peculiar utilities or probabilities chosen for this example. The next exercise helps answering your objection.\n\n\n\n\n\n\nExercise 4.2\n\n\n\n\nIs there any case in which the optimal predictor machine yields a strictly lower utility than the machine-learning classifier?\n\nTry using different utilities, for instance using ±5 instead of ±2, or whatever other values you please.\nTry using different probabilities as well.\n\nAs in the previous exercise, try to understand what’s happening. Consider this question: how many photos including Aladdin did each classifier miss?\nModify the hitsvsgain() function to output this result.\nDo the comparison using the following utilities: chooseAtrueA = +1, chooseAtrueB = -1, chooseBtrueB = 1, chooseBtrueA = -1. What’s the result? what does this tell you about the relationship between the machine-learning classifier and the optimal predictor machine?",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-1-ML.html#what-is-artificial-intelligence",
    "href": "connection-1-ML.html#what-is-artificial-intelligence",
    "title": "4  Connection with machine learning and AI",
    "section": "4.2 What is “Artificial Intelligence”?",
    "text": "4.2 What is “Artificial Intelligence”?\n\n“AI” as opposed to what?\nThe field of Artificial Intelligence is vast, and its boundaries are not clear-cut. Different books give slightly different definitions of AI. In everyday parlance the term “AI” is moreover used in ways that are not technically correct – a bit like it happens with physics terms such as “energy” or “force”. In this course we want to use AI in a technically more correct way.\nThe discussion of the possible definitions of AI could take several chapters. Let’s try a shorter approach, by examining why the two words “artificial” and “intelligence” are used specifically.\n\nArtificial as opposed to what? As opposed to natural for example. So it denotes something human-made, as opposed to something directly found in nature; say in an orangutan or in a dolphin.\n\nIntelligence as opposed to what? As opposed to stupidity. The definition of “intelligence” itself, even natural intelligence, is still quite open. Generally we mean something that is logical or rational. Thus an agent that breaks some logical procedure, or that does not follow a procedure that it claims to follow, is not “intelligent”.\n\nOf course neither term is fully dichotomous: we can distinguish different degrees of artificiality and of intelligence.\n\n\n“Intelligence” is not “human-likeness”\nWe can distinguish two distinct endeavours in the field of Artificial Intelligence, considered in its most general extension:\n\nachieving human-like behaviour;\nachieving intelligent reasoning, or we could say logical or rational reasoning.\n\nIt’s important to recognize immediately that these two endeavours may not be mutually compatible. We often associate human behaviour with error-making and irrationality. We may say that a person is very irrational, yet we don’t say that because of this the person is inhuman.\nGiven the incompatible character of the two endeavours above, we must be very clear and conscious about which goal we’re trying to achieve; otherwise we won’t achieve any goal at all. And in technical discussions we must be careful to adopt the correct terminology. In particular we should avoid the term “intelligent” when we instead mean “human-like”, and vice versa.\nAn example of such confusion is with present-day large language models (LLMs, which we also discuss in chapters 34 and 38) and in particular those with a Generative Pre-training Transformer (GPT) architecture. In many media they are referred to as “AI systems”; yet what they achieve is not intelligence, but rather human-like language processing – including non-intelligent processing.\nIf you have access to a large language model, you have surely witnessed examples of nonsensical output1. You can try a variation of the following experiment:\n1 often euphemistically called “hallucination” because this term may increase sales, whereas “stupid” would risk decreasing sales.\nAsk the LLM to write down a short list of some set, for instance of all Norwegian counties.\nAsk the LLM to select from the list only those item that have one or more letter “r” in their name. See the result.\nAsk the LLM to give you a step-by-step procedure to achieve the selection required in the previous step.\n\nTypically a LLM fails at task 2., even if it can give a completely sound procedure in task 3. Clearly it isn’t internally following that logical procedure.\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\nChapters 1–2 of Artificial Intelligence.",
    "crumbs": [
      "[**An invitation**]{.lightblue}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[Connection with machine learning and AI]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "Rintro1.html",
    "href": "Rintro1.html",
    "title": "Working with R, I",
    "section": "",
    "text": "R resources\nAs announced in the introduction, in these notes we use the R programming language to explore data science topics and for the software implementation of AI ideas.\nThere’s plenty of resources on installing R and learning its basics; you just have to do a short search and find the one that resonates with you. Examples:\nThere are also resources to run simple R scripts online:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "[Working with R, I]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro1.html#sec-R-resources",
    "href": "Rintro1.html#sec-R-resources",
    "title": "Working with R, I",
    "section": "",
    "text": "R Installation and Administration\nAn Introduction to R\nR Tutorial\nAnother Introduction to R\n\n\n\nRun R code online\nRun your R code\nR online compiler",
    "crumbs": [
      "[**Inference I**]{.green}",
      "[Working with R, I]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro1.html#sec-R-setup",
    "href": "Rintro1.html#sec-R-setup",
    "title": "Working with R, I",
    "section": "R setup",
    "text": "R setup\nThere are various ways of working with R, as well as various Integrated Development Environments, which you can find with an online search. Use whatever you like best. In these notes we’ll write the code to run, which you can simply paste into an R console or run through your chosen software (note the copy icon on the right of each code snippet). It’s understood that you are using some working directory or folder, where you can load data and other files from, and where you save your results to.\nLet’s start with something very simple. Here’s an input, and the output you should see:\n\n2 + 1\n\n[1] 3\n\n\nHere’s another simple example (character strings in R are delimited by single quotes ' or double quotes \"):\n\nprint('hello')\n\n[1] \"hello\"\n\n\nLet’s assign the value 5 to the variable x, then ask what x is; we also write a comment, introduced by one or more # signs:\n\n## assign value to x\nx &lt;- 5\n\n## output x\nx\n\n[1] 5\n\n\nNow let’s assign the sequence of values \\(0, -9, 4.7\\) to x; this is done using the function c():\n\n## assign values to x\nx &lt;- c(0, -9, 4.7)\n\n## output x\nx\n\n[1]  0.0 -9.0  4.7\n\n\nLet’s ask what is the second value in x; this is done with square brackets [ ]. Note that indexing starts from 1:\n\nx[2]\n\n[1] -9\n\n\n\n\nWe shall need some extra R packages to develop the material in these notes:\n\nlpSolve\nextraDistr\ncollapse\n\nso let’s start an R session and install them.1\n1 Depending on your operating system you can choose to install them as user, making them available only to you, or as superuser or administrator, making them available to every user in your machine.\ninstall.packages('lpSolve')\ninstall.packages('extraDistr')\ninstall.packages('collapse')\n\nWe shall also use some custom-made functions for plotting and reading data. They are defined in the tplotfunctions.R file, which you should download to your working directory. Once it’s downloaded, you can load the functions this way:\n\nsource('tplotfunctions.R')",
    "crumbs": [
      "[**Inference I**]{.green}",
      "[Working with R, I]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro1.html#sec-R-1st-explore",
    "href": "Rintro1.html#sec-R-1st-explore",
    "title": "Working with R, I",
    "section": "First exploration with R",
    "text": "First exploration with R\nAs a very simple task to get acquainted with R, let’s do the following:\n\nAssign the sequence of values form -5 to 5, in steps of 0.5, to the variable x.\nCalculate \\(x^2 + 1\\) for each value contained in x, and assign the resulting values to the variable y.\nCalculate \\(7 \\sin(x)\\) for each value contained in x, and assign the resulting values to the variable z.\nPlot the graphs of y vs x, and of z vs x, together, using different colours and line style, giving appropriate names to the axes, and choosing a range from -6 to 35 for the vertical axis.\n\nWe do all these operations, in sequence, below; note the comments:\n\nx &lt;- seq(from = -5, to = 6, by = 0.5)\n\ny &lt;- x^2 + 1\n\nz &lt;- 7 * sin(x)\n\nflexiplot(\n    x = x, y = y,\n    col = 1, lty = 1,         ## colour and type of line\n    ylim = c(-6, 35),         ## y-axis range\n    xlab = 'x', ylab = 'y, z' ## axes labels\n)\n\nflexiplot(\n    x = x, y = z,\n    col = 2, lty = 2, ## different colour & type\n    add = TRUE        ## add to previous plot\n)\n\n\n\n\n\n\n\n\nLooking at the code above you notice the following features:\n\nMany functions, such as seq(), have named arguments, for example from = -5, or to = 6, or xlab = 'x'. In some cases the argument names can be omitted; for instance we could write seq(-5, 6, by = 0.5). But in these cases one must pay attention to the order of the arguments.\nMost mathematical operations are performed element-wise on sequences of numbers.\nIn some circumstances it is possible to break a command over several lines; for instance, the arguments to the flexiplot() functions were not all given in one line. One must be careful because linebreaks do terminate some expressions; but in general it is safe – and clearer for people who read your code – to distribute function arguments across several lines, as done above.\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nGet familiar with basic mathematical operations in R, like +, -, *, /, ^, exp(), log().\nGet familiar to assigning single values and sequences of values to variables.\nCheck what happens when you change the colour and type of plot lines: try all numbers from 1 to 10.\nCheck what happens when you choose different ranges for the vertical axis in the first plot. Can you change the vertical range when you call the second plot?\nWhat happens if you omit the vertical-axis range ylim = c(-6, 35)? (pay attention not to leave spurious commas.)\nCheck what happens if you specify a range for the horizontal axis with xlim =.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "[Working with R, I]{.lightblue}"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "5  What is an inference?",
    "section": "",
    "text": "5.1 The wide scope and characteristics of inferences\nIn the assembly-line decision problem of § 1, the probability of early failure was very important in determining the optimal decision. If the probability had been \\(5\\%\\) instead of \\(10\\%\\), the optimal decision would have been different. Also, if the probability had been \\(100\\%\\) or \\(0\\%\\), it would have meant that we knew for sure what was the successful decision.\nIn that decision problem, the probabilities of the outcomes were already given. But in real decision problems the probabilities of the outcomes almost always need to be calculated, and their calculation can be the most time- and resource-demanding stage in solving a decision problem.\nWe’ll loosely refer to problems of calculating probabilities as “inference problems”, and to their calculation as “drawing an inference”. Drawing inferences is very often a goal or need in itself, without any underlying decision process.\nOur purpose now is to learn how to draw inferences – that is, how to calculate probabilities. In the next few chapters we’ll proceed by facing the following questions, in order:\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\nWe must keep these characteristics in mind if we want to build an AI agent capable of drawing inferences and to autonomously face inferential problems as diverse as in the examples above.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "5  What is an inference?",
    "section": "",
    "text": "Looking at the weather, we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects the patient, in order to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nThe computer of a self-driving car needs to assess, from the current set of camera frames, whether a particular patch of colours in the frames is a person, in order to slow down the car and stop if that’s the case.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change, if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a value of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test value could have been \\(100\\,\\mathrm{mV}\\) even if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test value could have been \\(80\\,\\mathrm{mV}\\) if the component had not been defective.\n\n\n\nFrom measurements of the Sun’s energy output, measurements of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and measurements of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\nExercise 5.1\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done with full certainty (that is, to know which decision is successful), based the data given explicitly and implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only”? That is, has their object or the data on which they are based never happened before and will never happen again?\nAre any of the inferences above based on data and information that come chronologically after the object of the inference?\nAre any of the inferences above about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\n\n\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say for sure whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nInferences are typically based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information. In other words, inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-inference-origin",
    "href": "inference.html#sec-inference-origin",
    "title": "5  What is an inference?",
    "section": "5.2 Where are inferences drawn from?",
    "text": "5.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking developments and theorems in the foundations of mathematics that appeared in the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n \n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on.\nProbably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that in order to prove some theorem, we must always start from some axioms. There are “inferences”, called tautologies, that can be drawn without requiring others, but they are all trivial: for example “this component failed early, or it didn’t”. These tautologies are of little use in a real problem, although they have a deep theoretical importance. Useful inferences, on the other hand, must always start from some axioms.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\nIn concrete applications, we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by everyone. In this case the final inference has a somewhat contingent character. We accept it (as well as the solution of any underlying decision problem) as the best available one for the moment. This is partly the origin of the term “model”.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "inference.html#sec-basic-elements-inference",
    "href": "inference.html#sec-basic-elements-inference",
    "title": "5  What is an inference?",
    "section": "5.3 Basic elements of an inference",
    "text": "5.3 Basic elements of an inference\nLet us introduce some mathematical notation and more precise terminology for inferences.\n\nEvery inference has an “object”: what is to be assessed or guessed. We call proposal the object of the inference.\nEvery inference also has data, information, hypotheses, or hypothetical scenarios on which it is based. We call conditional what the inference is based upon.\nWe separate proposal and conditional with a vertical bar  “ \\(\\pmb{\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}}\\) ”, which can be pronounced “given” or “conditional on”.\nFinally, we put parentheses around this and a “\\(\\mathrm{P}\\)” in front, short for “probability”:\n\n\n\nProposal is Johnson’s (1924) terminology; Keynes (1921) uses “conclusion”; modern textbooks do not seem to use any specialized term. Conditional is modern terminology; other terms used: “evidence”, “premise”, “supposal”. The vertical bar, originally a solidus, was introduced by Keynes (1921).\n\\[\n\\mathrm{P}( \\underbracket[1px]{\\color[RGB]{34,136,51}\\boldsymbol{\\cdots}}_{\\textit{proposal}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\underbracket[1px]{\\color[RGB]{68,119,170}\\boldsymbol{\\cdots}}_{\\textit{conditional}}\n) = {\\color[RGB]{238,102,119}\\boldsymbol{\\cdots}\\%}\n\\]\nthis means “the probability that [proposal], supposing [conditional], is . . . %”. Or also: “supposing [conditional], we can infer [proposal] with . . . % probability”.\nWe have remarked that in order to calculate the probability for an inference, we must use the probabilities of other inferences, which in turn are calculated by using the probabilities of other inferences, and so on, until we arrive at probabilities that are taken for granted. A basic inference process could therefore be schematized like this:\n\n\n\n\n\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects of an inference. Thereafter we shall study the rules for drawing correct inferences.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>[What is an inference?]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html",
    "href": "sentences.html",
    "title": "6  Sentences",
    "section": "",
    "text": "6.1 The central components of knowledge representation\nWe have seen that an inference involves, at the very least, two things: the object of the inference (proposal), and the data, information, or hypotheses on which the inference is based (conditional).\nWe also observed that wildly different “things” can be the object of an inference or the information on which the inference is based: measurement results, decision outcomes, hypotheses, not-real events, assumptions, data and information of all kinds (for example, images). In fact, such variety in some cases can make it difficult to pinpoint what an inference is about or what it is based upon.\nIs there a general, flexible, yet precise way of representing all these kinds of “things”?\nWhen speaking of “data”, what comes to mind to many people is numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of “things” exemplified above? Well, this option turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it.\nInstead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. You can do different things with this piece of information. For instance, if you had decided to send one probe to each official planet, now you know you have to build eight probes. Or maybe you can win at a pub quiz with it.\n“Data” is therefore not just numbers. A number is not “data” unless there’s an additional verbal and non-numeric context accompanying it – even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but is essentially non-numeric data. Sure, in some situations we could represent this data with numbers, say “1” for “recovered” and “0” for “not recovered”. But the opposite or some other convention could also be used: “0” for “recovered” and “1” for “not recovered”, or the numbers “0.3” and “174”. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\nThe examples above, however, actually reveal the answer to our needs! In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “things” that can be the proposal or the conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts – we’ll return to these later on.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "6  Sentences",
    "section": "",
    "text": "1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\n\n\n\n\n\n\n Study reading\n\n\n\nRead §7.1 in Artificial Intelligence.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-identify-sentences",
    "href": "sentences.html#sec-identify-sentences",
    "title": "6  Sentences",
    "section": "6.2 Identifying and working with sentences",
    "text": "6.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\nA sentence is something for which an agent can determine, at least in principle, whether it is true or false.\nLet’s make this definition clearer with some remarks:\n\n\n A sentence doesn’t have to contain only words. It can contain pictures, sounds, and other non-verbal items. For example, the following:\n“This:  is an animated picture of Saitama.”\nis a sentence, even if it contains animated graphics, because we can say that it is true. Likewise, the following:\n“This link leads to a song by Pink Floyd.”\nis also a sentence, even if it contains links and audio, because we can say that it is false (that’s a song by Monty Python).\n A meaningful phrase may not be a sentence. For instance, a phrase like “Apples are much tastier than pears” may not be a sentence, because it’s a matter of personal taste whether it’s true or false. Moreover, an agent’s opinion about apples and pears might change from time to time.\nThe phrase “Jenny right now finds apples tastier than pears”, on the other hand, could be a sentence; its truth being found by asking Jenny at that very moment.\nIn an engineering context, the phrase “This valve will operate for at least two months” is a sentence, even if its truth is unknown at the moment: one has to wait two months, and then its truth will be unambiguously known.\n\n\n\n An expression involving technical terms may not be a sentence (and not meaningful either). For instance, in a data-science context the phrase “This neural-network algorithm has better performance than that random-forest one” is not a sentence unless we have objectively specified what “better” means (higher accuracy? higher true-positive rate? faster?), for example by adopting a particular comparison metric.\nSome expressions involving technical terms may appear to be sentences at first; but a deeper analysis then reveals that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention. The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\n\n\n\n\n\n\n Stay alert even if you see technical terms\n\n\n\nBe particularly careful when reading scientific and engineering papers with a lot of technical terms and phrases. Technical jargon often makes it especially difficult to see whether what’s being said is true or at least meaningful!\n\n\n\n A sentence can be expressed in different ways by different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\n\n\nThere are many advantages in working with sentences (rather than just numbers), and in keeping in mind that every inference is about sentences:\nFirst, this point of view leads to clarity in engineering problems, and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing a solution, it is important to understand what information or outcomes the end user exactly wants. These “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, this point of view is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe therefore agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can be true or false.\nMany inferences, especially when they concern numerical measurements, involve more than one sentence. For example, an inference about the result of rolling a die actually consists of the probabilities for six separate proposals:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences of this kind. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false sentences. This analysis may reveal contradictions in our inference problem. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\nExercise 6.1\n\n\n\nRewrite each inference scenario of § 5.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "6  Sentences",
    "section": "6.3 Notation and abbreviations",
    "text": "6.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression. Example:\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed. Sometimes we may simply write\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. As mentioned before, in these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\), possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We express such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq\\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means that the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\).\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three sentences:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\]\nThe quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. The difference greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike in the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\nExercise 6.2\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\\[\n\\mathsfit{S}\n\\]\nSome key word appearing in the sentence:\n\\[\n\\textsf{\\small temperature}\n\\]\nAn equality:\n\\[\nT\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\]\nThe quantity appearing in the sentence:\n\\[\nT\n\\]\nThe value appearing in the sentence:\n\\[\nx\n\\]\n\nGet familiar with these kinds of abbreviations because they are all very common. Some texts may even jump from one abbreviation to another in the same page or paragraph!",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#sec-connecting-sentences",
    "href": "sentences.html#sec-connecting-sentences",
    "title": "6  Sentences",
    "section": "6.4 Connecting sentences",
    "text": "6.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences, out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following composite sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols:\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For example it might become necessary to consider atomic sentences with other values for the reported voltage, such as\n\\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\]\nand so on.\n\n\nConnectives\nHow do we construct composite sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives. You may have already encountered them in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n \n\n\n\n\n\nNot (symbol  \\(\\lnot\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\[1ex]\n\\lnot \\mathsfit{s} &= \\textsf{\\small`The component is broken after the shock test'}\n\\end{aligned}\\]\n\nAnd (symbols  \\(\\land\\)  also  \\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\) )\n\nexample:\n\n\n\\[\n\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\[1ex]\n\\mathsfit{s} \\land \\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\\\\n\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\end{aligned}\n\\]\n\nOr (symbol  \\(\\lor\\) )\n\nexample:\n\n\n\\[\\begin{aligned}\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\\\[1ex]\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} &= \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\end{aligned}\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly more complex composite sentences.\nThe and connective appears very frequently in probability formulae. Using its standard symbol “\\(\\land\\)” would consume a lot of horizontal space. For this reason a comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is often used as an alternative symbol. So the expressions \\(\\mathsfit{s} \\land \\mathsfit{h}\\) and \\(\\mathsfit{s} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{h}\\) are completely equivalent.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. The and connective may for instance correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take\n\\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq\\textsf{\\small`The component is broken'}\n  \\]\nBut in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that the sentences it connects can be both true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this:\n\\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full composite sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through §7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "6  Sentences",
    "section": "6.5 “If… then…”",
    "text": "6.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it is often the starting point of our inferences. We shall discuss this point in more detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in formal logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "sentences.html#actual-implementation",
    "href": "sentences.html#actual-implementation",
    "title": "6  Sentences",
    "section": "6.6 Actual implementation",
    "text": "6.6 Actual implementation\nAs we said at the beginning, sentences are the central components of knowledge representation in AI agents. But how are sentences and their relationships actually implemented in a concrete AI agent?\nThe notation and symbols we discussed above are tools that we, humans, use to study and discuss about sentences. These symbols and tools are independent of technology; there also lies their usefulness. But we cannot expect a concrete AI agent to work with “letters” or similar symbols internally.\nKnowledge representation is a whole AI field in itself, and unfortunately we don’t have time to delve into its present-day state and concrete implementations. Also because, remember, we’re trying to adopt a view that’s technology-independent, a view that allows us to see potential in new technologies.\nBut it’s good to get a glimpse of present-day implementations knowledge representations. Here are examples from NASA:\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through:\n\nOno & al. 2015: SMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in Ingham 2012: No More Band-Aids: Integrating FM into the Onboard Execution Architecture\n\n\npart IV in Williams & al. 2003: Model-based programming of intelligent embedded systems and robotic space explorers\n\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n\n\n@@ TODO: add connections to large language models (Gödel & Co.).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>[Sentences]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html",
    "href": "truth_inference.html",
    "title": "7  Truth inference",
    "section": "",
    "text": "7.1 A trivial inference\nSome inferences can be drawn with absolute certainty, that is, we can ascertain for sure the truth or falsity of their proposal. We call this particular “sure” kind of inferences truth inferences. Mathematical inferences are a typical example of this kind. You probably have some acquaintance with rules for drawing truth inferences, so we start from these.\nConsider again the assembly-line scenario of § 1, and suppose that an inspector has the following information about an electric component:\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "7  Truth inference",
    "section": "",
    "text": "This electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the shock test or the heating test. This component passed the shock test.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference-analysis",
    "href": "truth_inference.html#sec-trivial-inference-analysis",
    "title": "7  Truth inference",
    "section": "7.2 Analysis and representation of the problem",
    "text": "7.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with compact symbols.\n\n1. Atomic sentences\nWe can introduce the following atomic sentences and symbols:\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\n2. Proposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]\n\n\n\n3. Starting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the shock test or the heating test. We write this as\n\\[\n\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#sec-truth-inference-rules",
    "href": "truth_inference.html#sec-truth-inference-rules",
    "title": "7  Truth inference",
    "section": "7.3 Truth-inference rules",
    "text": "7.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when sure inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems, each one with a wide variety of possible notation conventions. These systems are all equivalent, of course.\nAs an example, look at how a proof of our inference appears in the so-called sequent calculus. This inference calculus consists of a dozen or so inference rules. Note that this system adopts a notation different from ours:\n\nit uses the symbol “\\(\\vdash\\)” instead of our conditional bar;\nit puts the proposal on the right and the conditional on the left (this is simply because of historical reasons), swapping our placement:\n\n\n\n\n\n\nThe bottom formula is the target inference. Each horizontal line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference and a tautology.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nTakeuti: Proof Theory\n\n\n\n\nOur specific choice and notation\nWe choose to compactly encode all truth-inference rules in the following way.\n\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor with  “\\({} = 0\\)”  if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n \n\n\n\n\n\n\\(\\boldsymbol{\\lnot}\\) “Not” rule\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n1 - \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{7.1}\\]\n\n\\(\\boldsymbol{\\land}\\) “And” rule\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.2}\\]\n\n\\(\\boldsymbol{\\lor}\\) “Or” rule\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{7.3}\\]\n\nTruth rule\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{7.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\nprovided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nAgain wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) \\ ,\n\\]\nstarting from the given ones\n\\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0 \\ .\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\nExercise 7.1\n\n\n\nRetrace the proof above step by step. At each step, how did we use the rule indicated on the right?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n \n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\nExercise 7.2\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\nEquivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (7.1)–(7.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nExercise 7.3\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (7.1)–(7.4) are more complicated than truth-tables, but have two important advantages. First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "7  Truth inference",
    "section": "7.4 Logical AI agents and their limitations",
    "text": "7.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in § 6.1, are programmed according to these rules.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free to take a look at\n\nHuth & Ryan: Logic in Computer Science\nBen-Ari: Mathematical Logic for Computer Science\nPelletier & Hazen: Natural Deduction Systems in Logic\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>[Truth inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html",
    "href": "probability_inference.html",
    "title": "8  Probability inference",
    "section": "",
    "text": "8.1 When truth isn’t known: beliefs and probability\nIn most engineering and data-science problems we don’t know the truth or falsity of outcomes and hypotheses that interest us. But this doesn’t mean that nothing can be said or done in such situations. Now we shall finally see how to draw uncertain inferences, that is, how to calculate the probability of something that interests us, given particular data, information, and assumptions.\nSo far we have used the term “probability” somewhat informally and intuitively. It’s time to make it more precise and to emphasize some of its most important aspects, especially for Artificial Intelligence. Then we’ll dive into the rules of probability-inference.\nIf we want to design an AI agent that can make decisions and act in uncertain situations, we need to equip it with probabilities and utilities. This is an inescapable necessity, as discussed in § 2.5. It is intuitively clear what utilities are: they quantify how desirable different outcomes are to the agent. What do probabilities quantify?\nA first tentative answer could be this: the probability of an outcome quantifies how often the agent has seen that outcome. That is, it represents the observed frequency of that outcome.\nAlthough it seems reasonable in many respects, the answer above still doesn’t capture several elements that appear in the way we make rational inferences, and that are extremely important in designing an AI agent that can operate in uncertain conditions.\nTo see what’s missing, consider the following scenario:\nDo you think Aisha’s judgement and choice are rational?\nAisha’s conclusion comes not only from the fact that the win is the same in either outcome, but also from the fact that her beliefs in the heads outcome and in the tails outcome are equal. If one belief were stronger than the other, she would definitely bet on that outcome instead.\nYet this equality of beliefs does not come from an equality of frequencies. In fact neither you or your friend have observed any frequencies whatsoever; you have never seen a similar tossing device in operation before. For all you know, the device might be designed to always produce heads, or always tails, or maybe one or the other with peculiar frequencies not equal to 50%/50%.\nLet’s continue:\nDo you think Aisha’s judgement and choice are rational?\nClearly Aisha’s beliefs in the two possible outcomes are almost equal. And this near-equality is different from the observed frequency: the frequency right now is 100% tails. The frequency does affect Aisha’s belief a little (she’d choose tails if offered an equal bet), but is different from her belief.\nFast-forward in time:\nDo you think Aisha’s judgement and choice are rational?\nNow we can say that her beliefs and the observed frequencies are aligned.\nFinal part of this scenario:\nDo you think Aiden’s judgement and choice are rational?\nIf you think that Aisha’s and Aiden’s behaviour and choices were rational, consider a situation in which two AI agents were in their place. It is apparent that we need to equip an AI agent with a sort of quantified, rational “degree of belief” in order for it to make rational decisions in uncertain situations. This is also the result discussed in § 2.5. This degree of belief can have connections with an observed frequency, but is generally different from it. The two notions must therefore be kept separate, and quantified separately. In fact, we may think of situations in which an agent may be uncertain about a frequency and needs to quantify its own belief about the frequency; we’ll meet these situations in the Inference III part.\nWhat we call probability is this quantified degree of belief:\nWe shall take the notion of degree of belief as intuitively understood, just as we did with the notion of truth. We shall use the terms probability, degree of belief, belief, plausibility, credibility1 as synonyms.\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of 0.5 represents a belief completely symmetric with respect to truth and falsity.\nSaid otherwise, if an agent assigns to a sentence a probability 1, it means that the agent is completely certain that the sentence is true. If the agent assigns a probability 0, it means that the agent is completely certain that the sentence is false. If the agent assigns a probability 0.5, it means that the agent is equally uncertain about the truth as about the falsity of the sentence.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "8  Probability inference",
    "section": "",
    "text": "Someone shows you and a friend of yours, called Aisha, a mechanical device designed to toss a coin. The device looks complicated; neither of you has a full grasp of its design. A normal coin is placed on the device, and your friend Aisha is required to place a bet on whether the tossed coin will land heads or tails. If she guesses correctly she wins $1; nothing otherwise. Note that not betting is not an option: feel free to imagine circumstances (possibly very nasty) where your friend can’t refuse to bet. Let’s also make clear that no “cheating” is taking place: for instance, the outcome does not depend on the betting choice (you can imagine that the coin is tossed right before Aisha bets, but the outcome hidden from her until she bets). From now on you can only observe Aisha’s choices and hear what she says, but cannot talk with her.\nIn this situation your friend says that it does not matter whether she bets on heads or tails; she has no more belief in one outcome than the other. In fact, she decides by tossing a coin she had in her pocket.\n\n\n\n\n\n\nNow you and your friend observe the toss outcome. It’s tails. The device is prepared again, exactly as in the first toss, and Aisha is required to bet again. However, now they propose Aisha two different bets: betting tails and guessing correctly, she wins $1; betting heads and guessing correctly, she wins $2. (And remember: the outcome won’t depend on her bet; no cheating is taking place).\nAisha says the following. If she had been proposed a $1-win on either heads or tails, then she would have bet on tails, just because the device had shown tails once. But she still has no much more belief in tails than heads; so given the double win on heads, she now bets heads.\n\n\n\n\n\nThe device is repeatedly used, say for 1000 times (plus some extra times until you or your friend says “stop”, if you like). In these repetitions you and Aisha count that heads occurred 781 times, and tails 219 times, without any recognizable pattern. Now Aisha is asked again to place a bet on the next toss.\nAisha says that she believes more strongly that the coin will land heads than tails. She even quantifies her belief to around 78% for heads and 22% for tails. It’s very close or identical to the frequency you both observed. Aisha bets accordingly.\n\n\n\n\n\nFinally, another friend of yours, called Aiden, is brought in, and shown the device. Aiden cannot communicate with you or Aisha; he’s in the same situation she and you were around 1000 tosses ago. Aiden is required to place a bet, and he says that he has no preferences for betting on heads or tails – with exactly the same reasoning Aisha did 1000 tosses ago.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nThe probability of a sentence is an agent’s quantified degree of belief in that sentence.\n\n\n\n\n1 credibility literally means “believability” (from Latin credo = to believe).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-aspects-probability",
    "href": "probability_inference.html#sec-aspects-probability",
    "title": "8  Probability inference",
    "section": "8.2 Important aspects of probabilities",
    "text": "8.2 Important aspects of probabilities\nFrom our discussion of the tossing-device scenario above we can gather several important aspects and facts about probabilities:\n\n Probabilities are not frequencies\nThis fact is clear by now. Indeed:\n\nthere can be a degree of belief when no frequencies are available;\na degree of belief can be very different from observed frequencies;\na degree of belief can be equal to an observed frequency.\n\nOne remarkable feature of the framework that we’re going to study is that the quantitative connection between these two notions is taken care of automatically! We shall see how this connection works in the Inference III part.\nNote also the different status of the notions “frequency” and “probability/belief” from an epistemological point of view.2 Frequencies can be unknown to some agents. Degrees of belief cannot be “unknown”: the agent must have them in order to act. At worst, degrees of belief can be difficult to calculate.\n2 That is, from the point of view of an agent’s knowledge.\n\n\n\n\n\n\n\n\n\nBe careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated; it’s “unknown” in the same sense that the value of  “\\(1-0.7 \\cdot 0.2/(1-0.3)\\)”  is “unknown” to you right now.\n\n\n\n\n Probabilities are agent- and knowledge-dependent\nThe tossing-device scenario shows that different agents can have different probabilities, that is, degrees of belief, about the same situation.\nThis happened when Aiden entered the scene. Aisha had beliefs around 78% for heads and 22% for tails; but Aiden had 50%/50% beliefs for the same toss. Both sets of beliefs were rational and appropriate to their respective situations. They simply reflected the different states of knowledge of the agents that held them.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some literature speaks of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n\n\n Probabilities are not physical properties\nThe fact that two agents can hold different probabilities in the same situation also shows that probabilities are not physical properties, which could be objectively measured with some meter.\nWhether a tossed coin lands heads or tails is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Diaconis & al. 2007: Dynamical Bias in the Coin Toss. \n\n\nWe can objectively measure frequencies, in several instances of a phenomenon. Frequencies, as opposed to probabilities, are physically measurable quantities. This shows again the difference between probabilities and frequencies.\n\n\n Probabilities are assigned to sentences\nWe already discussed this point in § 6.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different. If the engineer is the person who decides how to set that output, and has decided to set it to 100 MW, then the probability is obviously \\(100\\%\\) (or very close to), because the engineer already knows what the output will be. The probability depends not only on a number, but on what it’s being done with that number: measuring, setting, third-party reporting, and so on. Often we write simply “\\(O \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\,\\mathrm{W}\\)”, provided that the full sentence behind this shorthand is understood.\n\n\nThe points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance3 – or even cause deaths.\n3 This fact can be mathematically proven.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#the-many-uses-of-the-word-probability",
    "href": "probability_inference.html#the-many-uses-of-the-word-probability",
    "title": "8  Probability inference",
    "section": "8.3 The many uses of the word “probability”",
    "text": "8.3 The many uses of the word “probability”\nThe terms “degree of belief” and “frequency” are quite distinct and are used more or less consistently in the literature. Whenever you encounter these terms you more or less know what’s intended.\nSadly the situation is completely different with the term “probability”, which is used in the literature in incompatible ways. Some literature uses this term as a synonym of “frequency”. Other literature uses it as a synonym of “degree of belief”, as we do; use of “probability” as “degree of belief” is called Bayesian probability theory.\nSome recent literature in machine learning uses “probability” in yet another way, to denote the numeric output of some machine-learning algorithms. This numeric output is neither a frequency or a degree of belief, and only has vague associations with them; we’ll discuss this in ch.  42.\nThere are also a couple more different uses of the term “probability” in the literature. It’s a mess.\nIn this course we could have stuck to the terms “degree of belief” and “frequency” only, avoiding the problematic “probability”. But such a choice would not be very helpful to you, because you will nevertheless encounter this term in your readings and scientific discussions. In the AI literature the most common use is as “degree of belief”, so we also adopt it, sometimes using “(degree of) belief” and sometimes “probability”, interchangeably.\nBut beware of this term when you read the literature, or in your scientific discussions. You must try to understand the intended meaning from the context. You’re also free to choose (preferably consistently) the terminology you like most. What’s important is that the notions underlying these words are clear to you.\n\n\n\n\n\n\n Beware of likelihood as a synonym for probability\n\n\n\nIn everyday language, “likelihood” is synonym with “probability”. In technical writings about probability or statistics, however, “likelihood” means something different and is not a synonym of “probability”, as we explain below (§ 8.10.1).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "8  Probability inference",
    "section": "8.4 An unsure inference. Probability notation",
    "text": "8.4 An unsure inference. Probability notation\nConsider now the following variation of the trivial inference problem of § 7.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production either it didn’t pass the heating test or it didn’t pass the shock test. The probability that it passed neither test (that is, both tests failed) is 10%. There’s no reason to believe that the component passed the heating test, more than to believe that it passed the shock test.\n\nAgain the inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\nExercise 8.1\n\n\n\n\nTry to argue why a conclusion cannot be drawn with certainty in this case. One way to argue this is by presenting two different scenarios that fit the given data but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?\n\n\n\nFor this inference problem we cannot find a true or false final value. The truth-inference rules (7.1)–(7.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents a belief in the truth of the sentence which is as strong as the belief in its falsity.\nSecond, let’s symbolically write in the following way that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\):\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "8  Probability inference",
    "section": "8.5 Inference rules",
    "text": "8.5 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (7.1)–(7.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than only for \\(0\\) and \\(1\\).\nThis important result was taken more or less for granted at least since Laplace in the 1700s, but was formally proven for the first time in 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or composite sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n  THE FUNDAMENTAL LAWS OF INFERENCE  \n\n\n\n\n\n\\(\\boldsymbol{\\lnot}\\) “Not” rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 - \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\]\n\n\n\\(\\boldsymbol{\\land}\\) “And” rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n\\(\\boldsymbol{\\lor}\\) “Or” rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nTruth rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\n\nHow to use the rules:\nEach equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – maybe billions of times or more. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is just a study of how to apply these rules to particular kinds of problems.\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\nCh. 2 of Gregory: Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of O’Hagan: Probability\n§§1.0–1.2 of Sivia: Data Analysis\n\nSkim through:\n\nCox 1946: Probability, Frequency and Reasonable Expectation. Try to get the ideas behind the reasoning, even if you can’t follow the mathematical details.\nChs 1–2 of Jaynes: Probability Theory\n\n\n\n\nThe fundamental inference rules are used in the same way as their truth-inference counterpart of [§@truth-inference-rules]: Each equality can be rewritten in different ways according to the usual rules of algebra. The left and right side of the equality thus obtained can replace each other in a proof.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-solution-first-probinference",
    "href": "probability_inference.html#sec-solution-first-probinference",
    "title": "8  Probability inference",
    "section": "8.6 Solution of the uncertain-inference example",
    "text": "8.6 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual, we first analyse it and represent it in terms of atomic sentences; we find what are its proposal and conditional; and we find which initial inferences are given in the problem.\n\n1. Atomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\nThe background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\n2. Proposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\n3. Starting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass the heating test or the shock test (or neither). This is given as a sure fact. Let’s write it as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\tag{8.1}\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\tag{8.2}\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\tag{8.3}\\]\nNote the interesting situation above: we are not given the numerical values of these two probabilities; we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinding the target inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all paths will lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small starting inferences (8.1–2)}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small starting inference (8.3)}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it:\n\\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\nExercise 8.2\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then:\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#sec-algorithm-rules",
    "href": "probability_inference.html#sec-algorithm-rules",
    "title": "8  Probability inference",
    "section": "8.7 Use and implementation of the inference rules",
    "text": "8.7 Use and implementation of the inference rules\nIn the step-wise solution above you noticed that the equations of the fundamental rules were not only used to calculate the probability on their left side, given those on their right side. For example, in the very first step, when we went from the probability\n\\[\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\]\nto the sum\n\\[\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n+ \\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) \\,,\n\\]\nwe used the or-rule rewritten, by simple algebra, as follows:\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}{\\color[RGB]{238,102,119}\\lor} \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\,.\n\\]\nThe four rules in fact represent, first of all, constraints of logical consistency (the precise technical term is coherence) among probabilities. For instance, if we have probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z}) &= 0.1\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) &= 0.7\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) &= 0.2 \\,,\n\\end{aligned}\\]\nthen there must be an inconsistency in the agent’s degrees of belief, because these values violate the and-rule:\n\\[0.2 \\ne 0.1 \\cdot 0.7 \\,.\\]\nWhen this happens, the inconsistency must be found and solved. (However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.)\nThe fundamental rules also imply more general constraints. For example we must always have\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\nExercise 8.3\n\n\n\n\nTry to prove the two constraints above from the four fundamental rules.\nTranslate the two constraints above verbally. Do they make intuitive sense? (For example, the first constraint says, very roughly speaking, that our belief in two things can’t be stronger than our belief in either one of the two things alone.)\n\n\n\n\n\nIn following the step-wise solution you may have been asking yourself after some steps: “OK, why this peculiar step now, and not some other step?”. This is a very intelligent question. You essentially noticed the lack of an algorithm. Compare this situation with the solution of a basic decision problem: there we had a clear sequence of steps to do: writing down some tables, do some element-wise multiplications and then some sums, and finally find the largest of a list of numbers.\nIs there an algorithmic way of drawing inferences, that is, of calculating some target probabilities given some initial ones? If our inference problem involves a finite number of sentences, then the answer is yes, but with some caveats.\nGiven a set of initial probabilities, and a target probability whose value we want to find, there is an algorithm that yields the minimum and the maximum possible values of the target probability. Specifically it can yield the following results (note that some are particular instances of others):\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe complete proof that inference problems can be solved this way seems to have been given first by Hailperin:\n\n(1965): Best Possible Inequalities for the Probability of a Logical Function of Events,\n(1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications,\n\nalthough particular cases were explored already by Boole.\n\n\n\n   No values\n\nThis means that the initial probabilities have inconsistent values; that is, they violate some of the four rules, as in the “\\(0.2 \\ne 0.1 \\cdot 0.7\\)” example above.\n\n\n\n   \\(0\\) and \\(1\\)\n\nThat is, the target probability is completely unspecified. This means that the initial probabilities are not sufficient to determine the target probability. For example, if we have \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.2\\), and \\(\\mathsfit{Y}\\) is a sentence completely unrelated to \\(\\mathsfit{X}\\) (no atomic sentences in common), then \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\) could have any value.\n\n\n\n   Values \\(v\\), \\(V\\) with \\(v &lt; V\\) and at least one different from 0 or 1\n\nThat is, the target probability cannot have any value whatsoever, but a its value is still unspecified. This means that the initial probabilities constrain the target probability somewhat, but do not determine it. The two constraints discussed above are an example of this: if we have \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.2\\) and \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0.3\\), the we can say that \\(0 \\le \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\le 0.2\\), but its precise value is not determined.\n\n\n\n   One value \\(p\\)\n\nThe target probability is completely determined by the initial ones. This occurred in our example inference about the electronic component.\n\n\n\n\nThis algorithm is not mathematically difficult, but it is somewhat involved. It boils down to: (1) writing the sentences in the initial and target probabilities in disjunctive normal form; (2) rewriting the initial and target probabilities as sum of probabilities of basic conjunctions; (3) solving two linear-fractional optimization problems, equivalent to linear optimization oens (for which there are algorithms available).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#an-implementation-in-r",
    "href": "probability_inference.html#an-implementation-in-r",
    "title": "8  Probability inference",
    "section": "8.8 An implementation in R",
    "text": "8.8 An implementation in R\nThe algorithm described in the previous section is implemented in the R function inferP(); it requires the lpSolve R package, which we installed in the R introduction.\nThe inferP() function takes as first target = argument the probability we want to find, and as remaining arguments the values of the given probabilities, or equalities among probabilities. We must use the following notation:\n\n\\(\\lnot\\)  becomes  -\n\\(\\land\\)  becomes  & or equivalently *\n\\(\\lor\\)  becomes  +\n\\(=\\)  becomes  ==\nconditional bar \\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)  becomes  |\n\nThe output are the minimum and maximum possible values of the target probability, or NA if the starting probabilities are inconsistent.\nWe can apply this function in the example above about the electronic component:\n\nsource('inferP.R') ## load the function\n\ninferP(\n    ## target probability:\n    target = P(-h | f & J),\n    ## given probabilities:\n    P(-h + -s | f & J) == 1,\n    P(-h & -s | f & J) == 0.1,\n    P(h | f  & J) == P(s | f & J)\n)\n\n min  max \n0.55 0.55 \n\n\nObviously the algorithm becomes more expensive, the larger the number of sentences in the inference problem. In inference problems involving continuous quantities, such as energy, such an algorithm cannot be applied in practice. Also, the way the algorithm above works cannot be represented as a sequence of “logical” steps with consecutive applications of the rules, as shown in the step-wise solution above. Later on in this course we shall focus on specific kinds of inferences for which other, less opaque inference algorithms are available.\n\n\n\n\n\n\nExercise 8.4\n\n\n\n\nPlay with the function inferP(): test it in other inference problems, find problems where there is no solution and others where the min and max values are different.\nConsider the following inference:\n\n\ninferP(\n    target = P(hypothesis  |  evidence1 & evidence2 & I),\n    P(hypothesis  |  evidence1 & I) == 0.1,\n    P(hypothesis  |  evidence2 & I) == 0.9\n)\n\nmin max \n  0   1 \n\n\nWhat does this result tells us about “combining evidence” to prove a hypothesis?",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "8  Probability inference",
    "section": "8.9 Consequences of not following the rules",
    "text": "8.9 Consequences of not following the rules\nThe fundamental rules of inference guarantee that the agent’s uncertain reasoning is self-consistent, and that it follows logic when there’s no uncertainty. Breaking the rules means that the resulting inference has some logical or irrational inconsistencies.\nThere are many examples of inconsistencies that appear when the rules are broken. Imagine for instance an agent that gives an 80% probability that it rains4 in the next hour; and it also gives a 90% probability that it rains and that the average wind is above 3⋅m/s in the next hour. This is clearly unreasonable, because the raining scenario alone would be true with wind above 3 m/s and also below 3⋅m/s – therefore it should be more probable than the scenario where the wind is above 3 m/s. And indeed the two given probabilities break the and-rule, showing that they are unreasonable or illogical.\n4 to be precise, let’s say “it rains above 1 mm”\n\n\n\n\n\nExercise 8.5\n\n\n\nProve that the two probabilities in the example above break the and-rule.  (Hint: you must use the fact that probabilities are numbers between 0 and 1, and that multiplying a number by something between 0 and 1 can only yield a smaller number.)\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nRead §12.2.3 in Artificial Intelligence\nAs you continue your studies, skim through chapters 4–8 of Hastie & Dawes: Rational Choice in an Uncertain World, just to get the main messages and an overview of curious psychological phenomena.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "8  Probability inference",
    "section": "8.10 Remarks on terminology and notation",
    "text": "8.10 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”, and “likelihood” of “probability”. But in technical writings about probability, inference, and decision-making, “likelihood” has a very different meaning. Beware of this important difference in definitions:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nWe can also say:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\), is \\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\).\nthe likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\), is \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\).\n\n\n\n\n\n\n\n Probability and likelihood can be very different\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of probability.\n\n\nOmitting background information\nIn the analyses of the inference examples of § 7.1 and § 8.4 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that specific background information.\nIn many concrete inference problems the background information usually stays in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening in books where you see “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as\n\\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\]\nand then proceed as usual. If the background information was not explicitly written, this may lead to confusion and mistakes: there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the degrees of belief of different agents.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper in the quantum-theory literature: Brukner & Zeilinger 2000: Conceptual inadequacy of the Shannon information in quantum measurements arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in the scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, and it has several shortcomings:\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\n\nAs discussed in § 8.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nWe want a general AI agent to be able to deal with uncertainty and probability also in situations that do not involve mathematical sets.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown (simple example: we are uncertain about the mass of a particular block of concrete, so we speak of the probability of some mass value; this doesn’t mean that the mass of the block of concrete is changing).\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word. Strangely enough, texts that use that word never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\nFinally, some texts speak of the probability of an “event”. For all purposes, an “event” is just what’s expressed in a sentence.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>[Probability inference]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html",
    "href": "derived_rules.html",
    "title": "9  Shortcut rules",
    "section": "",
    "text": "9.1 Boolean algebra\nThe fundamental rules introduced in chapter  8 are all we need, and all an AI needs, in order to draw inferences from other inferences and from initial data.\nFrom them, however, it is possible to derive some “shortcut” rules than can make the inferences shorter and faster. The situation is similar to what happens with some rules in algebra: for instance, we know that whenever we find the expression\n\\[\n(a+b) \\cdot (a-b)\n\\]\nthen we can automatically substitute it with\n\\[\na^2 - b^2\n\\]\nno matter the values of \\(a\\) and \\(b\\). The rule “\\((a+b) \\cdot (a-b) = a^2-b^2\\)” is not a new algebraic rule: it’s simply the result of the application of the rules for addition \\(+\\) and multiplication \\(\\cdot\\), and indeed we could just apply them directly:\n\\[\n\\begin{aligned}\n(a+b) \\cdot (a-b)\n&=a\\cdot a + b\\cdot a - a\\cdot b - b\\cdot b\n\\\\\n&=a^2 + b\\cdot a - b\\cdot a - b^2\n\\\\\n&=a^2 - b^2\n\\end{aligned}\n\\]\nBut if we remember that they always lead to the result \\(a^2-b^2\\), then we can directly use the “shortcut” rule \\((a+b) \\cdot (a-b) = a^2-b^2\\) and save ourselves some time.\nLikewise with the four rules of inference. Some particular sequences of application of the rules occur very often. We can then simply memorize the starting and final steps of these sequences, and use them directly, skipping all the steps in between. These shortcut rules are not only useful for saving time, however. We shall see that they reveal interesting and intuitive inference patterns, which are implicit in the four inference rules.\nIt is possible and legitimate to implement these shortcut rules in an AI agent, besides the four fundamental ones. Such an agent will arrive at the same results and decisions of an identical AI agent that doesn’t use the shortcut rules – but a little faster.\nHere are the shortcut rules we’ll frequently use in the rest of the course.\nIt is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules of § 8.5. So we can always make the following convenient replacements anywhere in a probability expression:\nFor example, if we have the probability\n\\[\\mathrm{P}[\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Y}) \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} (\\lnot\\lnot\\mathsfit{Z}) \\land \\mathsfit{I}]\\]\nwe can directly replace it with\n\\[\\mathrm{P}[\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}\\land \\mathsfit{I}]\\]\nThe derivation of the Boolean-algebra rules from the four inference rules is somewhat involved. As as example, a partial proof of the rule \\(\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\), called “and-idempotence” goes as follows:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}| \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&&\n&&\\text{\\small truth-rule}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\nand with a similar procedure it can be shown that \\(\\mathsfit{X}\\land \\mathsfit{X}\\) can be replaced with \\(\\mathsfit{X}\\) no matter where it appears. The above proof shows that the and-idempotence rule is tightly connected with the truth-rule of inference.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-boolean",
    "href": "derived_rules.html#sec-boolean",
    "title": "9  Shortcut rules",
    "section": "",
    "text": "Shortcut rules: Boolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-generalized-and",
    "href": "derived_rules.html#sec-generalized-and",
    "title": "9  Shortcut rules",
    "section": "9.2 Generalized and-rule",
    "text": "9.2 Generalized and-rule\nThe and-rule extends in an easy way to any number of and-ed sentences. For example, for four sentences\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\land \\mathsfit{Z}) \\cdot {}&\n\\\\\n\\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\land \\mathsfit{Z}) \\cdot {} &\n\\\\\n\\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}_4 \\land \\mathsfit{Z}) \\cdot {} &\n\\\\\n\\mathrm{P}(\\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &\n\\end{aligned}\n\\]\nThat is, we take the probability of the first sentence given the ones on its left and the background information, times the probability of the second sentence given the ones on its left and the background information, and so on, up to the probability of the last sentence alone given the background information. Keep in mind that, before this operation, the and-ed sentences can be reordered as you please.\n\n\n\n\n\n\nExercise 9.1\n\n\n\nUsing the four fundamental rules, prove the generalized and-rule for three sentences:\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}_3 \\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n(Hint: start by treating \\(\\mathsfit{X}_2 \\land \\mathsfit{X}_3\\) as one sentence and apply the usual and-rule.)",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-generalized-or",
    "href": "derived_rules.html#sec-generalized-or",
    "title": "9  Shortcut rules",
    "section": "9.3 Generalized or-rule",
    "text": "9.3 Generalized or-rule\nAlso the or-rule extends to any number of or-ed sentences. For example, for four sentences:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\lor \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = {}\n\\\\[1ex]\n&\\quad\n\\quad\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[1ex]\n&\\quad\n{}- \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[1ex]\n&\\quad\n{}+ \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}+ \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}+ \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}+ \\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[1ex]\n&\\quad\n{}-\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\land \\mathsfit{X}_4 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\n\nNote the pattern:\n\nfirst we sum the probabilities of all sentences taken individually, with a plus sign;\nthen we sum the probabilities of all possible and-ed pairs, with a minus sign;\nthen we sum the probabilities of all possible and-ed triplets, with a plus sign;\nand so on, alternating the signs, until we arrive at the probability of the and of all sentences.\n\n\n\n\n\n\n\nExercise 9.2\n\n\n\nUsing the four fundamental rules, prove the generalized or-rule for three sentences:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = {}\n\\\\[1ex]\n&\\quad\n\\quad\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[1ex]\n&\\quad\n{}- \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n{}- \\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[1ex]\n&\\quad\n{}+ \\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\nTry to understand where the peculiar alternating signs come from.\n(Hint: start by treating \\(\\mathsfit{X}_2 \\lor \\mathsfit{X}_3\\) as one sentence and apply the usual or-rule. In the expression that you obtain, try to use the or-rule again; then the law from Boolean algebra\n\\[\n\\mathsfit{X}_1 \\land (\\mathsfit{X}_2 \\lor \\mathsfit{X}_3) =\n(\\mathsfit{X}_1 \\land \\mathsfit{X}_2) \\lor (\\mathsfit{X}_1 \\land \\mathsfit{X}_3) \\,,\n\\]\nthen again the or-rule considering and-ed sentences as one sentence.)",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-truth-stable",
    "href": "derived_rules.html#sec-truth-stable",
    "title": "9  Shortcut rules",
    "section": "9.4 Falsity and truth cannot be altered by additional knowledge",
    "text": "9.4 Falsity and truth cannot be altered by additional knowledge\nSuppose that sentence \\(\\mathsfit{X}\\) is judged to be completely impossible, conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\n\\]\nIt can then be proved, from the fundamental rules, that \\(\\mathsfit{X}\\) is also completely impossible if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 0\n\\]\n\n\n\n\n\n\nExercise 9.3\n\n\n\nTry to prove this. (Hint: try using the and-rule one or more times.)\n\n\n What if we use \\(\\lnot\\mathsfit{X}\\) for \\(\\mathsfit{Y}\\), that is, what if we acquire knowledge that \\(\\mathsfit{X}\\) is actually true? Then it can be proved that all probability calculations break down. The problem is that \\(\\lnot\\mathsfit{X}\\) and \\(\\mathsfit{Z}\\) turn out to be mutually contradictory, so all inferences are starting from contradictory premises. You probably know that in formal logic if we start from contradictory premises then we can obtain any conclusion whatsoever. The same happens with probability logic.\nNote that this problem does not arise, however, if \\(\\mathsfit{X}\\) is only extremely improbable conditional on \\(\\mathsfit{Z}\\), say with a probability of \\(10^{-100}\\), rather than flat-out impossible. In practical applications we often approximate extremely small probabilities by \\(0\\), or extremely large ones by \\(1\\). If the probability calculations break down, we must then step back and correct the approximation.\n\nBy using the not-rule it is possible to prove that full certainty about a sentence behaves in a similar manner. If sentence \\(\\mathsfit{X}\\) is judged to be completely certain conditional on sentence \\(\\mathsfit{Z}\\):\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 1\n\\]\nthen, from the fundamental rules, \\(\\mathsfit{X}\\) is also completely certain if we add information to \\(\\mathsfit{Z}\\). That is, for any sentence \\(\\mathsfit{Y}\\) we’ll also have\n\\[\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = 1\n\\]\n\n\n\n\n\n\nShortcut rules: permanence of truth and falsity\n\n\n\n\\[\n\\begin{aligned}\n&\\text{if}\\quad \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) = 0\\text{ or }1\n\\\\\n&\\text{then}\\quad \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\quad\\text{for any $\\mathsfit{Y}$ not contradicting $\\mathsfit{Z}$}\n\\end{aligned}\n\\]",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-extension-conversation",
    "href": "derived_rules.html#sec-extension-conversation",
    "title": "9  Shortcut rules",
    "section": "9.5 Law of total probability or “extension of the conversation”",
    "text": "9.5 Law of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(\\set{\\mathsfit{H}_1, \\mathsfit{H}_2, \\dotsc, \\mathsfit{H}_n}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given some background knowledge \\(\\mathsfit{Z}\\):\n\n\n\\[\n    \\mathrm{P}(\\mathsfit{H}_1\\land\\mathsfit{H}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n    \\mathrm{P}(\\mathsfit{H}_1\\land\\mathsfit{H}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n    \\mathrm{P}(\\mathsfit{H}_{n-1}\\land\\mathsfit{H}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n    \\]\n\n\nThey are exhaustive, meaning that the “or” of all of them is true, given the background knowledge \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{H}_1\\lor \\mathsfit{H}_2 \\lor \\dotsb \\lor \\mathsfit{H}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nIn other words, according to our background knowledge, one of those sentences must be true, but only one.\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{H}_1,\\mathsfit{H}_2,\\dotsc\\):\n\n\n\n\n\n\n\nShortcut rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\\\[2ex]\n&\\quad{}=\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{H}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{H}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\end{aligned}\n\\]\n\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probability of that sentence conditional on several auxiliary “scenarios” or hypotheses1. The name extension of the conversation for this shortcut rule comes from the fact that we are able to call these additional scenarios or hypotheses into play. This situation occurs very often in concrete applications.\n1 this is why we used the symbol \\(\\mathsfit{H}\\) for these sentences",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-bayes-theorem",
    "href": "derived_rules.html#sec-bayes-theorem",
    "title": "9  Shortcut rules",
    "section": "9.6 Bayes’s theorem",
    "text": "9.6 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability of a proposal \\(\\mathsfit{Y}\\) and a conditional \\(\\mathsfit{X}\\) to the probability where their proposal-conditional roles are exchanged:\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\nShortcut rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\nBayes’s theorem is extremely useful when we want to assess the probability of a hypothesis (the proposal) given some data (the conditional), and it is easy to assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) doesn’t always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) doesn’t always need to be “data”.\n\n\n\n\n\n\nExercise 9.4\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead §8.8 of Hastie & Dawes: Rational Choice in an Uncertain World",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-bayes-extension",
    "href": "derived_rules.html#sec-bayes-extension",
    "title": "9  Shortcut rules",
    "section": "9.7 Bayes’s theorem & extension of the conversation",
    "text": "9.7 Bayes’s theorem & extension of the conversation\nBayes’s theorem is often used with several sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nShortcut rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "href": "derived_rules.html#the-many-facets-of-bayess-theorem",
    "title": "9  Shortcut rules",
    "section": "9.8 The many facets of Bayes’s theorem",
    "text": "9.8 The many facets of Bayes’s theorem\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule for “updating an agent’s beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all the agent’s knowledge. The agent’s degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, the agent gets to know – maybe thanks to an observation or measurement – that the sentence \\(\\mathsfit{X}\\) is true. The agent’s whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). The agent’s degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows us to find the agent’s degree of belief about \\(\\mathsfit{Y}\\) conditional on the new state of knowledge, from the one conditional on the old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\nExercise 9.5\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change (note that this is different from the rule of truth-permanence of § 9.4).\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§§4.1–4.3 in Sox & al.: Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of O’Hagan: Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "derived_rules.html#sec-idempotent",
    "href": "derived_rules.html#sec-idempotent",
    "title": "9  Shortcut rules",
    "section": "9.9 Importance of seemingly trivial rules",
    "text": "9.9 Importance of seemingly trivial rules\nSome of the fundamental or shortcut rules may seem obvious or unimportant, but are of extreme importance in data science. For instance, the and-idempotence rule    \\(\\mathsfit{X}\\land\\mathsfit{X}= \\mathsfit{X}\\)   effectively asserts that whenever we draw inferences, redundant information or data is automatically counted only once.\nThis amazing feature saves us from a lot of headaches. Imagine that an AI decision agent at the assembly line has been given the following background information: if an electronic component passes the heating test (\\(\\mathsfit{h}\\)), then its probability of early failure (\\(\\mathsfit{f}\\)) is only 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 0.1\\]\nNow let’s say that a new voltage test has also been devised, and if a component passes this test (\\(\\mathsfit{v}\\)) then its probability of early failure is also 10%:\n\\[\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{Z}) = 0.1\\]\nHowever, it is discovered that the voltage test works in exactly the same way as the heating test – they’re basically the same test! \\(\\mathsfit{v}=\\mathsfit{h}\\). This means that if an element passes the heating test then it will automatically pass the voltage test, and vice versa (they’re the same test!):2\n2 We are assuming that a test, if repeated, will always give the same result.\\[\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z}) = 1\\]\nor equivalently   \\(\\mathsfit{v}\\land \\mathsfit{h}= \\mathsfit{h}\\land \\mathsfit{h}= \\mathsfit{h}\\).\nNow suppose that inadvertently we give our AI agent the redundant information that an electronic component has passed the heating test and the voltage test. What will the agent say about the probability of early failure, given this duplicate information? will it count the test twice? Let’s calculate:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{v}\\land \\mathsfit{h}\\land \\mathsfit{Z})&&\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n}\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= \\frac{\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})}{1}\n=\\mathrm{P}(\\mathsfit{f}\\land \\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small initial probability}\n\\\\[1ex]\n&\\qquad= \\mathrm{P}(\\mathsfit{v}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{h}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small ∧-rule}\n\\\\[1ex]\n&\\qquad= 1 \\cdot\n\\mathrm{P}(\\mathsfit{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{h}\\land \\mathsfit{Z})\n&&\\text{\\small truth cannot be altered}\n\\\\[1ex]\n&\\qquad= 0.1\n&&\\text{\\small initial probability}\n\\end{aligned}\n\\]\nCompare the first and next-to-last lines: in the final probability, the sentence \\(\\mathsfit{v}\\) has been dropped. Thus the AI agent, thanks to the truth-rule or equivalently the and-idempotence rule, correctly detected the redundancy of the sentence \\(\\mathsfit{v}\\) (“the element passed the voltage test”) and automatically discarded it.\n This feature is of paramount importance in machine learning and data-driven engineering: the “features” that we give as an input to a machine-learning classifier could contain redundancies that we don’t recognize, owing to the complexity of the data space. But if the classifier makes inferences according to the four fundamental rules, it will automatically discard any redundant features.\n\n\n\n\n\n\nExercise 9.6\n\n\n\n\nConfirm the result above using the inferP() function introduced in §.  8.7.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>[Shortcut rules]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html",
    "href": "monty.html",
    "title": "10  Monty Hall and related inference problems",
    "section": "",
    "text": "10.1 Motivation: calculation vs intuition\nThe “Monty Hall problem”, inspired by the TV show Let’s make a deal! hosted by Monty Hall, was proposed in the Parade magazine in 1990 (the numbers of the doors are changed here):\nThe web is full of insightful intuitive solutions and of informal probability discussions about this inference problem. Our purpose here is different: we want to solve it mechanically, by applying the fundamental rules of inference (§ 8.5) and the shortcut rules (§ 9) derived from them. No intuitive arguments. Our purpose is different because of two main reasons:\nIt is instructive, however, if you also check what your intuition told you about the problem:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-motivation",
    "href": "monty.html#sec-monty-motivation",
    "title": "10  Monty Hall and related inference problems",
    "section": "",
    "text": "Suppose you are on a game show and given a choice of three doors. Behind one is a car; behind the others are goats. You pick door No. 1, and the host, who knows what is behind them and wouldn’t open the door with the car, opens No. 2, which has a goat. He then asks if you want to pick No. 3. Should you switch?\n\n\n\n\n\n\nWe want to be able to implement or encode the procedure algorithmically in an AI agent.\nWe generally cannot ground inferences on intuition. Intuition is shaky ground, and hopeless in data-science problems involving millions of data with millions of numbers in abstract spaces of millions of dimensions. To solve such complex problems we need to use a more mechanical procedure, a procedure mathematically guaranteed to be self-consistent. That’s the probability calculus. Intuition is only useful for arriving at a method which we can eventually prove, by mathematical and logical means, to be correct; or for approximately explaining a method that we already know, again by mathematical and logical means, to be correct.\n\n\n\n\n\n\n\n Misleading intuition in high dimensions\n\n\n\nAs an example of our intuition can be completely astray in problems involving many data dimensions, consider the following fact.\nTake a one-dimensional Gaussian distribution of probability. You probably know that the probability that a data point is within three standard deviations from the peak is approximately \\(99.73\\%\\). If we take a two-dimensional (symmetric) Gaussian distribution, the probability that a data point (two real numbers) is within three standard deviations from the peak is \\(98.89\\%\\), slightly less than the one-dimensional case. For a three-dimensional Gaussian, the analogous probability is \\(97.07\\%\\), slightly smaller yet.\nNow try to answer this question: for a 100-dimensional Gaussian, what is the probability that a data point is within three standard deviations from the peak? The answer is \\(\\boldsymbol{(1.83 \\cdot 10^{-32})\\%}\\). This probability is so small that you would never observe a data point within three standard deviations from the peak, even if you checked one data point every second for the same duration as the present age of the universe – which is “only” around \\(4\\cdot 10^{17}\\) seconds.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nFor further examples of how our intuition leads us astray in high dimensions see\n\nCounterintuitive Properties of High Dimensional Space\nExercise 2.20 (and its solution) in MacKay: Information Theory, Inference, and Learning Algorithms\n\n\n\n\n\n\n\n\n\n\nExercise 10.1\n\n\n\nExamine what your intuition tells you the answer should be, without spending too much time thinking, just as if you were on the game show. Examine which kind of heuristics your intuition uses. If you already know the solution to this puzzle, try to remember what your intuition told you the first time you faced it. Keep your observations in mind for later on.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-agent",
    "href": "monty.html#sec-monty-agent",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.2 Which agent? whose knowledge?",
    "text": "10.2 Which agent? whose knowledge?\nA sentence can be assigned different probabilities by different agents having different background information, although in some cases different background information can still lead to numerically equal probabilities.\nIn the present case, who’s the agent solving the inference problem? And what background information does it have?\nFrom the problem statement it sounds like you (on the show) are the agent. But we can imagine that you have programmed an AI agent having your same background information, and ready to make the decision for you.\nWe must agree on which background information \\(\\mathsfit{K}\\) to give to this agent. Let’s define \\(\\mathsfit{K}\\) as the knowledge you have right before picking door 1. We make this choice so that we can add your door pick as additional information.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-sentences",
    "href": "monty.html#sec-monty-sentences",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.3 Define the atomic sentences relevant to the problem",
    "text": "10.3 Define the atomic sentences relevant to the problem\nThe following sentences seem sufficient:\n\\[\n\\begin{aligned}\n\\mathsfit{K}&\\coloneqq\\text{\\small[the background knowledge discussed in the previous section]}\n\\\\[1ex]\n\\mathsfit{\\small car1} &\\coloneqq\\textsf{\\small`The car is behind door 1'}\n\\\\\n\\mathsfit{\\small you1} &\\coloneqq\\textsf{\\small`You initially pick door 1'}\n\\\\\n\\mathsfit{\\small host2} &\\coloneqq\\textsf{\\small`The host opens door 2'}\n\\\\\n&\\text{\\small and similarly for the other door numbers}\n\\end{aligned}\n\\]\nWe could have used other symbols for the sentences, for instance “\\(C_1\\)” instead of “\\(\\mathsfit{\\small car1}\\)”. The specific symbol choice doesn’t matter. We could also have stated the sentences slightly differently, for instance “You choose door 1 at the beginning of the game”. What’s important is that we understand and agree on the meaning of the atomic sentences above.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-goal",
    "href": "monty.html#sec-monty-goal",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.4 Specify the desired inference",
    "text": "10.4 Specify the desired inference\nWe want the probabilities of the sentences \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\), given the knowledge that you picked door 1 (\\(\\mathsfit{\\small you1}\\)), that the host opened door 2 (\\(\\mathsfit{\\small host2}\\)), and the remaining background knowledge (\\(\\mathsfit{K}\\)). So in symbols we want the values of the following probabilities:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{aligned}\n\\]\nYou may object: “but we already know that there’s no car behind door 2, the one opened by the host; so that probability is 0%”. That’s correct, but how did you arrive at that probability value? Remember our goal: to solve this inference mechanically. Your intuitive probability must therefore either appear as an initial probability, or be derived via the inference rules. No intuitive shortcuts.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-prior",
    "href": "monty.html#sec-monty-prior",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.5 Specify all initial probabilities",
    "text": "10.5 Specify all initial probabilities\nAs discussed in § 5.2, any inference – logical or uncertain – can only be derived from other inferences, or taken for granted as a starting point (“initial probability”, or “axiom” in logic). The only inferences that don’t need any initial probabilities are tautologies. We must explicitly write down the initial probabilities implicit in the present inference problem:\n\nThe car is for sure behind one of the three doors, and cannot be behind more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small car1} \\lor \\mathsfit{\\small car2} \\lor \\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\nRemember from the shortcut rule for the permanence of truth and falsity (§ 9.4) that the \\(1\\) and \\(0\\) probabilities above do not change if we and additional information to \\(\\mathsfit{K}\\).\nThe host cannot open the door you picked or the door with the car. This translates in several initial probabilities. Here are some:\n\\[\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n  \\end{gathered}\n  \\]\nThe host must open one door, and cannot open more than one door:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host1} \\lor \\mathsfit{\\small host2} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\n\\\\[1ex]\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 0\n\\end{gathered}\n\\]\n\n\nThe probabilities above are all quite clear from the description of the puzzle. But implicit in that description are some more probabilities that will be needed in our inference. The values of these probabilities can be more open to debate, because the problem, as stated, provides ambiguous information. You shall later explore possible alternative values for these probabilities.\n\nIt is equally probable that the car is behind any of the three doors, and your initial pick doesn’t change this uncertainty:\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/3\n  \\end{aligned}\n  \\]\nRemember that a probability is not a physical property. We aren’t saying that the car should appear behind each door with a given frequency, or something similar. The values 1/3 are simply saying that in the present situation you have no reason to believe the car to be behind one specific door more than behind another.\nIf the host can choose between two doors (because the car is behind the door you picked initially), we are equally uncertain about the choice:\n\\[\n\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\n  \\]\n\nThis probability could be analysed into further hypotheses. Maybe the host, out of laziness, could more probably open the door that’s closer. But from the problem it isn’t fully clear which one is closer. The host could also more probably open the door that’s further from the one you choose. The host could have a predetermined scheme on which door to open. The hypotheses are endless. We can imagine some hypotheses that make \\(\\mathsfit{\\small host2}\\) more probable, and some that make \\(\\mathsfit{\\small host3}\\) more probable, conditional on \\(\\mathsfit{\\small you1} \\land \\mathsfit{\\small car1} \\land \\mathsfit{K}\\). The probability of 50% seems like a good compromise. You shall later examine the effects of changing this probability.\n\nSome peculiar probabilities\nWe defined the background knowledge \\(\\mathsfit{K}\\) as the one you have right before choosing door 1. In this way the sentence \\(\\mathsfit{\\small you1}\\), expressing your door pick, can be added as additional information: \\(\\mathsfit{\\small you1}\\land \\mathsfit{K}\\).\nIt is legitimate to ask: what is the probability that you pick door 1, given only the background information \\(\\mathsfit{K}\\):\n\\[\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\ ?\\]\nTo answer this question we would need to specify \\(\\mathsfit{K}\\) more in detail. It is possible, for instance, that you planned to pick door 1 already the day before. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1\\) or very nearly so. Or you could pick door 1 right on the spot, with no clear conscious thought process behind your choice. In this case we would have \\(\\mathrm{P}(\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 1/3\\) or a similar value.\nLuckily in the present problem these probabilities are not needed. If they are used, their numerical values turn out not to matter: they will “cancel out” of the computation.\n\n\n\n\n\n\n Silly literature\n\n\n\nSome texts on probability say that if you have decided something and therefore know for certain it in advance, then the probability of that something is undefined “because it is not random”. Obviously this is nonsense. If you (or more generally an agent) already know something, then the probability of that something – your degree of belief about it – is well-defined and its value is 100%, or something short of this value, if you want to make allowance for the occurrence of other possibilities.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-solution",
    "href": "monty.html#sec-monty-solution",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.6 Solution",
    "text": "10.6 Solution\nLet’s try first to calculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\), that is, the probability that the car is behind the door you picked.\nSeeing that we have several initial probabilities of the “\\(\\mathrm{P}(\\mathsfit{\\small host} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small car} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)” form, we can use Bayes’s theorem together with the “extension of the conversation” (§ 9.7) to swap the positions of “\\(\\mathsfit{\\small car}\\)” and “\\(\\mathsfit{\\small host}\\)” sentences between proposal and conditional. In the present case the exhaustive and mutually exclusive sentences are \\(\\mathsfit{\\small car1}\\), \\(\\mathsfit{\\small car2}\\), \\(\\mathsfit{\\small car3}\\):\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll probabilities in green are initial probabilities discussed in the previous steps. Let’s substitute their values:\n\\[\n\\begin{aligned}\n&\\qquad=\\frac{\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3}\n}{\n\\enspace\\left[\\,\\begin{gathered}\n{\\color[RGB]{34,136,51}1/2} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n{\\color[RGB]{34,136,51}0} \\cdot\n{\\color[RGB]{34,136,51}1/3} +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n{\\color[RGB]{34,136,51}1/3}\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\dotso\n\\end{aligned}\n\\]\nAll that’s left is to find \\(\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\). It’s intuitively clear that this probability is 100%, because the host is forced to choose door 2 if you picked door 1 and the car is behind door 3. But our purpose is to make a fully mechanical derivation, starting from the initial probabilities only. We can find this probability by applying the or-rule and the and-rule to the probabilities that the host opens at least one door and cannot open more than one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\lor \\mathsfit{\\small host1} \\lor \\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}+\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\\n&\\qquad\\quad{}-\n\\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{\\small host1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad= 1 - 0 - 0 + 0 + 0 + 0 - 0 = 1\n\\end{aligned}\n\\]\nas expected.\nFinally, using this probability in our previous calculation we find\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n1/3\n}\n\\\\[1ex]\n&\\qquad=\\frac{ 1/6\n}{\n1/6 +\n1 \\cdot\n1/3\n}\n= \\frac{1/6}{3/6} = \\boldsymbol{\\frac{1}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 1/3 probability that the car is behind the door we picked!\n\nWhat about door 3, that is, the probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)? Also in this case we can use Bayes’s theorem with the extension of the conversation. The calculation is immediate, because we have already calculated all the relevant pieces:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\qquad=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n}{\n\\enspace\\left[\\,\\begin{gathered}\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) +{}\\\\\n\\mathrm{P}(\\mathsfit{\\small host2}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car3} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\\n&\\qquad=\\frac{\n1 \\cdot\n1/3\n}{\n\\enspace\\left[\\,\\begin{gathered}\n1/2 \\cdot\n1/3 +{}\\\\\n0 \\cdot\n1/3 +{}\\\\\n1\n\\cdot\n1/3\n\\end{gathered}\\,\\right]\\enspace\n}\n\\\\[1ex]\n&\\qquad=\\frac{1/3}{1/2} = \\boldsymbol{\\frac{2}{3}}\n\\end{aligned}\n\\]\nthat is, there’s a 2/3 probability that the car is behind door 3. If we’d like to win the car, then we should switch doors.\n\n\n\n\n\n\nExercise 10.2\n\n\n\n\nPerform a similar calculation to find \\(\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\)\nConfirm the result using the inferP() function introduced in § 8.7.\n\n\n\n\nNote that we found these probabilities, and solved the Monty Hall problem, just by applying the fundamental rules of inference (§ 8.5), specifically the and-rule and or-rule, and the Boolean-algebra shortcut rules (§ 9), starting from given probabilities. Here is a depiction of how the fundamental and the shortcut rules connect the initial probabilities, at the top, to the final ones, at the bottom:",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-remarks",
    "href": "monty.html#sec-monty-remarks",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.7 Remarks on the use of Bayes’s theorem",
    "text": "10.7 Remarks on the use of Bayes’s theorem\nYou notice that at several points our calculations could have taken a different path. For instance, in order to find \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) we applied Bayes’s theorem to swap the sentences \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\) in their proposal and conditional positions. Couldn’t we have swapped \\(\\mathsfit{\\small car1}\\) and \\(\\mathsfit{\\small host2}\\land \\mathsfit{\\small you1}\\) instead? That is, couldn’t we have made a calculation starting with\n\\[\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n=\\frac{\n\\mathrm{P}(\\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\\dotso} \\enspace ?\n\\]\nafter all, this is also a legitimate application of Bayes’s theorem.\nThe answer is: yes, we could have, and the final result would have been the same. The self-consistency of the probability calculus guarantees that there are no “wrong steps”, as long as every step is an application of one of the four fundamental rules (or of their shortcuts). The worst that can happen is that we take a longer route – but to exactly the same result. In fact it’s possible that there’s a shorter calculation route to arrive at the probabilities that we found in the previous section. But it doesn’t matter, because it would lead to the same result that we found.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-sensitivity",
    "href": "monty.html#sec-monty-sensitivity",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.8 Sensitivity analysis",
    "text": "10.8 Sensitivity analysis\nIn § 10.5 we briefly discussed possible interpretations or variations of the Monty Hall problem, for which the probability that the host chooses among the available doors 2 and 3 (if the car is behind the door you picked) is different from 50%.\nWhen we want to know how an initial probability value can affect the final probabilities, we can leave its value as a variable, and check how the final probabilities change as we change this variable. This procedure is often called sensitivity analysis. Try to do a sensitivity analysis for the Monty Hall problem:\n\n\n\n\n\n\nExercise 10.3\n\n\n\nInstead of assuming\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1/2\\]\nassign a generic variable value \\(p\\)\n\\[\\mathrm{P}(\\mathsfit{\\small host2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = p\n\\qquad\n\\mathrm{P}(\\mathsfit{\\small host3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small car1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 1-p\\]\nwhere \\(p\\) could be any value between \\(0\\) and \\(1\\).\n\nCalculate \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) as was done in the previous sections, but keeping \\(p\\) as a generic variable. This way you’ll find a probability \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) that depends numerically on \\(p\\); it could be considered as a function of \\(p\\).\nPlot how the value of \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\) depends on \\(p\\), as the latter ranges from \\(0\\) to \\(1\\).\nFor which range of values of \\(p\\) is it convenient to switch door, that is, \\(\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small host2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &lt; 1/2\\) ?\nImagine and describe alternative scenarios or background information that would lead to values of \\(p\\) different from \\(0.5\\).",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "monty.html#sec-monty-variations",
    "href": "monty.html#sec-monty-variations",
    "title": "10  Monty Hall and related inference problems",
    "section": "10.9 Variations and further exercises",
    "text": "10.9 Variations and further exercises\n\n\n\n\n\n\nExercise 10.4\n\n\n\n: other variations - In § 10.2 we decided that the agent in this inference was you, with the knowledge \\(\\mathsfit{K}\\) right before you picked door 1. Try to change the agent: do you arrive at different probabilities?\n+ Consider a person in the audience, right before you picked door 1, as the agent, and re-solve the problem, adjusting all initial probabilities as needed.\n\n+ Consider the *host* as the agent, right before you picked door 1, and re-solve the problem, adjusting all initial probabilities as needed. Note that the host knows for certain where the car is, so you need to provide this additional, secret information. Consider the cases where the car is behind door 1 and behind door 3.\n\n\n\nSuppose a friend of yours, backstage, gave you partial information about the location of the car (you cheater!), which makes you believe that the car should be closer to door 1. Assign the probabilities\n\\[\\begin{aligned}\n\\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 + q\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3\n\\\\\n\\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') &= \\mathrm{P}(\\mathsfit{\\small car3} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\small you1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}') = 1/3 - q\n  \\end{aligned}\n  \\]\nwith \\(0 \\le q \\le 1/3\\) (this background information is different from the previous one, so we denote it \\(\\mathsfit{K}'\\)). Re-solve the problem keeping the variable \\(q\\), and find if there’s any value for \\(q\\) for which it’s best to keep door 1.\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.5\n\n\n\n: making decisions In this chapter we only solved the inference problem for the Monty Hall scenario. We calculated the probabilities of various outcomes. But no decision has been made yet.\n\nAssign utilities to winning the car or winning the goat from the point of view of an agent who values the car more. The available decisions are, of course, “keep door 1” vs “switch to door 3”. Then solve the decision-making problem according to the procedure of § 3.3. What’s the optimal decision?\nNow assign utilities from the point of view of an agent who values the goat more than the car. Then solve the decision-making problem according to the usual procedure. What’s the optimal decision?\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.6\n\n\n\n: the Sleeping Beauty problem\nTake a look at the inference problem presented in this video:\n \nand try to solve it, not using intuition, but using the mechanical procedure and steps as in the Monty Hall solution above.\nNote that the video asks “What do you believe is the probability that the coin came up heads?”. Since probability and degree of belief are the same thing, that is like asking “What do you believe is your belief that the coin came up heads?” which is a redundant or quirky question. Instead, simply answer the question “What is your degree of belief (that is, probability) that the coin came up heads?”.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>[Monty Hall and related inference problems]{.green}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html",
    "href": "connection-2-ML.html",
    "title": "11  Second connection with machine learning",
    "section": "",
    "text": "11.1 “Learning” and “output” from the point of view of inference & decision\nIn these first chapters we have been developing notions and methods about agents that draw inferences and make decisions, sentences expressing facts and information, and probabilities expressing uncertainty and certainty. Let’s draw some first qualitative connections between these notions and notions typically used in machine learning.\nA machine-learning algorithm is usually presented in textbooks as something that first “learns” from some training data, and thereafter performs some kind of task – typically it yields a response or outcome, for example a label, of some kind. More precisely, the training data are instances or examples of the task that the algorithm is expected to perform. These instances have a special status because their details are fully known, whereas new instances, where the algorithm will be applied, have some uncertain aspects. A new instance typically has an ideal or optimal outcome, for example “choosing the correct label”, but this outcome is unknown beforehand. The response given by the algorithm in new instances depends on the algorithm’s internal architecture and parameters (for brevity we shall just use “architecture” to mean both).\nLet’s try to rephrase this description from the point of view of the previous chapters. A machine-learning algorithm is given known pieces of information (the training data), and then forms some kind of connection with a new piece of information of similar kind (the outcome in a new application) that was not known beforehand. The connection depends on the algorithm’s architecture.\nThe remarks above reveal similarities with what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences \\({\\color[RGB]{34,136,51}\\mathsfit{D}_1}, {\\color[RGB]{34,136,51}\\mathsfit{D}_2}, {\\color[RGB]{34,136,51}\\dots}, {\\color[RGB]{34,136,51}\\mathsfit{D}_N}\\), together with some background or built-in information \\(\\color[RGB]{204,187,68}\\mathsfit{I}\\), in order to calculate the probability of a new piece of information of a similar kind, expressed by a sentence \\(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\):\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{D}_{N} \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1 \\color[RGB]{0,0,0}\\land {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\]\nWe can thus consider a first tentative correspondence:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis correspondence seems convincing for architecture and training data: in both cases we’re speaking about the use of pre-existing or built-in information, combined with additional information.\nBut the correspondence is less convincing with regard to the outcome. The “agents” that we have envisioned find the probabilities for several possible “outcomes” or “outputs”; they don’t yield only one output. This indicates that there must also be some decision involved among the possible outcomes.\nWe’ll return to this tentative connection later.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-outputs",
    "href": "connection-2-ML.html#sec-1stconn-outputs",
    "title": "11  Second connection with machine learning",
    "section": "11.2 Why different outputs?",
    "text": "11.2 Why different outputs?\nIn the previous chapters we have seen, over and over, what was claimed at the beginning of these lecture notes: that an inference & decision problem has only one optimal solution. Once we specify the utilities and the initial probabilities of the problem, the fundamental rules of inference and the principle of maximal expected utility lead to one unique answer (unless, of course, there are several optimal ones with equal expected utilities).\nDifferent machine-learning algorithms, trained with the same training data, often give different answers or outputs to the same problem. Where do these differences come from? From the point of view of decision theory there are three possibilities, which don’t exclude one another:\n\nThe initial probabilities given to the algorithms are different. Since the training data are the same, this means that the background information built into one machine-learning algorithm is different from those built into another.\nIt is therefore important to understand what are the built-in background information and initial probabilities of different machine-learning algorithms. The built-in assumptions of an algorithm must match those of the real problem as closely as possible, in order to avoid sub-optimal or even disastrously wrong answers and outputs.\nThe utilities built into one machine-learning algorithm are different from those built into another.\nIt is therefore also important to understand what are the built-in utilities of different machine-learning algorithms. The built-in utilities must also match those of the real problem as closely as possible.\nThe calculations made by the algorithms are approximate, and different algorithms use different approximations. This means that the algorithms don’t arrive at the unique answer determined by decision theory, but to some other answers which may be approximately close to the optimal one – or not!\nIt is therefore important to understand what are the calculation approximations made by different machine-learning algorithms. Some approximations may be too crude for some real problems, and may again lead to sub-optimal or even disastrously wrong answers and outputs.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "connection-2-ML.html#sec-1stconn-preprocess",
    "href": "connection-2-ML.html#sec-1stconn-preprocess",
    "title": "11  Second connection with machine learning",
    "section": "11.3 Data pre-processing and the data-processing inequality",
    "text": "11.3 Data pre-processing and the data-processing inequality\n“Data pre-processing” is a collective name given to very different operations on data before they are used in some algorithm to solve a decision or inference problem. Some of these operations are often said to be essential for the solution of these problems. This statement in not completely true, and needs qualification.\nWe can divide pre-processing procedures in roughly three categories:\n\nInconsistency checks\n\nProcedures in this category make sure that the data are what they were intended to be. For instance, if data should consist of the power outputs of several engines, but one datapoint is the physical weight of an engine, then that “datapoint” is actually no data at all for the present problem. It’s something included by mistake and should be removed. Such procedures are necessary and useful, but they are just consistency checks and do not change the information contained in the proper data.\n\n\n\n\n\n\n\n\n Common erroneous procedures\n\n\n\nIn later chapters we shall say more about some common procedures that are often used erroneously. One example is “tail trimming”, that actually removes proper data and lead to sub-optimal or completely erroneous solutions.\n\n\n\nFormatting\n\nThese procedures make sure that data are in the correct format to be inputted into the algorithm. They may also include rescaling of numerical values for avoiding numerical overflow or underflow errors during computation. Such procedures are often necessary and useful, but they just change the way data are encoded. They do not actually change the information contained in the data.\n\n“Mutilation” or information-alteration\n\nProcedures of this kind alter the content of data. For instance, such a procedure may replace, in a dataset of temperatures, a datapoint having value 20 °C with one having value 25 °C; this is not just a simple rescaling. Procedures of this kind include “de-noising”, “de-biasing”, “de-trending”, “filtering”, “dimensionality reduction” and similar ones (often having noble-sounding names). We must state, clearly and strongly, that within Decision Theory and Probability Theory, such information-altering pre-processing is not necessary , and is in fact detrimental. This is why we call it “mutilation” here.\n\n\nIt is important that you understand that such data pre-processing is not something that one, in principle, has to do in data science. Quite the opposite, in principle we should not do it, because it is a destructive procedure. Such pre-processing is done in order to correct deficiencies of the algorithms currently in use, as discussed below.\nIf we build an “optimal predictor machine” that fully operates according to the four rules of inference (§ 8.5) and of maximization of expected utility, then the data fed into this machine should not be pre-processed with any information-altering procedures. The reason is that the four fundamental rules automatically take care, in an optimal way, of factors such as noise, bias, systematic errors, redundancy. We briefly discussed this fact in § 9.9 and saw a simple example of how redundancy is accounted for by the four inference rules.\nIf we have information about noise or other factors affecting the data, then we should include this information in the background information provided to the “optimal predictor machine”, rather than altering the data given to it. The reason, in intuitive terms, is that the machine does the adjustments while fully exploring the data themselves; so it can “see” more deeply how to make optimal adjustments given the “inner structure” of the data. In the pre-processing phase – as the prefix “pre-” indicates – we don’t have the full picture about the data, so any adjustment risks to eliminate actually useful information.\nMore formally, this is the content of the data-processing inequality from information theory:\n\n\n\n\n\n\nData-processing inequality\n\n\n\n\n“No clever manipulation of the data can improve the inferences that can be made from the data”\n(Elements of Information Theory §2.8)\nor, from a complementary point of view:\n“Data processing can only destroy information”\n(Information Theory, Inference, and Learning Algorithms exercise 8.9)\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through:\n\n§2.8 of Cover & Thomas: Elements of Information Theory\nExercise 8.9 and its solution in MacKay: Information Theory, Inference, and Learning Algorithms\n\n\n\n\nThere are two main, partially connected reasons why one performs “mutilating” pre-processing of data:\n\nThe algorithm used is non-optimal: it’s only using approximations of the four fundamental rules, and therefore cannot remove noise, bias, redundancies, or similar factors in an optimal way, or at all. In this case, pre-processing is an approximate way of correcting the deficiency of the non-optimal algorithm.\nFull optimal processing is computationally too expensive. In this case we try to simplify the optimal calculation by doing, in advance and in a cruder, faster way, some of the “cleaning” that the full calculation would otherwise spend time doing in an optimal way.",
    "crumbs": [
      "[**Inference I**]{.green}",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>[Second connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html",
    "href": "quantities_types.html",
    "title": "12  Quantities and data types",
    "section": "",
    "text": "Motivation for the “Data I” part\nIn the “Inference I” part we surveyed the four fundamental rules of inference, which determine how an agent’s degrees of belief should propagate and be self-consistent. We explored some applications and consequences of the four fundamental rules. The rules can be used with any sentences whatsoever, so their application can be further developed and specialized in a wide spectrum of directions, with applications ranging from robotics to psychology. Each of these possible developments would require by itself a full university course!\nWe shall now restrict our attention to applications typical of “data science” and machine learning, like classification, forecast, prognosis, hypothesis testing, in situations that involve quantifiable and measurable phenomena. For this purpose we focus on sentences of particular kinds, which can express such quantification and measurement. In a sense, we develop a specialized “language” for this kind of situations.\nStill, since we’re nonetheless dealing with sentences (Ch.  6), the probability calculus and its inference rules apply without changes of any kind.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "12  Quantities and data types",
    "section": "12.1 Quantities",
    "text": "12.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of things that we can measure. We represent them by mathematical objects of different kinds. These objects have particular mathematical properties and can undergo particular operations.\nThe different mathematical properties of these things reflect the kind of activities that we can do with them. For instance, colours are represented by particular tuples of numbers. These tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation, called “convex combination”, represents the fact that colours can be obtained by mixing other colours in different proportions.\n\n\n25% #FF0000 + 75% #0000FF = #4000C0\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation. Yet it’s convenient if we find one, so we can discuss the general theory without getting bogged down in individual cases. To this purpose we’ll borrow the term quantity from physics and engineering.\n\n\n\n\n\n\n “Quantity” has very different meanings in different contexts\n\n\n\nThe definition of “quantity” we are using here is similar to the one having the maximum specific level as defined in §1.1 of the International vocabulary of metrology by the Joint Committee for Guides in Metrology.\nUsing the word “quantity” this way is just a convention between us. Other texts and scientists may use other words – for example “variable”, “event”, “state”. When you read a text or listen to a scientist, try to grasp the general idea behind their words.\nAs a general term, we prefer the word “quantity” to a word like “variable”, because the latter may give the idea of something changing in time, and that may very well not be the case (think of the mass of a block of concrete). Same goes with a word like “state”, for the opposite reason.\n\n\n\n\nWe distinguish between a quantity and its value. For instance, a quantity could be:\n“The temperature at the point having GPS coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”;\nand its value could be:\n\\(24\\,\\mathrm{°C}\\). To understand the difference between a quantity and its value, you may think of the quantity as a question, and of the value as the answer to that question:\n(quantity:) “What was the temperature at the point having GPS coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z?”\n(value:) “It was \\(24\\,\\mathrm{°C}\\).”\nThe distinction between a quantity and its value is important and necessary in inference and decision problems, because an agent may not know the value of a particular quantity, while still knowing what the quantity is. In this case the agent can consider every possible value that the quantity could have, and assign a probability to each. The set of possible values is called the domain of the quantity. Think of it as the collection of all meaningful answers that could be given to the question. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nKeep in mind that our definition of quantity is quite general. Here’s another example:\n\nQuantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say 128 × 128 × 3 integers between \\(0\\) and \\(255\\)).\nOne example value is this:  (corresponding to a grid of 128 × 128 × 3 specific numbers). Another example value: .\nDomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values).\n\n\nOther examples of quantities and their domains:\n\nThe distance between two objects in the Solar System at a specific Barycentric Coordinate Time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of a specific online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object at a specific time and place. The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, 0 to 10 000 nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\). Note that the domain in this case is not made of numbers.\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz (that is, 48 000 audio samples per second). The domain could be all possible sequences of 2 880 000 numbers in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket at a specific time and place, with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\nIn these notes we agree that a quantity has one, and only one, actual value.\n\n\n\n\n\n\n “Quantity” vs “variate” vs “variable”\n\n\n\nWe can consider something that changes with time, or with location, or from individual to individual, or from unit to unit. Then this “something” is not a quantity, according to our present terminology, but a collection of quantities: one for each time or location or individual. Later we shall call this collection a variate, especially when it refers to individuals or units; or a variable, especially when it refers to time.\nFor instance, your height at this exact moment is a quantity, but your height throughout your life is a variable, and the height at this moment across all living human beings is a variate.\nThese are just terminological conventions adopted in the present notes. As mentioned before, different scientists often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the three notions that we here call “quantity”, “variate”, “variable”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\), or \\(U\\), or \\(A\\). The sentences that appear in decision-making and inferences are therefore often of the kind:\n“the quantity \\(X\\) was observed to have value \\(x\\)”,\nwhere “\\(x\\)” stands for a specific value. This kind of sentences are often abbreviated in the form “\\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\)”.\n\n\n\n\n\n\n Pay attention to symbol meaning and notation\n\n\n\n\nKeep in mind our discussion from § 6.3: we must make clear what “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” means. It could mean “observed”, “set”, “reported”, and so on.\nNote the subtle difference between \\(X\\), in italics, and \\(\\mathsfit{X}\\), in sans-serif. The first denotes a quantity, the second denotes a sentence. Usually we don’t have to worry too much about these symbol differences, because the meaning of the symbol is clear from the context. But just in case, you know the convention.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-basic-types",
    "href": "quantities_types.html#sec-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.2 Basic types of quantities",
    "text": "12.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts, and with various degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that when we speak of a nominal quantity, it does not make sense to say, for instance, that one value is “twice” or “1.5 times” another; or that one value is “larger” or “later” than another. Nor does it make sense to “add” two quantities. In particular, there is no notion of cumulative probability, quantile, median, average, or standard deviation for a nominal quantity; these are notions that we’ll discuss in Ch.  21.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\njust because \\(3-2=2-1\\), or similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that that when we speak of a nominal quantity, it does not make sense to say that one value is “twice” or “1.5 times” another, and we cannot add or subtract two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, or “larger”, and similar. Owing to the ordering property, it does make sense to speak of cumulative probability, quantile, and median of an ordinal quantity; but there is no notion of average or standard deviation for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what a pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers do not make sense. For this reason the use of numbers to represent an ordinal quantity can be misleading. A less misleading possibility is to represent ordered values by alphabet letters.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that for such a quantity we can say, at the very least, whether the interval or “distance” between one pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether one value is larger than another. We can also take weighted sums of values, called convex combinations (keep in mind that simple addition of values may be meaningless for some quantities).\nOwing to these mathematical properties, it does make sense to speak of the cumulative probability, quantile, median, and also average and standard deviation for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous interval quantity.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue. For example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval type. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, then it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other type.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity: it could be measured, say, in seconds, as precisely as we want. But in a specific situation we could simply label its values as slow, medium, fast, thus turning it into an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\nExercise 12.1\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity.\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nStevens 1946: On the theory of scales of measurement",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "12  Quantities and data types",
    "section": "12.3 Other attributes of basic types",
    "text": "12.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. Ultimately all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. But in practice, in many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes. This can be convenient also because we can then use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and analogously with unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (upper-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\nExercise 12.2\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long a website (or the Earth?) can last, and how many people can live during such an extent of time.\nWhat about the age of a person? What upper bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or an infinite number (at least in theory) of possible values. The domain of a continuous quantity always has an infinite number of values. Note that a domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, because of the way it’s measured. In this case the quantity could be considered discrete rather than continuous.\n\n\n The Iris dataset from its original paper\nFor instance, the famous Iris dataset consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could of course have intermediate values. The age of a person is another frequent example of an in-principle continuous quantity which is often rounded, say to the year or to the month.\nRounding can impact the way we do inferences about such a quantity. In some situations, rounding can lead to quantities with different unrounded values to take on identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities.\n\n\n\n\n\n\n\n\n\nExercise 12.3\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository:\n\nRead the description of the quantities listed in the dataset (sometimes in a readme file included with the dataset download)\nAnalyse the values of some of the quantities in the dataset: check if they can be considered continuous, discrete, or rounded; bounded or unbounded; uncensored or censored; and so on.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-true-quantities",
    "href": "quantities_types.html#sec-true-quantities",
    "title": "12  Quantities and data types",
    "section": "12.4 “True” vs “measured” values",
    "text": "12.4 “True” vs “measured” values\nA difference is often drawn, especially in physics and engineering, between the “true” value of a quantity and the value “observed” or “measured” with a particular measuring instrument. What’s the difference? and how is the “true” value defined? There actually are deep philosophical questions and choices underlying this distinction, and it would take a whole university course to do them justice.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nBridgman: The Logic of Modern Physics\n\n\nIntuitively we define the “true” value as the value that would be measured with an instrument that is perfectly calibrated and as precise as theoretically possible. If we make a distinction between such value and the currently measured value then we’re implying that the current measurement is made with a less precise instrument, and that the “true” and “measured” values could be different.\nIn some circumstances this distinction is unimportant and an agent can use the “measured” value without worries, and consider it as the “true” one. Typically this is the case when the possible discrepancy between measured and true value is enough small to have no consequences. In other circumstances the discrepancy is important: slightly different values might lead to quite different consequences. In such circumstances it is then necessary for the agent to try to infer – using the probability calculus – the true value, using the measured one as “data” or “evidence”. Said otherwise, the agent doesn’t use the measured value directly, but only as an intermediate step to guess the true value. The latter, in turn, can be used for further inferences.\nFrom the point of view of inference and decision-making, the distinction between “true” and “measured” value doesn’t lead to anything methodologically new. It just means that an agent has to do a chain of inferences instead of just one, using the four rules of inference as usual. This situation often requires the definition of two distinct quantities, the “true” and the “observed”, which can have slightly different domains. For instance, we could have a voltage \\(V_\\text{obs}\\) measured with rounding to \\(1\\,\\mathrm{V}\\) and therefore with discrete domain \\(\\set{10\\,\\mathrm{V}, 11\\,\\mathrm{V}, 12\\,\\mathrm{V}, \\dotsc}\\); but we need the “true” voltage \\(V_\\text{true}\\) with a precision of at least \\(0.01\\,\\mathrm{V}\\), so this latter quantity could have a continuous domain.\nIn solving data-science and engineering problems it’s important to make clear whether a particular quantity value can be considered “true” and used as-is, or only “observed” with insufficient precision and used as data to infer the true value.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types.html#sec-metadata",
    "href": "quantities_types.html#sec-metadata",
    "title": "12  Quantities and data types",
    "section": "12.5 Importance of metadata for inference and decision",
    "text": "12.5 Importance of metadata for inference and decision\nThe characteristics of quantities and domains that we have discussed so far are examples of metadata. As the name implies, metadata is information that typically cannot be found in the data.\nAs a simple example, consider this collection of numerical data values:\n8   2   6   19   1   5   4   19   1   8   12   3   1   2   17\nand suppose that some machine-learning algorithm has to generate a new number “similar” to the ones above, or guess what a new one could be. Consider the following possible guesses; would they be acceptable?:\n\n13.  Note that this number does not appear among the values above. Could it be that it is impossible for some particular reason? For example, this value is omitted from the seat numbers of some airlines or street addresses, because of triskaidekaphobia.\n\n\n\n\n\n\n\n\n\n21.  This guess would be impossible if the reported values are rolls of a 20-sided die, typical of fantasy roleplay games. But if the values are, say, the ages of some people, then 21 could be an admissible guess.\n3.5.  The reported values are all integers. But they could be rounded values of say, temperature readings or ages. We don’t know whether a more precise, non-rounded guess such as 3.5 could be acceptable.\n-2.  If the reported values are people’s ages or objects’ weights, then a negative value would be impossible. But if the values were temperatures in degrees Celsius, then the guess -2 could be acceptable.\n\nThe collection of values does not allow us to determine which of the possible scenarios above applies.\n\nA simple piece of metadata can actually correspond to an infinite amount of datapoints. Even if we have a collection of one million positive numbers, they still don’t tell us whether negative values are impossible or not. When we are given the information that the domain of the relevant quantity is positive, this effectively corresponds to knowing that all future data – possibly an infinity – will not be negative.\nAn AI agent or machine-learning algorithm will therefore make better guesses and better decisions if it is given full metadata, besides “training” data. For this reason metadata are extremely important for inference and decision-making, and an optimal agent should make use of metadata.\n\n\n\n\n\n\nExercise 12.4\n\n\n\nExamine the adult-income dataset at the UC Irvine Machine Learning Repository. This set contains more than 30 000 datapoints.\nTake a look at the quantity native_country (what type of variate is this?).\n\nDo you see the values Norway or Finland or Sweden listed among these values?\nDoes this mean that in the adult USA population there are no people coming from these three countries?\nShould an AI agent or machine-learning algorithm exclude these three values from its future guesses?\nWhat would you do to give the algorithm this metadata information? (In a later chapter we shall see how to do this in a rigorous way.)",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>[Quantities and data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html",
    "href": "quantities_types_multi.html",
    "title": "13  Joint quantities and complex data types",
    "section": "",
    "text": "13.1 Joint quantities\nQuantities of more complex types can often be viewed and represented as sets (that is, collections) of quantities of basic and possibly different types.\nA simple collection of quantities of basic types, for instance “age, sex, nationality”, usually does not have any new mathematical properties appearing just because we’re considering those quantities together. We shall call such a collection a joint quantity. Note that a “joint quantity” it is still a quantity, but not a quantity of a basic type.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Its domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, {\\small\\verb;F;}, {\\small\\verb;Norwegian;})\\). The domain could be\n\\[\n[0,+\\infty)\\times\n\\set{{\\small\\verb;F;}, {\\small\\verb;M;}} \\times\n\\set{{\\small\\verb;Afghan;}, {\\small\\verb;Albanian;}, \\dotsc, {\\small\\verb;Zimbabwean;}}\n\\]",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-multiv",
    "href": "quantities_types_multi.html#sec-data-multiv",
    "title": "13  Joint quantities and complex data types",
    "section": "",
    "text": "1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\nExercise 13.1\n\n\n\nConsider again the examples of § 12.1.1. Do you find any examples of joint quantities?",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-complex",
    "href": "quantities_types_multi.html#sec-data-complex",
    "title": "13  Joint quantities and complex data types",
    "section": "13.2 Complex quantities",
    "text": "13.2 Complex quantities\nWe shall call “complex quantity” a quantity that is not of a basic type, nor a collection of quantities of basic type, that is, a joint quantity.\nFamiliar examples of complex quantities are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Other examples are images, sounds, videos.\nNote that a complex quantity may be represented as a collection of quantities of basic type. This collection, however, is “more than the sum of its parts”, in the sense that it has new mathematical properties that do not apply or do not make sense for the single components.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nWe can numerically represent these images as the matrices\n\\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nWith this representation, this quantity is made to correspond to 16 binary digits, or in other words 16 binary quantities.\nFrom the point of view of the individual binary quantities, these three values are “equally different” from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is a new mathematical property that does not belong to any of the 16 binary quantities individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. Machine-learning methods are important because they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or why they may classify correctly but for the wrong reason.",
    "crumbs": [
      "[**Data I**]{.yellow}",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>[Joint quantities and complex data types]{.yellow}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html",
    "href": "probability_distributions.html",
    "title": "14  Probability distributions",
    "section": "",
    "text": "Motivation for the “Inference II” part\nIn the “Data I” part we developed a language, that is, particular kinds of sentences, to approach inferences and probability calculations typical of data-science and engineering problems.\nIn the present part we focus on probability calculations that often occur with this kind of sentences and data. We also focus on how to visually represent such probabilities in useful ways.\nAlways keep in mind that at bottom we’re just using the four fundamental rules of inference over and over again – nothing more than that!",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-distribute-prob",
    "href": "probability_distributions.html#sec-distribute-prob",
    "title": "14  Probability distributions",
    "section": "14.1 Distribution of probability among values",
    "text": "14.1 Distribution of probability among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases – all the possible values that could be the true one.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting the temperature with \\(T\\), these sentences can be abbreviated as\n\\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these five sentences, conditional on the agent’s state of knowledge, which we may denote by the letter \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\). These probabilities could be, for instance, \n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\n\\]\nNote that they sum up to one:\n\\[\n\\begin{aligned}\n&\\quad\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\\\[1ex]\n&=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40}\n\\\\[1ex]\n&=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\nThis collection of probabilities is called a probability distribution, because we are distributing the probability among the possible alternatives.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to the agent.\n\n\n\n\n\n\n\n\n\n\n\nExercise 14.1\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\]\nProve, using the fundamental rules of inferences and any derived rules from § 8, that we must then have\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in § 12.\nWe start with probability distributions over discrete domains.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-discr-prob-distr",
    "href": "probability_distributions.html#sec-discr-prob-distr",
    "title": "14  Probability distributions",
    "section": "14.2 Discrete probability distributions",
    "text": "14.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\):\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, or discrete-interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. The rectangles might be contiguous or not. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents an area equal to 1. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations, for instance when we want to have a clearer idea of how the probability varies across values for ordinal or interval quantities; or when we want to compare several different probability distributions over the same values.\nIn these cases we can use standard line plots, or variations thereof. Compare the following example.\nA technician wonders which component of a laptop failed first (only one can fail at a time), with seven possible alternatives: \\(\\set{{\\small\\verb;hard-drive;}, {\\small\\verb;motherboard;}, {\\small\\verb;CPU;}, {\\small\\verb;keyboard;}, {\\small\\verb;screen;}, {\\small\\verb;graphics-card;}, {\\small\\verb;PCI;}}\\). This is a nominal quantity.\nBefore examining the laptop, the technician’s belief about which component failed first is distributed among the seven alternatives as shown by the blue histogram with solid borders. After a first inspection of the laptop, the technician’s belief has a new distribution, shown by the red histogram with dashed borders:\n\nIt requires some concentration to tell the two probability distributions apart, for example to understand where their peaks are. Let us represent them by two line plots instead: solid blue with circles for the pre-inspection belief distribution, and dashed red with squares for the post-inspection one:\n\nthis line plot displays more cleanly the differences between the two distributions. We see that at first the technician most strongly believed the \\({\\small\\verb;keyboard;}\\) to be the faulty candidate, the second strongest belief being for the \\({\\small\\verb;PCI;}\\). After the preliminary inspection, the technician most strongly believes the \\({\\small\\verb;PCI;}\\) to be the faulty candidate, followed by the \\({\\small\\verb;graphics card;}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-prob-densities",
    "href": "probability_distributions.html#sec-prob-densities",
    "title": "14  Probability distributions",
    "section": "14.3 Probability densities",
    "text": "14.3 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often using linguistic expressions that make only sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that this quantity has some value  \\(x\\)  we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small, because we never have infinite precision. For example, for double-precision values stored in a computer, the width1 must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) . You can check indeed that your computer might not distinguish between two numbers that differ in their 16th decimal digit:\n1 more precisely the relative width#### R code\n## difference in 15th decimal digit\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n## difference in 16th decimal digit\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nThe value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nWhen we consider a distribution of probability for a continuous quantity, the probabilities are therefore distributed among such small ranges, not among single values.\nSince these ranges are very small, they are also very numerous. But the total probability assigned to all of them must still sum up to \\(1\\). This means that each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real-valued quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.000 000 000 000 000 08\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around the value \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is\n\\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\]\nwhich is a more convenient number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still\n\\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942 \\ .\n\\]\nAs you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation:\n\\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\]\nThis definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nIf \\(X\\) is a continuous quantity, the expression “\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\)”. The probability that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2.5\\) exactly is, if anything, zero.\nThat expression instead means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probabilities and probability densities. We shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as\n\\[\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}X\\]\nwhere “\\(\\mathrm{d}X\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity.\n\nPhysical dimensions and units\nIn the International System of Units (SI), “Degree of belief” is considered to be a dimensionless quantity, or more precisely a quantity of dimension “1”. This is why we don’t write units such as “metres” (\\(\\mathrm{m}\\)), “kilograms” (\\(\\mathrm{kg}\\)) or similar together with a probability value.2\n2 See also the material at the International Bureau of Weights and Measures (BIPM)A probability density, however, is defined as the ratio of a probability amount and an interval \\(\\epsilon\\) of some quantity. This latter quantity might well have physical dimensions, say “metres” \\(\\mathrm{m}\\). Then the ratio, which is the probability density, has dimensions \\(1/\\mathrm{m}\\). So probability densities in general have physical dimensions.\nAs another example, suppose that an agent with background knowledge \\(\\mathsfit{I}\\) assigns a degree of belief \\(0.00012\\) to an interval of temperature of width \\(0.0001\\,\\mathrm{°C}\\), around the temperature \\(T = 20\\,\\mathrm{°C}\\). Then the probability density at \\(20\\,\\mathrm{°C}\\) is equal to\n\\[\n\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\frac{0.00012}{0.0001\\,\\mathrm{°C}} = 1.2\\,\\mathrm{°C^{-1}}\n\\]\nIt is an error to report probability densities without their correct physical units. In fact, keeping track of these units is often useful for consistency checks and finding errors in calculations, just like in other engineering or physics calculations.\nOn the other hand, if we write probability densities as previously suggested, in this case as “\\(\\mathrm{p}(T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}20\\mathrm{°C} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}T\\)”, then the density written this way does not need any units: the units “\\(\\mathrm{°C^{-1}}\\)” disappear because multiplied by \\(\\mathrm{d}T\\), which has the inverse units “\\(\\mathrm{°C}\\)”.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-represent-dens",
    "href": "probability_distributions.html#sec-represent-dens",
    "title": "14  Probability distributions",
    "section": "14.4 Representation of probability densities",
    "text": "14.4 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the example plots shown in the margin.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath the curve, then we just have a line-based, or rather curve-based, representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\nKeep in mind that the curve representing the probability density is not quite a function. In fact it’s best to call it a “density” or a “density function”. There are important reasons for keeping this distinction, which have also consequences for probability calculations, but we shall not delve into them for the moment.\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0.1\\), by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range containing 10 lines has twice the probability of a range containing 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\nExercise 14.2\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one of the examples above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain. For example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n@@ TODO Behaviour of representations under transformations of data.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead §§5.3.0–5.3.1 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "14  Probability distributions",
    "section": "14.5 Combined probabilities",
    "text": "14.5 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from Ch.  1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if it’s sold), and represent it by the quantity \\(T\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”:\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\]\nwhich we can shorten to  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsc \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24\\); note the slightly different meaning of the last value.\n\n\n\n\n\n\nExercise 14.3\n\n\n\nWhat is the basic type of the quantity \\(T\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(T\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nTheir values are stored in the file failure_probability.csv and plotted in the histogram on the side.\n\n\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(T\\):\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\[1ex]\n&\\qquad\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad\\qquad{}\\equiv\n(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1) \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2) \\lor \\dotsb \\lor (T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12)\n\\end{aligned}\n\\]\nThe probability needed by the agent is therefore\n\\[\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhich can be calculated using the or-rule, considering that the sentences involved are mutually exclusive:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\lor \\dotsb \\lor T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{t=1}^{12} \\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\nExercise 14.4\n\n\n\nUsing your favourite programming language:\n\nLoad the file failure_probability.csv containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months of use, or after a year of use”.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>[Probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "Rintro2.html",
    "href": "Rintro2.html",
    "title": "Working with R, II",
    "section": "",
    "text": "Plotting pairs of values in R\nR has a wide range of basic plotting functions, and in addition to them there are many plotting libraries. Here we shall see examples of how to create the kinds of plot discussed in chapter  14  Probability distributions, using basic R functions and custom functions used in these notes.\nA very simple kind of plot is the one where we plot a sequence of values against another sequence of the same length, interpreting the first as x-axis and the second as y-axis. This can be done with the base R function plot(). Here we consider sequences a and b, with six values each:\na &lt;- c(1, 2, 3, 4, 5, 6)\nb &lt;- c(0.05, 0.3, 0.4, 0.2, 0.03, 0.02)\n\nplot(x = a, y = b)\nYou notice that the names of the variables appear automatically on the axes, and the pairs of points are represented by small, empty circles. If we want to join the points with lines we can use the type = 'l' argument; with type = 'b' we get both circles and lines. The line width is controlled by the lwd = argument, and the line type (solid, dashed, dotted, and so on) by the lty = argument. The colour by col =. We can also change the axes’ labels with xlab = and ylab =, and the ranges of the two axes with xlim = and ylim =. Let’s see two examples:\nplot(x = a, y = b,\n    type = 'l',\n    lwd = 1, lty = 1, col = 1,\n    xlab = 'quantity A', ylab = 'quantity B',\n    xlim = c(-3, 7), ylim = c(-1, 1))\n\n\n\n\n\n\n\nplot(x = a, y = b,\n    type = 'b',\n    lwd = 3, lty = 2, col = 2,\n    xlab = 'quantity A', ylab = 'quantity B',\n    xlim = c(0.5, 6.5), ylim = c(0, 0.5))\nNote that the sum of b values is 1:\nsum(b)\n\n[1] 1\nso b could be a discrete probability distribution for the quantity a.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, II]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro2.html#sec-R-plotting-pair",
    "href": "Rintro2.html#sec-R-plotting-pair",
    "title": "Working with R, II",
    "section": "",
    "text": "A custom plot function\nIf the quantity a is non-numeric, for instance a sequence of character strings, then we must first somehow convert it to numbers in order to use plot(), otherwise we get an error.\nIn these notes we use the custom function flexiplot(), which takes care of such conversions internally. We can load it from the file tplotfunctions.R, which you should have already downloaded:\n\nsource('tplotfunctions.R')\n\nConsider a quantity animal that could be one of six possible animals, and the distribution of probability over them, assigned to a variable prob. With flexiplot() we can plot the probability distribution as follows:\n\nanimal &lt;- c('cat', 'dog', 'chicken', 'cow', 'pig', 'horse')\nprob1 &lt;- c(0.05, 0.3, 0.4, 0.2, 0.03, 0.02)\n\nflexiplot(x = animal, y = prob1, type = 'b', ylim = c(0, NA))\n\n\n\n\n\n\n\n\nNote how the animal names appear in the x-axis, in the order specified. The range specification ylim = c(0, NA) (not available for plot()) says that the lower range should be \\(0\\), and the upper range should be the maximum available among the y-axis values.\n\nSuppose we need to plot two different probability distributions (or two different sets of y-values more generally), stored in the variables prob1 and prob2, for the same x-values, as in the last plot of § Discrete probability distributions. We can do this by column-binding them together into an array with two columns with the cbind() function, and then giving this array as the y = argument:\n\nprob2 &lt;- c(0.2, 0.6, 0.05, 0.04, 0.01, 0.1)\n\nflexiplot(x = animal, y = cbind(prob1, prob2),\n    type = 'b', ylim = c(0, NA),\n    ylab = 'two probability distributions')\n\n\n\n\n\n\n\n\nWe could also issue two separate flexiplot() commands, giving the argument add = TRUE to the second. But in this case we must make sure to give the correct y-range already in the first plot, and we must also explicitly use different colours col = and line types lty =. Check for instance what would happen for the case above:\n\n## Plot for prob1\nflexiplot(x = animal, y = prob1,\n    type = 'b', ylim = c(0, NA),\n    ylab = 'two probability distributions')\n\n## Plot for prob2\nflexiplot(x = animal, y = prob2,\n    type = 'b', col = 2, lty = 2,\n    add = TRUE)\n\n\n\n\n\n\n\n\nYou see that the curve in the second plot was partly cut off, because the first plot adapted the y-range to the first curve only.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, II]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro2.html#sec-R-plotting-densities",
    "href": "Rintro2.html#sec-R-plotting-densities",
    "title": "Working with R, II",
    "section": "Plotting probability densities in R",
    "text": "Plotting probability densities in R\nIf \\(X\\) is a continuous quantity, we know the mathematical formula for a probability density over its domain, then the density can be plotted as pairs of values, as we did above. We need to generate many values of \\(X\\) in the range of interest, and calculate the probability density for each from the mathematical formula.\nSuppose for instance that \\(X\\) has all real numbers as its domain, and probability density \\(\\mathrm{p}(X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}I)\\) given by a Gaussian, or “normal”, distribution with mean \\(5\\) and standard deviation \\(3\\). Its mathematical formula is\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} I) = \\frac{1}{\\sqrt{2\\pi\\cdot 3^2}}\\,\n\\exp\\biggl[\n-\\frac{(x - 5)^2}{2\\cdot 3^2}\n\\biggr]\\,.\n\\]\nWe’d like to visualize this probability density between \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-7\\) and \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\). Then we proceed as follows:\n\nGenerate a large number of values, say hundreds, for \\(X\\) from \\(-5\\) to \\(10\\). The number of values to generate depends on the resolution at which the final plot will be shown.\nIn this example we’ll generate 257 values using the function seq() with argument length =, and store them in the variable x.\nCalculate the probability density for each \\(X\\) value generated. In R this is often easy, as we can give the whole sequence of numbers to a mathematical function.\nIn R, the formula for the Gaussian distribution is given by the function dnorm() with arguments mean = and sd =.\nPlot the pairs of values generated.\n\n\nx &lt;- seq(from = -7, to = 10, length = 257)\n\np &lt;- dnorm(x, mean = 5, sd = 3)\n\nflexiplot(x = x, y = p, ylim = c(0, NA), ylab = 'p(x | I)')\n\n\n\n\n\n\n\n\nIf we need to display more densities in the same plot we can proceed as described in the previous section.\nR has built-in functions for many common probability densities; they all have the form d...(), take a look at the index. Packages like extraDistr provide even more densities.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, II]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro2.html#sec-R-1Dscatter",
    "href": "Rintro2.html#sec-R-1Dscatter",
    "title": "Working with R, II",
    "section": "1D scatter plots in R",
    "text": "1D scatter plots in R\nWhat if we want to represent a probability density as a scatter plot, as was done in § Representation of probability densities?\nFirst of all we need to generate sample points that reflect the probability density. R has built-in functions to generate sample points for many common densities; they have the form r...() corresponding to the d...() one. For instance, to generate 100 points from a Gaussian density we can use rnorm() with arguments n = for the number of points, and mean =, sd = for the mean and standard deviation. Packages like extraDistr also provide point generators.\nHow many sample points should we generate? The answer is heavily context- and problem-dependent. Too few points, say a dozen, may not give any clear idea of the density. Too many, say thousands, may end up producing a compact blob where no density differences are discernible. It also depends on the structure of the probability density itself. Around a 100 sample points or so may be a good starting point. But if you need to explore regions of the domain where the probability density is low, then you’ll need more points.\nLet’s generate 100 sample points for the Gaussian density of the previous example, storing them in the samples variable:\n\nsamples &lt;- rnorm(n = 100, mean = 5, sd = 3)\n\nThen we need to display these points in a graph.\nR has the built-in function rug() to display the sample points as thin vertical lines on the x-axis. The argument ticksize = specifies the length of these lines: 1 means the whole plot’s height, 0.5 half the plot’s height, and so on. You can also choose colour and line type and width with the usual col =, lty =, lwd = arguments.\nLet’s use this function to add to the previous plot the sample points we just generated (we also redraw the plot):\n\n## Redraw the plot\nflexiplot(x = x, y = p, ylim = c(0, NA), ylab = 'p(x | I)')\n\nrug(samples, ticksize = 0.25, col = 2)\n\nWarning in rug(samples, ticksize = 0.25, col = 2): some values will be\nclipped\n\n\n\n\n\n\n\n\n\nOur custom function flexiplot() can also draw 1D scatter plots. In the present example, we simply need to give it the sample points as the x = argument alone:\n\nflexiplot(x = samples)\n\n\n\n\n\n\n\n\nYou notice that the density around \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5\\) is not very clear; there are too many overlapping circles. We can use small bars instead, with the argument pch = '|':\n\nflexiplot(x = samples, pch = '|')\n\n\n\n\n\n\n\n\nStill another alternative is to generate many sample points and represent them as a cloud of dots, which also extends in the y-direction. This is done by using the arguments pch = '.' and yjitter = TRUE. Let’s first generate 10 000 sample points and then plot them this way:\n\n## generate new sample points\nsamples &lt;- rnorm(n = 10000, mean = 5, sd = 3)\n\nflexiplot(x = samples, pch = '.', yjitter = TRUE)\n\n\n\n\n\n\n\n\n\n\nFinally, let’s suppose we want two visualize and compare two different probability densities for \\(X\\): the Gaussian distribution from before, and a Cauchy distribution. We can do this by generating sample points for both, and then plot them together with flexiplot() as discussed above.\nFirst let’s generate 10 000 samples from the Cauchy distribution. In R this is done with the function rcauchy(), with arguments n =, location =, and scale =. The location and scale determine where the centre of the distribution is located, and how wide it is; we can for instance choose a location equal to \\(5\\), as for the Gaussian, and scale equal to \\(1\\):\n\nsamplescauchy &lt;- rcauchy(n = 10000, location = 5, scale = 1)\n\nNow we plot them together, displaying only the range from \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-10\\) to \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}10\\). We add labels for the two distributions as y = arguments. For both x = and y = argument we use cbind(). Let’s also add some nicer x- and y-labels:\n\nflexiplot(\n    x = cbind(samples, samplescauchy),\n    y = cbind('Gaussian', 'Cauchy'),\n    pch = '.', yjitter = TRUE,\n    xlab = 'x', ylab = 'densities',\n    xlim = c(-10, 10)\n)",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, II]{.lightblue}"
    ]
  },
  {
    "objectID": "joint_probability.html",
    "href": "joint_probability.html",
    "title": "15  Joint probability distributions",
    "section": "",
    "text": "15.1 Joint probability distributions\nSo far we have considered probability distributions for quantities of a basic (binary, nominal, ordinal, interval) type. These distributions have a sort of one-dimensional character and can be represented by ordinary histograms, line plots, and scatter plots. We now consider probability distributions for the kind of joint quantities that were discussed in § 13.1.\nA joint quantity is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the person's sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values:\n\\[\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}31\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{m}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\dotsc\n\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s state of knowledge, and the probabilities sum up to 1. We call each of these probabilities a joint probability, and their collection a joint probability distribution. Usually these probabilities are written in a much abbreviated form. A comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” is typically used instead of “\\(\\land\\)” (§ 6.4). You can commonly encounter the following notation:\n\\[\n\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}25 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}S\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nor even just\n\\[\n\\mathrm{P}(25, \\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-prob",
    "href": "joint_probability.html#sec-repr-joint-prob",
    "title": "15  Joint probability distributions",
    "section": "15.2 Representation of joint probability distributions",
    "text": "15.2 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest of the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a joint quantity consists of two, discrete and finite component quantities, the joint probabilities can be reported as a table, sometimes called a contingency table1.\n1 this term is most often used for joint distributions of frequencies rather than probabilityExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means; and the possibility that the patient will need \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care. We can represent these possibilities by two quantities \\(T\\) (nominal) and \\(U\\) (binary). Now suppose that an agent has the following joint probability distribution, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\n\nTable 15.1: Joint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\n\n\nFrom the table we see that the most probable possibility is that the next patient will arrive by other transportation means than ambulance and helicopter, and will not require urgent care:\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.64\\]\n\n\n Probability distribution over the 27 × 27 possible bigrams \\(xy\\) in an English language document. Probabilities are represented by the areas of white squares. From MacKay’s Information Theory, Inference, and Learning Algorithms\nIn this kind of tables it is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n\n\n\n\n\n\nExercise 15.1\n\n\n\n– never forget the agent! Who could be the agent whose degrees of belief are represented in the table above? What could be the background information leading to such beliefs?\n\n\n\n\nScatter plots and similar\nWe saw in § 14.2 that probability distributions for nominal, ordinal, or discrete-interval quantities can be represented by histograms or line plots. Histograms could be generalized to quantities consisting of two joint discrete quantities: a probability could be represented by a cuboid or rectangular prism, or cylinder, or similar. This representation, even if it can look flamboyant, is often inconvenient because some of the three-dimensional objects can be hidden from view, as in the example in the margin illustration.\n\n\n\n\n Examples of a density histogram and a generalized histogram (from Mathematica)\nAlternatively, one can replace the numerical values of the probabilities in the tabular representation of the previous section with some graphical encoding. An example is a colour scheme with white for probability \\(0\\), black for probability \\(1\\), and grey levels for intermediate probabilities. This is sometimes called a “density histogram”; see the example in the margin figure. This representation can be useful for qualitative or semi-quantitative assessments, for example for seeing which joint values have highest probabilities.\n\nAnother representation, similar to the scatter plot (§ 14.4.2), is to encode the probability values with a proportional number of points or other shapes, as illustrated here for the probabilities of table 15.1:\n\n\n\n\n\n\nFigure 15.1: Scatter plot for the urgency-transportation joint probability distribution\n\n\n\nthe points do not need to be scattered in regular fashion as long as it’s clear which quantity value they are associated with. The scatter plot above has 100 points, and therefore we can see for instance that \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small urgent} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textrm{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}}) =\n0.03\\), since the corresponding region has 3 points out of 100.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-joint-prob-densities",
    "href": "joint_probability.html#sec-joint-prob-densities",
    "title": "15  Joint probability distributions",
    "section": "15.3 Joint probability densities",
    "text": "15.3 Joint probability densities\nIf a joint quantity consists in several continuous interval quantities, then its joint probability distribution is usually represented by a joint probability density, which generalizes the one-dimensional discussion of § 14.3 to several dimensions.\nFor instance, if \\(X\\) and \\(Y\\) are two continuous interval quantities, then the notation\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.001\n\\]\nmeans that the joint sentence “\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\), and \\(Y\\) between \\(y-\\delta/2\\) and \\(y+\\delta/2\\)”, or in symbols\n\\[\n\\bigl(x-\\tfrac{\\epsilon}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} X \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} x+\\tfrac{\\epsilon}{2}\\bigr)\n\\land\n\\bigl(y-\\tfrac{\\delta}{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} Y \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small&lt;}\\nonscript\\mkern 0mu}\\mathopen{} y+\\tfrac{\\delta}{2}\\bigr)\n\\]\nin being given a degree of belief \\(0.001\\cdot\\epsilon\\cdot\\delta\\), conditional on the background knowledge \\(\\mathsfit{I}\\). Visually, the rectangular region of values around \\((x,y)\\) with sides of lengths \\(\\epsilon\\) and \\(\\delta\\) is assigned a probability \\(0.001\\cdot\\epsilon\\cdot\\delta\\).\nRemember that a density typically has physical units, as in the one-dimensional case (§ 14.3). For instance, if \\(X\\) above is a temperature measured in kelvin (\\(\\mathrm{K}\\)) and \\(Y\\) a resistance measured in ohm (\\(\\Omega\\)), then we should write\n\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\frac{0.001}{\\mathrm{K}\\,\\Omega}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-joint-dens",
    "href": "joint_probability.html#sec-repr-joint-dens",
    "title": "15  Joint probability distributions",
    "section": "15.4 Representation of joint probability densities",
    "text": "15.4 Representation of joint probability densities\nFor one-dimensional densities we discussed line-based representations and scatter plots (§ 14.4). The first of these representations can be generalized to two-dimensional densities, leading to a surface plot. Below you see the surface density plot for the probability density given by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\n\nThis kind of representation can be neat, but it has three drawbacks: 1. It sometimes hides from view some features of the density (in the plot above, can you exclude that there’s a small peak right behind the main one?). 2. It cannot be extended to three-dimensional densities. 3. Sometimes the analytical expression for the probability density (like the formula above) is not available.\nThe scatter plot overcomes the three drawbacks above. It does not hides features; it can also be used for three-dimensional densities; it can be generated in cases where the analytical formula of a probability distribution is not available or too complicated, but we can still obtain “representative” points from it. The representation of a scatter plot is, however, quantitatively more imprecise. Here is a scatter plot, using 10 000 points, for the probability density given above:\n\n\n\n\n\n\nFigure 15.2: Scatter-plot representation of the joint probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) above\n\n\n\nThe probability of a small region is proportional to the density of points in that region. If we had a joint density for three continuous quantities, its scatter plot would consist of three-dimensional clouds of points instead.\nClearly both kinds of representation have advantages and disadvantages. The choice between them depends on the problem, on the probability density, and on what we wish to visually emphasize. It is also possible to use both, of course.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-joint-mix-distr",
    "href": "joint_probability.html#sec-joint-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.5 Joint mixed discrete-continuous probability distributions",
    "text": "15.5 Joint mixed discrete-continuous probability distributions\nFrequently occurring in engineering and data-science problems are joint quantities composed by some discrete and some continuous quantities. Their joint probability distribution is a density with respect to the continuous component quantity.\nSuppose for instance that \\(Z\\) is a binary quantity with domain \\(\\set{{\\small\\verb;low;}, {\\small\\verb;high;}}\\), and \\(X\\) a real-valued continuous quantity with domain \\(\\mathbf{R}\\). Together they form the joint quantity \\((Z,X) \\in \\set{{\\small\\verb;low;}, {\\small\\verb;high;}} \\times \\mathbf{R}\\). Then the probability expression\n\\[\n\\mathrm{p}(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0.07\n\\]\nmeans that the agent with background information  \\(\\mathsfit{I}\\)  has a degree of belief equal to   \\(0.07\\cdot \\epsilon\\)   in the joint sentence “\\(Z\\) has value \\({\\small\\verb;low;}\\) and \\(X\\) has value between \\(3-\\epsilon/2\\) and \\(3+\\epsilon/2\\)”.  As usual, this is only valid for any small \\(\\epsilon\\), and if \\(X\\) has physical dimensions, say metres \\(\\mathrm{m}\\), then the probability density above has value  \\(0.07\\,\\mathrm{m^{-1}}\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-mix-distr",
    "href": "joint_probability.html#sec-repr-mix-distr",
    "title": "15  Joint probability distributions",
    "section": "15.6 Representation of mixed probability distributions",
    "text": "15.6 Representation of mixed probability distributions\nMixed discrete-continuous probability distributions can be somewhat tricky to represent graphically. Here we consider line-based representations and scatter plots. We take as example the probability that the next patient who arrives at a particular hospital has a given age (positive continuous quantity) and arrives by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) transportation means (table 15.1).\n\nMulti-line plots\nA line plot can be used to represent the probability density for the continuous quantity and each specific value of the discrete quantity:\n\n\n\n\n\n\nFigure 15.3: Line plot for the age-transportation joint probability distribution (table 15.1)\n\n\n\nWith the plot above it’s important to keep in mind that the three curves are three pieces of the same probability density, not three different densities. This is also clear from the fact that the three areas under them (which partly overlap) cannot each be equal to 1, as would instead be required for a probability density. The probability density is separated into three curves owing to the presence of the discrete quantity, which has three possible values.\nThe area under the solid blue curve is equal to \\(0.55\\), the area under the dashed red curve is \\(0.25\\), and the area under the dotted green curve is \\(0.20\\) . The total area under the three curves (counting also the overlapping regions) is equal to \\(1\\), as it should.\nA possible disadvantage of this kind of plots is that some details, such as peaks, of the densities for some values of the discrete quantity, may be barely discernible.\n\n\nScatter plots\nAs discussed before, in a scatter plot we represent the probability density by a cloud of “representative” objects, such as points, obtained from it. The density of these objects is approximately proportional to the density of probability.\nHere is an example of scatter plot for the probability density of table 15.1:\n\nIn the plot above, the probability density is reflected by the density of vertical lines. Using points instead of vertical lines, the density would have been difficult to discern, since the points would all lie on three lines.\nWe can use points if we give some variation, usually called jitter, to their vertical coordinate; but we must keep in mind that such vertical variation has no meaning. The idea is similar to the one of fig.  15.1. In our current example of table 15.1 we obtain a plot like this:\n\n\n\n\n\n\nFigure 15.4: Point-scatter plot for the age-transportation joint probability\n\n\n\n\n\n\n\n\n\nExercise 15.2\n\n\n\nCompare the line plot of fig.  15.3 and the point-scatter plot of fig.  15.4, which represent the same joint probability density. Do some introspection, and analyse the contrasting impressions that the two kinds of representations may give you. For instance, does the line plot give you a wrong intuition about the sharpness of the peaks in the density?\nCompare with what you did in the exercise of § 14.4.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "joint_probability.html#sec-repr-general-distr",
    "href": "joint_probability.html#sec-repr-general-distr",
    "title": "15  Joint probability distributions",
    "section": "15.7 Representation of more general probability distributions and densities",
    "text": "15.7 Representation of more general probability distributions and densities\nProbability distributions for complex types of quantity can be quite tricky to visualize and represent in an informative way. They typically require a case-by-case approach.\nOften the idea behind the scatter plot works also in these complex cases: the probability distribution or density is represented by a “representative” sample of objects. The objects can even depict the quantity itself.\n\n\n A voltage-current converter\nFor instance, imagine the complex quantity \\(L\\) defined as “the linear relationship between input voltage and output current of a specific electronic component”. The possible values of this quantity are straight lines, that is, functions of the form “\\(y=m\\,x + q\\)”, where \\(x\\) is the input voltage and \\(y\\) the output current. These possible values – straight lines – can differ in their angular coefficient \\(m\\) or in their intercept \\(q\\). One possible value could be the straight line\n\\(y= (2\\,\\mathrm{A/V})\\, x - 3\\,\\mathrm{A}\\)\nanother possible value could be the straight line\n\\(y= (-1\\,\\mathrm{A/V})\\, x + 5\\,\\mathrm{A}\\)\nand so on. The quantity \\(L\\) so defined is a continuous quantity, but it isn’t a quantity of a basic type.\nAn agent may be uncertain about the actual value of \\(L\\), that is, about what is the straight line that correctly expresses the voltage-current relationship of this particular electronic component. The agent therefore assignes a probability density over all possible values: over all possible straight lines. How to visually represent such a “probability density over lines”?\nOne way is to use a scatter plot. The probability distribution is represented by a collection of straight lines, whose density is approximately proportional to the probability density. Here is an example using 360 representative straight lines:\n\n\n\n\n\n\nFigure 15.5: Scatter plot for a probability density over the voltage-current relationship\n\n\n\nFrom this plot we can read some important semi-quantitative information about the agent’s degrees of belief. For instance:\n\nIt’s most probable that the voltage-current relationship has a positive angular coefficient \\(m\\) with value around \\(0.5\\,\\mathrm{A/V}\\), and an intercept \\(q\\) around \\(3\\,\\mathrm{A}\\).\nIt is improbable, but not impossible, that the voltage-current relationship has a negative angular coefficient (that is, the output current decreases as the input voltage is increased).\nIt’s practically impossible that the voltage-current relationship is almost vertical (say, changes in current larger than \\(\\sim 5\\,\\mathrm{A}\\) with changes in voltage smaller than \\(\\sim 0.2\\,\\mathrm{V}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 15.3\n\n\n\n\nExplore datasets in a database such as the UC Irvine Machine Learning Repository, for example\n\nThe adult-income dataset\nThe heart-disease dataset\n\nAssume that the data given are representative “points” of a probability distribution or density (of which we don’t know the analytic formula). Plot the probability distributions and probability densities as scatter plots using some of these representative points.\nLook around for analytic formulae of some probability distributions and densities of simple and joint quantities, and plot them using different representations.\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead\n\n§5.3.2 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§12.2.2 of Artificial Intelligence",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>[Joint probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "Rintro3.html",
    "href": "Rintro3.html",
    "title": "Working with R, III",
    "section": "",
    "text": "Storing joint quantities in R\nIn this R interlude we shall see how to represent tables of joint probability distributions for two quantities (“contingency tables”), how to generate sample points from discrete probability distributions, and how to create 2D scatter plots of such samples.\nIn a previous example we had a quantity \\(\\mathrm{animal}\\) with a domain having six possible values, and a probability distribution over them. The domain and the probability distributions were simply stored in two R variables: animal and prob1:\nanimal &lt;- c('cat', 'dog', 'chicken', 'cow', 'pig', 'horse')\nprob1 &lt;- c(0.05, 0.3, 0.4, 0.2, 0.03, 0.02)\nand we can do many things simply with these: draw plots, generate sample points, and so on.\nNow consider the example from § Representation of joint probability distributions, with two joint quantities: urgency \\(U\\) and transportation \\(T\\). The joint quantity \\((U, T)\\) has a domain with 2 × 3 possible values. Table  15.1 reported a joint probability distribution for this joint quantity.\nHow can we store information about this joint quantity?\nThere are many way to do that. One way is to follow the example of the \\(\\mathrm{animal}\\) quantity, and just create two R variables: one containing all six possible values of \\((U, T)\\), and one containing the six values of the joint probability distribution.\nHere we follow another, slightly more elegant way, which will be implicitly used in later chapters.\nFirst we can store the quantities’ names and their domains as an R list(). It’s actually easier to show this with a concrete example than by wordy explanations. Take the quantity \\(U\\) with domain \\(\\set{\\texttt{urgent}, \\texttt{non-urgent}}\\), and the quantity \\(T\\) with domain \\(\\set{\\texttt{ambulance}, \\texttt{helicopter}, \\texttt{other}}\\). We store the joint quantity \\((U, T)\\) as the list UT:\nUT &lt;- list(\n    U = c('urgent', 'non-urgent'),\n    T = c('ambulance', 'helicopter', 'other')\n)\nEasy! You clearly understand how this could be generalized to three or more quantities with discrete domains. In fact, we could also use this representation for just one quantity. Each domain does not need to consist in character string; it could also be numeric. We can use any quantity names we like, but if they contain special characters such as spaces, hyphens -, and some others, then we must write them within back-ticks `...`, for instance `farm-animal`.\nThe individual domains stored in UT can be accessed by using the $ operator followed by the quantity’s name, as illustrated in this example:\nUT$U\n\n[1] \"urgent\"     \"non-urgent\"\n\nUT$T\n\n[1] \"ambulance\"  \"helicopter\" \"other\"\nOnce we have the variable UT containing name and domain of each quantity, we can also get the domain for the joint quantity by using the R function expand.grid() with UT as argument. We must also add the argument stringsAsFactors = FALSE owing to how we treat some quantities in these notes; we won’t explain its meaning or reason.\nLet’s generate the domain of \\((U, T)\\) and store it in the variable UTdomain:\nUTdomain &lt;- expand.grid(UT, stringsAsFactors = FALSE)\nThe resulting object UTdomain is a so-called data.frame. Let’s simply print it to understand how it looks like:\nUTdomain\n\n           U          T\n1     urgent  ambulance\n2 non-urgent  ambulance\n3     urgent helicopter\n4 non-urgent helicopter\n5     urgent      other\n6 non-urgent      other\nIt has a column for each quantity, and each row shows a combination of values for those quantities. The numbers you see in front of each row are simply printed as reference. The total number of joint values, equal to the number of rows, can be obtained with the nrow() function:\nnrow(UTdomain)\n\n[1] 6\nWe can access any joint value, say the 3rd one, by using square brackets as follows; note the comma:\nUTdomain[3, ]\n\n       U          T\n3 urgent helicopter",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, III]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro3.html#sec-R-store-joint-prob",
    "href": "Rintro3.html#sec-R-store-joint-prob",
    "title": "Working with R, III",
    "section": "Storing joint probability distributions in R",
    "text": "Storing joint probability distributions in R\nHow can we store a joint probability distribution, like that in Table  15.1, for a joint quantity such as UT? There are many ways to do this as well. Let’s see three.\n\nWe can simply create a variable that contains all six probability values. We must make sure that the ordering correspond to that of UTdomain:\n\n\nUTprob1 &lt;- c(0.11, 0.17, 0.04, 0.01, 0.03, 0.64)\n\n(check that the order above is correct by comparing with Table  15.1 and UTdomain).\n\n\nWe can add the probability values as an additional column in the UTdomain object; this is done with cbind() as follows:\n\n\nUTprob2 &lt;- cbind(\n    probability = c(0.11, 0.17, 0.04, 0.01, 0.03, 0.64),\n    UTdomain\n)\n\nTake a look at how the UTprob2 object looks like:\n\nUTprob2\n\n  probability          U          T\n1        0.11     urgent  ambulance\n2        0.17 non-urgent  ambulance\n3        0.04     urgent helicopter\n4        0.01 non-urgent helicopter\n5        0.03     urgent      other\n6        0.64 non-urgent      other\n\n\n\n\n\nWe could use a representation that allows us to read the probabilities just like in Table  15.1. In R this is easily done with the built-in array() function. This function has the following arguments which we use to store our information:\n\n\ndata =: the probabilities values (again in appropriate order).\ndim =: the sizes of the domains of the joint quantities. In our case they are \\(3\\) and \\(2\\).\ndimnames =: The list representing the joint quantity, UT in our case.\n\nLet’s call the resulting object UTtable:\n\nUTtable &lt;- array(\n    data = c(0.11, 0.17, 0.04, 0.01, 0.03, 0.64),\n    dim = c(2, 3),\n    dimnames = UT\n)\n\nHere is how the UTtable object looks like:\n\nUTtable\n\n            T\nU            ambulance helicopter other\n  urgent          0.11       0.04  0.03\n  non-urgent      0.17       0.01  0.64\n\n\nVery neat to read.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, III]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro3.html#sec-R-sample-joint",
    "href": "Rintro3.html#sec-R-sample-joint",
    "title": "Working with R, III",
    "section": "Sampling from discrete joint probability distributions in R",
    "text": "Sampling from discrete joint probability distributions in R\nOur next task is to graphically represent the probability distribution of a joint quantity (for the case of two quantities). Here we focus on [scatter-plots representations](joint_probability.html#scatter-plots-and-similar].\nAs usual, in order to represent a probability distribution as a scatterplot we first need to generate some sample points from it.\nThe easiest way to do this in the case of a joint quantity is to numerically index its possible values, for instance with integers, and then to draw samples of these indices instead. We can then retrieve the quantity values corresponding to the sampled indices. The sampling is done again with the `sample()’ function.\nAll this procedure is more easily explained by a concrete example. Let’s generate 100 samples from the joint domain of the quantity \\((U, T)\\), and then print the first ten:\n\nindexsamples &lt;- sample(nrow(UTdomain), size = 100, prob = UTtable, replace = TRUE)\n\n## print first ten\nindexsamples[1:10]\n\n [1] 6 2 2 6 2 6 2 6 6 6\n\n\nWhen we printed the UTdomain object in the previous section, an indexing from 1 to 6 had already been done for us.\nWe can now prepare a sequence of 100 samples of joint values, using the index samples:\n\nUTsamples &lt;- UTdomain[indexsamples, ]\n\nNote the syntax within the square brackets: we are saying to take the rows – possibly with repetitions – listed in indexsamples; the comma followed by nothing means to leave the columns as they are. Check the first ten rows of UTsamples:\n\nUTsamples[1:10, ]\n\n             U         T\n6   non-urgent     other\n2   non-urgent ambulance\n2.1 non-urgent ambulance\n6.1 non-urgent     other\n2.2 non-urgent ambulance\n6.2 non-urgent     other\n2.3 non-urgent ambulance\n6.3 non-urgent     other\n6.4 non-urgent     other\n6.5 non-urgent     other\n\n\nnote how the most probable value, \\((\\texttt{non-urgent}, \\texttt{other})\\) appears more often. You may also notice the curious indexing on the left, done automatically by R: the number after the period . tells us the numbers of new repetitions of that value. For instance 6.2 tells that that’s the 3rd time the 6th value appears in the list.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, III]{.lightblue}"
    ]
  },
  {
    "objectID": "Rintro3.html#sec-R-plot-joint",
    "href": "Rintro3.html#sec-R-plot-joint",
    "title": "Working with R, III",
    "section": "Plotting joint probability distributions in R",
    "text": "Plotting joint probability distributions in R\nWe can finally generate a scatterplot with the sample points just generated. It’s easily done with the flexiplot() function discussed in our previous R interlude. We extract the \\(U\\)-values by using the $U operator, and give them as y-axis values. Similarly for the \\(T\\)-values, using $T, as x-axis values. It’s important to use the xjitter = TRUE and yitter = TRUE arguments.\n\n## load the function\nsource('tplotfunctions.R')\n\nflexiplot(\n    x = UTsamples$T,\n    y = UTsamples$U,\n    xjitter = TRUE,\n    yjitter = TRUE,\n    xlab = 'T', ylab = 'U'\n)\n\n\n\n\n\n\n\n\nYou notice that the ordering of the \\(U\\)- and \\(T\\)-values on the axes does not match the one we chose initially. To adjust that, we can explicitly specify the xdomain = and ydomain = arguments:\n\n## load the function\nsource('tplotfunctions.R')\n\nflexiplot(\n    x = UTsamples$T,\n    y = UTsamples$U,\n    xjitter = TRUE,\n    yjitter = TRUE,\n    xlab = 'T', ylab = 'U',\n    xdomain = UT$T,\n    ydomain = UT$U\n)\n\n\n\n\n\n\n\n\n\n\nThe same procedure can be used to draw the scatterplot of a joint probability density. As an example, let’s first generate 10 000 sample points for the joint quantity \\((X, Y)\\), where both \\(X\\) and \\(Y\\) are continuous, and store them in a data.frame. Don’t pay too much attention to how the points are generated, as this is a very special case. We then show the first ten samples:\n\nXYsamples &lt;- data.frame(X = rnorm(n = 10000), Y = rcauchy(n = 10000))\n\nXYsamples[1:10, ]\n\n             X         Y\n1   0.00894987  0.550596\n2  -0.99420478  1.700663\n3   2.35361755  2.173250\n4   1.60492517 -2.221509\n5   0.10926966 -0.580429\n6   0.06184837 -0.267218\n7   0.57646086 -0.788744\n8   1.28930182 -0.259627\n9   0.23928751  0.501042\n10  0.03986775  0.875586\n\n\nNow we draw the scatter plot with the points above. With continuous quantities we don’t need to use the ...jitter = arguments. It is often useful to state the x- and y-ranges of the plot explicitly with xlim = and ylim =; note that this means that some sample points may end up outside the plot:\n\nflexiplot(\n    x = XYsamples$X, y = XYsamples$Y,\n    type = 'p', pch = '.',\n    xlim = c(-4, 4), ylim = c(-4, 4)\n)",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Working with R, III]{.lightblue}"
    ]
  },
  {
    "objectID": "marginal_probability.html",
    "href": "marginal_probability.html",
    "title": "16  Marginal probability distributions",
    "section": "",
    "text": "16.1 Marginal probability: neglecting some quantities\nIn some situations an agent has a joint distribution of degrees of belief for the possible values of a joint quantity, but it needs to consider its belief in the value of one component quantity alone, irrespective of what the values for the other components quantities might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table  15.1 from § 15.2, with joint quantity \\((U,T)\\). We may be interested in the probability that the next patient will need \\({\\small\\verb;urgent;}\\) care, independently of how the patient is transported to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from § 8.5.\nThe sentence of interest is “The next patient will require urgent care”, or in symbols\n\\[U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\]\nThis sentence is equivalent to “The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means”, or in symbols\n\\[\nU \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land\n(\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\lor\nT \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\n)\n\\]\nUsing the derived rules of Boolean algebra of § 9.1 we can rewrite this sentence in yet another way:\n\\[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\]\nThis last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule, with the and terms being zero (we shall now use the comma “\\(\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\)” for and):\nWe have found that the probability for a value of the urgency quantity \\(U\\), independently of the value of the transportation quantity \\(T\\), can be calculated by summing all joint probabilities with all possible \\(T\\) values. Using the \\(\\sum\\)-notation we can write this compactly:\n\\[\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{t}\n\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nwhere it’s understood that the sum index \\(t\\) runs over the values \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\).\nThis is called a marginal probability.\nConsidering now a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\), the probability for a specific value of \\(\\color[RGB]{34,136,51}X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(\\color[RGB]{238,102,119}Y\\) might be, is given by\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from § 14.5. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the sentence \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-probs",
    "href": "marginal_probability.html#sec-marginal-probs",
    "title": "16  Marginal probability distributions",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[1ex]\n&\\quad{}=\n\\mathrm{P}\\bigl[\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}) \\lor\n(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\quad{}=\\begin{aligned}[t]\n&\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) +{}\n\\\\\n&\\quad\\mathrm{P}(U \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.1\n\n\n\nUsing the values from table 15.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 16.2\n\n\n\n: test your understanding\nUsing again the values from table 15.1, calculate the probability that the next patient will need urgent care and will be transported either by ambulance or by helicopter.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-dens",
    "href": "marginal_probability.html#sec-marginal-dens",
    "title": "16  Marginal probability distributions",
    "section": "16.2 Marginal density distributions",
    "text": "16.2 Marginal density distributions\nIn the example of the previous section, suppose now that the quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous. Then the joint probability is expressed by a density:\n\\[\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwith the usual meaning. The marginal probability density for \\(\\color[RGB]{34,136,51}X\\) is still given by a sum, but this sum occurs over intervals of values of \\(\\color[RGB]{238,102,119}Y\\), intervals with very small widths. As a consequence the sum will have a very large number of terms. To remind ourselves of this fact, which can be very important in some situations, we use a different notation in terms of integrals:\n\\[\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}y}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) represents the domain of the quantity \\(\\color[RGB]{238,102,119}Y\\).\nThis is called a marginal probability density.\nThe appearance of integrals is sometimes extremely useful, because it allows us to use the theory of integration to calculate marginal probabilities quickly and precisely, instead of having to compute sums of a large numbers of small terms – a procedure that can be computationally expensive and lead to numerical errors owing to underflow or similar computation problems.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-marginal-scatter",
    "href": "marginal_probability.html#sec-marginal-scatter",
    "title": "16  Marginal probability distributions",
    "section": "16.3 Marginal probabilities and scatter plots",
    "text": "16.3 Marginal probabilities and scatter plots\nIn the previous chapters we have often discussed scatter plots (§ 14.4.2, § 15.6.2) for representing probability distributions of various kinds: discrete, continuous, joint, mixed, and so on.\nOne more advantage of representing a joint distribution with a scatter plot is that it can be quickly modified to represent any marginal distribution, again with a scatter plot. Whereas the use of a surface plot would require analytical calculations or approximations thereof.\nConsider for instance the joint probability density from § 15.4, represented by the formula\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\]\n\nand suppose we would like to visualize the marginal probability density for \\(X\\):\n\\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\nIn order to represent this marginal probability density with a line plot, we would first need to calculate the integral of the formula above over \\(Y\\):\n\n\\[\n\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\int_{-\\infty}^{\\infty}\n\\Bigl[\n\\tfrac{3}{8\\,\\pi}\\, \\mathrm{e}^{-\\frac{1}{2} (x-1)^2-(y-1)^2}+\n\\tfrac{3}{64\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{32} (x-2)^2-\\frac{1}{2} (y-4)^2}+ \\tfrac{1}{40\\,\\pi}\\,\\mathrm{e}^{-\\frac{1}{8} (x-5)^2-\\frac{1}{5} (y-2)^2}\n\\Bigr]\n\\, \\mathrm{d}y\n\\]\n\nNow instead suppose that we have stored the points used to represent the joint probability density  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  as a scatter plot, as in fig.  15.2. Each of these points is a pair of coordinates \\((x, y)\\), representing an \\(X\\)-value and a \\(Y\\)-value. It turns out that these same points can be used to make a scatter-plot of the marginal density for \\(X\\), simply by considering their \\(x\\)-coordinates only, that is, by discarding their \\(y\\)-coordinates. Often we use a subsample (unsystematically chosen) of them, so that the resulting one-dimensional scatter plot doesn’t become too congested and difficult to read.\nAs an example, here is a scatter plot for the marginal probability density  \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  above, obtained by selecting a subset of 400 points from the scatter plot (fig.  15.2) for the joint distribution. The points are replaced by vertical lines for better visibility:\n\n\n\n\n\n\nFigure 16.1: Scatter-plot representation of the marginal probability density \\(\\mathrm{p}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n\nExercise 16.3\n\n\n\nThe points for the scatter plot of fig.  15.2 (§ 15.4) are saved in the file scatterXY_samples.csv. Use them to represent the marginal probability density \\(\\mathrm{p}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), for the other quantity \\(Y\\), as a scatter plot.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "marginal_probability.html#sec-use-pitfall-marginal",
    "href": "marginal_probability.html#sec-use-pitfall-marginal",
    "title": "16  Marginal probability distributions",
    "section": "16.4 Uses and pitfalls of marginal probability distributions",
    "text": "16.4 Uses and pitfalls of marginal probability distributions\nAn agent’s distribution of degrees of belief for a multi-dimensional joint quantity is not easily – or at all – visualizable. This shortcoming is especially bad because, as discussed in § 10.1, our intuition often fails us horribly in multi-dimensional problems.\nMarginal probability distributions for one or two of the component quantities are useful because they offer us a little glimpse of the multi-dimensional “monster” distribution. In concrete engineering and data-science problem, when we need to discuss a multi-dimensional distribution it is good practice to visually report at least its one-dimensional marginal distributions.\nIn the machine-learning literature, this low-dimensional glimpse is often used to qualitatively assess whether two multi-dimensional distributions are similar. Their one-dimensional marginals are visually compared and, if they overlap, one hopes (but some works in the literature even erroneously conclude) that the multi-dimensional distributions are somewhat similar as well.\nKeep in mind that this may very well not be the case. Marginal distributions can also be quite deceiving:\n\n\n\n\n\n\nExercise 16.4\n\n\n\nHere are three different joint probability densities for the joint quantity \\((X,Y)\\), each density represented by a scatter plot with 200 points. the files containing the coordinates of the scatter-plot points are also given:\nA. File scatterXY_A.csv:\n\nB. File scatterXY_B.csv:\n\nC. File scatterXY_C.csv:\n\n\n\n\nReproduce the three scatter plots above using the points from the three files, just to confirm that they are correct.\nFor each density, plot the marginal density for the quantity \\(X\\) as a scatter plot. Use the method described in § 16.3; do not subsample the points.\nWhat can you say about the three marginal densities you obtain?\n\nDo the same, but for the marginal densities for \\(Y\\).\nWhat can you say about the three marginal densities you obtain?\nIf two joint probability distributions have the same marginals, can we conclude that they are identical, or at least similar?\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§§5.3.2–5.3.3 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§12.3 of Artificial Intelligence\n\nSkim through:\n\n§§5.1–5.5 of O’Hagan: Probability",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>[Marginal probability distributions]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html",
    "href": "conditional_probability.html",
    "title": "17  Conditional probability and learning",
    "section": "",
    "text": "17.1 The meaning of the term “conditional probability”\nWhen we introduced the notion of degree of belief – a.k.a. probability – in chapter  8, we emphasized that every probability is conditional on some state of knowledge or information. So the term “conditional probability” sounds like a pleonasm, just like saying “round circle”.\nThis term must be understood in a way analogous to “marginal probability”: it applies in situations where we have two or more sentences of interest. We speak of a “conditional probability” when we want to emphasize that additional sentences appear in the conditional (right side of “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”) of that probability. For instance, in a scenario with these two probabilities:\n\\[\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\qquad\n\\mathrm{P}(\\mathsfit{A} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwe call the first conditional probability of \\(\\mathsfit{A}\\) (given \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\)) to emphasize or point out that its conditional includes the additional sentence \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\), whereas the conditional of the second probability doesn’t include this sentence.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-prob_learning",
    "href": "conditional_probability.html#sec-conditional-prob_learning",
    "title": "17  Conditional probability and learning",
    "section": "17.2 The relation between learning and conditional probability",
    "text": "17.2 The relation between learning and conditional probability\nWhy do we need to emphasize that a particular degree of belief is conditional on an additional sentence? Because the additional sentence usually represents new information that the agent has learned.\nRemember that the conditional of a probability usually contains all factual information known to the agent1. Therefore if an agent acquires new data or a new piece of information expressed by a sentence \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), it should draw inferences and make decisions using probabilities that include \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\) in their conditional. In other words, the agent before was drawing inferences and making decisions using some probabilities\n1 Exceptions are, for instance, when the agent does counterfactual or hypothetical reasoning, as we discussed in § 5.1.\\[\n\\mathrm{P}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\]\nwhere \\(\\mathsfit{K}\\) is the agent’s knowledge until then. Now that the agent has acquired information or data \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), it will draw inferences and make decisions using probabilities\n\\[\n\\mathrm{P}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\]\nVice versa, if we see that an agent is calculating new probabilities conditional on an additional sentence \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\), then it means2 that the agent has acquired that information or data \\(\\color[RGB]{204,187,68}\\mathsfit{D}\\).\n2 But keep again in mind exceptions like counterfactual reasoning; see the previous side note.Therefore conditional probabilities represent an agent’s learning and should be used when an agent has learned something.\nThis learning can be of many different kinds. Let’s examine two particular kinds by means of some examples.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-dis",
    "href": "conditional_probability.html#sec-conditional-joint-dis",
    "title": "17  Conditional probability and learning",
    "section": "17.3 Learning about a quantity from a different quantity",
    "text": "17.3 Learning about a quantity from a different quantity\nConsider once more the next-patient arrival scenario of § 15.2, with joint quantity \\((U,T)\\) and an agent’s joint probability distribution as in table  15.1, reproduced here:\n\nJoint probability distribution for transportation and urgency\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\nSuppose that the agent must forecast whether the next patient will require \\({\\small\\verb;urgent;}\\) or \\({\\small\\verb;non-urgent;}\\) care, so it needs to calculate the probability distribution for \\(U\\) (that is, the probabilities for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)).\nIn the first exercise of § 16.1 you found that the marginal probability that the next patient will need urgent care is\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) = 18\\%\\]\nthis is the agent’s degree of belief if it has nothing more and nothing less than the knowledge encoded in the sentence \\(\\mathsfit{I}_{\\text{H}}\\).\nBut now let’s imagine that the agent receives a new piece of information: it is told that the next patient is being transported by helicopter. In other words, the agent has learned that the sentence  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\)  is true. The agent’s complete knowledge is therefore now encoded in the anded sentence\n\\[T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\ \\land\\ \\mathsfit{I}_{\\text{H}}\\]\nand this composite sentence should appear in the conditional. The agent’s belief that the next patient requires urgent care, given the new information, is therefore\n\\[\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\\]\nCalculation of this probability can be done by just one application of the and-rule, leading to a formula connected with Bayes’s theorem (§ 9.6):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\cdot\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\\\[3ex]\n&\\quad\\implies\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\n\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\end{aligned}\n\\]\n\nLet’s see how to calculate this. The agent already has the joint probability for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\land T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) that appears in the numerator of the fraction above. The probability in the denominator is just a marginal probability for \\(T\\), and we know how to calculate that too from § 16.1. So we find\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\sum_u\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\]\nwhere it’s understood that the sum index \\(u\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\).\nThis is called a conditional probability; in this case, the conditional probability of  \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\).\nThe collection of probabilities for all possible values of the quantity \\(U\\), given a specific value of the quantity \\(T\\), say \\({\\small\\verb;helicopter;}\\):\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\qquad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\]\nis called the conditional probability distribution for \\(U\\)  given  \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\). It is indeed a probability distribution because the two probabilities sum up to 1.\n\n\n\n\n\n\n Probabilities with different conditionals are not a probability distribution\n\n\n\nNote that the collection of probabilities for, say, \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), but for different values of the conditional quantity \\(T\\), that is:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}}) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\end{aligned}\n\\]\nis not a probability distribution. Calculate the three probabilities above and check that in fact they do not sum up to 1.\n\n\n\n\n\n\n\n\nExercise 17.1\n\n\n\n\nUsing the values from table 15.1 and the formula for marginal probabilities, calculate:\n\nThe conditional probability that the next patient needs urgent care, given that the patient is being transported by helicopter.\nThe conditional probability that the next patient is being transported by helicopter, given that the patient needs urgent care.\n\nNow discuss and find an intuitive explanation for these comparisons:\n\nThe two probabilities you obtained above. Are they equal? why or why not?\nThe marginal probability that the next patient will be transported by helicopter, with the conditional probability that the patient will be transported by helicopter given that it’s urgent. Are they equal? if not, which is higher, and why?",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-sim",
    "href": "conditional_probability.html#sec-conditional-joint-sim",
    "title": "17  Conditional probability and learning",
    "section": "17.4 Learning about a quantity from instances of similar quantities",
    "text": "17.4 Learning about a quantity from instances of similar quantities\nIn the previous section we examined how learning about one quantity can change an agent’s degree of belief about a different quantity, for example knowledge about “transportation” affects beliefs about “urgency”, or vice versa. The agent’s learning and ensuing belief change are reflected in the value of the corresponding conditional probability.\nThis kind of change can also occur with “similar” quantities, that is, quantities that represent the same kind of phenomenon and have the same domain. The maths and calculations are identical to the ones we explored above, but the interpretation and application can be somewhat different.\nAs an example, imagine a scenario similar to the next-patient arrival above, but now consider the next three patients to arrive and their urgency. Define the following three quantities:\n\\(U_1\\) : urgency of the next patient\n\\(U_2\\) : urgency of the second future patient from now\n\\(U_3\\) : urgency of the third future patient from now\n\nEach of these quantities has the same domain: \\(\\set{{\\small\\verb;urgent;},{\\small\\verb;non-urgent;}}\\).\nThe joint quantity \\((U_1, U_2, U_3)\\) has a domain with \\(2^3 = 8\\) possible values:\n\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n. . .\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\)\n\n\nSuppose that an agent, with background information \\(\\mathsfit{I}\\), has a particular joint belief distribution for the joint quantity \\((U_1, U_2, U_3)\\). For example consider the joint distribution implicitly given as follows: \n\nIf \\({\\small\\verb;urgent;}\\) appears in the probability 0 times out of 3:  probability = \\(53.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 1 times out of 3:  probability = \\(11.4\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 2 times out of 3:  probability = \\(3.6\\%\\)\nIf \\({\\small\\verb;urgent;}\\) appears 3 times out of 3:  probability = \\(1.4\\%\\)\n\nHere are some examples of how the probability values are determined by the description above:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036 \\quad&&\\text{\\small(${\\small\\verb;urgent;}$ appears twice)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.114 &&\\text{\\small(${\\small\\verb;urgent;}$ appears once)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.036 &&\\text{\\small(${\\small\\verb;urgent;}$ appears twice)}\n\\\\[1ex]\n&\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n= 0.536 &&\\text{\\small(${\\small\\verb;urgent;}$ doesn't appear)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nExercise 17.2\n\n\n\n\nCheck that the joint probability distribution as defined above indeed sums up to \\(1\\).\nCalculate the marginal probability for \\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\), that is,  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\). \nCalculate the marginal probability that the second and third patients are non-urgent cases, that is\n\n\\[\\mathrm{P}(U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\ .\\] \n\n\nFrom this joint probability distribution the agent can calculate, among other things, its degree of belief that the third patient will require urgent care, regardless of the urgency of the preceding two patients. It’s the marginal probability\n\\[\n\\begin{aligned}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})  &=\n\\sum_{u_1}\\sum_{u_2}\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]\n&= 0.114 + 0.036 + 0.036 + 0.014\n\\\\[1ex]\n&= \\boldsymbol{20.0\\%}\n\\end{aligned}\n\\]\nwhere each index \\(u_1\\) and \\(u_2\\) runs over the values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\). This double sum therefore involves four terms. The first term in the sum corresponds to “\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)” and therefore has probability \\(0.014\\) . The second term corresponds to “\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\)” and therefore has probability \\(0.036\\). And so on.\nTherefore the agent, with its current knowledge, has a \\(20\\%\\) degree of belief that the third patient will require urgent care.\n\n\nNow fast-forward in time, after two patients have arrived and have been taken good care of; or maybe they haven’t arrived yet, but their urgency conditions have been ascertained and communicated to the agent. Suppose that both patients were or are non-urgent cases. The agent now knows this fact. The agent needs to forecast whether the third patient will require urgent care.\nThe relevant degree of belief is obviously not  \\(\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\),  calculated above, because this belief represents an agent knowing only \\(\\mathsfit{I}\\). Now, instead, the agent has additional information about the first two patients, encoded in this anded sentence:\n\\[\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\n\\]\nThe relevant degree of belief is therefore the conditional probability\n\\[\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nWhich we can calculate with the same procedure as in the previous section:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.114}{0.65}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{17.5\\%}\n\\end{aligned}\n\\]\nThis conditional probability \\(17.5\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\) is lower than \\(20.0\\%\\) calculated previously, which was based only on knowledge \\(\\mathsfit{I}\\). Learning about the two first patients has thus affected the agent’s degree of belief about the third.\n\nLet’s also check how the agent’s belief changes in the case where the first two patients are both urgent instead. The calculation is completely analogous:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[1ex]\n&\\qquad{}=\\frac{0.030}{0.107}\n\\\\[2ex]\n&\\qquad{}\\approx\n\\boldsymbol{28.0\\%}\n\\end{aligned}\n\\]\nIn this case the conditional probability \\(28.0\\%\\) for \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) is higher than the \\(20.0\\%\\), which was based only on knowledge \\(\\mathsfit{I}\\).\nOne possible intuitive explanation of these probability changes, in the present scenario, is that observation of two non-urgent cases makes the agent slightly more confident that “this is a day with few urgent cases”. Whereas observation of two urgent cases makes the agent more confident that “this is a day with many urgent cases”.\n\n\n\n\n\n\n The diversity of inference scenarios\n\n\n\nIn general we cannot say that the probability of a particular value (such as \\({\\small\\verb;urgent;}\\) in the scenario above) will decrease or increase as similar or dissimilar values are observed. Nor can we say how much the increase or decrease will be.\nIn a different situation the probability of \\({\\small\\verb;urgent;}\\) could actually increase as more and more \\({\\small\\verb;non-urgent;}\\) cases are observed. Imagine, for instance, a scenario where the agent initially knows that there are 10 urgent and 90 non-urgent cases ahead (maybe these 100 patients have already been gathered in a room). Having observed 90 non-urgent cases, the agent will give a much higher, in fact 100%, probability that the next case will be an urgent one. Can you see intuitively why this conditional degree of belief must be 100%?\nThe differences among scenarios are reflected in differences in joint probabilities, from which the conditional probabilities are calculated. One particular joint probability can correspond to a scenario where observation of a value increases the degree of belief in subsequent instances of that value. Another particular joint probability can instead correspond to a scenario where observation of a value decreases the degree of belief in subsequent instances of that value.\nAll these situations are, in any case, correctly handled with the four fundamental rules of inference and the formula for conditional probability derived from them!\n\n\n\n\n\n\n\n\nExercise 17.3\n\n\n\n\nUsing the same joint distribution above, calculate\n\\[\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nthat is, the probability that the first patient will require urgent care given that the agent knows the second and third patients will not require urgent care.\n\nWhy is the value you obtained different from  \\(\\mathrm{P}(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) ?\nDescribe a scenario in which the conditional probability above makes sense, and patients 2 and 3 still arrive after patient 1. That is, a scenario where the agent learns that patients 2 and 3 are non-urgent, but still doesn’t know the condition of patient 1.\n\n\n\n\n\nDo an analysis completely analogous to the one above, but with different background information \\(\\mathsfit{J}\\) corresponding to the following joint probability distribution for \\((U_1, U_2, U_3)\\):\n• If \\({\\small\\verb;urgent;}\\) appears 0 times out of 3:  probability = \\(0\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 1 times out of 3:  probability = \\(24.5\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 2 times out of 3:  probability = \\(7.8\\%\\)\n• If \\({\\small\\verb;urgent;}\\) appears 3 times out of 3:  probability = \\(3.1\\%\\)\n\nCalculate\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\] \nand\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\] and compare them.\nFind a scenario for which this particular change in degree of belief makes sense.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-joint-general",
    "href": "conditional_probability.html#sec-conditional-joint-general",
    "title": "17  Conditional probability and learning",
    "section": "17.5 Learning in the general case",
    "text": "17.5 Learning in the general case\nTake the time to review the two sections above, focusing on the application and meaning of the two scenarios and calculations, and noting the similarities and differences:\n\n  The calculations were completely analogous. In particular, the conditional probability was obtained as the quotient of a joint probability and a marginal one.\n  In the first (urgency & transportation) scenario, information about one aspect of the situation changed the agent’s belief about another aspect. The two aspects were different (transportation and urgency). Whereas in the second (three-patient) scenario, information about analogous occurrences of an aspect of the situation changed the agent’s belief about a further occurrence.\n\n\nA third scenario is also possible, which combines the two above. Consider the case with three patients, where each patient can require \\({\\small\\verb;urgent;}\\) care or not, and can be transported by \\({\\small\\verb;ambulance;}\\), \\({\\small\\verb;helicopter;}\\), or \\({\\small\\verb;other;}\\) means. To describe this situation, introduce three pairs of quantities, which together form the joint quantity\n\\[\n(U_1, T_1, \\ U_2, T_2, \\ U_3, T_3)\n\\]\nwhose symbols should be obvious. This joint quantity has \\((2\\cdot 3)^3 = 216\\) possible values, corresponding to all urgency & transportation combinations for the three patients.\nGiven the joint probability distribution for this joint quantity, it is possible to calculate all kinds of conditional probabilities, and therefore consider all the possible ways the agent may learn new information. For instance, suppose the agent learns this:\n\nthe first two patients have not required urgent care\nthe first patient was transported by ambulance\nthe second patient was transported by other means\nthe third patient is arriving by ambulance\n\nand with this learned knowledge, the agent needs to infer whether the third patient will require urgent care. The required conditional probability is\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n}{\n\\mathrm{P}(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\nand is calculated in a way completely analogous to the ones already seen.\n\n\nAll three kinds of inference scenarios that we have discussed occur in data science and engineering. In machine learning, the second scenario is connected to “unsupervised learning”; the third, mixed scenario to “supervised learning”. As you just saw, the probability calculus “sees” all of these scenarios as analogous: information about something changes the agent’s belief about something else. And the handling of all three cases is perfectly covered by the four fundamental rules of inference.\nSo let’s write down the general formula for all these cases of learning.\nLet’s consider a more generic case of a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). Their joint probability distribution is given. Each of these two quantities could be a complicated joint quantity by itself.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), given that the agent has learned that \\(\\color[RGB]{34,136,51}X\\) has some specific value \\(\\color[RGB]{34,136,51}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.1}\\]\nwhere the index \\(\\color[RGB]{238,102,119}\\upsilon\\) runs over all possible values in the domain of \\(\\color[RGB]{238,102,119}Y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-conditional",
    "href": "conditional_probability.html#sec-conditional-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.6 Conditional probabilities as initial information",
    "text": "17.6 Conditional probabilities as initial information\nUp to now we have calculated conditional probabilities, using the derived formula (17.1), starting from the joint probability distribution, which we considered to be given. In some situations, however, an agent may initially possess not a joint probability distribution but conditional probabilities together with marginal probabilities.\nAs an example let’s consider a variation of our next-patient scenario one more time. The agent has background information \\(\\mathsfit{I}_{\\text{S}}\\) that provides the following set of probabilities:\n\nTwo conditional probability distributions  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) for transportation \\(T\\) given urgency \\(U\\), as reported in the following table:\n\n\n\n\nTable 17.1: Probability distributions for transportation given urgency\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\)\n\ntransportation at arrival  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\ngiven urgency  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U\\)\nurgent\n0.61\n0.22\n0.17\n\n\nnon-urgent\n0.21\n0.01\n0.78\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table has two probability distributions: on the first row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\); on the second row, one conditional on \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\). Verify that the probabilities on each row indeed sum up to 1.\n\n\n\n\n\nMarginal probability distribution  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\\) for urgency \\(U\\):\n\n\\[\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.18 \\ ,\n\\quad\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}}) = 0.82\n\\tag{17.2}\\]\n\n\nWith this background information, the agent can also compute all joint probabilities simply using the and-rule. For instance, the joint probability for \\(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) is\n\\[\n\\begin{aligned}\n&P(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}= 0.22 \\cdot 0.18 = \\boldsymbol{3.96\\%}\n\\end{aligned}\n\\]\nAnd from the joint probabilities, the marginal ones for transportation \\(T\\) can also be calculated. For instance\n\\[\n\\begin{aligned}\n&P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n\\\\[1ex]\n&\\quad{}=\n0.22 \\cdot 0.18 +\n0.01 \\cdot 0.82\n\\\\[1ex]\n&\\quad{}= \\boldsymbol{4.78\\%}\n\\end{aligned}\n\\]\n\nNow suppose that the agent learns that the next patient is being transported by \\({\\small\\verb;helicopter;}\\), and needs to forecast whether \\({\\small\\verb;urgent;}\\) care will be needed. This inference is the conditional probability  \\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\), which can also be rewritten in terms of the conditional probabilities given initially:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{H}})\n\\\\[2ex]\n&\\quad{}=\\frac{\n\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}{\n\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{\nP(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}{\n\\sum_u P(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}}) \\cdot\nP(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{S}})\n}\n\\\\[1ex]\n&\\quad{}=\\frac{0.0396}{0.0478}\n\\\\[2ex]\n&\\quad{}=\\boldsymbol{82.8\\%}\n\\end{aligned}\n\\]\nThis calculation was slightly more involved than the one in § 17.3, because in the present case the joint probabilities were not directly available. Our calculation involved the steps  \\(T\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}U \\enspace\\longrightarrow\\enspace T\\land U \\enspace\\longrightarrow\\enspace U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}T\\) .\n\nIn this same scenario, note that if the agent were instead interested, say, in forecasting the transportation means knowing that the next patient requires urgent care, then the relevant degree of belief  \\(\\mathrm{P}(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{S}})\\) would be immediately available and no calculations would be needed.\n\n\nLet’s find the general formula for this case, where the agent’s background information is represented by conditional probabilities instead of joint probabilities.\nConsider a joint quantity with component quantities \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\). The conditional probabilities  \\(\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  and  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  are encoded in the agent from the start.\nThe conditional probability for \\(\\color[RGB]{238,102,119}Y \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), given that the agent has learned that \\(\\color[RGB]{34,136,51}X \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*\\), is then\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{238,102,119}\\upsilon}\n\\mathrm{P}( {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^*}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{P}( {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n}\n\\tag{17.3}\\]\nIn the above formula we recognize Bayes’s theorem from § 9.6.\nThis formula is often exaggeratedly emphasized in the literature; some texts even present it as an “axiom” to be used in situations such as the present one. But we see that this formula is simply a by-product of the four fundamental rules of inference in a specific situation. An AI agent who knows the four fundamental inference rules, and doesn’t know what “Bayes’s theorem” is, will nevertheless arrive at this very formula.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-conditional-dens",
    "href": "conditional_probability.html#sec-conditional-dens",
    "title": "17  Conditional probability and learning",
    "section": "17.7 Conditional densities",
    "text": "17.7 Conditional densities\nThe discussion so far about conditional probabilities extends to conditional probability densities, in the usual way explained in §§15.3 and 16.2.\nIf \\(\\color[RGB]{34,136,51}X\\) and \\(\\color[RGB]{238,102,119}Y\\) are continuous quantities, the notation\n\\[\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = {\\color[RGB]{68,119,170}q}\n\\]\nmeans that, given background information \\(\\mathsfit{I}\\) and given the sentence “\\(\\color[RGB]{34,136,51}X\\) has value between \\(\\color[RGB]{34,136,51}x-\\delta/2\\) and \\(\\color[RGB]{34,136,51}x+\\delta/2\\)”, the sentence “\\(\\color[RGB]{238,102,119}Y\\) has value between \\(\\color[RGB]{238,102,119}y-\\epsilon/2\\) and \\(\\color[RGB]{238,102,119}y+\\epsilon/2\\)” has probability \\({\\color[RGB]{68,119,170}q}\\cdot{\\color[RGB]{238,102,119}\\epsilon}\\), as long as \\(\\color[RGB]{34,136,51}\\delta\\) and \\(\\color[RGB]{238,102,119}\\epsilon\\) are small enough. Note that the small interval \\(\\color[RGB]{34,136,51}\\delta\\) for \\(\\color[RGB]{34,136,51}X\\) is not multiplied by the density \\(\\color[RGB]{68,119,170}q\\).\nThe relation between a conditional density and a joint density or a different conditional density is given by\n\\[\n\\begin{aligned}\n&\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon}\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\\\[1ex]\n&\\quad{}=\n\\frac{\\displaystyle\n\\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\displaystyle\n\\int_{\\color[RGB]{238,102,119}\\varUpsilon} \\mathrm{p}({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\cdot\n\\mathrm{p}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\upsilon} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\mathrm{d}{\\color[RGB]{238,102,119}\\upsilon}\n}\n\\end{aligned}\n\\]\nwhere \\(\\color[RGB]{238,102,119}\\varUpsilon\\) is the domain of \\(\\color[RGB]{238,102,119}Y\\).",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_probability.html#sec-repr-conditional",
    "href": "conditional_probability.html#sec-repr-conditional",
    "title": "17  Conditional probability and learning",
    "section": "17.8 Graphical representation of conditional probability distributions and densities",
    "text": "17.8 Graphical representation of conditional probability distributions and densities\nConditional probability distributions and densities can be plotted in all the ways discussed in chapters 15 and 16. If we have two quantities \\(A\\) and \\(B\\), often we want to compare the different conditional probability distributions for \\(A\\), conditional on different values of \\(B\\):\n\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;one-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\mathrm{P}(A\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} B\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;another-value;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\),\n\\(\\dotsc\\)\n\nand so on. This can be achieved by representing them by overlapping line plots, or side-by-side scatter plots, or similar ways.\n\nIn § 16.3 we saw that if we have the scatter plot for a joint probability density, then from its points we can often obtain a scatter plot for its marginal densities. Unfortunately no similar advantage exists for the conditional densities that can be obtained from a joint density. In theory, a conditional density for \\(Y\\), given that a quantity \\(X\\) has value in some small interval \\(\\delta\\) around \\(x\\), could be obtained by only considering scatter-plot points having \\(X\\) coordinate in a small interval between \\(x-\\delta/2\\) and \\(x+\\delta/2\\). But the number of such points is usually too small and the resulting scatter plot could be very misleading.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§5.4 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n§§12.2.1, 12.3, and 12.5 of Artificial Intelligence\n\nSkim through:\n\n§§4.1–4.3 in Sox & al.: Medical Decision Making\n§§5.1–5.5 of O’Hagan: Probability – yes, once more!",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>[Conditional probability and learning]{.green}</span>"
    ]
  },
  {
    "objectID": "conditional_summary.html",
    "href": "conditional_summary.html",
    "title": "Learning and conditional probability: a summary",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\n\n\nThe previous chapter  17  Conditional probability and learning discussed many concepts that are important for what follows, and for artificial intelligence and machine learning in general. So let’s stop for a moment to emphasize and point out some things to keep in mind.\n\n\n\n What does it means that an agent has “learned”? It means that the agent has acquired new information, knowledge, or data. But this acquisition is not just some passive memory storage. As a result of this acquisition, the agent modifies its degrees of belief about any inferences it needs to draw, and consequently may make different decisions (ch.  3  Basic decision problems).\nThis change is an important aspect of learning. Think of when a person receives useful information; but the person, in actions or word, doesn’t seem to make use of it. We typically say “that person hasn’t learned anything”.\n\n\n\n\n The relation between the acquired knowledge and the change in beliefs is perfectly represented and quantified by conditional probabilities. These probabilities take account of the acquired information in their conditionals (the right side of the bar “\\(\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\)”). And the probability calculus automatically determines how to calculate the modified belief based on this new conditional.\nIn other words, the probability calculus already has everything we need to deal with and calculate “learning”. This is therefore the optimal, self-consistent way to deal with learning. We may use approximate versions of it in some situations, for instance when the computations would be too expensive. But we must keep in mind that such approximations are also deviations from optimality and self-consistency.\n\n\n\n\n The formula for conditional probability – that is, for the belief change corresponding to learning – involves and requires a joint distribution over several possibilities (ch.  15  Joint probability distributions). Therefore such distribution must be somehow built into the agent from the beginning, for the agent to be able to learn.\n\n\n\n\n The beliefs and behaviour arising from learning can be very different, depending on the context. For example, in some situations frequently observing a phenomenon may increase an agent’s belief in observing that phenomenon again; but in other situations such frequent observation may decrease an agent’s belief instead. Both kinds of behaviour can make sense in their specific circumstances.\nThese differences in behaviour are also encoded in the joint distribution built into the agent.\n\n\n\n\n The formula for belief change arising from learning is amazingly flexible and universal: it holds whether the agent is learning about different kinds of quantities or about past instances of similar quantities.\nFrom a machine-learning point of view, this must therefore be the formula underlying the use of “features” by a classifier, as well as its “training”.\nThis formula is moreover extremely simple in principle: it only involves addition and division! Computational difficulties arise from the huge amount of terms that may need to be added in specific data-science problems, not because of complicated mathematics. (A data engineer should keep this in mind, in case new hardware technologies may make it possible to deal with larger number of terms.)",
    "crumbs": [
      "[**Inference II**]{.green}",
      "[Learning and conditional probability: a summary]{.green}"
    ]
  },
  {
    "objectID": "information.html",
    "href": "information.html",
    "title": "18  Information, relevance, independence, association",
    "section": "",
    "text": "18.1 Independence of sentences\nIn an ordinary situation represented by background information \\(\\mathsfit{I}\\), if you have to infer whether a coin will land heads, then knowing that it is raining outside has no impact on your inference. The information about rain is irrelevant for your inference. In other words, your degree of belief about the coin remains the same if you include the information about rain in the conditional.\nIn probability notation, representing “The coin lands heads” with \\(\\mathsfit{H}\\), and “It rains outside” with \\(\\mathsfit{R}\\), this irrelevance is expressed by this equality:\n\\[\n\\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{H} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{R} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nMore generally two sentences \\(\\mathsfit{A}\\), \\(\\mathsfit{B}\\) are said to be mutually irrelevant or informationally independent given knowledge \\(\\mathsfit{I}\\) if any one of these three conditions holds:\nThese three conditions turn out to be equivalent to one another. In the first condition, \\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is undefined if \\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})=0\\), but in this case independence still holds; analogously in the second condition.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-indep-sentences",
    "href": "information.html#sec-indep-sentences",
    "title": "18  Information, relevance, independence, association",
    "section": "",
    "text": "“independEnt” is written with an E, not with an A.\n\n\\(\\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{B}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\\(\\mathrm{P}(\\mathsfit{A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(\\mathsfit{A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(\\mathsfit{B}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\n\n\n\n\n\n\n\n\n Irrelevance is not absolute and is not a physical notion\n\n\n\n\nIrrelevance or independence is not an absolute notion, but relative to some background knowledge. Two sentences may be independent given some background information, and not independent given another.\nIndependence as defined above is an informational or logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the sentences \\(\\mathsfit{A}\\) and \\(\\mathsfit{B}\\). It’s simply stating that information about one does not affect an agent’s beliefs about the other.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-indep-quantities",
    "href": "information.html#sec-indep-quantities",
    "title": "18  Information, relevance, independence, association",
    "section": "18.2 Independence of quantities",
    "text": "18.2 Independence of quantities\nThe notion of irrelevance of two sentences can be generalized to quantities. Take two quantities \\(X\\) and \\(Y\\). They are said to be mutually irrelevant or informationally independent given knowledge \\(\\mathsfit{I}\\) if any one of these three equivalent conditions holds for all possible values \\(x\\) of \\(X\\) and \\(y\\) of \\(Y\\):\n\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)   all \\(x,y\\)\n\n\nNote the difference between independence of two sentences and independence of two quantities. The latter independence involves not just two, but many sentences: as many as the combinations of values of \\(X\\) and \\(Y\\).\nIn fact it may happen that for some particular values \\(x^*\\) of \\(X\\) and \\(y^*\\) \\(Y\\) the probabilities become independent:\n\\[\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y^* \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = \\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x^* \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nwhile at the same time this equality does not occur for other values. In this case the quantities \\(X\\) and \\(Y\\) are not independent given information \\(\\mathsfit{I}\\). The general idea is that two quantities are independent if knowledge about one of them cannot change an agent’s beliefs about the other, no matter what their values might be.\n\n\n\n\n\n\n Irrelevance is not absolute and not physical\n\n\n\n\nAlso in this case, irrelevance or independence is not an absolute notion, but relative to some background knowledge. Two quantities may be independent given some background information, and not independent given another.\nAlso in this case, independence is an informational or logical, not physical, notion. It isn’t stating anything about physical dependence between phenomena related to the quantities \\(X\\) and \\(Y\\). It’s simply stating that information about one quantity does not affect an agent’s beliefs about the other quantity.\n\n\n\n\n\n\n\n\n\nExercise 18.1\n\n\n\nConsider our familiar next-patient inference problem with quantities urgency \\(U\\) and transportation \\(T\\). Assume a different background information \\(\\mathsfit{J}\\) that leads to the following joint probability distribution:\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}t\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\)\n\ntransportation at arrival \\(T\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\nurgency \\(U\\)\nurgent\n0.15\n0.08\n0.02\n\n\nnon-urgent\n0.45\n0.04\n0.26\n\n\n\n\nCalculate the marginal probability distribution \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\) and the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare them. Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\) relevant for inferences about \\(U\\)? \nCalculate the conditional probability distribution \\(\\mathrm{P}(U \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\), and compare it with the marginal \\(\\mathrm{P}(U\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{J})\\). Is the value \\(T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;helicopter;}\\) relevant for inferences about \\(U\\)? \nAre the quantities \\(U\\) and \\(T\\) independent, given the background knowledge \\(\\mathsfit{J}\\)?",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-info-uncertainty",
    "href": "information.html#sec-info-uncertainty",
    "title": "18  Information, relevance, independence, association",
    "section": "18.3 Information and uncertainty",
    "text": "18.3 Information and uncertainty\nThe definition of irrelevance given above appears to be very “black or white”: either two sentences or quantities are independent, or they aren’t. But in reality there is no such dichotomy. We can envisage some scenario \\(\\mathsfit{I}\\) where for instance the probabilities \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) and \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) are extremely close in value, although not exactly equal:\n\\[\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n= \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\delta(x,y)\n\\]\nwith \\(\\delta(x,y)\\) very small. This would mean that knowledge about \\(X\\) modifies an agent’s belief just a little. And depending on the situation such modification could be unimportant. In this situation the two quantities would be “independent” for all practical purposes. Therefore there really are degrees of relevance, rather than a dichotomy “relevant vs irrelevant”.\nThis suggests that we try to quantify such degrees. This quantification would also give a measure of how “important” a quantity can be for inferences about another quantity.\nThis is the domain of Information Theory, which would require a course by itself to be properly explored. In this chapter we shall just get an overview of the main ideas and notions of this theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nElements of Information Theory\n\n\n\n\nThe notion of “degree of relevance” is important in data science and machine learning, because it rigorously quantifies two related, intuitive ideas often occurring in these fields:\n\n“Correlation” or “association”: in its general meaning, it’s the idea that if an agent’s beliefs about some quantity change, then beliefs about another quantity may change as well.\n“Feature importance”: it’s the idea that knowledge about some aspects of a given problem may lead to improved inferences about other aspects.\n\nIn the next section we explore, through examples, some tricky aspects and peculiarities of these ideas. These examples also tell us which kind of properties a quantitative measure for “relevance” or “importance” or “informativeness” should possess.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-importance-scenarios",
    "href": "information.html#sec-importance-scenarios",
    "title": "18  Information, relevance, independence, association",
    "section": "18.4 Exploring “importance”: some scenarios",
    "text": "18.4 Exploring “importance”: some scenarios\nThe following examples are only meant to give an intuitive motivation of the notions presented later.\n\nFirst two required properties: a lottery scenario\nA lottery comprises 1 000 000 tickets numbered from 000000 to 999999. One of these tickets is the winner. Its number is already known, but unknown to you. You are allowed to choose any ticket you like (you can see the ticket numbers), before the winning number is announced.\nBefore you choose, you have the possibility of getting for free some clues about the winning number. The clues are these:\n\nClue A:       \n\nThe first four digits of the winning number.\n\nClue B:       \n\nThe 1st, 2nd, 3rd, and 5th digits of the winning number.\n\nClue C:       \n\nThe last three digits of the winning number.\n\n\nNow consider the following three “clue scenarios”.\n\nScenario 1: choose one clue\nYou have the possibility of choosing one of the three clues above. Which would you choose, in order of importance?\nObviously A or B are the most important, and equally important, because they increase your probability of winning from 1/1 000 000 to 1/100. C is the least important because it increases your probability of winning to 1/1000.\n\n\nScenario 2: discard one clue\nAll three clues are put in front of you (but you can’t see their digits). If you could keep all three, then you’d win for sure because they would give you all digits of the winning number.\nYou are instead asked to discard one of the three clues, keeping the remaining too. Which would you discard, in order of least importance?\nIf you discarded A, then B and C together would give you all digits of the winning number; so you would still win for sure. Analogously if you discarded B. If you discarded C, then A and B together would not give you the last digit; so you’d have a 1/10 probability of winning.\nObviously C is the most important clue to keep, and A and B are the least important.\n\n\nScenario 3: discard one more clue\nIn the previous Scenario 2, we saw that discarding A or B would not alter your 100% probability of winning. Either clue could therefore be said to have “importance = 0”.\nIf you had to discard both A and B, however, your situation would suddenly become worse, with only a 1/1000 probability of winning. Clues A and B together can therefore be said to have high “importance &gt; 0”.\n\nLet’s draw some conclusions by comparing the scenarios above.\n\nIn Scenario 1 we found that the “importance ranking” of the clues is\nA = B &gt; C\nwhereas in Scenario 2 we found the completely opposite ranking\nC &gt; A = B\nWe conclude:\n\n\n\n\n\n\nImportance is context-dependent\n\n\n\n\nIt doesn’t make sense to ask which aspect or feature is “most important” if we don’t specify the context of its use. Important if used alone? Important if used with others? and which others?\nDepending on the context, an importance ranking could be completely reversed. A quantitative measure of “importance” must therefore take the context into account.\n\n\n\n\nIn Scenario 3 we found that two clues may be completely unimportant if considered individually, but extremely important if considered jointly.\nWe conclude:\n\n\n\n\n\n\nImportance is non-additive\n\n\n\n\nA quantitative measure of importance cannot be additive, that is, it cannot quantify the importance of two or more features as the sum of their individual importance.\n\n\n\n\n\n\nThird required property: A two-quantity scenario\nSuppose we have a discrete quantity \\(X\\) with domain \\(\\set{1,2,3,4,5,6}\\) and another discrete quantity \\(Y\\) with domain \\(\\set{1,2,3,4}\\). We want to infer the value of \\(Y\\) after we are told the value of \\(X\\).\nThe conditional probabilities for \\(Y\\) given different values of \\(X\\) are as follows (each column sums up to \\(1\\))\n\n\n\nTable 18.1: Example conditional distribution for two discrete quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)\n\n  \\({}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\)\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n  \\(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{}\\)\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.50\n\n\n2\n0.00\n1.00\n0.00\n0.00\n0.50\n0.00\n\n\n3\n0.00\n0.00\n1.00\n0.50\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.50\n0.50\n0.50\n\n\n\n\n\n\nLet’s see what kind of inferences could occur.\nIf we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\), then we know for sure that \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\). Similarly if we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}2\\) or that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}3\\). These three values of \\(X\\) are therefore “most informative” for inference about \\(Y\\). If we instead learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}4\\), then our uncertainty about \\(Y\\) is split between two of its values. Similarly if we learn that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5\\) or that \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}6\\). These three values of \\(X\\) are therefore “least informative” for inference about \\(Y\\).\nBut what if we want to quantify the importance of a quantity or feature like \\(X\\), and not just one specific value? What is the “overall importance” of \\(X\\)?\n\nConsider again three scenarios.\n\nIn the first, we have 33% probability each of learning one of the values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 99%. And 0.33% probability of learning any one of the remaining values, for a total of 1%. (Grand total 100%.)\nIn this scenario we expect to make an almost exact inference about \\(Y\\) after learning \\(X\\). The quantity \\(X\\) has therefore large “overall importance”.\nIn the second scenario the reverse occurs. We have 0.33% probability each of learning one of the values \\(1\\), \\(2\\), \\(3\\) of \\(X\\), for a total of 1%. And 33% probability of learning any one of the remaining values, for a total of 99%.\nIn this scenario we expect to be roughly equally uncertain between two values of \\(Y\\) after we learn \\(X\\). The quantity \\(X\\) has therefore lower “overall importance”.\nIn the third scenario, we have around 16.7% probability each of observing any one of the values of \\(X\\).\nThis scenario is in between the first two. We expect to make an exact inference about \\(Y\\) half of the time, and to be equally undecided between two values of \\(Y\\) half of the time. The quantity \\(X\\) has therefore some “overall importance”: not as low as in the second scenario, not as high as in the first scenario.\n\nWhat determines the “overall importance” of the quantity or feature \\(X\\) is therefore its probability distribution.\nWe conclude:\n\n\n\n\n\n\nThe importance of a quantity depends on its probability distribution\n\n\n\n\nThe importance of a quantity is not only determined by the relation between its possible values and what we need to infer, but also by the probability with which its values can occur.\nA quantitative measure of “importance” of a quantity must therefore take the probability distribution for that quantity into account.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-entropy-mutualinfo",
    "href": "information.html#sec-entropy-mutualinfo",
    "title": "18  Information, relevance, independence, association",
    "section": "18.5 Entropies and mutual information",
    "text": "18.5 Entropies and mutual information\nThe thought-experiments above suggest that a quantitative measure of the importance of a quantity must have at least these three properties:\n\nContext-dependence: take the context somehow into account.\nNon-additivity: do not calculate the importance of two quantities as the sum of their importance.\nProbability-awareness: take the probability distribution for the quantity into account.\n\nDo measures with such properties exist? They do. Indeed they are regularly used in Communication Theory and Information Theory, owing to the properties above. They even have international standards on their definitions and measurement units.\nBefore presenting them, let’s briefly present the mother of them all:\n\nShannon entropy\nConsider an agent with background knowledge \\(\\mathsfit{I}\\) and a belief distribution \\(\\mathrm{P}(Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) about the finite quantity \\(Y\\). The agent’s uncertainty about the value of \\(Y\\) can be quantified by the Shannon entropy:\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq-\\sum_y \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\, \\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\;\\mathrm{Sh}\n\\]\nwhose unit is the shannon (symbol \\(\\mathrm{Sh}\\)) when the logarithm is in base 2, as above.1\n1 With the logarithm is in base 10, the unit is the hartley (\\(\\mathrm{Hart}\\)); with the natural logarithm, the unit is the natural unit of information (\\(\\mathrm{nat}\\)). \\(1\\,\\mathrm{Sh} \\approx 0.301\\,\\mathrm{Hart} \\approx 0.693\\,\\mathrm{nat}\\).Shannon entropy lies at the foundation of the whole fields of Information Theory and Communication Theory, and would require a lengthy discussion. Let’s just mention some of its properties and meanings:\n\nIt also quantifies the information that would be gained by the agent, if it learned the value of \\(Y\\).\nIt is always positive or zero.\nIt is zero if, and only if, the agent knows the value of \\(Y\\), that is, if the probability distribution for \\(Y\\) gives 100% to one value and 0% to all others.\nIts maximum possible value is \\(\\log_2 N\\;\\mathrm{Sh}\\), where \\(N\\) is the number of possible values of \\(Y\\). This maximum is attained by the uniform belief distribution about \\(Y\\).\nThe value in shannons of the Shannon entropy can be interpreted as the number of binary digits that we lack for correctly identifying the value of \\(Y\\), if the possible values were listed as integers in binary format. Alternatively, a Shannon entropy equal to  \\(h\\,\\mathrm{Sh}\\)  is equivalent to being equally uncertain among \\(2^h\\) possible alternatives.\n\nA Shannon entropy of 1 Sh quantifies the uncertainty of an agent that gives 50%/50% probability to two possibilities. An entropy of 3 Sh quantifies an equal 12.5% uncertainty among eight possibilities.\n\n\n Plot of the Shannon entropy for a binary quantity \\(Y\\in\\set{1,2}\\), for different distributions \\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nAs a curiosity, an entropy of 0.5 Sh quantifies the uncertainty of an agent giving 89% probability to one possibility and 11% to another. So we can say that an 89%/11% belief distribution is half as uncertain as a 50%/50% one.\n\nHere are two useful measures of “informativeness” or “relevance” or “importance” of a quantity about another quantity. Both are based on the Shannon entropy:\n\n\nConditional entropy\nThe conditional entropy2 of a quantity \\(Y\\) conditional on a quantity \\(X\\) and additional knowledge \\(\\mathsfit{I}\\), is defined as\n2 or “equivocation” according to ISO standard.\n\\[\n\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) \\coloneqq\n-\\sum_x \\sum_y\n\\mathrm{P}( X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\,\n\\log_2 \\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\;\\mathrm{Sh}\n\\]\n\nIt satisfies the three requirements above:\n\nContext-dependence\n\nDifferent background knowledge \\(\\mathsfit{I}\\), corresponding to different contexts, leads to different probabilities and therefore to different values of the conditional entropy.\n\nNon-additivity\n\nIf the quantity \\(X\\) can be split into two component quantities, then the conditional entropy conditional on them jointly is more than the sum of the conditional entropies conditional on them individually.\n\nProbability-awareness\n\nThe probability distribution for \\(X\\) appears explicitly in the definition of the conditional entropy.\n\n\nThe conditional entropy has additional, remarkable properties:\n\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is zero. Vice versa, if the conditional entropy is zero, then \\(Y\\) is a function of \\(X\\).\nIf knowledge of \\(X\\) is irrelevant, in the sense of § 18.2, to knowledge of \\(Y\\), then the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) takes on its maximal value, determined by the marginal probability for \\(Y\\). Vice versa, if the conditional entropy takes on its maximal value, then \\(X\\) is irrelevant to \\(Y\\).\nThe value in shannons of the conditional entropy has the same meaning as for the Shannon entropy: if the conditional entropy amounts to \\(h\\,\\mathrm{Sh}\\), then after learning \\(X\\) an agent’s uncertainty about \\(Y\\) is the same as if the agent were equally uncertain among \\(2^h\\) possible alternatives.\n\nFor instance, in the case of an agent with belief distribution of table  18.1, the conditional entropy has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_1) = 0.01\\,\\mathrm{Sh}\\) , almost zero. Indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case.\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_2) = 0.99\\,\\mathrm{Sh}\\) , almost 1. Indeed in this case the agent is approximately uncertain between two values of \\(Y\\).\n\\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_3) = 0.5\\,\\mathrm{Sh}\\) . Indeed this case is intermediate between the previous two.\n\n\n\nMutual information\nSuppose that, according to background knowledge \\(\\mathsfit{I}\\), for any value of \\(X\\) there’s a 100% probability that \\(Y\\) has one and the same value, say \\(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\). The conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) is then zero. In this case it is true that \\(Y\\) is formally a function of \\(X\\). But it is also true that we could perfectly predict \\(Y\\) without any knowledge of \\(X\\). Learning the value of \\(X\\) doesn’t really help an agent in forecasting \\(Y\\). In other words, \\(X\\) is not relevant for inference about \\(Y\\).3\n3 There’s no contradiction with the second remarkable property previously discussed: in this case the maximal value that the conditional entropy can take is zero.If we are interested in quantifying how much learning \\(X\\) “helped” in inferring \\(Y\\), we can subtract the conditional entropy for \\(Y\\) conditional on \\(X\\) from the maximum value it would have if \\(X\\) were not learned.\nThis is the definition of mutual information4 between a quantity \\(Y\\) and a quantity \\(X\\), given background knowledge \\(\\mathsfit{I}\\). It is defined as\n4 or “mean transinformation content” according to ISO standard.\n\\[\n\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\sum_x \\sum_y\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\n\\log_2 \\frac{\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})}{\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\cdot\n\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n} \\;\\mathrm{Sh}\n\\]\n\nIt also satisfies the three requirements – context-dependece, non-additivity, probability-awareness – for a measure of “informativeness” or “importance”. Its properties are somehow complementary to those of the conditional entropy:\n\nIf \\(Y\\) and \\(X\\) are informationally independent, in the sense of § 18.2, then their mutual information is zero. Vice versa, if their mutual information is zero, then these quantities are informationally independent.\nIf knowledge of \\(Y\\) is completely determined by that of \\(X\\), that is, if \\(Y\\) is a function of \\(X\\), then their mutual information attains its maximal value (which could be zero). Vice versa, if their mutual information attains its maximal value, then \\(Y\\) is a function of \\(X\\).\nIf the mutual information between \\(Y\\) and \\(X\\) amounts to \\(h\\,\\mathrm{Sh}\\), then learning \\(X\\) reduces, on average, \\(2^h\\)-fold times the possibilities regarding the value of \\(Y\\).\nMutual information is symmetric in the roles of \\(X\\) and \\(Y\\), that is, \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\mathrm{H}(X : Y\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).\n\nIn the case of an agent with belief distribution as in table  18.1, the mutual information has the following values in the three scenarios:\n\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_1) = 1.61\\,\\mathrm{Sh}\\) , almost equal to the maximal value achievable in this scenario (\\(1.62\\,\\mathrm{Sh}\\)). Indeed \\(Y\\) can almost be considered a function of \\(X\\) in this case. Since \\(2^{1.61}\\approx 2.1\\), learning \\(X\\) roughly halves the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_2) = 0.81\\,\\mathrm{Sh}\\) ; this means that learning \\(X\\) reduces by \\(2^{0.81}\\approx 1.8\\) or almost 2 times the number of possible values of \\(Y\\).\n\\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_3) = 1.5\\,\\mathrm{Sh}\\) ; learning \\(X\\) reduces by \\(2^{1.5} \\approx 2.8\\) or almost 3 times the number of possible values of \\(Y\\).\n\n\n\nUses\nLet’s emphasize that Shannon entropy, conditional entropy, and mutual information are not just fancy theoretical ways of quantifying uncertainty and informativeness. Their numerical values have concrete technological importance; for instance they determine the maximal communication speed of a communication channel. See references on the margin for concrete applications.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nInformation Theory, Inference, and Learning Algorithms\nProbability and Information Theory, with Applications to Radar\n\n\n\n\nWhether to use the conditional entropy \\(\\mathrm{H}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) or the mutual information \\(\\mathrm{H}(Y : X\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) depends on the question we are asking.\nConditional entropy is the right choice if we want to quantify to what degree \\(Y\\) can be considered a function of \\(X\\) – including the special case of a constant function. It is also the right choice if we want to know how many binary-search iterations it would take to find \\(Y\\), on average, once \\(X\\) is learned.\nMutual information is the right choice if we want to quantify how much learning \\(X\\) helps, on average, for inferring \\(Y\\). Or equivalently how many additional binary-search iterations it would take to find \\(Y\\), if \\(X\\) were not known. Mutual information is therefore useful for quantifying “correlation” or “association” of two quantities.\nIf we simply want to rank the relative importance of alternative quantities \\(X_1\\), \\(X_2\\), etc. in inferring \\(Y\\), then conditional entropy and mutual information are equivalent in the sense that they yield the same ranking, since they basically differ by a zero point that would be constant in this scenario.\n\n\n\n\n\n\n Mutual information is superior to the correlation coefficient\n\n\n\nThe Pearson correlation coefficient is actually a very poor measure of correlation or association. It is more a measure of “linearity” than correlation. It can be very dangerous to rely on in data-science problems, where we can expect non-linearity and peculiar associations in large-dimensional data. The Pearson correlation coefficient is widely used not because it’s good, but because of (1) computational easiness, (2) intellectual inertia.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead §12.4 of Artificial Intelligence\nSkim through Chapter 8 of MacKay: Information Theory, Inference, and Learning Algorithms",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "information.html#sec-utility-importance",
    "href": "information.html#sec-utility-importance",
    "title": "18  Information, relevance, independence, association",
    "section": "18.6 Utility Theory to quantify relevance and importance",
    "text": "18.6 Utility Theory to quantify relevance and importance\nThe entropy-based measures discussed in the previous section originate from, and have deep connections with, the problem of repeated communication or signal transmission. They do not require anything else beside joint probabilities.\nIn a general decision problem – where an agent has probabilities and utilities – another approach may be required, however.\nConsider questions like “What happens if I discard quantity \\(X\\) in this inference?” or “If I have to choose between learning either quantity \\(U\\) or quantity \\(V\\), which one should I choose?“.  Such questions are decision-making problems. They must therefore be solved using Decision Theory (this is an example of the recursive capabilities of Decision Theory, discussed in § 2.5). The application of decision theory in these situations if often intuitively understandable. For example, if we need to rank the importance of quantities \\(U\\) and \\(V\\), we can calculate how much the expected utility would decrease if we discarded the one or the other.\nWe’ll come back to these questions in chapters 42 and 43.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>[Information, relevance, independence, association]{.green}</span>"
    ]
  },
  {
    "objectID": "connection-3-ML.html",
    "href": "connection-3-ML.html",
    "title": "19  Third connection with machine learning",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\n\n\nIn chapter  11 we made a second tentative connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some built-in background information (corresponding to the algorithm’s architecture), has received pieces of information (corresponding to the data about perfectly known instances of the task, and possibly partial data about a new instance), and is assessing a not-previously known piece of information (other partial aspects of a new task instance):\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThe correspondence about training data and architecture seems somewhat convincing, the one about outcome needs more exploration.\nHaving introduced the notion of quantity in the latest chapters 12 and 13, we recognize that “training data” are just quantities, the values of which the agent has learned. So a datum \\(\\mathsfit{D}_i\\) can be expressed by a sentence like \\(Z_i\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_i\\), where\n\n\\(i\\) is the instance: \\(1,2,\\dotsc,N, N+1\\).\n\\(Z_i\\), a quantity, describes the type of data at instance \\(i\\), for example “128 × 128 image with 24-bit colour depth, with a character label”.\n\\(z_i\\) is the value of the quantity \\(Z_i\\) at instance \\(i\\), for example the specific image & label displayed here:\n\n\n\n\n\n\nlabel = “Saitama”\n\n\nWe can therefore rewrite the correspondence above as follows:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{ Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_2 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis is the kind of inference that we explored in the “next-three-patients” scenario of § 17.4 and in some of the subsequent sections. In chapter  24, after a review of conventional machine-learning methods and terminology, we shall discuss with more care what these inferences are about, what kind of information they use, and how they can be concretely calculated.\n\nIn the last sections we have often been speaking about “instances”, “instances of similar quantities”, “task instances”, and similar expression. What do with mean with “instance”, more exactly? It is time that we make this and related notions more precise: the whole idea of “learning from examples” hinges on them. In the next few chapters we shall therefore make these ideas more rigorous and quantifiable. Statistics is the theory that deals with these ideas. As a bonus we shall find out that a rigorous analysis of the notion of “instances” also leads to concrete formulae for calculating the kind of probabilities discussed in the present chapter.",
    "crumbs": [
      "[**Inference II**]{.green}",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>[Third connection with machine learning]{.midgrey}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html",
    "href": "populations_variates.html",
    "title": "20  Populations and variates",
    "section": "",
    "text": "20.1 Collections of similar quantities: motivation\nIn the latest chapters we gradually narrowed our focus on a particular kind of inferences: inferences that involve collections of similar quantities, each of which can be simple, joint, or complex. “Similar” means that all these quantities have a similar meaning and measurement procedure, and therefore have the same domain. For instance, each quantity might have possible values \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\); or possible values between \\(0\\,\\mathrm{\\textcelsius}\\) and \\(100\\,\\mathrm{\\textcelsius}\\). These quantities can be considered different “instances” of the same quantity, so to speak. We saw an example in the three-patient hospital scenario of § 17.4, with the three “urgency” quantities \\(U_1\\), \\(U_2\\), \\(U_3\\), corresponding to the urgency of three consecutive patients. Here are other examples:\nIt is easy to think of many other and very diverse examples, with even more complex variates, such as images or words. We shall now try to abstract and generalize this similarity.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-collections",
    "href": "populations_variates.html#sec-collections",
    "title": "20  Populations and variates",
    "section": "",
    "text": "Stock exchange\n\nWe are interested in the daily change in closing price of a stock, during 1000 days. Each day the change can be positive (or zero), or negative.\n\n\nThe daily change on any day can be considered as a binary quantity, say with domain \\(\\set{{\\small\\verb;+;}, {\\small\\verb;-;}}\\). The daily changes in 1000 days are a set of 1000 binary quantities with exactly the same domain; but note that each one can have a different value.\n\n\n\n\n\n\n\nMars prospecting\n\nSome robot examines 1000 similar-sized rocks in a large crater on Mars. Each rock either contains haematite, or it doesn’t contain haematite.\n\n\nThe haematite-content of any rock can be considered as a binary quantity, say with domain \\(\\set{{\\small\\verb;Y;}, {\\small\\verb;N;}}\\). The haematite contents of the 1000 rocks are a set of 1000 binary quantities with exactly the same domain; note again that each one can have a different value.\n\n\n\n\n\n\n\nGlass forensics\n\nA criminal forensics department has 215 glass fragments collected from many different crime scenes. Each fragment is characterized by a refractive index (between \\(1\\) and \\(\\infty\\)), a percentage of Calcium (between \\(0\\%\\) and \\(100\\%\\)), a percentage of Silicon (ditto), and a type of origin (for example “from window of building”, “from window of car”, and similar).\n\n\nThe refractive index, Calcium percentage, Silicon percentage, and type of origin of one fragment constitute a joint quantity, having a joint domain. The refractive index, Calcium percentage, Silicon percentage, and type of origin of the 215 fragments are a set of 215 joint quantities, having identical domains.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-variates-populations",
    "href": "populations_variates.html#sec-variates-populations",
    "title": "20  Populations and variates",
    "section": "20.2 Units, variates, statistical populations",
    "text": "20.2 Units, variates, statistical populations\nConsider a large collection of entities that are somehow similar to one another, as in the preceding examples. We shall call these entities units. Units could be, for instance:\n\nphysical objects such as cars, windmills, planets, or rocks from a particular place;\ncreatures such as animals of a particular species, or human beings, maybe with something in common such as geographical region; or plants of a particular kind;\nautomatons having a particular application;\nsoftware objects such as images;\nabstract objects such as functions or graphs;\nthe rolls of a particular die or the tosses of a particular coin;\nthe weather conditions on several different days.\n\nThese units are similar to one another in that they have some set of attributes1 common to all. These attributes can present themselves in a specific number of mutually-exclusive guises. For instance, the attributes could be:\n1 The term features is frequently used in machine learning\n“colour”, each unit being, say, green, blue, or yellow;\n“mass”, each unit having a mass between \\(0.1\\,\\mathrm{kg}\\) and \\(10\\,\\mathrm{kg}\\);\n“health condition”, each unit (an animal or human in this case) being healthy or ill; or maybe being affected by one of a specific set of diseases;\ncontaining something, for instance a particular chemical substance;\n“having a label”, each unit having one of the labels A, B, C;\na complex combination of several simpler attributes like the ones above.\n\nThe units may also have additional attributes which we simply don’t consider or can’t measure.\n\nFrom the definition above it’s clear that the attributes of each unit are a quantity, as defined in § 12.1.1; often a joint quantity. Once the units and their attributes are specified, we have a set of as many quantities as there are units. All these quantities have identical domains.\nWe call variate the collection of all similar quantities of all the units. When we speak about a “variate”, it is understood that there is some set of units, each having a similar quantity.\nNote the difference between a variate and a quantity. For example, suppose we have three patients A, B, C, and we consider their health condition, which can be healthy or ill. Then “health condition” is a variate, while “the health condition of patient B” is a quantity. There’s a difference because the sentence “the health condition is ill” cannot be said to be true or false, while the sentence “the health condition of patient B is ill” can. If I ask you “is the health condition healthy? or ill?”, you’ll ask me “the health condition of which patient?”.\n\nWe call a collection of units characterized by a variate, as discussed above, a statistical population, or just population when there’s no ambiguity. The number of units is called the size of the population.\nThe notion of statistical population is extremely general. Many different things and collections can be thought of as a statistical population. When we speak of “data”, what we often mean, more precisely, is a particular statistical population.\nThe specification of a population requires precision, especially when it is used to draw inferences, as we shall see later. A statistical population has not been properly specified until two things are precisely specified:\n\nA way to determine whether something is a unit or not: inclusion and exclusion criteria, means of collection, and so on.\nA definition of the variate considered, its possible values, and how it is measured.\n\n\n\n\n\n\n\nExercise 20.1\n\n\n\n\nWhich of the following descriptions does properly define a statistical population? explain why it does or does not.\n\nPeople.\nElectronic components produced in a specific assembly line, since the line became operational until its discontinuation, and measured for their electric resistance, with possible values in \\([0\\,\\mathrm{\\Omega}, \\infty\\,\\mathrm{\\Omega}]\\), and for their result on a shock test, with possible values \\(\\set{{\\small\\verb;pass;}, {\\small\\verb;fail;}}\\).\nPeople born in Norway between 1st January 1990 and 31st December 2010.\nThe words contained in all websites of the internet.\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater (as defined by contours on a map), and tested to contain haematite, with possible values \\(\\set{{\\small\\verb;Y;}, {\\small\\verb;N;}}\\).\n\nBrowse some datasets at the UC Irvine Machine Learning repository. Each dataset is a statistical population. The variate in most of these populations is a joint variate (to be discussed below), that is, a collection of several variates.\nExamine and discuss the specification of some of those datasets:\n\nIs it well-specified what constitutes a “unit”? Are the criteria for including or excluding datapoints, their origin, and so on, well explained?\nAre the variates well-defined? Is it explained what they mean, how they were measured, what is their domain, and so on?\n\n\n\n\n\n\n\n\n\n\n\n\n Subtleties in the notion of statistical population\n\n\n\n\n  A statistical population is only a conceptual device for simplifying and facing some decision or inference problem. There is no objectively-defined population “out there”.\nAny entity, object, person, and so on has some characteristics that makes it completely unique (say, its space-time coordinates). Otherwise we wouldn’t be able to distinguish it from other entities. From this point of view any entity is just a one-member population in itself. If we consider two or more entities as being “similar” and belonging to the same population, it’s because we have decided to disregard some characteristics of these entities, and only focus on some other characteristics. This decision is arbitrary, a matter of convention, and depends on the specific inference and decision problem.\nTo test whether an entity belongs to a given population, we have to check whether that entity satisfies the agreed-upon definition of that population.\n   Any physical entity, object, person, etc. can be a “unit” in very different and even statistical populations. For instance, a 100 cm³ rock found in the Schiaparelli crater on Mars could be a unit in these populations:\nA. Rocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and tested for haematite\nB. Rocks, of volume between 10 cm³ and 200 cm³, found in the Schiaparelli crater and tested for haematite\nC. Rocks, of volume between 10 cm³ and 200 m³, found in any crater on any planet of the solar system, and tested for haematite\nD. Rocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and measured for the magnitude of their magnetic field.\nNote the following differences. Populations A, B, C above have the same variate but differ in their definition of “unit”. Populations A and D have the same definition of unit but different variates. Population B is a subset of population A: they have the same variate, and any unit in B is also a unit in A; but not every unit in A is also a unit in B. Populations A and C have some overlap: they have the same variate, and some units of A are also units of C, and vice versa.\n   Recall the difference between variate and quantity, discussed previously. Consider the population of glass fragments introduced above, and suppose I say “\\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8.1\\)”. Can you check if what I said is true? No, because you don’t know which unit I’m referring to.\nThe variate for a specific unit is a quantity instead. We can indicate this by appending the unit label to the variate symbol, as we did with “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\)” above. If I tell you “\\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}8\\)”, you can check that what i said is false; therefore \\(\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\) is a quantity.\n   The units’ IDs don’t need to be consecutive numbers; in fact they don’t even need to be numbers: any label that completely distinguishes all units will do.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "populations_variates.html#sec-joint-variates",
    "href": "populations_variates.html#sec-joint-variates",
    "title": "20  Populations and variates",
    "section": "20.3 Populations with joint variates",
    "text": "20.3 Populations with joint variates\nThe quantity associated with each unit of a statistical population can be of arbitrary complexity. In particular it could be a joint quantity (§ 13.1), that is, a collection of quantities of a simpler type.\nWe saw an example at the beginning of this chapter, with a population relevant for glass forensics. The statistical population was defined as follows:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of four variates of a simple kind:\n\n\\(\\mathit{R}\\)efractive \\(\\mathit{I}\\)ndex of the glass fragment (interval continuous variate), with domain from \\(1\\) (included) to \\(+\\infty\\)\nweight percent of \\(\\mathit{Ca}\\)lcium in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\nweight percent of \\(\\mathit{Si}\\)licon in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed, vehicle_windows_non_float_processed, containers, tableware, headlamps\n\n\nHere is a table with the values of the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) for ten units:\n\n\n\nTable 20.1: Glass fragments\n\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n\n\n\nThe variate value for unit 4, for instance, is\n\\[\n\\mathit{RI}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1.52247 \\land\n\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}9.76 \\land\n\\mathit{Si}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}70.26 \\land\n\\mathit{Type}_{\\color[RGB]{204,187,68}4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\n\n\n\n\n\n\nExercise 20.2\n\n\n\n\nDownload the dataset2 income_data_nominal_nomissing.csv (4 MB):\n\nHow many variates does this population have?\nWhat types of variate (binary, nominal, etc.) do they seem to be?\nWhat are their domains?\n\nExplore datasets from the UC Irvine Machine Learning Repository, answering the three questions above.\n\n\n\n\n\n2 This is an adapted version of the UCI “adult-income” dataset",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>[Populations and variates]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "21  Statistics",
    "section": "",
    "text": "21.1 What’s the difference between Probability Theory and Statistics?\n“Probability theory” and “statistics” are often mentioned together. We shall soon see why, and what are the relationship between them. But first let’s try to define them more precisely:\nThere are clear and crucial differences between the two:\nMany texts do not clearly distinguish between probability and statistics. The distinction is important for us because we will have to solve problems involving the uncertainty about particular statistics, so the two must be kept clearly separate. This distinction was observed by James Clerk Maxwell who used it to develop the theories of statistical mechanics and kinetic theory.\nIn many concrete problems, however, probability theory and statistics do go hand in hand and interact. This happens mainly in two ways:\nLet’s now discuss some important statistics.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-diff-prob-stat",
    "href": "statistics.html#sec-diff-prob-stat",
    "title": "21  Statistics",
    "section": "",
    "text": "Probability theory\n\nis the theory that describes and norms the quantification and propagation of uncertainty, as we saw in § 8.1.\n\nStatistics\n\nis the study of collective properties of the variates of populations or, more generally, of collections of data.\n\n\n\n\nThe fact that we are uncertain about something doesn’t mean that there are populations or replicas involved. We can apply probability theory in situations that don’t involve any statistics.\nIf we have full information about a population – the value of each variate for each unit – then we can calculate summaries and other properties of the variates. And there’s no uncertainty involved: at all times we can exactly calculate any information we like about any variates. So we do statistics, but probability theory plays no role.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMaxwell explains the statistical method and its use in the molecular description of matter:\n\nIntroductory Lecture on Experimental Physics\nMolecules\n\n\n\n\n\nThe statistics of a population give information that can be used in the conditional of an inference.\nWe want to draw inferences about some statistics of a population, whose values we don’t know.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-freq-distr",
    "href": "statistics.html#sec-freq-distr",
    "title": "21  Statistics",
    "section": "21.2 Frequencies and frequency distributions",
    "text": "21.2 Frequencies and frequency distributions\nConsider a statistical population of \\(N\\) units, with a variate \\(X\\) having a finite set of \\(K\\) values as domain. To keep things simple let’s just say these values are \\(\\set{1, 2, \\dotsc, K}\\) (without any ordering implied). Our discussion applies for any finite set. The variate \\(X\\) could be of any non-continuous type: nominal, ordinal, interval, binary (§ 12.2), or of a joint or complex type (§ 13). Let’s denote the variate associated with unit \\(i\\) by \\(X_i\\). For instance, we express that unit #3 has \\(X\\)-variate value \\(5\\) and unit #7 has \\(X\\)-variate value \\(1\\) by writing\n\\[\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\land X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\quad\\text{\\small or equivalently}\\quad\nX_3 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}5 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_7 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\n\\]\nFor each value \\(a\\) in the domain of the variate \\(X\\), we count how many units have that particular value. Let’s call the number we find \\(n_a\\). This is the absolute frequency of the value \\(a\\) in this population. Obviously \\(n_a\\) must be an integer between \\(0\\) (included) and \\(K\\) (included). The set of absolute frequencies of all values is called the absolute frequency distribution of the variate in the population. We must have\n\\[\\sum_{a=1}^K n_a = N \\ .\\]\nIt is often useful to give the fraction of counts with respect to the population size, which we denote by \\(f_a\\):\n\\[f_a \\coloneqq n_a/N\\]\nThis is called the relative frequency of the value \\(a\\). Obviously \\(0 \\le f_a \\le 1\\). The collection of relative frequencies for all values, \\(\\set{f_1, f_2, \\dotsc, f_K}\\), satisfies\n\\[\\sum_{a=1}^K f_a = 1 \\ .\\]\nWe call this collection of relative frequencies the relative frequency distribution. We shall denote it with the boldface symbol \\(\\boldsymbol{f}\\) (boldface indicates that it is a tuple of numbers):\n\\(\\boldsymbol{f} \\coloneqq(f_1, f_2, \\dotsc, f_K)\\)\nwith an analogous convention if other letters are used instead of “\\(f\\)”.\n\n\n\n\n\n\n \n\n\n\nIn the following we shall call relative frequencies simply “frequencies”, and explicitly use the word “absolute” when we speak about absolute frequencies.\n\n\n\nThe frequency distribution of values in a population does not give us full information about the population, because it doesn’t tell which unit has which value. In many situations, however, the frequencies are all we need to know, or all we can hope to know.\nFrequencies and frequency distributions are quantities in the technical sense of § 12.1.1. In fact we can say, for instance, “The frequency of the value C is 0.3”, or “The frequency distribution for the values A, B, C is \\((0.2, 0.7, 0.1)\\)”. We shall denote the quantity, as separate from its value, by the corresponding capital letter, for example \\(F_1\\), so that we can write sentences about frequencies in our usual abbreviated form. For instance\n\\[\nF_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}f_3\n\\]\nmeans “The frequency of the variate value \\(3\\) is equal to \\(f_3\\)”, where \\(f_3\\) must be a specific number.\n\n\n\n\n\n\nExercise 21.1\n\n\n\nConsider the statistical population defined as follows:\n\nunits: the bookings at a specific hotel during a specific time period\nvariate: the market segment of the booking\nvariate domain: the set of five values \\(\\set{{\\small\\verb;Aviation;}, {\\small\\verb;Complementary;},  {\\small\\verb;Corporate;}, {\\small\\verb;Offline;},  {\\small\\verb;Online;}}\\)\n\nThe population data is stored in the file hotel_bookings-market.csv. Each row of the file corresponds to a unit, and lists the unit id (this is not a variate in the present population) and the market segment.\nUse any method you like (a script in your favourite programming language, counting by hand, or whatever) to answer these questions:\n\nWhat is the size of the population?\nWhat are the absolute frequencies of the five values?\nWhat are their relative frequencies?\nWhich units have the value \\({\\small\\verb;Corporate;}\\)?\n\n\n\n\nDifferences between frequencies and probabilities\nThe fact that frequencies are non-negative and sum up to 1 makes them somewhat similar to probabilities, from a purely numerical point of view. The two notions, however, are completely different and have different uses. Here is a list of some important differences:\n\n\nNot few works in machine learning tend to call “probabilities” any set of positive numbers that sum up to one. Be careful when reading them. Mentally replace probability with degree of belief and see if the text mentioning “probabilities” still makes sense.\n\n\nA probability expresses a degree of belief.\nA frequency is the count of how many times something occurs.\n\n\n\n\nThe probability of a sentence depends on an agent’s state of knowledge and background information. Two agents can assign different probabilities to the same sentence.\nThe frequency of a value in a population is an objective physical quantity. All agents agree on the frequency (if they have the possibility of counting the occurrences).\n\n\n\n\nProbabilities refer to sentences.\nFrequencies refer to values in a population, not to sentences. \n\n\n\n\nA probability can refer to a specific unit in a population. An agent can consider, for instance, the probability that a variate for unit #7 has value 3.\nA frequency cannot refer to a specific unit in a population. It is meaningless to “count how many times the value 3 appears in unit #7”.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-joint-freq",
    "href": "statistics.html#sec-joint-freq",
    "title": "21  Statistics",
    "section": "21.3 Joint frequencies",
    "text": "21.3 Joint frequencies\nConsider the following population consisting of ten units with joint variate \\((\\mathit{age}, \\mathit{race}, \\mathit{sex}, \\mathit{income})\\), whose component variates have the following properties:\n\n\\(\\mathit{age}\\):   interval discrete with domain \\(\\set{17, 18, \\dotsc, 90+}\\)\n\\(\\mathit{race}\\):   nominal with domain \\(\\set{{\\small\\verb;Amer-Indian-Eskimo;}, {\\small\\verb;Asian-Pac-Islander;} , {\\small\\verb;Black;}, {\\small\\verb;Other;}, {\\small\\verb;White;}}\\)\n\\(\\mathit{sex}\\):   binary with domain \\(\\set{{\\small\\verb;F;}, {\\small\\verb;M;}}\\)\n\\(\\mathit{income}\\):   binary with domain \\(\\set{{\\small\\verb;`&lt;=50K';}, {\\small\\verb;`&gt;50K';}}\\)\n\n\n\n\nTable 21.1: Income\n\n\n\n\n\n\\(\\mathit{age}\\)\n\\(\\mathit{race}\\)\n\\(\\mathit{sex}\\)\n\\(\\mathit{income}\\)\n\n\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n26\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;F;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;White;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n53\n\\({\\small\\verb;Black;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&lt;=50K';}\\)\n\n\n48\n\\({\\small\\verb;Amer-Indian-Eskimo;}\\)\n\\({\\small\\verb;M;}\\)\n\\({\\small\\verb;`&gt;50K';}\\)\n\n\n\n\n\n\nThe joint frequency distribution for the joint variate of the population above gives the frequencies of all possible joint variate values, for instance the value\n\\(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\\)\nIn this population, most joint values appear each only once, and the remaining values never appear; this is because of the population’s small size and the large number of possible variate values. A couple of joint values appear twice. We have for example\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;M;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{2}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Black;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Amer-Indian-Eskimo;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;F;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = 0\n\\end{aligned}\\]\n\n\n\n\n\n\nExercise 21.2\n\n\n\nTry to write a function that takes as input a dataset with a small number of variates and outputs the joint frequency distribution for all combinations of variate values. The best output format is a multidimensional array having one dimension per variate, and for each dimension a length equal to the number of possible values of that variate. The value of the array in each cell is the corresponding frequency.\nFor instance, consider the case of the income dataset above but without the age variate. The output of the function would then be an array with \\(5 \\times 2 \\times 2\\) dimensions",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-marginal-freq",
    "href": "statistics.html#sec-marginal-freq",
    "title": "21  Statistics",
    "section": "21.4 Marginal frequencies",
    "text": "21.4 Marginal frequencies\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the population of the example above, for instance, we might be interested only in the \\(\\mathit{age}\\) and \\(\\mathit{income}\\) variates. These two variates together are then called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes.\n\n\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\), we define the marginal frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\) as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the marginal population with the variate \\({\\color[RGB]{238,102,119}Y}\\) alone. This frequency is simply written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\n\\]\nA conditional frequency can be calculated as the sum of the joint frequencies for all values \\({\\color[RGB]{34,136,51}x}\\), in a way analogous to marginal probabilities (§ 16.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}) = \\sum_{\\color[RGB]{34,136,51}x} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nFor example, if from the population of table  21.1 we consider the marginal population with variates \\((\\mathit{age}, \\mathit{income})\\), some of the marginal frequencies are\n\\[\\begin{aligned}\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}53 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{3}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}26 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&lt;=50K';}\n) = \\frac{1}{10}\n\\\\[2ex]\n&f(\\mathit{age}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}48 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;`&gt;50K';}\n) = \\frac{3}{10}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nExercise 21.3\n\n\n\n\nDownload again the dataset income_data_nominal_nomissing.csv:\n\nCalculate the marginal frequencies of some of its variates.\nDoes any variate have a value appearing with marginal absolute frequency equal to 1?",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-summary-stat",
    "href": "statistics.html#sec-summary-stat",
    "title": "21  Statistics",
    "section": "21.5 Summary statistics",
    "text": "21.5 Summary statistics\nIn communicating statistics about a population it is always best to report and, when possible, visually show (for instance as marginal distributions) the full joint frequency distribution of the population’s variates.\nSometimes one wants to share some sort of “summary” of the frequency distribution, emphasizing particular aspects of it; because these are also aspects of the population. Different kinds of aspects can be chosen; some of them are only defined for specific types of variates. They are often called “summary statistics” or “descriptive statistics”. Below we give a brief description of some common ones, emphasizing when they are appropriate and when they are not. These summaries can also be used for probability distributions.\n\nMode\nThe mode is the value having the highest frequency (or probability, if we’re speaking about an agent’s beliefs rather than a population). There can be more than one mode.\nThe mode is defined for any distribution over discrete values, also for nominal quantities.\n\n\n\n\n\n\n Beware of modes\n\n\n\nBe careful in relying too much on the “mode” for a continuous quantity. Continuous quantities can be transformed in a one-to-one way into other, equivalent ones; and such a transformation also give the equivalent frequency or probability density for the new quantity.\nThere is no general relationship between the modes of the densities for the two equivalent quantities. In fact, the density for one quantity can have one mode, whereas the density for the equivalent quantity can have no mode, or many modes. This is true for all kinds of distributions represented by densities, for example a continuous distribution of energy.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSome paradoxes, errors, and resolutions concerning the spectral optimization of human vision\n\n\n\n\nMedian and quartiles\nRecall (§ 12.2) that an ordinal or an interval quantity or variate have values that can be ranked in a specific order. If there is a value for which the sum of the frequencies of all values of rank lower than that value equals the sum of the frequencies of all values of rank higher than that value, then that value is called the median of the distribution. See the following histogram as an example:\n The value \\({\\small\\verb;e;}\\) is the median of this frequency distribution, because \\(f({\\small\\verb;c;})+f({\\small\\verb;d;}) = f({\\small\\verb;f;})+f({\\small\\verb;g;})+f({\\small\\verb;h;})+f({\\small\\verb;i;})= 47.6\\%\\)\nIf there is no such separating value, then sightly different definitions of median exist in the literature; but the approximate idea is the same: a value that somehow divides the domain into two parts of roughly equal (50%) total frequency. This idea can be also applied to continuous distributions represented by densities.\nThe notion of median can be generalized to that of a value that separates the domain into a lower-rank part with total frequency 1/4, and a higher-rank part with total frequency 3/4; and also to that of a value separating into a 3/4 vs 1/4 proportion instead. These values are called the first quartile and third quartile. The two quartiles and the median (also called second quartile) divide the domain into four parts of roughly equal 25% frequencies.\nIf the variate or quantity under consideration is of interval type, then it’s possible to take the difference between the third and first quartile, called the interquartile range.\n\n\nMean and standard deviation\nFor an interval quantity \\(X\\) with values \\(\\set{x_1, x_2, \\dotsc}\\) for which it makes sense to take the sum, it is possible to define the mean and standard deviation:\n\\[\n\\bar{X} \\coloneqq\\sum_i x_i\\cdot f(x_i)\n\\qquad\n\\sigma(X) \\coloneqq\\sqrt{\\sum_i (x_i-\\bar{X})^2\\cdot f(x_i)}\n\\]\nwe assume that their meaning is more or less familiar to you.\n\n\nUses and pitfalls\nFor a nominal variate or quantity it doesn’t make sense to speak of median, quartiles, mean, standard deviation, because its possible values cannot be ranked or added.\nFor an ordinal variate or quantity it doesn’t make sense to speak of mean or standard deviation, because its possible values cannot be added.\n\n\nThe mean and standard deviation can make sense and can be useful in some circumstances. But note that even if the values of a quantity can be summed, their mean (and standard deviation) may not quite make sense.\nConsider the number of patients visiting a hospital in 100 consecutive days. It is possible to consider the mean number of patients per day. This number has a meaning: if this number of patients visited the hospital every day for 100 days, then the total number of visits would be equal to the actual total. The same reasoning can be made for the number of nurses working in the hospital every day for 100 days, and their mean.\nNow consider the daily ratios of patients to nurses, for those 100 days. These ratios are numbers, so we can take their mean. But what does such a mean represent? if we multiply it by 100, we don’t obtain the total of anything. Also, if we consider the total number of patients and total number of nurses in 100 days, their ratio will not be equal to the “mean ratio” we calculated.\nThe example above is not meant to say that a mean of ratios never makes sense, but to point out that mean and standard deviation are often overused. In chapter  23 we will discuss other problems that may arise in using mean and standard deviation.\nIn general, when in doubt, we recommend to use median and quartiles or median and interquartile range, which are more generally meaningful and enjoy several other properties (for example so-called “robustness”) useful in doing statistics.\n\n\nNote, in any case, that the present discussion regards the question of how to provide summary information besides the full frequency (or probability) distribution. If our problem is to choose one value out of the possible ones, then that’s a decision-making problem, which must be solved by specifying utilities and maximizing the expected utility, as preliminary discussed in chapter  2 and as will be discussed more in detail towards the final chapters.\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§2.6 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n\nSkim through:\n\n§ “The median estimate” of Cox & O’Hagan 2022: Meaningful expression of uncertainty in measurement\nGould 1985/2013: The Median Isn’t the Message",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-outliers",
    "href": "statistics.html#sec-outliers",
    "title": "21  Statistics",
    "section": "21.6 Outliers vs out-of-population units",
    "text": "21.6 Outliers vs out-of-population units\nThe term “outlier” frequently appears in problems related to statistics and probability, often in conjunction with some summary statistics described above. Unfortunately the definitions of this term can be confusing or misleading. With the notion of outlier often there also comes a barrage of “methods” or rules meant to “deal” with outliers. Some such rules, for instance the rule of discarding any datapoints lying at more than three standard deviations from the mean, are often mindless and dangerous.\nSo let’s avoid the term “outlier” for the moment, and let’s take a different perspective.\n\n\nOne reason why we consider a population of units is that we are interested in making inferences about some units in this population, for which we lack the values of some variates. As we shall see in the forthcoming chapters, such inferences can be made if we first try to infer the full joint frequency distribution for the variates of the population of interest.\nThis kind of inference becomes more difficult if we have reckoned into the population some units that actually don’t belong there.\nSuppose for instance that a hospital is interested in the age of female patients admitted in a year. In collecting data, some male patients are counted in. Then obviously the age frequencies obtained from the collected data will not reflect the age frequencies among females. The problem is that some out-of-population units have been counted in by mistake.\nThe way out-of-population units affect and distort the frequency count can be different from problem to problem.\nIn our example, suppose that the wast majority of female patients could have age between 45–55 years, and that the male patients erroneously counted in also have age in the same range. Then the bulk of the frequency distribution will appear more inflated than it should be. Or suppose instead that the male patients erroneously reckoned have age between 80–90 years. In this case the old-age tail of the distribution will appear more inflated. As you see we can’t a priori point to any “tail” or “bulk” as a problem.\nLow frequencies are relatively affected by out-of-population units more than high frequencies. Suppose 10 female patients out of 100 have age 52; frequency 10%. If one 52-year-old male patient is now included by mistake, the frequency becomes 11/101 ≈ 10.1%, or a 1% relative error. But if one female patient out of 100 has age 96 (1% frequency), and a male patient of the same age is now included by mistake, the frequency becomes 2/101 ≈ 1.98%, with a 98% relative error. This is the reason some people focus on distribution tails and “outliers”, defined as data having with low-frequency values. (Note that this reasoning would concern any regions of low frequency, for example among two modes; not just tails.)\nYet we cannot mindlessly attack low-frequency regions and data just because they could be more affected by out-of-population units. In many problems of data science, engineering, medicine, low-frequency cases are the most important ones (think of rare diseases, rare mineral elements, rare astronomical events, and so on). So if we alter or eliminate low-frequency data only because they might be out-of-population units, then we have dangerously affected all our inferences about such rare events.\nMoreover, how could we judge what the “correct” frequency should be? Many outlier methods assume that the true frequency of the population has a Gaussian shape, and alter or cut the tails based on this assumption. But how can we know if such an assumption is correct? It turns out that the tails of a distribution are important for checking such assumption. Then you see the full circularity behind such mindless methods.\n\n\n\n\n\n\n Study reading\n\n\n\nRead §2.1 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n\n\nWhich method should one use to face this problem, then? – The answer is that there’s no universal method. The approach depends on the specific problem. A data scientist must carefully examine all possible sources of out-of-population units, make inferences about them, and integrate these inferences in the general inference about the population of interest.\nThere is literature discussing first-principle approaches of this kind for different scenarios, but we cannot discuss them in the present course.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 21 of Probability Theory",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>[Statistics]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html",
    "href": "subpopulations.html",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "",
    "text": "22.1 Subpopulations\nWhen we have a statistical population with a joint variate, it is often of interest to focus on a subset of units that share the same value of a particular variate.\nConsider for instance the following population, related to the glass-forensics example we encountered before:\nLet’s say we are interested only in units that have the \\(\\mathit{Type}\\) variate equal to \\({\\small\\verb;tableware;}\\). Discarding all others we obtain a new, smaller population with four units:\nwere a bar “\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\)” indicates the variate used for the selection.\nAs another example, we could be interested instead in those units that have both \\(\\mathit{Ca}\\) and \\(\\mathit{Si}\\) variates equal to \\({\\small\\verb;medium;}\\). We obtain a smaller population with five units:\nPopulations formed in this way are called subpopulations of the original one. They are statistical populations in their own right. The notion of “subpopulation” is a relative notion. Any population can often be considered as a subpopulation of some larger population having additional variates.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-subpopulations",
    "href": "subpopulations.html#sec-subpopulations",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "",
    "text": "units: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of three simple variates:\n\nweight fraction of \\(\\mathit{Ca}\\)lcium in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\nweight fraction of \\(\\mathit{Si}\\)licon in the fragment (ordinal variate), with three possible values \\(\\set{{\\small\\verb;low;}, {\\small\\verb;medium;}, {\\small\\verb;high;}}\\)\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values \\(\\{{\\small\\verb;building_windows_float_processed;}\\), \\({\\small\\verb;building_windows_non_float_processed;}\\), \\({\\small\\verb;containers;}\\), \\({\\small\\verb;tableware;}\\), \\({\\small\\verb;headlamps;}\\}\\)\n\n\n\n\n\nTable 22.1: Simplified glass-fragment population data\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\nTable 22.2: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\nTable 22.3: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\) and \\(\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n3\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n4\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n6\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;containers;}\\)\n\n\n9\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n10\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 22.1\n\n\n\n\nFrom the population of table  22.1:\n\nConstruct the marginal population with variate \\(\\mathit{Ca}\\)\nReport the frequency distribution for the marginal population above (remember that \\(\\mathit{Ca}\\) has three possible values)\nConstruct the subpopulation with variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;high;}\\)\nConstruct the subpopulation with variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) and the variate \\(\\mathit{Si}\\) equal to \\({\\small\\verb;medium;}\\)\n\n\nCheck your understanding of the reasoning behind the notions of marginal population and subpopulation with this exercise:\n\nFrom the population of table  22.1, construct the subpopulation with variate \\(\\mathit{Type}\\) equal to either \\({\\small\\verb;headlamps;}\\) or \\({\\small\\verb;tableware;}\\).",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-conditional-freqs",
    "href": "subpopulations.html#sec-conditional-freqs",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.2 Conditional frequencies",
    "text": "22.2 Conditional frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\) (and possibly others), we define the conditional frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\), given or “conditional on” the value \\({\\color[RGB]{34,136,51}x}\\) of \\({\\color[RGB]{34,136,51}X}\\), as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the subpopulation selected by \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). This frequency is usually written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})\n\\]\nwhere \\(f\\) is the symbol for the joint frequency of the population.\nConsider for instance the glass-fragment population of table  22.1. The conditional frequency of \\({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}}\\) given \\({\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}\\) is the (marginal) frequency of \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\) in the subpopulation of table  22.2, from which we find\n\\[\nf({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\]\nThe collection of these conditional frequencies for all values of \\({\\color[RGB]{238,102,119}Y}\\) constitutes the conditional frequency distribution of \\({\\color[RGB]{238,102,119}Y}\\) conditional on \\({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}\\). In our example this distribution has three conditional frequencies:\n\\[\\begin{aligned}\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{1}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = \\frac{3}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}}) = 0\n\\end{aligned}\\]\nwhich sum up to \\(1\\) as they should.\n\n\n\n\n\n\n Conditional on a value of a variate\n\n\n\nIt doesn’t make sense to speak of the conditional frequency distribution of \\(Y\\) “conditional on \\(X\\)”. Conditional frequencies and frequency distributions are always conditional on some value of a variate. If we consider all possible values of \\(Y\\) and of \\(X\\) we obtain a collection of frequencies that is not a distribution.\n\n\nA conditional frequency can be calculated as the ratio of a joint and a marginal frequencies, in a way analogous to conditional probabilities (§ 17.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}) =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{f({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})} =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}{\n\\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x})}\n\\]\n\n\n\n\n\n\nExercise 22.2\n\n\n\nCalculate the conditional frequency distributions corresponding to the subpopulations of tables 22.2 and 22.3. For example, for table  22.2 this means calculating\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\\ ,\n\\\\\n&\\dotsc\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{Si}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;})\n\\end{aligned}\\]",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "subpopulations.html#sec-association",
    "href": "subpopulations.html#sec-association",
    "title": "22  Subpopulations and conditional frequencies",
    "section": "22.3 Associations",
    "text": "22.3 Associations\nThe analysis of subpopulations and conditional frequencies is important because it often reveals peculiar associations1 among different variates and groups of variates. Let’s illustrate what we mean by “association” with an example.\n1 In everyday language this is the same as “correlation”. The term “association” is used in statistics to avoid confusion with the Pearson correlation coefficient (see § 18.5)Extract the subpopulation having variate \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) from the population of table  22.1:\n\n\n\nTable 22.4: Selection according to \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n\n\n\nwe notice that all units have variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\). In terms of conditional frequencies, this means\n\\[\n\\begin{aligned}\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;medium;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\\\\n&f(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;high;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 0\n\\end{aligned}\n\\]\nIt is therefore impossible to observe other values of \\(\\mathit{Ca}\\) in this new population.2\n2 We are not claiming that this fact will be true if new units are considered; this important question will be discussed later.On the other hand, if we extract the subpopulation having variate \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) we obtain\n\n\n\nTable 22.5: Selection according to \\(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\)\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n2\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n5\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n7\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;medium;}\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n8\n\\({\\small\\verb;low;}\\)\n\\({\\small\\verb;high;}\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n\n\n\nwith conditional frequencies such as\n\\[\n\\begin{aligned}\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\\\[1ex]\n&f(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{1}{5}\n\\end{aligned}\n\\]\nand so on. The reverse is therefore not true: if \\(\\mathit{Ca}\\) is equal to \\({\\small\\verb;low;}\\), that does not mean that it’s impossible to observe other \\(\\mathit{Type}\\) values besides \\({\\small\\verb;headlamps;}\\). Note especially how these frequencies differ:\n\\[\n\\begin{gathered}\nf(\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}) = 1\n\\\\[1ex]\nf(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}) = \\frac{2}{5}\n\\end{gathered}\n\\]\nIn the original population we have, figuratively speaking, the following interesting association:\n\\[\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\\ \\mathrel{\\color[RGB]{34,136,51}\\Rightarrow}\\\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\n\\qquad\\text{\\small but}\\qquad\n\\mathit{Ca}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;low;}\\  \\mathrel{\\color[RGB]{238,102,119}\\nRightarrow}\\\n\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;headlamps;}\n\\]\nThis kind of associations is often useful. Suppose for instance that you are asked to pick a unit with \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\) in the original population; but it’s difficult to measure a unit’s \\(\\mathit{Ca}\\) value, while it’s easy to measure its \\(\\mathit{Type}\\) value. Then you could instead search for a unit having \\(\\mathit{Type}\\) equal to \\({\\small\\verb;headlamps;}\\) (easier search), and you would be sure that the unit you found also has \\(\\mathit{Ca}\\) equal to \\({\\small\\verb;low;}\\).\n\n\nThe example above, where some values of a variate completely exclude some values of another, is a special one. More often we find that there are small or large changes in the frequency distribution of some variate, depending on the subpopulation considered.\n\n\n\n\n\n\nExercise 22.3\n\n\n\n\nCalculate the (marginal) frequency distribution for the \\(\\mathit{Ca}\\) variate for the glass-fragment population of table  22.1. Is the value \\({\\small\\verb;low;}\\) more frequent than \\({\\small\\verb;medium;}\\)? or vice versa?\nCalculate the frequency distribution for \\(\\mathit{Ca}\\), conditional on \\(\\mathit{Type}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tableware;}\\) (see table  22.2). How does this frequency distribution differ from the one you calculated above? Come up with possible ways to exploit this difference in concrete applications.\n\n\n\n\nAssociations can be very counter-intuitive\nIt is usually best to assess associations by explicitly calculating all relevant conditional frequencies, rather than jumping to intuitive conclusions after having examined just a few. Here’s an example.\n\n\nConsider the statistical population defined as follows:\n\n\n\n\nunits: all reparations done by a repair company on a particular kind of electronic components, which is extremely delicate and usually very difficult to repair. The population has 26 units (every unit actually represents a batch 100 reparations, so the population really refers to 2600 reparations).\na joint variate, consisting in three binary ones:\n\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\) of the repair procedure, with values \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) and \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\);\nrepair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), with values \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\), representing a traditional reparation method and one introduced more recently;\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) of the repair procedure, with values \\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\) and \\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\).\n\n\n\n\n\nTable 22.6: Reparations (each row is one unit, representing 100 reparations).\nfile repair_data.csv\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\)\n\\(\\mathit{\\color[RGB]{34,136,51}Method}\\)\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\)\n\n\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;success;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\\(\\color[RGB]{238,102,119}{\\small\\verb;fail;}\\)\n\\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\)\n\\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\)\n\n\n\n\n\n\nThe repair company claims that, in this population, the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\). Can you back up their claims?:\n\n\n\n\n\n\nExercise 22.4\n\n\n\n(one of the most fun of the course!)\nUse the population data above. The calculations can be done with any tools you like.\n\nExamine the whole population first:\n\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) (note that we are disregarding the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\)).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\).\nCompare the two conditional frequency distributions above. Which of the two repair methods seems more effective?\nAre the claims of the repair company justified?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\):\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for onsite reparations?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;onsite;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in step 3.?\n\nNow examine the reparations that have been done \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\)ly:\n\nBefore doing any calculations, what do you expect to find? should the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method be more effective than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) one, for reparations done remotely?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\).\nCompare the two conditional frequency distributions for this \\({\\color[RGB]{102,204,238}{\\small\\verb;remote;}}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in steps 3. and 7.?\n\nSummarize and explain all your findings.\nCan the repair company claim that the \\({\\color[RGB]{34,136,51}{\\small\\verb;new;}}\\) repair method is better than the \\({\\color[RGB]{34,136,51}{\\small\\verb;old;}}\\)?\n\nSuppose you need to send an electronic component for repair to this company.\n\nIf you could choose both the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\) and the \\(\\mathit{\\color[RGB]{34,136,51}Method}\\) of the repair, which would you choose? why?\nIf you could only choose the repair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), but have no control over the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\), which method would you choose? why?\n\nIs there other information, missing from the description of the population, that should be known before answering the questions above?\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nRead:\n\n§§2.2–2.4 and 2.7–2.10 of Fenton & Neil: Risk Assessment and Decision Analysis with Bayesian Networks\n\nSkim through:\n\nLindley & Novick 1981: The role of exchangeability in inference This can be a difficult reading. Try to get the main message.\nMalinas & Bigelow 2004/2016: Simpson’s paradox",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>[Subpopulations and conditional frequencies]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html",
    "href": "samples.html",
    "title": "23  Infinite populations and samples",
    "section": "",
    "text": "23.1 Infinite populations\nThe examples of populations that we explored so far comprised a small number of units, and all their data were exactly and fully known. In concrete inference and decision problems of the kind we have been focusing on in chapters 17 and 19, we usually deal with populations that are much larger or potentially infinite; and data are known only for a small collection of their units.\nIn the glass-forensic example (table  20.1), for instance, many more glass fragments could be examined beyond the 10 units reported there, with no clear bound on the total number. We could even extend that population considering glass fragments from past and future crime scenes:\nthe imaginary example above also shows that the values of some variates for some units might be unknown; this is a situation we shall discuss in depth later.\nWe shall henceforth focus on statistical populations with a number of units that is in principle infinite, or so large that it can be considered practically infinite. “Practically” means that the number of units we’ll use as data or draw inferences about is a very small fraction, say less than 0.1%, of the total population size.\nThis is often the case. Consider for example (as in § 12.1.1) the collection of all possible 128 × 128 images with 24-bit colour depth. This collection has \\(2^{24 \\times 128 \\times 128} \\approx 10^{118 370}\\) units. Even if we used 100 billions of such images as data, and wanted to draw inferences on another 100 billions, these would constitute only \\(10^{-118 357}\\,\\%\\) of the whole collection. This collection is practically infinite.\nNote that we can’t say whether a population, per se, is “practically infinite” or not. It could be practically infinite for a particular inference problem, but not for another.\nWhen we use the term “population” it will often be understood that we’re speaking about a statistical population that is practically infinite with respect to the inference or decision problem under consideration.",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-infinite-populations",
    "href": "samples.html#sec-infinite-populations",
    "title": "23  Infinite populations and samples",
    "section": "",
    "text": "Table 23.1: Glass fragments, extended\n\n\n\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\nnotes\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\n\\({\\small\\verb;tableware;}\\)\n\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\n\\({\\small\\verb;building_windows_float_processed;}\\)\n\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\n\\({\\small\\verb;vehicle_windows_float_processed;}\\)\n\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\n\\({\\small\\verb;building_windows_non_float_processed;}\\)\n\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\n\\({\\small\\verb;containers;}\\)\n\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\n\\({\\small\\verb;headlamps;}\\)\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n351\n\\(1.52101\\)\n\\(8.75\\)\n\\(71.78\\)\n?\nfrom unsolved-crime scene in 1963\n\n\n…\n…\n…\n…\n…\n…\n\n\n1027\n\\(1.51761\\)\n\\(7.83\\)\n\\(72.73\\)\n?\ncrime scene in 2063\n\n\n…\n…\n…\n…\n…\n…",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-limit-freqs",
    "href": "samples.html#sec-limit-freqs",
    "title": "23  Infinite populations and samples",
    "section": "23.2 Limit frequencies",
    "text": "23.2 Limit frequencies\nIn § 21.2 we defined relative frequencies. Relative frequencies are ratios of two integers, the denominator being the population size \\(N\\). So a frequency \\(f\\) can only take on \\(N+1\\) rational values \\(0/N, \\dotsc, N/N\\) between \\(0\\) and \\(1\\). As the population size increases, the number of distinct, possible frequencies increases and eventually can be considered practically continuous. Frequencies in this case are sometimes called limit frequencies and they are treated as real numbers between \\(0\\) and \\(1\\).",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "samples.html#sec-samples",
    "href": "samples.html#sec-samples",
    "title": "23  Infinite populations and samples",
    "section": "23.3 Samples",
    "text": "23.3 Samples\n\nLearning from samples\nIn chapters 17 and 19 we considered an agent that must draw an inference about some units from a population. The agent’s degrees of belief in that inference relied (that is, were conditional on) units already observed in the population, the “learning” or “training” data. We saw that the agent’s degrees of belief changed, often becoming sharper, thanks to the information about the observed units.\nUnits for which we have full (or almost full) information, and that an agent can use to update its beliefs, are called a population sample or “sample” for short. Almost all data considered in engineering and data-science problems can be considered to be population samples.\nIt is extremely important to specify how a sample is extracted or collected from a population. For instance, if we consider table  20.1 to be a full population, we could extract a sample in such a way that \\(\\mathit{Type}\\) only has value \\({\\small\\verb;headlamps;}\\) (similarly to when we construct a subpopulation, § 22.1, but for a subpopulation we would select all units having that variate value). The marginal frequency of the value \\({\\small\\verb;headlamps;}\\) in the sample would then be \\(1\\), whereas in the original population it is \\(3/10 \\approx 0.333\\) – two very different frequencies.\n\n\n“Representative” and biased samples\nIf samples from a population are used as conditional information to calculate probabilities about other units, then they should of course be “relevant”, in some sense (not the technical sense of chapter  18), for the inference. The very definition of statistical population (§ 20.2) is meant to have such a relevance built-in: the “similarity” of the units makes each of them relevant for inferences about any other.\nStill, the procedure with which samples are selected from a population may lead to quirky and unreasonable inferences. For instance suppose we are interested in prognosing a disease for a person from a particular population, having observed a sample of people from the same population. If the sample was chosen to consist only of people having the disease, then it is obviously meaningless for our inference.\nThe specific problem in this example is that our inference is based on guessing a frequency distribution in the full population (as we’ll see more in detail in later chapters), but the sample, owing to the way it was chosen, cannot show a frequency distribution similar to the full-population frequency distribution.\n\n\nA sampling procedure may generate a sample that is pointless for some inferences, but still useful for others.\nIn the inference and decision problems under our focus we would like to use a sample for which particular frequencies – most often the full joint frequency – don’t differ very much from those in the full population. We’ll informally call this a “representative sample”. This is a difficult notion; the International Organization for Standardization for instance warns (item 3.1.14):\n\nThe notion of representative sample is fraught with controversy, with some survey practitioners rejecting the term altogether.\n\n\n\nIn many cases it is impossible for a sample of given size to be fully “representative”:\n\n\n\n\n\n\nExercise 23.1\n\n\n\nConsider the following population of 16 units, with four binary variates \\(W,X,Y,Z\\), each with values \\(0\\) and \\(1\\):\n\n\n\n\n\nTable 23.2: Four-bit population\n\n\n\n\n\n\\(W\\)\n\\(X\\)\n\\(Y\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n1\n0\n1\n\n\n0\n0\n1\n1\n\n\n1\n0\n1\n1\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nThe joint variate \\((W,X,Y,Z)\\) has 16 possible values, from \\((0,0,0,0)\\) to \\((1,1,1,1)\\). Each of these values appear exactly once in the population, so it has frequency \\(1/16\\). The marginal frequency distribution for each binary variate is also uniform, with frequencies of 50% for both \\(0\\) and \\(1\\).\n\nExtract a representative sample of size four units. In particular, the marginal frequency distributions of the four variates should be as close to 50%/50% as possible.\n\n\n\nLuckily, the probability calculus allows an agent to draw inferences also when the sample is too small to correctly reflect full-population frequencies, if appropriate background information is provided.\n\n\nObviously we cannot expect a population sample to exactly reflect all frequency distributions – joint, marginal, conditional – of the original population; some discrepancy is to be expected. How much discrepancy should be allowed? And what is the minimal size for a sample not to exceed such discrepancy?\nInformation Theory, briefly mentioned in chapter  18, can give reasonable answers to these questions. Let us summarize some examples here.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapters 1–10 of Information Theory, Inference, and Learning Algorithms\nVideo lectures 1–9 from the Course on Information Theory, Pattern Recognition, and Neural Networks\n\n\n\nFirst we need to introduce the Shannon entropy of a discrete frequency distribution. It is defined in a way analogous to the Shannon entropy for a discrete probability distribution, discussed in § 18.5. Lets say the distribution is \\(\\boldsymbol{f} \\coloneqq(f_1,f_2, \\dotsc)\\). Its Shannon entropy \\(\\mathrm{H}(\\boldsymbol{f})\\) is\n\\[\n\\mathrm{H}(\\boldsymbol{f}) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n\\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nand is measured in shannons when the base of the logarithm is 2.\nIf we have a population with joint frequency distribution \\(\\boldsymbol{f}\\), then a representative sample from it must have at least size\n\\[\n2^{\\mathrm{H}(\\boldsymbol{f})} \\equiv\n\\frac{1}{{f_1}^{f_1}\\cdot {f_2}^{f_2}\\cdot {f_3}^{f_3}\\cdot \\dotsb}\n\\] \nThis particular number has important practical consequences; for example it is related to the maximum rate at which a communication channel can send symbols (which can be considered as values of a variate) with an error as low as we please.\n\n\n\n\n\n\nExercise 23.2\n\n\n\n\nCalculate the Shannon entropy of the joint frequency distribution for the four-bit population of table  23.2.\nCalculate the minimum representative-sample size according to the Shannon-entropy formula. Is the result intuitive?\n\n\n\n\n\nIf we are only interested in a smaller number of variates of a population, then the representative sample can be smaller as well: its size would be given by the entropy of the corresponding marginal frequency distribution of the variates of interest. In the example of table  23.2, if we are only interested in the variate \\(X\\), then any sample consisting of two units, one having \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}0\\) and the other having \\(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}1\\), would be a representative sample of the marginal frequency distribution \\(f(X)\\).\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Kruskal & al. 1979: Representative Sampling, I\n\n\nA sample that presents some aspects, such as frequency distributions, which are at variance with the original population, is sometimes called biased. This term is used in many different ways by different authors. Unfortunately, most samples are “biased” in this sense.\nThe only way to counteract the misleading information given by a biased sample is to specify appropriate background information, which comes not from data samples but from a general meta-analysis, often based on physical, medical, and similar principles, of the problem and population.\n\n\n\n\nQuirks of samples for mean and standard deviation\nFor some populations, the mean and standard deviation calculated in a sample can be wildly different from those of the full population – even when the sample comprises half of the full population! This does not happen with the median and quartiles. Here is a demonstration in R. Try it out in your favourite programming language.\n\n\n\n\n\n\nExercise 23.3:  Guided exercise\n\n\n\nWe imagine to have a population of 1 000 000 units. These units have continuous interval variates \\(X\\) and \\(Y\\), each with an approximately standard Gaussian frequency distribution. These variates are not actual part of the population definition, however. Rather, an agent only has access to, or maybe it’s only interested in, the ratio of these two variates \\(Z \\coloneqq X/Y\\).\nThe agent is in particular interested in the mean of the variate \\(Z\\) in the full population, but has only access to the values of \\(Z\\) in a sample. How does the mean calculated from a sample of increasing size compare with the actual mean of the full population? For comparison, we also study the median of the full population and of the samples.\nFirst let’s create the values of the variates \\(X\\) and \\(Y\\), and construct \\(Z\\) from them:\n\n## Load custom plot functions\nsource('tplotfunctions.R')\nset.seed(1000) ## to reproduce results\n\nN &lt;- 1000000 ## population size\n\nX &lt;- rnorm(N) ## variate invisible to agent\nY &lt;- rnorm(N) ## variate invisible to agent\n\nZ &lt;- X / Y ## variate considered by agent\n\n## mean and median of Z in the full population\npopmean &lt;- mean(Z)\ncat('\\nThe full-population mean is', popmean, '(unknown to the agent)\\n')\n\n\nThe full-population mean is -8.04481 (unknown to the agent)\n\npopmedian &lt;- median(Z)\ncat('\\nThe full-population median is', popmedian, '(unknown to the agent)\\n')\n\n\nThe full-population median is 0.00188701 (unknown to the agent)\n\n\nNow we imagine that the agent accumulates samples from the population, starting from 100, increasing by 100 units, until half of the population has been sampled. At each sample increase the agent calculates the sample mean. We plot how the sample mean changes with the sample size. We also plot indicate the full-population mean, which the agent doesn’t know and is trying to guess:\n\n## sizes of successive samples\nsamplesizes &lt;- seq(from = 100, to = N / 2, by = 100)\n\n## empty vectors to contain the means and medians of the increasing samples\nsamplemeans &lt;- numeric(length(samplesizes))\nsamplemedians &lt;- numeric(length(samplesizes))\n\n## loop through the increasing samples, calculate mean for each\nfor(sample in seq_along(samplesizes)){\n    samplemeans[sample] &lt;- mean(Z[1:samplesizes[sample]])\n    samplemedians[sample] &lt;- median(Z[1:samplesizes[sample]])\n\n}\n\n## plot how sample means change with sample size, and the actual population mean\ncommonmax &lt;- 1.01 * max(abs(c(popmean, popmedian, samplemeans, samplemedians)))\nflexiplot(x = samplesizes, y = samplemeans,\n      xlab = 'sample size', ylab = 'sample mean',\n      col = 2, lwd = 4,\n      ylim = c(-commonmax, commonmax))\nabline(h = popmean, lty = 2, lwd = 3, col = 7)\ntext(y = popmean, x = max(samplesizes),\n    labels = 'population mean (unknown to agent)',\n    adj = c(1, 1), col = 7, cex = 1.2)\n\n\n\n\n\n\n\n## plot how sample medians change with sample size, and the actual population median\nflexiplot(x = samplesizes, y = samplemedians,\n      xlab = 'sample size', ylab = 'sample median',\n      col = 3, lwd = 4,\n      ylim = c(-commonmax, commonmax))\nabline(h = popmedian, lty = 2, lwd = 3, col = 7)\ntext(y = popmedian, x = max(samplesizes),\n    labels = 'population median (unknown to agent)',\n    adj = c(1, 1), col = 7, cex = 1.2)\n\n\n\n\n\n\n\n\nTest this again with several pseudorandom seeds.\n\n\n\nNow try a similar exercise but for the standard deviation of \\(Z\\)",
    "crumbs": [
      "[**Data II**]{.yellow}",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>[Infinite populations and samples]{.yellow}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html",
    "href": "categorization-inferences.html",
    "title": "24  A categorization of inferences",
    "section": "",
    "text": "24.1 A task- and information-oriented categorization of some inference problems\nIn this and the next few chapters we shall focus on particular kinds of inferences and predictions, and on how an AI agent should do them. Later on we shall also explore ways to make our agent faster, at the expense of optimality; most present-day machine-learning algorithms are examples of such fast, sub-optimal approximations.\nAll sorts of inferences must be faced in everyday life and in highly technological applications; in situations without serious consequences and in others, like medicine, where lives may be at stake.\nSuch variety of inferences cannot be separated into clear-cut categories. But an informal categorization can provide a starting point to examine some new kind of inference that we may have to face. Many inference tasks will fall in between categories; every data-engineering or data-science problem is unique.\nThe important questions for us are these:\nSo let’s focus on a categorization based on the types of desired information and of available information. For simplicity here we exclude all tasks that require an agent to continuously and actively interact with its environment for acquiring information, making choices, getting feedback, and so on. These tasks are the domain of Decision Theory in its most complex form, with ramified decisions, strategies, and possibly with the interaction among several decision-making agents. To explore and analyse this complex kind of tasks is beyond the purpose of this course.\nWe focus on tasks where multiple “instances” with similar characteristics are involved, and the agent has some question related to a “new instance”. According to the conceptual framework developed in the Data II part (chapters 20–23), we can view these “instances” as units of a practically infinite population. The “characteristics” that the agent observed or must guess are variates common to all these units.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html#sec-cat-problems",
    "href": "categorization-inferences.html#sec-cat-problems",
    "title": "24  A categorization of inferences",
    "section": "",
    "text": "What do we need or want to infer?\nFrom which kind of information?\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nDecision Analysis\nChapters 16–18 in Artificial Intelligence\nGames and Decisions\n\n\n\n\n\n\nRemember that you can adopt any terminology you like. If you prefer “instance” and “characteristics” or some other words to “unit” and “variate”, then use them. What’s important is that you understand the ideas and methods behind these words, and that you can exactly explain what you mean to others.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html#sec-categ-probtheory",
    "href": "categorization-inferences.html#sec-categ-probtheory",
    "title": "24  A categorization of inferences",
    "section": "24.2 Flexible categorization using probability notation",
    "text": "24.2 Flexible categorization using probability notation\nAn extremely useful way to express an inference task is directly through probability notation “\\(\\mathrm{P}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)”, once the relevant variates – or more generally the relevant sentences (ch.  6) – have been defined. It has two main advantages:\n\nIt directly gives us the probability or probabilities that the agent eventually needs to calculate.\nIt often eliminates ambiguity or vagueness in the inference task.\n\nThis latter advantage is often highly underestimated. As was briefly mentioned in § 6.2, many apparent difficulties in inference tasks arise not because of computational difficulties, but because it isn’t clear what the inference is about. You can witness, for example, different conclusions and debates in trying to determine “which model ois better” – which is not a clear inference at all, until it is precisely stated what “better” means, how it is measured, and by which variate it is represented.\nIt is therefore to your advantage if you learn and practice to translate an inference and its information into probability notation, and vice versa to read from probability notation what type of inference is being drawn.\nWe shall use this probability-notation method for delineating approximate categories of inference. Recall (§ 5.3) that in probability notation\n\\[\\mathrm{P}(\\text{\\color[RGB]{238,102,119}\\small[proposal]}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\text{\\color[RGB]{34,136,51}\\small[conditional]} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nthe proposal contains what the agent’s belief is about, and the conditional contains what’s supposed to be known to the agent, together with the background information \\(\\mathsfit{I}\\).\n\nNotation\nFirst let’s introduce some symbol conventions to be used in this and the next chapters. We consider a population, its units being the instances where an agent learned or guessed something. Denote the variates involved in the inferences by letters like  \\(X\\), \\(Y\\),  etc; keep in mind that each of these variates might itself be a composite one, for instance \\(X = (A, B)\\).  Subscripts, usually  \\({}_1\\) , \\({}_2\\) , \\({}_n\\) ,  etc., identify the individual instances; each subscript might be associated to the time or place of the instance.\nFocus on the inference that the agent is currently making, let’s say on unit \\(N+1\\). We can then divide all population variates intro three roles:\n\nThe predictands1 are the variates that the agent must guess for this unit, because it doesn’t know their values. We shall usually denote the predictands, jointly, with the symbol \\(Y\\). An inference always has a predictand.\nThe predictors are variates that the agent has observed in this unit; it knows their value. We shall usually denote the predictors, jointly, with the symbol \\(X\\). An inference may not have any predictors.\nThe nuisance variates are the remaining ones, which the agent neither needs to guess nor has observed in unit; it doesn’t know their values, and is not interested in their values. Nuisance variates, jointly, shall usually be denoted by \\(W\\). An inference may not have any nuisance variates.\n\n1 literally “what has to be predicted”In probability notation these roles are clear as we write\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X, W\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotso)\n\\end{aligned}\n\\]\nwhere the dots \\(\\dotso\\) do not refer to the present unit. You see that, for this \\((N+1)\\)th unit, the predictand “\\(Y\\)” is in the proposal, at the left of the conditional bar; this is what the agent’s belief is about. In the conditional, at the right of the conditional bar, we see there’s a predictor “\\(X\\)”; this means that its value is assumed to be known to the agent. We don’t see the variate “\\(W\\)”: not on the left of the conditional bar, because this is not what the belief is about; and not on the right, because its value is unknown. But we know this is a population variate; thus it is a nuisance variate. Note that the variate \\(W\\) might be known to the agent for other units.2\n2 In machine learning, instead of “predictand” the terms “dependent variable”, “class” or “label” (for nominal variates) are often; instead of “predictor”, the terms “independent variable” or “feature” are often used.\n\n\n\n\n\n Flexibility of “predictand”, “predictor”, “nuisance variate”\n\n\n\nThe roles of “predictand”, “predictor”, “nuisance variate” do not need to be fixed once and for all. Take the hospital example (§ 15.2): for the present inference the predictand might be \\(\\mathit{Urgency}\\):\n\\[\n\\mathrm{P}(U_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso) \\ ,\n\\]\nbut in the next inference the predictand might be \\(\\mathit{Transportation}\\):\n\\[\n\\mathrm{P}(T_{N+2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso) \\ .\n\\]\nThere are some inference tasks where the predictand and predictor are always the same. An example is image classification: an agent designed for it always takes an image as predictor, and tries to guess the image’s label as predictand:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathit{Label}_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathit{Image}_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(\\mathit{Label}_{N+2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathit{Image}_{N+2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso) \\ ,\n\\\\[1ex]\n&\\dotso\n\\end{aligned}\n\\]\nIn other inference tasks the predictand and predictor may be different from time to time, and even be exchanged. Examples occur in medicine: for a patient we may need to infer the disease given the observed symptoms, and for another patient we may need to forecast the symptoms, say to reduce their severity, given the patient’s known disease:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\mathit{Disease}_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathit{Symptom}_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso) \\ ,\n\\\\[1ex]\n&\\mathrm{P}(\\mathit{Symptom}_{N+2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathit{Disease}_{N+2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso) \\ .\n\\\\[1ex]\n&\\dotso\n\\end{aligned}\n\\]\n\n\nThe agent generally has mixed information about the three groups of variates for other units \\(1, 2, \\dotsc, N\\), usually called the training data. It may know \\(Y\\) for one unit, \\(X\\) for another, \\(W\\) for another, \\((Y,X)\\) for another still, and so on. Consider for example this expression:\n\\[\n\\mathrm{P}(Y_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{5}  \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{5}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{4}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{3}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{3}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{2}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}w_{1}  \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nThis agent, having background information \\(\\mathsfit{I}\\), is calculating its belief that the predictand \\(Y\\) has value \\(y_{5}\\) for unit #5. It knows the predictor \\(X\\) for this unit. The agent also knows the variate \\(Y\\), but not \\(X\\), for unit #4; it knows both \\(Y\\) and \\(X\\) for unit #3; it knows only \\(X\\) for unit #2; finally it knows \\(Y\\) and \\(W\\) for unit #1. This task might have additional variates, whose values are unknown to the agent for all units so far – but which may come into play for future units.\nIn fact, if any of these variate is composite, say \\(Y = (A,B)\\), the agent may have information only about component variate \\(A\\) rather than for all \\(Y\\). As already remarked, the number of possibilities is endless.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html#sec-example-inference-categories",
    "href": "categorization-inferences.html#sec-example-inference-categories",
    "title": "24  A categorization of inferences",
    "section": "24.3 Examples of inference tasks",
    "text": "24.3 Examples of inference tasks\nLet’s have an overview of some common inference tasks\n\nNew unit: given vs generated\nA first important distinction can be made between\n\nTasks where an agent is given a new unit, of which it must guess some or all variates.\nTasks where an agent must generate a new unit, with all its variates.\n\nExamples of the first type of task are image generation and word generation, which Large Language Models do. An algorithm is given a collection of images or a corpus of texts, and is asked to generate a new image or text based on, or “inspired” by, them.\nIt’s very important to keep in mind that despite the use of the words “generate” and “guess”, both tasks above require the computation of probabilities; that is, the agent must assess its beliefs. They are both inference tasks. In generation, the agent needs to assess what’s best to generate; in guessing, it needs to assess the possible guesses.\nIndeed we shall see that these two types of task are actually quite close to each other from the point of view of Decision Theory & Probability Theory.\n\nIn machine learning, the terms “generative” and “discriminative” are sometimes associated with the two types of task above.\n\n\n\nGuessing variates: all or some\nFocusing on the second type of task above – the agent must make guesses about a unit given to it – we can further divide it into two subtypes:\n\nThe agent must guess all variates of the new unit:\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\n\\end{aligned}\n\\]\n\nThe agent must guess some variates of the new unit, but can observe all others:\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotso)\n\\end{aligned}\n\\]\nAn example of the first subtype of task is the “urgent vs non-urgent” problem of § 17.4: having observed incoming patients, some of which where urgent and some non-urgent, the agent must guess whether the next incoming patient will be urgent or not. No other kinds of information (transport, patient characteristics, or others) are available.\n\nIn machine learning, the terms “unsupervised learning” and “supervised learning” are sometimes associated with these two subtypes of task. But the association is loose. “Clustering” tasks for example, discussed below, are usually called “unsupervised” but they are examples of the second subtype above, where the agent has some predictors.\n\n\n\nInformation available in previous units\nFinally we can further divide the second subtype above into two or three subsubtypes, depending on the information available to the agent about previous units:\n\nPredictor and predictand are known for all previous units \\(N,\\dotsc,1\\):\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{N} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotso\nY_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nThe predictor variate, but not the predictand one, is known for all previous units \\(N,\\dotsc,1\\):\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotso\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nThe predictand variate, but not the predictor one, is known for all previous units \\(N,\\dotsc,1\\):\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotso\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\end{aligned}\n\\]\n\nAn example of the first subsubtype of task above is image classification. The agent is for example given the following 128 × 128-pixel images (predictor) and character-labels (predictand) from the One Punch Man series:\n\nand is then given one new 128 × 128-pixel image:\n\n\n\n\n\nof which it must guess the character-label.\nA pictorial representation of the probability notation for this case could be as follows:\n\nwhere \\({\\color[RGB]{238,102,119}y} \\in \\set{\\color[RGB]{238,102,119}{\\small\\verb;Saitama;}, {\\small\\verb;Fubuki;}, {\\small\\verb;Genos;}, {\\small\\verb;MetalBat;}, \\dotsc \\color[RGB]{0,0,0}}\\).\n\n\nAn example similar to the second subtype is clustering. The agent is for example given the following set of points in \\((\\alpha, \\beta)\\) coordinates:\n\n\n\n\n\nEach point is a unit, and the predictor variate is the pair of coordinates \\((\\alpha, \\beta)\\).\nThe agent must now guess the label of each point: this is the predictand. One possible guess could be the following:\n\n\n\n\n\nThis kind of inference can be expressed as follows:\n\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ \\mathit{label}, \\alpha, \\beta\n\\\\[1ex]\n&\\mathrm{P}(\n\\mathit{label}_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\n\\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, \\dotso \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\mathit{label}_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\alpha_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\beta_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\n\\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, \\dotso \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\alpha_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\beta_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\n\\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nIn machine learning, the term “supervised learning” typically refer to the first subsubtype above.\nThe term “unsupervised learning” can refer to the second subsubtype.\nThe third subsubtype is very rarely considered in machine learning, yet it is not an unrealistic task.\n\n\n\nThe types, subtypes, subsubtypes described above are obviously not mutually exclusive or comprehensive. Consider for instance the following task, which doesn’t fit into any of the types discussed above. The agent must guess the predictand \\(Y\\) for the new unit #3, observing that its predictor \\(X\\) has value \\(x_3\\). Of two previous units, the agent knows the predictor value \\(x_1\\) of the first, and the predictand value \\(y_2\\) of the second. This task is expressed by\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_3\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_3\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\end{aligned}\n\\]\n\nIn machine learning, hybrid situations like these are categorized as “missing data” or “imputation”.\n\n\n\n\n\n\n\n\nExercise 24.1\n\n\n\n\nFind an example, invented or realistic, for the last type of task discussed above:\n\\[\n\\begin{aligned}\n&\\text{\\small population variates: }\\ Y, X\n\\\\[1ex]\n&\\mathrm{P}(Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotso\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\end{aligned}\n\\]\nWhat kind of task does the following probability express? what kind of peculiarities does it have?\n\\[\n\\mathrm{P}(\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1} \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nW_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}w_{N} \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nW_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}w_{1} \\,\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\mathsfit{I})\n\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html#sec-underlying-distribution",
    "href": "categorization-inferences.html#sec-underlying-distribution",
    "title": "24  A categorization of inferences",
    "section": "24.4 The underlying distribution",
    "text": "24.4 The underlying distribution\nFrom the discussion and all examples above we can draw the following conclusion:\n\n\n\n\n\n\n \n\n\n\n\nProbability distribution such as those discussed above intrinsically enter all types of inference algorithms, including machine-learning algorithms.\n\n\n\nThis is the condition for any inference algorithm to be optimal and self-consistent. The less an algorithm satisfies that condition, the less optimal and the less consistent it is.\nA remarkable feature of all the probabilities discussed in the above task categorization is that they can all be calculated from one and the same probability distribution. We briefly discussed and used this feature in chapter  17.\nA conditional probability such as \\(\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) can always be written, by the and-rule, as the ratio of two probabilities:\n\\[\n\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}B} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\mathsfit{\\color[RGB]{238,102,119}A}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}B} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\\mathsfit{\\color[RGB]{34,136,51}B} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nTherefore we have, for the probabilities of some of the tasks above,\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\\\[2em]\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}(\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nWe also know the marginalization rule (chapter  16.1): any quantity \\(\\color[RGB]{204,187,68}C\\) with values \\(\\color[RGB]{204,187,68}c\\) can be introduced into the proposal of a probability via the or-rule:\n\\[\n\\mathrm{P}( {\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\sum_{\\color[RGB]{204,187,68}c}\\mathrm{P}({\\color[RGB]{204,187,68}C\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}c} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}\\boldsymbol{\\dotsb}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nUsing the marginalization rule we find these final expressions of the probabilities for some of the tasks discussed so far:\n\n\n\n\n\n\n\n \n\n\n\n\nAll previous predictors and predictands known:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}y}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nPrevious predictors known, previous predictands unknown:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\quad{}=\n\\frac{\n\\sum_{\\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{{\\color[RGB]{170,51,119}y}, \\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\nAll these formulae, even for hybrid tasks, involve sums and ratios of only one distribution:\n\\[\\boldsymbol{\n\\mathrm{P}(\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nStop for a moment and think about the consequences:\n\n  An agent that can perform one of the tasks above can, in principle, also perform all other tasks.\nThis is why a perfect agent, working with probability, in principle does not have to worry about “supervised”, “unsupervised”, “missing data”, “imputation”, and similar situations. This also shows that all these task typologies are much closer to one another than it might look like from the perspective of current machine-learning methods.\n\n\n\nThe acronym OPM  can stand for Optimal Predictor Machine or Omni-Predictor Machine\n\n  The probability distribution above encodes the agent’s background knowledge and assumptions; different agents differ only in the values of that distribution.\nIf two agents yield different probability values in the same task, with the same variates and same training data, the difference must come from the joint probability distribution above. And, since the data given to the two agents are exactly the same, the difference must lie in the agents’ background information \\(\\mathsfit{I}\\).\n  Data cannot “speak for themselves”\nGiven some data, we can choose two different joint distributions for these data, and therefore get different results in our inferences and tasks. This means that the data alone cannot determine the result: specific background information and assumptions, whether acknowledged or not, always affect the result.\n\nThe qualification “in principle” in the first consequence is important. Some of the sums that enter the formulae above are computationally extremely expensive and, with current technologies and maths techniques, cannot be performed within a reasonable time. But new technologies and new maths discoveries could make these calculations possible. This is why a data engineer cannot simply brush them aside and forget them.\nAs regards the third consequence, we shall see that there are different states of knowledge which can converge to the same results, as the number of training data increases.\n\n\n\n\n\n\nExercise 24.2\n\n\n\nIn a previous example of “hybrid” task we had the probability distribution\n\\[\n\\mathrm{P}(\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nX_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{2}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nRewrite it in terms of the underlying joint distribution.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "categorization-inferences.html#plan-for-the-next-few-chapters",
    "href": "categorization-inferences.html#plan-for-the-next-few-chapters",
    "title": "24  A categorization of inferences",
    "section": "24.5 Plan for the next few chapters",
    "text": "24.5 Plan for the next few chapters\nOur goal in building an “Optimal Predictor Machine” is now clear: we must find a way to\n\n\n\n\nassign the joint probability distribution above, in such a way that it reflects some reasonable background information\nencode the distribution in a computationally useful way\n\nThe “encode” goal sounds quite challenging, because the number \\(N\\) of units can in principle be infinite; we have an infinite probability distribution.\nIn the next Inference III part we shall see that partially solving the “assign” goal actually makes the “encode” goal feasible.\n\nOne question arises if we now look at present-day machine-learning algorithms. Many popular machine-learning algorithms don’t give us probabilities about values. They return one definite value. How do we reconcile this with the probabilistic point of view above? We shall answer this question in full in the last chapters; but a short, intuitive answer can already be given now.\nIf there are several possible correct answers to a given guess, but a machine-learning algorithm gives us only one answer, then the algorithm must have internally chosen one of them. In other words, the machine-learning algorithm is internally doing decision-making. We know from chapters 2 and 3 that this process should obey Decision Theory and therefore must involve:\n\n  the probabilities of the possible correct answers\n  the utilities of the possible answer choices\n\nNon-probabilistic machine-learning algorithms must therefore be approximations of an Optimal Predictor Machine that, after computing probabilities, selects one particular answer by using utilities.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>[A categorization of inferences]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html",
    "href": "exchangeable_probabilities.html",
    "title": "25  Exchangeable beliefs",
    "section": "",
    "text": "25.1 Recap\nIn the chapters of part Inference I we had an overview of how an agent can draw inferences and make predictions of the most general kind, expressed by general sentences, using the four fundamental rules of inference.\nThen, in part Inference II, we successively narrowed our focus on more and more specialized kinds of inference, typical of engineering and data-science problems and of machine-learning algorithms. First we considered inferences about measurements and observations, then inferences about multiple instances of similar measurements and observations. The idea was that an agent can arrive at sharper degrees of belief – that is, it can learn – by using information about “similar instances”.\nFor these purposes we introduced a specialized language about quantities and data types in part Data I, and about “populations” of similar “units” in part Data II.\nIn ch.  24 we had an overview of possible types of tasks, many of which are typical of machine learning algorithms such as deep networks and random forests. We found a remarkable result: a perfect agent – one that operates according to Probability Theory – can in principle perform any and all of those tasks by using the joint probability distribution\n\\[\n\\mathrm{P}(\nY_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhere \\(Y\\) and \\(X\\) are the variates of interest.\nThe agent only needs to do some calculations with this joint distribution, involving sums and divisions. This distribution must be specified for all possible values of \\(x_{1}, \\dotsc, x_{N+1}\\), \\(y_{1}, \\dotsc, y_{N+1}\\), and \\(N\\).\nFor example take the task of predicting some variates (predictands) for a new unit, from knowledge of other variates (predictors) for the same unit and of all variates for \\(N\\) other units. Solving this task corresponds to calculating\nTo build an AI agent that deals with these kinds of task we must:\nIn order to reach these two goals we shall now narrow our focus further, upon inferences satisfying a condition that greatly simplifies the calculations, and that is also reasonable in many real inference problems – and it is moreover typical of many “supervised” and “unsupervised” machine-learning applications.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "href": "exchangeable_probabilities.html#sec-recap-before-exchang",
    "title": "25  Exchangeable beliefs",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}}\n    \\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n    \\color[RGB]{0,0,0}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    {\\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\n\n\nchoose a joint distribution according to reasonable assumptions and background information,\nencode it in a computationally feasible way.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "href": "exchangeable_probabilities.html#sec-excheable-beliefs",
    "title": "25  Exchangeable beliefs",
    "section": "25.2 States of knowledge with symmetries",
    "text": "25.2 States of knowledge with symmetries\nAn agent’s degrees of belief about a particular population may satisfy a special symmetry called exchangeability. This symmetry can be understood from different points of view. Let’s start from one of these viewpoints, and then make connections with alternative ones.\nTake again two populations briefly mentioned in § 20.1:\n\n\nStock exchange\n\nThe daily change in closing price of a stock during 1000 consecutive days. Each day the change can be positive or zero: \\({\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\), or negative: \\({\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\).\n\n\n\n\n\n\n\n\nMars prospecting\n\nA collection of 1000 similar-sized rocks gathered from a specific, large crater on Mars. Each rock either contains haematite: \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), or it doesn’t: \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\n\n\n\n\n\n\n\nSuppose that, in each of these populations, you (the agent) don’t know the variate value for unit #735, and for some reason would like to infer it. You are given the variate values for 100 other units, which you can use to improve your inference. Now consider this question:\n\nHow much does the relative order of the 100 known units and the unknown unit matter to you, for drawing your inference?\n\nWe know, from information theory, that it never hurts having extra information, such as the units’ order. But you probably judge the units’ order to be much more important for your inference in the stock-exchange case than in the Mars-prospecting one. In the stock-exchange case it would be more informative to have data from units temporally close to unit #735; for example units #635–#734, or #736–#835, or #685–#734 & #736–#785, or similar ranges. But in the Mars-prospecting case you might find it acceptable if the 100 known units were picked up in some unsystematic way from the catalogue of remaining 999 units. There are reasons, boiling down to physics, behind this kind of judgement.\nThe question above could also be replaced by others, slightly different but still connected to the same issue. For example:\n\nHow strongly would you like to be able to choose which 100 units you can have data from, in order to draw your inference?\n\nor\n\nHow much would you be upset if the original order of the population units were destroyed by accidental shuffling?\n\nor\n\nWould it be acceptable to you if only the frequencies of the values (\\(\\set{{\\color[RGB]{34,136,51}{\\small\\verb;+;}},{\\color[RGB]{238,102,119}{\\small\\verb;-;}}}\\) in one case, \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\) in the other) for the 100 known units were given to you?\n\n\n\n\n\n\n\nExercise 25.1\n\n\n\n\nFind examples of populations where the units have some kind of ordering that you think would be very important for drawing inferences about some units, given other units. Examine why you judge such ordering to be important. (The ordering doesn’t need to be one-dimensional. For instance, the pixel intensities of an image also have a two-dimensional relative order or position: is that important if you want to draw inferences about the intensities of some pixels from those of other pixels?)\nFind examples of populations where any potential ordering of the units would not be very important for drawing inferences about some units, given other units. Or, put it otherwise, you wouldn’t be excessively upset or worried if such order were lost owing to accidental shuffling of the units.\n\n\n\nMany kinds of inference considered in data science and engineering, and all inferences done with “supervised” or “unsupervised” machine-learning algorithms, are examples where any ordering of the data used for learning is deemed irrelevant and is, in fact, often lost. This irrelevance is clear from the data-shuffling involved in many procedures that accompany these algorithms.\nWe shall thus restrict our attention to situations and kinds of background information where this judgement of irrelevance is considered appropriate. In reality this is not a black-or-white situation: it is possible that some kind of ordering information would improve our inferences; what we are assuming here is that this improvement is so small that it can be neglected altogether.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "href": "exchangeable_probabilities.html#sec-exchaneable-distr",
    "title": "25  Exchangeable beliefs",
    "section": "25.3 Exchangeable probability distributions",
    "text": "25.3 Exchangeable probability distributions\nLet’s take the Mars-prospecting problem as a concrete example. Denote by \\(H\\) the variate expressing haematite presence, with domain \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}, {\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\).\nIf an agent’s background information or assumption \\(\\mathsfit{I}\\) says that the relative order of units – rocks in this case – is irrelevant for inferences about other units, then it means that a probability such as\n\\[\n\\mathrm{P}(R_{735}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{734}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{733}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{732}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{731}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{730}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nmust be equal to the probability\n\\[\n\\mathrm{P}(R_{735}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{87}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{16}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{52}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{988}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nand in fact to any probability like\n\\[\n\\mathrm{P}(R_{i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{j}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{k}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{l}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{m}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{n}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nfor instance\n\\[\n\\mathrm{P}(R_{356}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\nR_{952}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{103}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{69}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{740}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{679}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I})\n\\]\nwhere \\(i\\), \\(j\\), and so on are different but otherwise arbitrary indices.\nIn other words, the probability depends on whether we are inferring \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) or \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\), and on how many \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appear in the conditional; in the example above, three \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and two \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\). This property should also apply if the agent makes inferences about more than one unit, conditional on any number of units. It can be proven that this property is equivalent, in our present example, to this general requirement:\n\nThe value of a joint probability such as\n\\[\\mathrm{P}(R_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\n\\dotso\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{\\scriptscriptstyle\\dotso}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\ \\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I})\n\\]\ndepends only on the total number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) values and total number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) values that appear in it, or, equivalently, on the absolute frequencies of the values that appear in it.\n\n\n\nLeaving the Mars-specific example and generalizing, we can define the following property, called exchangeability:\n\n\n\n\n\n\n \n\n\n\n\nA joint probability distribution is called exchangeable if the probabilities for any number of units depend only on the absolute frequencies of the values appearing in them.\n\n\n\nLet’s see a couple more examples.\n\n\nDon’t forget that\n\\(\\mathrm{P}(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nand\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)\nmean exactly the same, because and (symbol “\\(,\\)”) is commutative!\n\nConsider an infinite population with variate \\(Y\\) having domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;low;}},{\\color[RGB]{204,187,68}{\\small\\verb;medium;}},{\\color[RGB]{34,136,51}{\\small\\verb;high;}}}\\). If the background information \\(\\mathsfit{J}\\) guarantees exchangeability, then these three joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{6}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{283}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{91}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{72}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1838}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause they all have one \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), one \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), two \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\). The same is true for these two probabilities:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nY_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3024}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\\\[2ex]\n&=\\mathrm{P}(\nY_{26}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{611}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{78}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J}\n)\n\\end{aligned}\n\\]\nbecause both have zero \\({\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\), two \\({\\color[RGB]{204,187,68}{\\small\\verb;medium;}}\\), one \\({\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\).\n\n\n\nConsider an infinite population with variates \\((U,V)\\) having joint domain \\(\\set{{\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}} \\times \\set{{\\color[RGB]{102,204,238}-1}, {\\color[RGB]{119,119,119}0}, {\\color[RGB]{204,187,68}1}}\\)  (six possible joint values). If the background information \\(\\mathsfit{K}\\) guarantees exchangeability, then these two joint probabilities must have the same value:\n\\[\\begin{aligned}\n&\\quad\\mathrm{P}(\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{337}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{825}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{66}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[3ex]\n&=\\mathrm{P}(\nU_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{421}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{55}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{43}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{14}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad\\qquad\\qquad\\quad\nU_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{928}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;fail;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{700}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{119,119,119}0}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\\nU_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;pass;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}V_{39}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}-1}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\end{aligned}\n\\]\nbecause both have: one \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{102,204,238}-1})\\),  two \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{119,119,119}0})\\),  zero \\(({\\color[RGB]{238,102,119}{\\small\\verb;fail;}},{\\color[RGB]{204,187,68}1})\\),  three \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{102,204,238}-1})\\),  zero \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{119,119,119}0})\\),  one \\(({\\color[RGB]{34,136,51}{\\small\\verb;pass;}},{\\color[RGB]{204,187,68}1})\\). From this example, note that it’s important to count the occurrences of the joint values, not of the values of the single variates independently.\n\n\n\n\n\n\n\n\nExercise 25.2\n\n\n\n\nFirst let’s check that you haven’t forgotten the basics about connectives (§ 6.4), Boolean algebra § 9.1), and the four fundamental rules of inference (§ 8.5):\n\nHow much is  \\(\\mathrm{P}(Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{J})\\) ?\nSimplify the probability\n\n\\[\\mathrm{P}(X_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{28}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nwhat are the absolute frequencies of the values \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) among the units in the probability above?\n\n\n\n\nFor each collection of probabilities below (the sentences \\(\\mathsfit{I'}, \\mathsfit{I''}, \\mathsfit{J'}\\dotsc\\) indicate different states of knowledge), say whether they cannot come from an exchangeable probability distribution, or if they might (to guarantee exchangeability, one has to check an infinite number of inequalities, so we can’t be sure about it unless they give us a general formula for the joint probabilities):\n\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(C_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 31.6\\%\n  \\\\\n  &\\mathrm{P}(C_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}-1\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I'}) = 24.8\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(Z_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{53}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%\n  \\\\\n  &\\mathrm{P}(Z_{3904}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;on;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{29}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;off;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I''}) = 9.7\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(A_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{87}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 6.2\\%\n  \\\\\n  &\\mathrm{P}(A_{99}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{10}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}A_{13}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J'}) = 8.9\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(W_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{97}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{300}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 6.2\\%\n  \\\\\n  &\\mathrm{P}(W_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{86}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{107}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{J''}) = 8.9\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(B_{1190}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{1152}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{233}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 7.5\\%\n  \\\\\n  &\\mathrm{P}(B_{1185}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}B_{424}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K'}) = 12.3\\%\n  \\end{aligned}\\)\n\\(\\begin{aligned}[c]\n  &\\mathrm{P}(S_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{21}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{33}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 5.0\\%\n  \\\\\n  &\\mathrm{P}(S_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;-;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{5}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;low;}}\\ \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\  S_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;+;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{34,136,51}{\\small\\verb;high;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K''}) = 2.9\\%\n  \\end{aligned}\\)\n\n\n\n\n\nFurther constraints\nThe exchangeability property greatly reduces the number of probabilities that an agent needs to specify. For a population with a binary variate, a joint probability distribution for 1000 units would require the specification of around \\(2^{1000} \\approx 10^{300}\\) probabilities. But if this distribution is exchangeable, only \\(1000\\) probabilities need to be specified (the absolute frequency of one of the two values, ranging between 0 and 1000; minus one because of normalization).1\n1 For the general case of a variate with \\(n\\) values, and \\(k\\) units, the number of independent probabilities is \\(\\binom{n+k-1}{k}\\).Moreover, the exchangeable joint distributions for different numbers of units satisfy additional restrictions, owing to the fact that each of them is the marginal distribution of all distributions with a larger number of units. In the Mars-prospecting case, for instance, if \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is the degree of belief that rock #1 contains haematite, we must also have\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n&=\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\\n&= \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nfor any two different units #\\(a\\) and #\\(b\\). Therefore, if the agent has specified \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\), then three of these four probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\\\[1ex]\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\qquad\n\\mathrm{P}(R_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\n\\]\nare completely determined if we specify just one of them.\n\n\n\n\n\n\nExercise 25.3\n\n\n\nAssume that the state of knowledge \\(\\mathsfit{I}\\) implies exchangeability, and a population has binary variate \\(R\\in \\set{{\\color[RGB]{204,187,68}{\\small\\verb;N;}},{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}\\).\nIf\n\\[\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.75 \\qquad \\mathrm{P}(R_{4}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{9}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = 0.60\\]\nThen how much are the probabilities\n\\[\\mathrm{P}(R_{15}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?} \\qquad\n\\mathrm{P}(R_{7}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{11}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) = \\mathord{?}\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>[Exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html",
    "href": "inference_from_freqs.html",
    "title": "26  Inferences from frequencies",
    "section": "",
    "text": "26.1 If the population frequencies were known\nLet’s now see how the exchangeability of an agent’s degrees of belief allows it to calculate probabilities about the units of a population. We shall do this calculation in two steps. First, in the case where the agent knows the joint frequency distribution (§§21.2, 21.3, 23.2) for the full population. Second, in the more general case where the agent lacks this population-frequency information.\nWhen the full-population frequency distribution is known, the calculation of probabilities is very intuitive and analogous to the stereotypical “drawing balls from an urn”. We shall rely on this intuition; keep in mind, however, that the probabilities are not assigned “by intuition”, but actually fully determined by the two basic pieces of knowledge or assumptions: exchangeability and known population frequencies. Some simple proof sketches of this will also be given.\nWe consider an infinite population with any number of variates. For concreteness we assume these variates to have finite, discrete domains; but the formulae we obtain can be easily generalized to other kinds of variates. In this and the following chapters we shall often use the simplified income dataset (file income_data_nominal_nomissing.csv and its underlying population as an example. This population has nine nominal variates. The variates, their domain sizes, and their possible values are listed at this link.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-pop-freq-known",
    "href": "inference_from_freqs.html#sec-pop-freq-known",
    "title": "26  Inferences from frequencies",
    "section": "",
    "text": "Notation recap\nWe shall mainly use the notation introduced in § 24.2:\nIn applications where the agent wants to infer the values of some predictand variates, given the observation of predictor variates, the former are denoted \\(Y\\), the latter \\(X\\). In the income problem, for instance, the agent (some USA census agency) would like to infer the \\(\\mathit{income}\\) variate of a person from the other eight demographic characteristics \\(\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\) of that person. So in this inference problem we define\n\\[\n\\begin{aligned}\nY &\\coloneqq{\\mathit{income}}\n\\\\[1ex]\nX &\\coloneqq({\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{native\\_country}})\n\\end{aligned}\n\\]\nWe shall, however, also consider slightly different inference problems, for example with \\(({\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex}})\\) as predictand and the remaining seven variates \\(({\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}})\\) as predictors.\nSometimes we shall use red colour for quantities that are not known in the problem, and green for quantities that are known.\nAll population variates, jointly, are denoted \\(Z\\). In the case of the income dataset, for instance, the variate \\(Z\\) stands for the joint variate with nine components:\n\\[\n\\begin{aligned}\nZ &\\coloneqq(Y, X) = (\n\\mathit{workclass} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{education} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{marital\\_status} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n{}\\\\ &\\qquad\n\\mathit{relationship} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{race} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{native\\_country} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathit{income}\n)\n\\end{aligned}\n\\]\nWhen we write \\(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\), the symbol \\(z\\) stands for some definite joint values, for instance \\(({\\color[RGB]{68,119,170}{\\small\\verb;Without-pay;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Doctorate;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Ireland;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&gt;50K;}})\\).",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-know-freq",
    "href": "inference_from_freqs.html#sec-know-freq",
    "title": "26  Inferences from frequencies",
    "section": "26.2 Knowing the full-population frequency distribution",
    "text": "26.2 Knowing the full-population frequency distribution\nNow suppose that the agent knows the full-population joint frequency distribution. Let’s make clearer what this means. In the income problem, for instance, consider these two different joint values for the joint variate \\(Z\\):\n\\[\n\\begin{aligned}\n{ z^{*}}&\\coloneqq(\n{\\small\\verb;Private;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Machine-op-inspct;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\\\[2ex]\n{ z^{**}}&\\coloneqq(\n{\\small\\verb;Self-emp-not-inc;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;HS-grad;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Married-civ-spouse;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{}\n\\\\\n&\\qquad {\\small\\verb;Farming-fishing;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Husband;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;White;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;Male;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;United-States;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\small\\verb;&lt;=50K;}\n)\n\\end{aligned}\n\\]\nThe agent knows that the value \\(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) occurs in the full population of interest (in this case all 340 millions or so USA citizens, considered in a short period of time) with a relative frequency \\(0.860 369\\%\\); it also knows that the value \\(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) occurs with a relative frequency \\(0.260 058\\%\\). We write this as follows:\n\\[\nf({ Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}}) = 0.860 369\\% \\ ,\n\\qquad\nf({ Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}}) = 0.260 058\\%\n\\]\nThe agent knows not only the frequencies of the two particular joint values \\(z^{*}\\), \\(z^{**}\\), but for all possible joint values, that is, for all possible combinations of values from the single variate. In the income example there are 54 001 920 possible combinations, and therefore just as many relative frequencies. All these frequencies together form the full-population frequency distribution for \\(Z\\), which we denote collectively with \\(\\boldsymbol{f}\\) (note the boldface). Let’s introduce the quantity \\(F\\), denoting the full-population frequency distribution. Knowledge that the frequencies are \\(\\boldsymbol{f}\\) is then expressed by the sentence \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\).\n\n\n\n\n\n\n Don’t confuse the full population with a sample from it\n\n\n\nNote that the frequencies reported above are not the ones found in the income_data_nominal_nomissing.csv dataset, because that dataset is only a sample from the full population, not the full population. The frequency values reported above are purely hypothetical (but not inconsistent with the frequencies observed in the sample).\n\n\nIn other cases, these hypothetically known frequencies would refer to the full population of units: maybe even past, present, future, if they span a possibly unlimited time range.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-1unit-freq-known",
    "href": "inference_from_freqs.html#sec-1unit-freq-known",
    "title": "26  Inferences from frequencies",
    "section": "26.3 Inference about a single unit",
    "text": "26.3 Inference about a single unit\nNow imagine that the agent, given the information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies and some background information \\(\\mathsfit{I}\\), must infer all \\(Z\\) variates for a specific unit \\(u\\). In the income case, it would be an inference about a specific USA citizen. This unit \\(u\\) could have any particular combination of variate values; in the income case it could have any one of the 54 001 920 possible combined values. The agent must assign a probability to each of these possibilities.1 Which probability values should it assign?\n1 Remember that this is what we mean when we say “drawing an inference”! (See chap.  5 and § 14.1)Intuitively we would say that the probability for a particular value \\(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\) should be equal to the frequency of that value in the full population:\n\n\n\n\n\n\n \n\n\n\n\nif \\(\\mathsfit{I}\\) leads to an exchangeable probability distribution, then\n\\[\n\\mathrm{P}(Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) = f(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z})\n\\]\nfor any unit \\(u\\).\n\n\n\nFor instance, the probabilities that unit \\(u\\) has the values \\(z^{*}\\) or \\(z^{**}\\) above is\n\\[\n\\begin{aligned}\n&\\mathrm{P}(Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{*}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{*}}) = 0.860 369\\%\n\\\\[1ex]\n&\\mathrm{P}(Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\nf(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}}) = 0.260 058\\%\n\\end{aligned}\n\\]\nThis intuition is the same as in drawing balls, which may have different sets of labels, from a collection, given that we know the proportion of balls with each possible label set.\nBut the equality above can actually be proven mathematically in this specific case: it follows from the assumption of exchangeability. Let’s examine a very simple case to get an idea of how this proof works.\n\nExact calculation of the probabilities in a simple case\nSuppose we have three rocks from our Mars-prospecting collection. They are marked #1, #2, #3. They look alike, but we know that two of them have haematite, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) for them, and one doesn’t, so \\(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) for that rock. This background information – let’s call it \\(\\mathsfit{K}_{\\textsf{3}}\\) – is a simple case of a finite population with three units and a binary variate \\(R\\). We know that the frequency distribution for this population is\n\n\n\n\\[f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = 2/3 \\qquad f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) = 1/3\\]\nOur information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the frequencies corresponds to the following composite sentence:\n\n\\[\nF\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\ \\Longleftrightarrow\\\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nGiven \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\), we know that \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) is true: \\(\\mathrm{P}( F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})=1\\), which means\n\n\\[\n\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}, \\mathsfit{K}_{\\textsf{3}}\\bigr] = 1\n\\]\n\nNow use the or-rule, considering that the three ored sentences are mutually exclusive:\n\n\\[\n\\begin{aligned}\n1&=\\mathrm{P}\\bigl[(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\lor\n(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}\\bigr]\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n+{}\n\\\\&\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\end{aligned}\n\\]\n\nAccording to our background information \\(\\mathsfit{K}_{\\textsf{3}}\\), our degrees of belief are exchangeable. This means that the three probabilities summed up above must all have the same value, because in each of them \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) once. But if we are summing the same value thrice, and the sum is \\(1\\), that that value must be \\(1/3\\). Hence we find that\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\\n&\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n= 1/3\n\\\\[1ex]\n&\\text{all other probabilities are zero}\n\\end{aligned}\n\\]\nNow let’s find the probability that a rock, say #1, has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)), given that we haven’t observed any other rocks: \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\). This is a marginal probability (§ 16.1), so it’s given by the sum\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) &=\n\\sum_{i={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\sum_{j={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) + {}\n\\\\ &\\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\n\\\\[1ex]\n&= 0 + 1/3 + 1/3 + 0\n\\\\[1ex]\n&= 2/3\n\\end{aligned}\n\\]\nwhich is indeed equal to \\(f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\).\n\n\nThis simple example gives you an idea why our intuition for equating – in specific circumstances – probability with full-population frequencies, is actually a mathematical theorem: it follows from (1) knowledge of the full-population frequencies, and (2) exchangeability.\n\n\n\n\n\n\nExercise 26.1\n\n\n\n\nCalculate \\(\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), that is, the probability that rock #2 has haematite, given that we don’t know the haematite content of any other rock. Is it different from \\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), or not? Why?\nBuild a similar proof for a slightly different case; for example four rocks; or two units from a population with a variate having three possible values (instead of just the two \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\)).\nConsider the same calculation we did above, but in the case of background knowledge \\(\\mathsfit{K}_{\\text{NE}}\\) where our degrees of belief are \\(\\text{N}\\)ot \\(\\text{E}\\)xchangeable. For instance, give three different values to the probabilities\n\\[\n\\begin{gathered}\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\\\\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nR_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\text{NE}})\n\\end{gathered}\n\\]\nin such a way that they still sum up to \\(1\\). Then find by marginalization the probability that rock #1 contains haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). Is this probability still equal to the relative frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)?",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-moreunit-freq-known",
    "href": "inference_from_freqs.html#sec-moreunit-freq-known",
    "title": "26  Inferences from frequencies",
    "section": "26.4 Inference about several units",
    "text": "26.4 Inference about several units\nLet’s continue with the Mars-prospecting example of the previous section, with just three rocks. We found that the probability that rock #1 has haematite (\\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)) was \\(2/3\\), given that we haven’t observed any other rocks. This probability was equal to the frequency of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks in the urn.\nNow suppose that we observe rock #2, and it turns out to have haematite (\\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)). What is the probability that rock #1 has haematite?\nThe probability we are asking about is \\(\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}})\\), and it can be calculated with the usual rules. The result is again the same as the frequency of the \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks, but with respect to the new situation: there are now two rocks left in front of us, and one must contain haematite, while the other doesn’t. The probability is therefore \\(1/2\\), a value different from that we found before, \\(2/3\\):\n\\[\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}_{\\textsf{3}}) = 1/2\n\\]\nThis situation is quite general: in a collection of many rocks, the probabilities for new observations change accordingly to information about previous observations (and also subsequent ones, if already known).\n\n\nBut consider now the case \\(\\mathsfit{K}\\) of a large collection of 3 000 000 rocks, 2 000 000 of which have haematite and the rest doesn’t.2 The population’s relative frequencies are exactly as in the case with three rocks, and for the probability that rock #1 contains haematite we still have\n2 Note how this scenario becomes very similar to that of coin tosses.\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) = \\frac{2 000 000}{3 000 000} = 2/3\n\\]\nNow suppose we examine rock #2 and find haematite. What is the probability that rock #1 also contains haematite? In this case we find\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) =\n\\frac{1 999 999}{2 999 999} \\approx 2/3\n\\]\nwith an absolute error of only \\(0.000 000 1\\). That is, the probability and frequency are almost the same as before examining rock #2. The reason is clear: the number of rocks is so large that observing some of them doesn’t practically change the content and proportions of the whole collection.\nThe joint probability that rock #2 contains haematite and rock #1 doesn’t is therefore, by the and-rule,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[1ex]\n&\\approx f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\end{aligned}\n\\]\n\nthe approximation being the better, the larger the collection of rocks.\nIt is easy to see that this will hold for more observations, and for different and more complex variate domains, as long as the number of units considered is enough small compared with the population size. For instance\n\n\\[\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\approx\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\]\n\nwhere \\(\\boldsymbol{f}\\) is the initial frequency distribution for the population.\n\n\nThis situation applies to more general populations: if the full-population frequencies are known, the agent’s beliefs are exchangeable, and the population is practically infinite, then the joint probability that some units have a particular set of values is equal to the product of the frequencies of those values.\n\n\n\n\n\n\n\n \n\n\n\n\nIf an agent:\n\nhas background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nhas full information \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) about the population frequencies \\(\\boldsymbol{f}\\) for the variate \\(Z\\)\n\nthen\n\\[\n\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\approx\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'}) \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''}) \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'''}) \\cdot\n\\dotsb\n\\]\nfor any (different) units \\(u', u'', u''', \\dotsc\\) and any (even equal) values \\(z', z'', z''', \\dotsc\\).\n\n\n\n\nThe formula above solves our initial problem: How to calculate and encode the joint probability distribution for the full population?, although it does so only in the case where the full-population frequencies \\(\\boldsymbol{f}\\) are known. In this case this probability is encoded in the \\(\\boldsymbol{f}\\) itself (which can be represented as a multidimensional array), and can be calculated for any desired joint probability distribution just by a multiplication.\nIn the income example from § 26.2, the probability that two units (citizens) #\\(a\\), #\\(c\\) have value \\(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{**}\\) and one unit #\\(b\\) has value \\(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z^{*}\\) is\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\nZ_{a}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{b}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{*}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{c}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n&\\approx\nf(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}}) \\cdot\nf(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{*}}) \\cdot\nf(Z \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z^{**}})\n\\\\[1ex]\n&=\n0.260 058\\% \\cdot\n0.860 369\\% \\cdot\n0.260 058\\%\n\\\\\n&= 0.000 005 818 7\\%\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n Always check whether the approximate formula above is appropriate\n\n\n\nAs we have seen, the product formula above is strictly speaking only approximate. In situations where the full population has practically infinite size compared to (1) the number of units that the agent uses for learning, and (2) the number of units the agent will draw inferences about, then the formula can be used as if it were exact.\nBut how much is “practically infinite”? No general answer is possible: it depends on the precision required in the specific problem. In some problems, even if learning and inference involve 10% of the units from the full population, the approximation might still be acceptable; but in other problems it might not be. If learning and inference involve 50% or more units from the full population, then the formula above is hardly acceptable.\nThe probability calculus and the four fundamental rules allow us to handle problems with any population size exactly (see the Study reading below), but the exact computation becomes involved and expensive. This is why the approximate product formula above is valuable, when it can be properly used.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_from_freqs.html#sec-no-learn-freqs",
    "href": "inference_from_freqs.html#sec-no-learn-freqs",
    "title": "26  Inferences from frequencies",
    "section": "26.5 No learning when full-population frequencies are known",
    "text": "26.5 No learning when full-population frequencies are known\nImagine an agent with exchangeable beliefs \\(\\mathsfit{I}\\) and knowledge \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\) of the full-population frequencies, who also has observed several units with values (possibly some identical) \\(Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\). What is this agent’s degree of belief that a new unit #\\(u\\) has value \\(Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\\)?\nFrom our basic formula for this question,\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\color[RGB]{34,136,51}\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}Z_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u'''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n}\n\\\\[2ex]\n&\\qquad{}\\approx\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\nf({\\color[RGB]{68,119,170}{\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''}) \\cdot\n\\dotsb\n}\n\\\\[2ex]\n&\\qquad{}=\\frac{\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z}) \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}{\n\\underbracket[0.2ex]{\\sum_{\\color[RGB]{170,51,119}z}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\color[RGB]{170,51,119}z})}_{{}=1} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z''})} \\cdot\n\\cancel{f({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z'''})} \\cdot\n\\cancel{\\dotsb}\n}\n\\\\[2ex]\n&\\qquad{}=\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z})\n\\\\[3ex]\n&\\qquad{}\\equiv\n\\mathrm{P}(\\color[RGB]{238,102,119}Z_{u}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\end{aligned}\n\\]\nso the information from the units \\(u'\\), \\(u''\\), and so on is irrelevant to this agent. In other words, this agent’s inferences about some units are not affected by the observation of other units.\nThe reason for this irrelevance is that the agent already knows the full-population frequencies. So the observation of the frequencies of some values provides no new information to the agent.\nObviously this is not what we desire. But it is not a problem: the crucial point is that knowledge of full-population frequencies is only a hypothetical, idealized situation. In the next chapter we shall see that learning occurs when we go beyond this idealization.\n\n\n\n\n\n\n “Learning” about what?\n\n\n\nIn this and the following sections, and sometimes in the following chapters, when we say “the agent is learning” or “the agent is not learning” we mean specifically the change in an agent’s beliefs about observation of variates of some units which had not yet been observed.\nNote that there is always learning about something whenever we put new information in the conditional of a probability. In the Mars-prospecting example above, for example, we have\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 2/3\n\\]\nand the agent has (practically) not learned anything about the sentence \\(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) from the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\).\nBut we also have\n\\[\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 2/3\n\\qquad\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) = 0\n\\]\nthe probability for the sentence \\(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) has changed. So the agent has learned something: that rock #2 doesn’t contain haematite (\\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through Ch. 3 of Jaynes: Probability Theory. This chapter is extremely instructive in general to understand how probability theory works.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>[Inferences from frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html",
    "href": "inference_about_freqs.html",
    "title": "27  Inference about frequencies",
    "section": "",
    "text": "27.1 Inference when population frequencies aren’t known\nIn chapter  26 we considered an agent that has exchangeable beliefs and that knows the full-population frequencies. The degrees of belief of such an agent have a very simple form: products of frequencies. But for such an agent the observations of units doesn’t give any useful information for drawing inferences about new units: such observations provide frequencies which the agent already knows.\nSituations where we have complete frequency knowledge can be common in engineering problems, where the physical laws underlying the phenomena involved are known and computable. They are far less common in data-science and machine-learning applications: here we must consider agents that do not know the full-population frequencies.\nHow does such an agent calculate probabilities about units? The answer is actually a simple application of the “extension of the conversation” (§ 9.5, which boil down to applications of the and and or rules). A probability given that the frequency distribution is not known is equal to the average of the probabilities given each possible frequency distribution, weighted by the probabilities of the frequency distributions:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\n)\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nBut we saw in § 26.4 that the probability for a sequence of values given a known frequency is just the product of the value’s frequencies. We thus have our long-sought formula:\nThis result is called de Finetti’s representation theorem for exchangeable belief distributions. It must be emphasized that this result is actually independent of any real or imaginary population frequencies. We took a route to it through the idea of population frequencies only to help our intuition. If for any reason you find the idea of a “limit frequency for an infinite population” somewhat suspicious, then don’t worry: the formula above actually does not rely on it. The formula results from the assumption has exchangeable beliefs about a collection of units that can potentially be continued without end.\nLet’s see how this formula works in the simple Mars-prospecting example (with 3 million rocks or more) from § 26.4. Suppose that the agent:\nWhat is the agent’s degree of belief that rock #1 contains haematite? According to the derived rule of extension of the conversation, that is, the main formula written above, we find:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) +\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot \\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[1ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[1ex]\n&= \\boldsymbol{62.5\\%}\n\\end{aligned}\n\\]\nIn an analogous way we can calculate, for instance, the agent’s belief that rock #1 contains haematite, rock #2 doesn’t, and rock #3 does:\nThis formula generalizes to any population, any variates, and any number of hypotheses about the frequencies.\nMathematical and, even more, computational complications arise when we consider all possible frequency distributions, since there is a practically infinite number of them; they form a continuum in fact. But do not let these practical difficulties affect the intuitive picture behind them, which is simple to grasp once you’ve considered some simple examples.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-freq-not-known",
    "href": "inference_about_freqs.html#sec-freq-not-known",
    "title": "27  Inference about frequencies",
    "section": "",
    "text": "de Finetti’s representation theorem\n\n\n\n\nIf an agent has background information \\(\\mathsfit{I}\\) about a population saying that\n\nbeliefs about units are exchangeable\nthe population size is practically infinite\n\nthen\n\\[\n\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'}) \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nfor any (different) units \\(u', u'', \\dotsc\\) and any (even equal) values \\(z', z'', \\dotsc\\).\nIn the sum above, \\(\\boldsymbol{f}\\) runs over all possible frequency distributions for the full population.\nProperly speaking the sum is an integral, because \\(F\\) is a continuous quantity. We should write \\(\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n) = \\int\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'})  \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{p}(\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}\\boldsymbol{f}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nForesight: Its logical laws, its subjective sources. This essay gives much insight on our reasoning process in making forecasts and learning from experience.\n\n\n\n\n\n\n\n\nknows that the rock collection consists of:\n\neither a proportion 2/3 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/3 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}'\\)\nor a proportion 1/2 of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks and 1/2 of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks; denote these frequencies with \\(\\boldsymbol{f}''\\)\n\nassigns a \\(75\\%\\) degree of belief to the first hypothesis, and \\(25\\%\\) to the second (so the sentence \\((F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}') \\lor (F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'')\\) has probability \\(1\\)):\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{3} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n&\\approx\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) + {}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot  f''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot {\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot {\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot 25\\%\n\\\\[2ex]\n&\\approx \\boldsymbol{14.236\\%}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nExercise 27.1\n\n\n\nConsider a state of knowledge \\(\\mathsfit{K}'\\) according to which:\n\n\nThe rock collection may have a proportion \\(0/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks (and \\(10/10\\) of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks); call this frequency distribution \\(\\boldsymbol{f}_0\\)\nThe rock collection may have a proportion \\(1/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_1\\)\nand so on… up to\na proportion \\(10/10\\) of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rocks; call this \\(\\boldsymbol{f}_{10}\\)\n\n\n\nThe probability of each of these frequency hypotheses is \\(1/11\\), that is:\n\\[\n  \\begin{aligned}\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{0} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11 \\\\[1ex]\n  &\\dotso\\\\[1ex]\n  &\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}_{10} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}') = 1/11\n  \\end{aligned}\n  \\]\n\nCalculate the probabilities\n\\[\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}') \\qquad\n\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{K}')\n\\] \nDo they all have the same value? Try to explain why or why not.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-learning-general",
    "href": "inference_about_freqs.html#sec-learning-general",
    "title": "27  Inference about frequencies",
    "section": "27.2 Learning from observed units",
    "text": "27.2 Learning from observed units\nStaying with the same Mars-prospecting scenario, let’s now ask what’s the agent’s degree of belief that rock #1 contains haematite, given that the agent has found that rock #2 doesn’t contain haematite. In the case of an agent that knows the full-population frequencies we saw § 26.5 that this degree of belief is actually unaffected by other observations. What happens when the population frequencies are not known?\nThe calculation is straightforward:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\frac{\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&\\approx\n\\frac{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\cdot f(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\sum_{\\boldsymbol{f}}\nf(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[2ex]\n&=\n\\frac{\n{\\color[RGB]{102,204,238}\\frac{2}{3}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{102,204,238}\\frac{1}{2}}\\cdot {\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}{\n{\\color[RGB]{204,187,68}\\frac{1}{3}}\\cdot 75\\% +\n{\\color[RGB]{204,187,68}\\frac{1}{2}}\\cdot 25\\%\n}\n\\\\[2ex]\n&\\approx\\frac{\n22.9167\\%\n}{\n37.5000\\%\n}\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\nKnowledge that \\(R_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) thus does affect the agent’s belief about \\(R_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\):\n\\[\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  \\mathsfit{K}) = 62.5\\%\n\\qquad\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\approx 61.1\\%\n\\]\nIn particular, the observation of one \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has somewhat decreased the probability of observing a new \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\)-rock.\n\n\n\n\n\n\nExercise 27.2\n\n\n\n\nCalculate the minimal number of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) observations needed for lowering the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(55\\%\\) or less. \nDoes it seem possible to lower the agent’s belief to less than \\(50\\%\\)? Explain why.\nCalculate the minimal number of \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) observations needed for increasing the agent’s degree of belief of observing a \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) to \\(65\\%\\) or more. \nDoes it seem possible to increase the agent’s belief to more than \\(2/3\\)? Explain why.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-learn-freqs",
    "href": "inference_about_freqs.html#sec-learn-freqs",
    "title": "27  Inference about frequencies",
    "section": "27.3 How learning works: learning about frequencies",
    "text": "27.3 How learning works: learning about frequencies\nAn agent having full-population frequency information does not learn1 from observation of units, whereas an agent not having such information does learn from observation of units. This fact shows how learning from observed to unobserved units actually works. Crudely speaking, observations do not directly affect the beliefs about unobserved units, but instead affect the beliefs about the population frequencies. And these in turn affect the beliefs about unobserved units. Graphically this could be represented as follows:\n1 remember the warning of § 26.5 about “learning” \nas opposed to this:\n\n\n\n\n \n\n\n\n\n\n\n\n\n Information connections\n\n\n\nThe graphs above represent informational connections, not “causal”. The directed arrows roughly mean “…provides information about…”; they do not mean “…causes…”.\nIn the first graph, the lack of an arrow from observed units to unobserved units means that all information provided by the observed units for the unobserved ones is fully contained in the information about frequencies.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the notion of cause\n\n\n\n\nThe informational relation between observed units, frequencies, and unobserved units becomes clear if we check how the agent’s beliefs about the frequency hypotheses change as observations are made. In the Mars-prospecting example of § 27.1, the agent has initial probabilities\n\\[\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 75\\%\n\\qquad\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K}) = 25\\%\n\\]\nwhere \\(\\boldsymbol{f}'\\) gives frequency \\(2/3\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\), and \\(\\boldsymbol{f}''\\) gives frequency \\(1/2\\) to \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\). How do these probabilities change, conditional on the agent’s observing that rock #2 doesn’t contain haematite? We just need to use Bayes’s theorem. For the first hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\):\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) &=\n\\frac{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\n\\mathrm{P}(R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}{\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n+\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n}\n\\\\[1ex]\n&=\n\\frac{\n\\frac{1}{3} \\cdot\n75\\%\n}{\n\\frac{1}{3} \\cdot\n75\\%\n+\n\\frac{1}{2} \\cdot\n25\\%\n}\n\\\\[2ex]\n&=\n\\boldsymbol{66.667\\%}\n\\end{aligned}\n\\]\n\nand an analogous calculation yields \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})=33.333\\%\\).\nThis result makes sense, because according to the hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) there is a higher proportion of \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rocks than according to \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\), and a \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\)-rock has been observed. The hypothesis \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\) therefore becomes slightly more plausible, and \\(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\) slightly less.\nThe updated degree of belief above for the frequencies also gives us an alternative (yet equivalent) way to calculate the conditional probability \\(\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\\). Use the derived rule of “extension of the conversation” in a different manner:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&=\n\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n\\text{\\color[RGB]{187,187,187}\\scriptsize(no learning if frequencies are known)}\\enspace\n&=\\sum_{\\boldsymbol{f}}\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\\mathrm{P}(R_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}  F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\nf'(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}'\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n+{}\n\\\\[1ex]\n&\\qquad\nf''(R\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}''\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_{2} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&=\n{\\color[RGB]{102,204,238}\\frac{2}{3}} \\cdot 66.667\\%\n+\n{\\color[RGB]{102,204,238}\\frac{1}{2}} \\cdot 33.333\\%\n\\\\[2ex]\n&= \\boldsymbol{61.111\\%}\n\\end{aligned}\n\\]\n\nThe result is exactly as in § 27.2 – as it should be: remember from chapter  8 that the four rules of inference are built so as to mathematically guarantee this kind of logical self-consistency.\n\nAn intuitive interpretation of population inference\nThe general expression for the updated belief about frequencies has a very intuitive interpretation. Again using Bayes’s theorem, but omitting the proportionality constant,\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n&\\propto\n\\mathrm{P}(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\n\\\\[2ex]\n&\\propto\n\\overbracket[0.1ex]{f(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\\color[RGB]{0,0,0})  \\cdot \\,\\dotsb\\,\n\\cdot f(\\color[RGB]{34,136,51}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0})}^{\\color[RGB]{119,119,119}\\mathclap{\\text{how well the frequency \"fits\" the data}}}\n\\ \\cdot \\\n\\underbracket[0.1ex]{\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})}_{\\color[RGB]{119,119,119}\\mathclap{\\text{how reasonable the frequency is}}}\n\\end{aligned}\n\\]\n\nThis product can be interpreted as follows.\nTake a hypothetical frequency distribution \\(\\boldsymbol{f}\\). If the data have high frequencies according to it, then the product\n\\[f(\\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\\color[RGB]{0,0,0})  \\cdot \\,\\dotsb\\,\n\\cdot f(\\color[RGB]{34,136,51}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0})\\]\nhas a large value. Vice versa, if the data have low frequency according to it, that product has a small value. This product therefore expresses how well the hypothetical frequency distribution \\(\\boldsymbol{f}\\) “fits” the observed data.\nOn the other hand, if the factor\n\\[\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\]\nhas a large value, then the hypothetical \\(\\boldsymbol{f}\\) is probable, or “reasonable”, according to the background information \\(\\mathsfit{K}\\). Vice versa, if that factor has a low value, then the hypothetical \\(\\boldsymbol{f}\\) is improbable or “unreasonable”, owing to reasons expressed in the background information \\(\\mathsfit{K}\\).\nThe agent’s belief in the hypothetical \\(\\boldsymbol{f}\\) is a balance between these two factors, the “fit” and the “reasonableness”. This has a very important consequence:\n\n\n\n\n\n\n \n\n\n\n\nProbability inference does not need any “regularization methods” or any procedures against “over-fitting” or “under-fitting”.\nIn fact, the very notions of over- or under-fitting refer to the background information \\(\\mathsfit{K}\\) and the initial belief \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\).\nThink about it: How can we judge that an algorithm is over- or under-fitting, given that we do not know the “ground truth”? (If we knew the ground truth we wouldn’t be making inferences.) Such judgement reveals that we have some preconceived notion of what a reasonable distributions would look like – that’s exactly what \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{K})\\) encodes.\n\n\n\nThe agent’s belief about new data is then an average of what the frequency of the new data would be for all possible frequency distributions \\(\\boldsymbol{f}\\):\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1} \\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}) \\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\color[RGB]{34,136,51}Z_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_N\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{K})\n\\end{aligned}\n\\]\n\nEach possible \\(\\boldsymbol{f}\\) is weighed by its credibility, which takes into account the fit of the possible frequency to observed data, and its reasonableness against the agent’s background information.\n\n\nOther uses of the belief distribution about frequencies\nThe fact that the agent is actually learning about the full-population frequencies allows it to draw improved inferences not only about units, but also about characteristics intrinsic to the population itself, and also about its own performance in future inferences. For instance, the agent can even forecast the maximal accuracy that can be obtained in future inferences. We shall quickly explore these possibilities in a later chapter.\n\n\n\n\n\n\n Study reading\n\n\n\nSkim through:\n\nCh. 4 of Jaynes: Probability Theory\n§§8.1–8.6 of O’Hagan: Probability\nHeath & Sudderth 1976: De finetti’s theorem on exchangeable variables",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "inference_about_freqs.html#sec-prob-for-freqs",
    "href": "inference_about_freqs.html#sec-prob-for-freqs",
    "title": "27  Inference about frequencies",
    "section": "27.4 How to assign the probabilities for the frequencies?",
    "text": "27.4 How to assign the probabilities for the frequencies?\nThe general formula we found for the joint probability:\n\\[\n\\mathrm{P}(\nZ_{u'}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{u''}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\approx\n\\sum_{\\boldsymbol{f}}\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z'}) \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{ z''}) \\cdot\n\\,\\dotsb\\\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nallows us to draw many kinds of predictions about units, which we’ll explore in the next chapter.\nBut how does the agent assign  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) ,  that is, the probability distribution (in fact, a density) over all possible frequency distributions? There is no general answer to this important question, for two main reasons.\nFirst, a proper answer is obviously problem-dependent. In fact  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is the place where the agent encodes any background information relevant to the problem.\nTake the simple example of the tosses of a coin. If you (the agent) examines the coin and the tossing method and they seem ordinary to you, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx 0\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{o}}) \\approx \\text{\\small very high}\n\\end{aligned}\n\\]\nBut if you are told that the coin is a magician’s one, with either two heads or two tails, and you don’t know which, then you might assign probabilities like these:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always heads'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`always tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 1/2\n\\\\\n&\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\textsf{\\small`50\\% heads 50\\% tails'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}}) = 0\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise 27.3\n\n\n\nAssume the state of knowledge \\(\\mathsfit{I}_{\\text{m}}\\) above and calculate:\n\n\\(\\mathrm{P}(\\textsf{\\small`heads 1st toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{m}})\\), the probability of heads at the first toss.\n\\(\\mathrm{P}(\\textsf{\\small`heads 2nd toss'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`heads 1st toss'} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\text{m}})\\), the probability of heads at the second toss, given that heads was observed at the first.\n\nExplain your findings.\n\n\nSecond, for complex situations with many variates of different types it is may be mathematically and computationally difficult to write down and encode  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) .  Moreover, the multidimensional characteristics and quirks of this belief distribution can be difficult to grasp and understand.\nYet it is a result of probability theory (§ 5.2) that we cannot avoid specifying \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). Any “methods” that claim to avoid the specification of that probability distribution are covertly specifying one instead, and hiding it from sight. It is therefore best to have this distribution at least open to inspection rather than hidden.\nLuckily, if  \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\)  is “open-minded”, that is, if it doesn’t exclude a priori any frequency distribution \\(\\boldsymbol{f}\\), or in other words if it doesn’t assign strictly zero belief to any \\(\\boldsymbol{f}\\), then with enough data the updated belief distribution   \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{data} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\)  will actually converge to the true frequency distribution of the full population. The tricky word here is “enough”. In some problems a dozen observed units might be enough; in other problems a million observed units might not be enough yet.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nBayesian statistical inference for psychological research\n\n\n\n\nIn chapter  28 we shall discuss and implement a mathematically concrete belief distribution \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{D})\\) appropriate to task involving nominal variates.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>[Inference about frequencies]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html",
    "href": "dirichlet-mixture.html",
    "title": "28  Example of belief over frequencies: the Dirichlet-mixture distribution",
    "section": "",
    "text": "28.1 A belief distribution for frequencies over nominal variates\nThere are infinite possible ways to choose a belief distribution \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) over frequencies. In this chapter we discuss one, called the Dirichlet-mixture belief distribution, in some detail. The state of knowledge underlying this distribution will be denoted \\(\\mathsfit{I}_{\\textrm{d}}\\).\nThe Dirichlet-mixture distribution is appropriate for statistical populations with nominal, discrete variates, or joint variates with all nominal components. It is not appropriate to discrete ordinal variates, because it implicitly assumes that there is no natural order to the variate values.\nSuppose we have a simple or joint nominal variate \\(Z\\) which can take on \\(M\\) different possible values; these can be joint values, as in the examples of § 24.2. As usual \\(\\boldsymbol{f}\\) denotes a specific frequency distribution for the variate values. For a given value \\({\\color[RGB]{68,119,170}z}\\), we denote by \\(f({\\color[RGB]{68,119,170}z})\\) the relative frequency with which that value occurs in the full population.\nThe Dirichlet-mixture distribution assigns to \\(\\boldsymbol{f}\\) a probability density given by the following formula:\nBesides some multiplicative constants, the probability of a particular frequency distribution is simply proportional to the product of all its individual frequencies, raised to some powers. The product “\\(\\prod_{{\\color[RGB]{68,119,170}z}}\\)” is over all \\(M\\) possible values of \\(Z\\). The sum “\\(\\sum_{k}\\)” is over an integer (positive or negative) index \\(k\\) that runs between the minimum value \\(k_{\\text{mi}}\\) and the maximum value \\(k_{\\text{ma}}\\). In the applications of the next chapters these minimum and maximum are chosen as follows:\n\\[\nk_{\\text{mi}}=0\n\\qquad\nk_{\\text{ma}}=20\n\\]\nso the sum “\\(\\sum_k\\)” runs over 21 terms. In most applications it does not matter if we take a lower \\(k_{\\text{mi}}\\) or a higher \\(k_{\\text{ma}}\\).",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Example of belief over frequencies: the Dirichlet-mixture distribution]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-intro-dirichlet-mix",
    "href": "dirichlet-mixture.html#sec-intro-dirichlet-mix",
    "title": "28  Example of belief over frequencies: the Dirichlet-mixture distribution",
    "section": "",
    "text": "For the extra curious\n\n\n\nSome of the theoretical basis for the choice of this belief distribution can be found in chapters 4–5 of The Estimation of Probabilities.\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\\[\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) =\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Biggl[\\prod_{{\\color[RGB]{68,119,170}z}} f({\\color[RGB]{68,119,170}z})^{\\frac{2^k}{M} -1} \\Biggr]\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\tag{28.1}\\]\n\n\n\n\n\n\n\nMeaning of the \\(k_{\\text{mi}}, k_{\\text{ma}}\\) parameters\nThe parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\) encode, approximately speaking, the agent’s built-in belief about how many data are needed to change its initial beliefs. More precisely, \\(2^{k_{\\text{mi}}}\\) and \\(2^{k_{\\text{ma}}}\\) represent a lower and an upper bound on the amount of data necessary to overcome initial beliefs. Values  \\(k_{\\text{mi}}=0\\),  \\(k_{\\text{ma}}=20\\)  represent the belief that such amount could be anywhere between 1 unit and approximately 1 million units. The belief is spread out uniformly across the orders of magnitude in between.\nIf \\(2^{k_{\\text{mi}}}\\) is larger than the amount of training data, the agent will consider these data insufficient, and tend to give uniform probabilities to its inferences, for example a 50%/50% probability to a binary variate.\nIf the amount of data that should be considered “enough” is known, for example from previous studies on similar populations, the parameters \\(k_{\\text{mi}},k_{\\text{ma}}\\) can be set to that order of magnitude, in base 2, minus or plus some magnitude range.\nNote that if such an order of magnitude is not known, then it does not make sense to “estimate” it from training data with other methods, because an agent with a Dirichlet-mixture belief will already do that internally and in an optimal way, provided an ample range is given with \\(k_{\\text{mi}}, k_{\\text{ma}}\\).\n\n\n\n\n\n\nExercise 28.1\n\n\n\nIn the calculations and exercises that follow, try also other \\(k_{\\text{mi}}\\) and \\(k_{\\text{ma}}\\) values, and see what happens.\n\n\n\n\nLet’s see how this formula looks like in a concrete, simple example: the Mars-prospecting scenario, which has many analogies with coin tosses.\nThe variate \\(R\\) can take on two values \\(\\set{{\\color[RGB]{102,204,238}{\\small\\verb;Y;}},{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\\), so \\(M=2\\) in this case. The frequency distribution consists of two frequencies:\n\\[f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}) \\qquad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\\]\nof which only one can be chosen independently, since they must sum up to 1. For instance we could consider \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.5, f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.5\\),  or \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.84, f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.16\\),  and so on.\nIn the present example we choose\n\\[k_{\\text{mi}}=0 \\qquad k_{\\text{ma}}=2\\]\nso that the sum “\\(\\sum_k\\)” runs over 3 terms.\nLet’s use these specific values of \\(M\\), \\(k_{\\text{mi}}\\), \\(k_{\\text{ma}}\\) in the agent’s belief distribution for the frequencies, and simplify its expression a little:\n\\[\n\\begin{aligned}\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) &=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\Biggl[\\prod_{{\\color[RGB]{68,119,170}z}={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}} f({\\color[RGB]{68,119,170}z})^{\\frac{2^k}{2} -1} \\Biggr]\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[2ex]\n&=\n\\frac{1}{3}\n\\Biggl[\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{0}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{0}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{0} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{0}}{2} - 1\\bigr)!}^2\n}\n+{} \\\\[1ex]&\\qquad\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{1}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{1}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{1} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{1}}{2} - 1\\bigr)!}^2\n}\n+{} \\\\[1ex]&\\qquad\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})^{\\frac{2^{2}}{2}-1}\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})^{\\frac{2^{2}}{2}-1}\n\\cdot\n\\frac{\n\\bigl(2^{2} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{2}}{2} - 1\\bigr)!}^2\n}\n\\Biggr]\n\\\\[2ex]\n&=\n\\frac{1}{3}\n\\Biggl[\n\\frac{1}{\\sqrt{f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})}}\\cdot \\frac{1}{\\sqrt{f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})}}\n\\cdot \\frac{1}{\\pi}\n+{} \\\\[1ex]&\\qquad\n1 \\cdot 1 \\cdot \\frac{1}{1}\n+{} \\\\[1ex]&\\qquad f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\cdot \\frac{6}{1}\n\\Biggr]\n\\\\[2ex]\n&=\n\\frac{1}{3\\pi} \\cdot\n\\frac{1}{\\sqrt{f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})}}\\cdot \\frac{1}{\\sqrt{f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})}}\n+\n\\frac{1}{3}\n+ 3 \\cdot\nf({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\cdot f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})\n\\end{aligned}\n\\]\n\n\nWe can visualize this belief distribution (with \\(k_{\\text{mi}}=0, k_{\\text{ma}}=2\\)) with a generalized scatter plot (§ 15.7) of 100 frequency distributions; each distribution is represented by a line (recall § 14.2):\n\nAlternatively we can represent the probability density of the frequency \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})\\):\n\nYou can see some characteristics of this belief:\n\nall possible frequency distributions are taken into account, that is, no frequency is judged impossible and given zero probability\na higher probability is given to frequency distributions that are almost 50%/50%, or that are almost 0%/100% or 100%/0%\n\nThe second characteristic expresses the belief that the agent may more often deal with tasks where frequencies are almost symmetric (think of coin toss), or the opposite: tasks where, once you observe a phenomenon, you’re quite sure you’ll keep observing it. This latter case is typical of some physical phenomena; an example is given by Jaynes:\n\nFor example, in a chemical laboratory we find a jar containing an unknown and unlabeled compound. We are at first completely ignorant as to whether a small sample of this compound will dissolve in water or not. But having observed that one small sample does dissolve, we infer immediately that all samples of this compound are water soluble, and although this conclusion does not carry quite the force of deductive proof, we feel strongly that the inference was justified.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPrior probabilities\n\n\n\n\n\n\n\n\nExercise 28.2\n\n\n\nCalculate the formula above for these three frequency distributions:\n1.   \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.5\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.5\\)\n2.   \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.75\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.25\\)\n3.   \\(f({\\color[RGB]{102,204,238}{\\small\\verb;Y;}})=0.99\\quad f({\\color[RGB]{204,187,68}{\\small\\verb;N;}})=0.01\\)",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Example of belief over frequencies: the Dirichlet-mixture distribution]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-formulae-with-Dirmix",
    "href": "dirichlet-mixture.html#sec-formulae-with-Dirmix",
    "title": "28  Example of belief over frequencies: the Dirichlet-mixture distribution",
    "section": "28.2 Examples of inference by a Dirichlet-mixture agent",
    "text": "28.2 Examples of inference by a Dirichlet-mixture agent\n\nDegree of belief about a data sequence\nRecall that an agent, in order to learn, needs to be initially provided with the full joint belief distribution\n\\[\n\\mathrm{P}(\nZ_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\n)\n\\]\nwhere \\(L\\) is the total number of data that will be used to learn, as well as those that will be subsequently analysed.\nA mathematical advantage of the Dirichlet-mixture belief distribution is that it allows us to compute this belief distribution, and several others, in an exact way. Start from de Finetti’s representation theorem (§ 27.1) and use the Dirichlet-mixture (28.1) for \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\). The integral “\\(\\int\\,\\mathrm{d}\\boldsymbol{f}\\)” can be solved explicitly and we find the following result:\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\nZ_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n&=\n\\int\nf( Z_{L}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{L} ) \\cdot\n\\,\\dotsb\\, \\cdot\nf( Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1 ) \\cdot\n\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\,\\mathrm{d}\\boldsymbol{f}\n\\\\[2ex]\n&=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\end{aligned}\n\\tag{28.2}\\]\n\nwhere \\(\\#{\\color[RGB]{68,119,170}z}\\) is the multiplicity with which the specific value \\({\\color[RGB]{68,119,170}z}\\) occurs in the sequence \\(z_1,\\dotsc,z_{L}\\).\nTo understand better the formula above, let’s use it in an example from the Mars-prospecting scenario of ch.  25. Take the sequence1\n1 Remember that the agent has exchangeable beliefs, so the units’ IDs don’t matter (§ 25.3)!\\[\nR_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\n\\]\nIn this sequence of four values, value \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears thrice and value \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) once. So in this case we have\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3\\ ,\\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1 \\ ,\\qquad L = 4\\ .\\]\nWe also have \\(M=2\\) (two possible distinct values), and we still take \\(k_{\\text{mi}}=0, k_{\\text{ma}}=2\\). The formula above then gives\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\n\\underbracket[0.1ex]{R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}_{\n\\color[RGB]{187,187,187}L=4\\quad \\#{\\small\\verb;Y;}=3\\quad \\#{\\small\\verb;N;}=1\n}\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}\n)\n\\\\\n&\\qquad{}=\n\\frac{1}{k_{\\text{ma}}-k_{\\text{mi}}+1}\n\\sum_{k=k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\frac{\n\\prod_{{\\color[RGB]{68,119,170}z}} \\bigl(\\frac{2^{k}}{M} + \\#{\\color[RGB]{68,119,170}z}- 1\\bigr)!\n}{\n\\bigl(2^{k} + L -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{M} - 1\\bigr)!}^M\n}\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + \\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}- 1\\bigr)!\n\\cdot \\bigl(\\frac{2^{k}}{2} + \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}- 1\\bigr)!\n}{\n\\bigl(2^{k} + 4 -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2^{k} + 3 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.048 735 1}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nExercise 28.3\n\n\n\n\nImplement the calculation above in your favourite programming language.\nUsing the formula above, calculate:\n\n\\(\\mathrm{P}(R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\) . Does the result make sense?\n\\(\\mathrm{P}(R_{32}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{102}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_{8}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\\)\n\n\n\nTry doing the calculation above on a computer with \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\). What happens?\n\n\n\n\n\nUsing formula (28.2) we can solve all the kinds of task discussed in § 24.2. Let’s see a couple of simple examples.\n\n\nExample 1: Forecast about one variate, given previous observations\nIn the following example we consider only one predictand variate \\(R\\): the presence of haematite. There are no predictors. The Dirichlet-mixture agent observes the value of this variate in several rocks, and tries to forecast its value in a new rock. The variate of the population of interest has two possible values, in formula (28.2) we have  \\(M=2\\). We still take \\(k_{\\text{mi}}=0\\), \\(k_{\\text{ma}}=2\\).\nThe agent has collected three rocks. Upon examination, two of them contain haematite, one doesn’t. The agent’s data are therefore\n\\[\\text{\\small data:}\\quad R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\]\nWhat belief should the agent have about finding haematite in a newly collected rock? That is, what value should it assign to the probability\n\\[\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\ ?\\]\nThis conditional probability is given by\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{{\\color[RGB]{170,51,119}r}={\\color[RGB]{102,204,238}{\\small\\verb;Y;}}}^{{\\color[RGB]{204,187,68}{\\small\\verb;N;}}}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}r} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad{}\\equiv\n\\frac{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}}) +\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\end{aligned}\n\\]\n\nand we can calculate the probabilities at the numerator and denominators using formula (28.2):\n –   The fraction above requires the computation of two joint probabilities:\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\end{aligned}\n\\]\nNote how they are associated with the two possible hypotheses about the new rock:\n\\[R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\,? \\qquad R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\,?\\]\nof which the first one interests us.\n\n –  In the first joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears thrice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears once, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 3\\ ,\\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 1\\ ,\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}4}\\ .\\]\nFormula (28.2) gives\n\\[\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}3} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}1} - 1\\bigr)!\n}{\n\\bigl(2^{k} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.048 735 1}\n\\end{aligned}\n\\]\n\n\n –  In the second joint probability, \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) appears twice and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) appears twice, so\n\\[\\#{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}= 2\\ ,\\qquad \\#{\\color[RGB]{204,187,68}{\\small\\verb;N;}}= 2\\ ,\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}4}\\ .\\]\nWe find\n\\[\\begin{aligned}\n&\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\, R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{102,204,238}2} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{2} + {\\color[RGB]{204,187,68}2} - 1\\bigr)!\n}{\n\\bigl(2^{k} + {\\color[RGB]{119,119,119}4} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{2} - 1\\bigr)!}^2\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.033 209 3}\n\\end{aligned}\n\\]\n\n\n\n –   We can finally substitute the values we just found into the initial fraction. The probability for the hypothesis of interest is\n\\[\n\\begin{aligned}\n\\mathrm{P}(R_4\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} R_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}R_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n& =\n\\frac{0.048 735 1}{0.048 735 1 + 0.033 209 3}\n\\\\[1ex]\n&=\n\\boldsymbol{59.47\\%}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise 28.4\n\n\n\n\nThe inference problem above has some analogy with coin tossing: there’s just one, binary, variate. This agent could have been used to make forecasts about coin tosses.\nConsider the result above from the point of view of this analogy. Let’s say that \\({\\color[RGB]{102,204,238}{\\small\\verb;Y;}}\\) would be “heads”, and \\({\\color[RGB]{204,187,68}{\\small\\verb;N;}}\\) “tails”. Having observed four coin tosses, with three heads and one tail, the agent is giving a 58% probability for heads at the next toss.\n\nDo you consider this probability reasonable? Why?\nIn which different coin-tossing circumstances would you consider this probability reasonable (given the same previous observation data)?\n\nTry doing the calculation above on a computer, using the values  \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\).\nIf you use the formulae above as they’re given, you’ll probably get just NaNs. The formulae above must be rewritten in a different way in order not to generate overflow. The result would be \\(\\boldsymbol{51.42\\%}\\).\n\n\n\n\n\nExample 2: Forecast given predictor but no previous observations\nLet’s go back to the hospital scenario of § 17.5. The units are patients coming into a hospital. The population is characterized by two nominal variates:\n\n\\(T\\): the patient’s means of transportation at arrival, with domain \\(\\set{{\\small\\verb;ambulance;}, {\\small\\verb;helicopter;}, {\\small\\verb;other;}}\\)\n\\(U\\): the patient’s need of urgent care, with domain \\(\\set{{\\small\\verb;urgent;}, {\\small\\verb;non-urgent;}}\\)\n\nThe combined variate \\((U \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T)\\) has  \\(M = 2\\cdot 3 = 6\\)  possible values. We still use parameters \\(k_{\\text{mi}}=0\\) and \\(k_{\\text{ma}}=2\\) in formula (28.2).\nThe agent’s task is to forecast whether the next incoming patient will require urgent care, given information about the patient’s transportation. Therefore variate \\(U\\) is the predictand, and \\(T\\) the predictor.\nTwo patients previously arrived to the hospital, but for the moment we assume that no information about them was given to the agent. Thus we have a Dirichlet-mixture agent that hasn’t learned anything yet.\nA third patient is incoming by ambulance:\n\n\\(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\)\n\nWhat is the agent’s belief that this patient requires urgent care?\n\nSince no information about the previous patients is available to the agent, its belief is expressed by\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad {}= \\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\sum_{u = {\\small\\verb;urgent;}}^{{\\small\\verb;non-urgent;}}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\\\[1ex]\n&\\qquad {}= \\frac{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}{\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n+ \\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n}\n\\end{aligned}\n\\]\n\n\n\n –   We need to calculate the two joint probabilities\n\n\\[\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\qquad\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\]\n\ncorresponding to the two hypotheses of interest: \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\) and \\(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\).\n\n\n –  In the first joint probability, the joint value \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;})\\) appears once, and the remaining five joint values, like \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;})\\), \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;})\\), etc., appear zero times:\n\n\\[\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1}\\ ,\\qquad\\text{\\small five others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\\ ,\\qquad\n{\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}1}\\ .\\]\n\nFormula (28.2) gives\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}five factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}1} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{1/6}\n\\end{aligned}\n\\]\n\n\n\n –  In the second joint probability, the joint value \\((U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;})\\) appears once, and the remaining five joint values appear zero times:\n\n\\[\\#({\\small\\verb;non-urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1}\\ ,\\qquad\\text{\\small five others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\\ ,\\qquad {\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}1}\\ .\\]\n\nFormula (28.2) gives again:\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{2-0+1}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}five factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}1} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{1/6}\n\\end{aligned}\n\\]\n\n\n\n –   The probability that the third incoming patient is urgent, given that the agent doesn’t know anything about the previous two, is then\n\\[\n\\begin{aligned}\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\n&=\n\\frac{1/6 }{1/6 + 1/6}\n\\\\[1ex]\n&=\n\\boldsymbol{50\\%}\n\\end{aligned}\n\\]\n\n\nThis result makes sense. The agent’s background information says that, a priori, urgent and non-urgent patients are equally plausible. And the agent hasn’t had the opportunity to learn any statistical association between transportation and urgency, so the information \\(T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\) is irrelevant in the present situation.\nIt is actually possible to modify the agent so as to give it a-priori different beliefs about \\({\\small\\verb;urgent;}\\) and \\({\\small\\verb;non-urgent;}\\); but we shall not pursue this possibility here.\n\n\n\nExample 3: Forecast given predictor and previous observations\nImagine that at the last minute, just a little while the third patient arrives, someone gives the agent information about the previous two patients:\n\n\\(U_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\)\n\\(U_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\)\n\nLearning this, what is the agent’s belief about the urgency of the third patient, arriving by ambulance?\nThe agent’s updated belief is expressed by the probability\n\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\n\\]\n\nwhich can be written as a fraction in the usual way (write the fraction explicitly as an exercise).\n\n\n –   We need the joint probabilities\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\end{aligned}\n\\]\n\n\n\n –  The first joint probability has the following counts:\n\n\\[\n\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{68,119,170}2}\\ ,\\quad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;other;}) = {\\color[RGB]{102,204,238}1}\\ ,\\quad\n\\text{\\small four others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\\ ,\\quad\n{\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}3}\\ .\\]\n\nTherefore\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=0}^{2}\n\\frac{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{68,119,170}2} - 1\\bigr)! \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)! \\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}four factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}3} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.005 915 64}\n\\end{aligned}\n\\]\n\n\n\n –  Counts for the second joint probability:\n\n\\[\n\\begin{gathered}\n\\#({\\small\\verb;urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1}\\ ,\\qquad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;ambulance;}) = {\\color[RGB]{102,204,238}1}\\ ,\\qquad\n\\#({\\small\\verb;non-urgent;}, {\\small\\verb;other;}) = {\\color[RGB]{102,204,238}1}\\ ,\n\\\\[1ex]\n\\text{\\small three others }\\#(\\dotsc,\\dotsc) = {\\color[RGB]{204,187,68}0}\\ ,\\qquad\n{\\color[RGB]{119,119,119}L} = {\\color[RGB]{119,119,119}3}\\ .\n\\end{gathered}\n\\]\n\nTherefore\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{1}{3}\n\\sum_{k=0}^{2}\n\\frac{\n\\underbracket[0.1ex]{\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)!\n\\cdot\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{102,204,238}1} - 1\\bigr)!\n}_{\\text{\\color[RGB]{187,187,187}three factors}}\n\\cdot\n\\underbracket[0.1ex]{\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)! \\cdot\n\\,\\dotsb\\, \\cdot\n\\bigl(\\frac{2^{k}}{6} + {\\color[RGB]{204,187,68}0} - 1\\bigr)!}_{\\text{\\color[RGB]{187,187,187}three factors}}\n}{\n\\bigl( 2^{k} + {\\color[RGB]{119,119,119}3} -1 \\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl( 2^{k} -1 \\bigr)!\n}{\n{\\bigl(\\frac{2^{k}}{6} - 1\\bigr)!}^6\n}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{0.001 594 65}\n\\end{aligned}\n\\]\n\n\n\n –   Finally, the agent’s belief that the third incoming patient is urgent, knowing that the patient is arriving by ambulance and having learned about the two previous patients, is\n\n\\[\n\\begin{aligned}\n&\n\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\n\\\\[1ex]\n&\\qquad{}=\n\\frac{0.005 915 64}{0.005 915 64 + 0.001 594 65}\n\\\\[1ex]\n&\\qquad{}=\n\\boldsymbol{78.77\\%}\n\\end{aligned}\n\\]\n\n\n\nNote the difference from the belief before learning. The agent has learned a statistical association between \\({\\small\\verb;ambulance;}\\) and \\({\\small\\verb;urgent;}\\) from the first patient.\nYou may find the belief 78.77% a little too strong: after all, the agent has learned about only one case of urgency & ambulance. This strong belief comes form the particular parameters \\(k_{\\text{mi}}=0, k_{\\text{ma}}=2\\) our agent was built with. If we had used parameters \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\), the result would have been more conservative: \\(\\boldsymbol{54.90\\%}\\).\n\n\n\n\n\n\n\nExercise 28.5\n\n\n\n\nDo the inverse inference, using urgency \\(U\\) as predictor, and transportation \\(T\\) as predictand. That is, calculate the probability\n\\[\\mathrm{P}( T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\\]\nImagine that the urgency variate for the first patient, \\(U_1\\), is not known (missing data). Using the formula for marginalization (see § 24.4), calculate the corresponding probability\n\\[\\mathrm{P}(U_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;urgent;}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} T_3\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nU_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;non-urgent;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}T_2\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;other;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nT_1\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;ambulance;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}_{\\textrm{d}})\\]\nMake similar kinds of inferences, freely trying other combinations of information about the two previous patients.\nDo the same, but with three previous patients instead of two.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Example of belief over frequencies: the Dirichlet-mixture distribution]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-formulae-dirmix",
    "href": "dirichlet-mixture.html#sec-formulae-dirmix",
    "title": "28  Example of belief over frequencies: the Dirichlet-mixture distribution",
    "section": "28.3 Other useful formulae from the Dirichlet-mixture belief distribution",
    "text": "28.3 Other useful formulae from the Dirichlet-mixture belief distribution\nWe have how some experience with calculating the beliefs of a Dirichlet-mixture agent from formula (28.2), which in turn comes from the agent’s initial belief (28.1) about frequencies.\nThank again to particular mathematical form of the Dirichlet-mixture distribution, we can actually find even more compact formulae for the beliefs of a Dirichlet-mixture agent that has learned from \\(N\\) units. We list these formulae here and we’ll use them in ch.  31.\nTo consider the general case, suppose we have a population with variates \\(Y\\), which can take on \\(M_Y\\) possible values; \\(X\\), which can take on \\(M_X\\) possible values; and \\(W\\), which can take on \\(M_W\\) possible values, for a total of \\(M = M_Y \\cdot M_X \\cdot M_W\\) possible joint values. Also suppose that the agent has learned the values of \\(N\\) units, which we collectively call \\(\\mathsfit{\\color[RGB]{34,136,51}data}\\):\n\\(\\mathsfit{\\color[RGB]{34,136,51}data}\\coloneqq\n( Y_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}w_{N} \\,\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}w_{1}\n)\\)\nUse also the following notation:\n\n\\(\\#(y, x, w)\\) is the number of times (the absolute frequency) with which the specific triplet of values \\((y, x, w)\\) appear in the data.\n\\(\\#(y, x)\\) is the number of times (the absolute frequency) with which the specific pair of values \\((y, x)\\) of the joint variate \\((Y, X)\\) appear in the data.\n\\(\\#y\\) is the number of times with which the specific value \\(y\\) of variate \\(Y\\) appears in the data.\n\nFor example, if \\(Y\\) has domain \\(\\set{{\\small\\verb;pass;}, {\\small\\verb;fail;}}\\);  \\(X\\) domain \\(\\set{{\\small\\verb;on;}, {\\small\\verb;off;}}\\);  \\(W\\) domain \\(\\set{{\\small\\verb;+;}, {\\small\\verb;-;}}\\);  and our data are\n\n\\(\\mathsfit{\\color[RGB]{34,136,51}data}= (\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;pass;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;on;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;-;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;pass;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;on;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;-;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;fail;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;off;}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W_{3}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;+;}\n)\\)\nthen we have \\(M_X = 2\\), \\(M_Y = 2\\), \\(M_W = 2\\), \\(M = 8\\), \\(N = 3\\), and\n\\[\\begin{aligned}\n&\\#({\\small\\verb;pass;}, {\\small\\verb;on;}, {\\small\\verb;+;}) = 0 \\ ,\n&&\\#({\\small\\verb;pass;}, {\\small\\verb;on;}, {\\small\\verb;-;}) = 2 \\ ,\n&&\\#({\\small\\verb;pass;}, {\\small\\verb;off;}, {\\small\\verb;+;}) = 0 \\ ,\n&&\\#({\\small\\verb;pass;}, {\\small\\verb;off;}, {\\small\\verb;-;}) = 0 \\ ,\n\\\\\n&\\#({\\small\\verb;fail;}, {\\small\\verb;on;}, {\\small\\verb;+;}) = 0 \\ ,\n&&\\#({\\small\\verb;fail;}, {\\small\\verb;on;}, {\\small\\verb;-;}) = 0 \\ ,\n&&\\#({\\small\\verb;fail;}, {\\small\\verb;off;}, {\\small\\verb;+;}) = 1 \\ ,\n&&\\#({\\small\\verb;fail;}, {\\small\\verb;off;}, {\\small\\verb;-;}) = 0 \\ ;\n\\\\[1ex]\n&\\#({\\small\\verb;pass;}, {\\small\\verb;on;}) = 2 \\ ,\n&&\\#({\\small\\verb;pass;}, {\\small\\verb;off;}) = 0 \\ ,\n&& &&\n\\\\\n&\\#({\\small\\verb;fail;}, {\\small\\verb;on;}) = 0 \\ ,\n&&\\#({\\small\\verb;fail;}, {\\small\\verb;off;}) = 1 \\ ;\n&& &&\n\\\\[1ex]\n&\\#{\\small\\verb;pass;}= 2 \\ ,\n&&\\#{\\small\\verb;fail;}= 1 \\ .\n&& &&\n\\end{aligned}\n\\]\n\n\n\nUsing this notation, it is possible to prove the following conditional-probability formulae for the Dirichlet-mixture agent:\n\n\n\n\n\n\n\n \n\n\n\n\\[\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y} + \\#y\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}{\n\\sum_{y}\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y} + \\#y\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}\n\\tag{28.3}\\]\n\\[\n\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}{\n\\sum_{y}\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}\n\\tag{28.4}\\]\nwith\n\\[\n\\operatorname{aux}(k)\n\\coloneqq\n\\frac{\n\\prod_{x,y,w} \\Bigl(\\frac{2^{k}}{M} + \\#(x,y,w) - 1\\Bigr)!\n}{\n\\bigl(2^{k} + N\\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\Bigl(\\frac{2^{k}}{M} - 1\\Bigr)!}^M\n}\n\\tag{28.5}\\]\n\n\n\nAlthough they may not look like a gain with respect to the previous formulae, the formulae above have several computational advantages:\n\nThe values of \\(\\operatorname{aux}(k)\\), for \\(k\\) from \\(k_{\\text{mi}}\\) to \\(k_{\\text{ma}}\\), need to be computed only once.\nThe main fractions in (28.4) and (28.5) have some quantities at the numerator, divided by the sum of the same quantities at the denominator. So they can be simply calculated as .../sum(...).\n\n\n\n\n\n\n\nExercise 28.6\n\n\n\nTry to prove the formulae above, starting from formula (28.2) and the rules of probability.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Example of belief over frequencies: the Dirichlet-mixture distribution]{.green}</span>"
    ]
  },
  {
    "objectID": "dirichlet-mixture.html#sec-critique-dirmix",
    "href": "dirichlet-mixture.html#sec-critique-dirmix",
    "title": "28  Example of belief over frequencies: the Dirichlet-mixture distribution",
    "section": "28.4 When is the Dirichlet-mixture belief distribution appropriate?",
    "text": "28.4 When is the Dirichlet-mixture belief distribution appropriate?\nThe two examples above reveal some characteristics of an agent based on the Dirichlet-mixture belief distribution:\n\nIn absence of previous data, it assigns uniform probability distributions to any variate.\nIt can be “eager” to learn from previous examples, that is, its probabilities may vary appreciably even with only few observations. The “eagerness” is determined by the parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\). For a general-purpose agent, the values \\(k_{\\text{mi}}=0, k_{\\text{ma}}=20\\) are more reasonable.\n\nThere are also other subtle characteristics connected to the two above, which we won’t discuss here.\nThese characteristics can be appropriate to some inference tasks, but not to others. It is again a matter of background information about the task one wants to solve.\nThe background information implicit in the Dirichlet-mixture belief distribution can be reasonable in situations where:\n\nThere is very little information about the physics or science behind the (nominal) variates and population, so one is willing to give a lot of weight to observed data. Contrast this with the coin-tossing scenario, where our physics knowledge about coin tosses make us appreciably change our probabilities only after a large number of observations.\n\n\n\n\n\n\n\n\n\n Study reading (again)\n\n\n\nDiaconis & al. 2007: Dynamical Bias in the Coin Toss \n\n\n\nA large number of previous observations is available, “large” relative to the domain size \\(M\\) of the joint variate \\(Z\\).\nThe joint variate \\(Z\\) has a small domain.\n\nIt is possible to modify the Dirichlet-mixture belief distribution in order to alter the characteristics above. Some modifications can assign more a-priori plausibility to some variate values than others, or make the initial belief less affected by observed data.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nThe Dirichlet-tree distribution: discusses a more flexible generalization of the Dirichlet distribution\nMonkeys, kangaroos, and N: is an insightful discussion of how to investigate and represent prior beliefs.\n\n\n\nThese possibilities should remind us about the importance of assessing and specifying appropriate background information. No matter the amount of data, what the data eventually “tell” us acquires meaning only against the background information from which they are observed.",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>[Example of belief over frequencies: the Dirichlet-mixture distribution]{.green}</span>"
    ]
  },
  {
    "objectID": "summary_formulae.html",
    "href": "summary_formulae.html",
    "title": "29  Final inference formulae for exchangeable beliefs",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\n\n\nWe finally have all theoretical ingredients and formulae to use the probability calculus for drawing many kinds of inferences about some units in a population, given observations from other units. Keep in mind the minimal assumptions we are making in these formulae – which also underlie all machine-learning algorithms for “supervised” and “unsupervised” learning:\n\nbeliefs about units are exchangeable,\nthe population size is practically infinite.\n\nIn the next part, A prototype Optimal Predictor Machine, we shall computationally implement these formulae and use them in a couple of simple and not-so-simple inference problems.\nHere we collect the main formulae for exchangeable beliefs and tasks about\n\n  forecasting all variates (no predictors)\n  forecasting predictands given predictors; all previous predictors and predictands known\n\nWe still use the general scenario and notation of § 24.2.\n\n\nAll inferences about units of a population rely on the joint probability for any number of units, which is given by the following formula (§ 27.1):\n\n\n\n\n\n\n\nMain formulae for some inference tasks under exchangeable beliefs\n\n\n\nde Finetti’s representation\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\nZ_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({ Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}) \\cdot\nf({ Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({ Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\nor, in terms of predictand \\({\\color[RGB]{68,119,170}Y}\\) and predictors \\({\\color[RGB]{68,119,170}X}\\) variates:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\nY_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({ Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({ Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\) is problem-dependent and must be specified by the agent.\n\n\n\n\nInferences about all variates \\({\\color[RGB]{68,119,170}Z}\\) of a new unit, given observed units\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\n    \\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}\n    =\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}z} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Z_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}Z_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Z_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n    \\\\[3ex]\n    &\\qquad{}\n    =\n    \\frac{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N+1}}\\color[RGB]{0,0,0}) \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}}\\color[RGB]{0,0,0}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}}\\color[RGB]{0,0,0})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n\n\nInferences about predictands \\({\\color[RGB]{68,119,170}Y}\\) of a new unit, given its predictors \\({\\color[RGB]{68,119,170}X}\\) and given both predictands & predictors of observed units\n\n\n\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n    Y_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N\n    \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}} {\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{170,51,119}y} \\mathrm{P}\\bigl(\n    \\color[RGB]{238,102,119}Y_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y} \\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\color[RGB]{34,136,51}X_{N+1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}\n        \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_N \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_N \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    \\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n    Y_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_1 \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_1\n    \\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}  {\\mathsfit{I}} \\bigr)\n}\n    \\\\[3ex]\n    &\\qquad{}=\n    \\frac{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N+1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}) \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N+1}}) \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]",
    "crumbs": [
      "[**Inference III**]{.green}",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>[Final inference formulae for exchangeable beliefs]{.green}</span>"
    ]
  },
  {
    "objectID": "look_behind.html",
    "href": "look_behind.html",
    "title": "30  A look behind",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\n\n\nIn the past chapters we have learned a lot of theory and fundamentals, illustrated with some simple examples. Now we shall finally put the theory into practice! We shall build a prototype AI agent that implements the theory as closely as possible.\nBefore proceeding, let’s take a quick look back at the road behind us and recall the most important milestones:\n\n\n  In order to make decisions and to act in an optimal way, an agent needs to maximize expected utility. The agent must quantify its “values” (utilities) and its degrees of belief (probabilities) about the consequences of the problem.\n\n\n  In order to calculate its degrees of belief in a logically consistent way, an agent must use the four fundamental rules of inference:\n\\(\\mathrm{P}(\\lnot X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z) = 1 - \\mathrm{P}(X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z)\\)\n\\(\\mathrm{P}(X \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z)\n= \\mathrm{P}(X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z) \\cdot \\mathrm{P}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z)\n= \\mathrm{P}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z) \\cdot \\mathrm{P}(X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z)\\)\n\\(\\mathrm{P}(X \\lor Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z) =\n  \\mathrm{P}(X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z) + \\mathrm{P}(Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z) - \\mathrm{P}(X \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Z)\\)\n\\(\\mathrm{P}(X \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z) = 1\\)\nAny departure from these rules will lead to small or large logical errors.\n\n\n  When an agent must draw inferences about populations, having observed \\(N\\) units (training data) from the population, the four rules lead to the general formula\n\\(\\mathrm{P}( Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n  Z_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n  \\frac{\n  \\mathrm{P}( Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n  Z_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n  }{\n\\sum_z \\mathrm{P}( Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n  Z_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n  }\\)\nand some slight variations of it.\nThe probability distribution \\(\\mathrm{P}( Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) must be built-in in the agent.\n\n\n  When an agent must draw inferences about an approximately infinite population, having observed \\(N\\) units (training data) from the population, and the agent has exchangeable beliefs about the population, the four rules lead to the general formula\n\\(\\mathrm{P}( Z_{N+1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n  Z_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}Z_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n  \\frac{\n\\sum_{\\boldsymbol{f}}\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z) \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}) \\cdot\n\\, \\dotsb\\, \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\boldsymbol{f}}\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}) \\cdot\n\\, \\dotsb\\, \\cdot\nf(Z\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\\)\nand some slight variations of it.\nThe probability distribution \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\) must be built-in in the agent.\n\n\n\n\nPay attention to the fact that all inference formulae about populations came straight from the four fundamental rules of inference. We did not use intuition, and we did not use any “models”.\nIn the next chapter we address the practical question of implementing the formulae above into code.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>[A look behind]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html",
    "href": "code_design.html",
    "title": "31  Implementing an OPM",
    "section": "",
    "text": "31.1 Desired characteristics of the OPM\nWe now try to build up a real prototype AI agent from basic principles, using the formulae summarized in ch.  29 and in the previous chapter. By design, this agent is as close to optimal as theoretically possible; so let’s call it an\nor OPM for short.\nBefore starting, let’s agree on some terminology so as not to get confused in the discussion below.\nWe design our Optimal Predictor Machine with the following specific characteristics:",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>[Implementing an OPM]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-characteristics-opm",
    "href": "code_design.html#sec-characteristics-opm",
    "title": "31  Implementing an OPM",
    "section": "",
    "text": "It handles variates of nominal type (§ 12.2).\nIt handles inferences and decisions about approximately infinite populations, and its beliefs about the population are exchangeable (ch.  25).\nIts initial beliefs about the population frequencies are represented by a Dirichlet-mixture distribution (ch.  28).\nBefore deployment, it learns from a set of \\(N\\) units.\n\n\n\nExample of what kind of agent we want\nLet’s give an example of what we want our agent to be able to do. Suppose we have a population having three nominal variates  \\(Z = (Y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W)\\) (keep in mind that \\(X\\), \\(Y\\), \\(W\\) could each be a joint variate). Abbreviate the set of \\(N\\) training data as\n\\(\\mathsfit{\\color[RGB]{34,136,51}data}\\coloneqq\n( Z_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{2} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1} )\\)\n\n\nRecall that \\(Z\\) denotes all (nominal) variates of the population\nwhere \\(z_N, \\dotsc, z_2, z_1\\) are specific values, stored in some training dataset. To simplify things, we assume that no values are missing.\nWe want an agent that can draw inferences like the following ones, as often as required:\n\n\\(P(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\),  \\(P(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\), etc.: inference about a predictand variate, without knowledge of any predictors.\n\\(P(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): same but for any two predictand variates.\n\\(P(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): same but for all three variates.\n\\(P(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): inference about any one predictand variate, given information about one predictor variate.\n\\(P(X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): same, but given information about any pair of predictors.\n\\(P(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}W\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\dotso \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\): inference about any two predictand variates, given information about one predictor.\n\nNote that we are not fixing beforehand which variates are predictands and which are predictors. Once the agent has learnt from the training data, we want to be able to change on the fly, at each new application, what the predictands are, and what the predictors are (if any).\nPause for a second and ponder about the flexibility that we are requesting from our prototype agent! Consider that virtually all present-day machine-learning algorithms only work one way: a machine-learning algorithm designed to guess a label from some features cannot guess features from a label. Will we really manage to build an agent with the amazing versatility illustrated above?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>[Implementing an OPM]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-code-computations",
    "href": "code_design.html#sec-code-computations",
    "title": "31  Implementing an OPM",
    "section": "31.2 Computations needed and computational challenges",
    "text": "31.2 Computations needed and computational challenges\nThe examples above of requested inferences show that the OPM agent must essentially use formulae (28.3)–(28.5) from § 28.3, which we repeat here:\n\n\n\n\n\n\n \n\n\n\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y} + \\#y\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}{\n\\sum_{y}\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y} + \\#y\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}\\)   (28.3)\n\\(\\mathrm{P}(Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}_{\\textrm{d}})\n=\n\\frac{\n\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}{\n\\sum_{y}\\sum_{k= k_{\\text{mi}}}^{k_{\\text{ma}}}\n\\Bigl(\\tfrac{2^k}{M_Y \\cdot M_X} + \\#(y,x)\\Bigr)\n\\cdot \\operatorname{aux}(k)\n}\\)   (28.4)\nwith  \\(\\operatorname{aux}(k)\n\\coloneqq\n\\frac{\n\\prod_{x,y,w} \\Bigl(\\frac{2^{k}}{M} + \\#(x,y,w) - 1\\Bigr)!\n}{\n\\bigl(2^{k} + N\\bigr)!\n}\n\\cdot\n\\frac{\n\\bigl(2^{k} -1 \\bigr)!\n}{\n{\\Bigl(\\frac{2^{k}}{M} - 1\\Bigr)!}^M\n}\\)   (28.5)\n\n\nThe values of \\(\\operatorname{aux}(k)\\) can be calculated just once, when the OPM agent is built, and stored. Subsequently the agent will draw inferences by using (28.3) or (28.4) as needed. To use those formulae, the agent needs to store the counts \\(\\#(x,y,w,\\dotsc)\\), which it found in the training data, for all combinations of values \\(x,y,w,\\dotsc\\).\n\nThis kind of storage and computation could be implemented in a straightforward way if we had unlimited storage and computation precision. But in a real implementation we must face several difficulties. Here are the main difficulties and their solutions:\n\n Finite precision\n\nOwing to finite precision, the operations in the formulae may easily lead to overflow or underflow: large numbers are treated as infinity, and small non-zero numbers as 0. For instance this is what happens if we directly compute something like \\((2^{10})! / (2^{10})!\\), obviously equal to \\(1\\):\n\n\n\nfactorial(2^10) / factorial(2^10)\n\n[1] NaN\n\n\nOne way to bypass this problem is by rewriting the formulae in ways that are mathematically equivalent but less prone to over- and under-flow. For example we can use identities like\n\\[\nx / y = \\exp\\bigl(\\ln x - \\ln y\\bigr)\\ ,\\quad x, y &gt; 0 \\ .\n\\]\nNow indeed it works; note that lfactorial() is log(factorial()) in R:\n\nexp( lfactorial(2^10) - lfactorial(2^10) )\n\n[1] 1\n\n\nAnother useful identity that avoids over- and under-flow, if \\(\\pmb{x}\\) is a vector of positive numbers, is the following:\n\\[\n\\frac{\\pmb{x}}{\\operatorname{\\texttt{sum}}(\\pmb{x})}\n=\n\\frac{\n\\operatorname{\\texttt{exp}}\\bigl(\\operatorname{\\texttt{log}}(\\pmb{x})\n- \\operatorname{\\texttt{max}}(\\operatorname{\\texttt{log}}(\\pmb{x}))\\bigr)\n}{\\operatorname{\\texttt{sum}}\\bigl(\n\\operatorname{\\texttt{exp}}\\bigl(\\operatorname{\\texttt{log}}(\\pmb{x})\n- \\operatorname{\\texttt{max}}(\\operatorname{\\texttt{log}}(\\pmb{x}))\\bigr)\n\\bigr)}\n\\]\n\n\n\n Storage\n\nWith many variates and large domains, we may run out of memory in storing all possible counts \\(\\#(x,y,w,\\dotsc)\\). For instance if we have four variates with 20 possible values each, we would need to store \\(4^{20}\\) integers, which would take more than 4 000 GB:\n\n\n\ntry( x &lt;- integer(length = 4^20) )\n\nError : cannot allocate vector of size 4096.0 Gb\n\n\nWe can bypass this problem again by using smart mathematical manipulations. In the case of formulae (28.3)–(28.5), the product over all possible values \\((x,y,w,\\dotsc)\\) can be rewritten in one over all different values of the counts, which usually has much fewer terms. For example, if we have \\(N=10000\\) datapoints, and \\(4^{20} -1\\) counts are equal to \\(9000\\), while one count is equal to \\(1000\\), then we only need to store these four numbers rather than \\(4^{20}\\) numbers!\n\n\n\n Speed\n\nThe formulae that the agent uses may involve sums over many terms, or repeated computations for many different variate values. Computation speed may therefore become an issue. There are two kinds of solutions to this problem. The first is, again, to use mathematical identities to rewrite formulae in ways that require fewer computations. The second is to exploit computation features of the programming language used to code the agent, such as vectorization or parallel computing. In our case we shall use some R functions that performs computations in a vectorized way, that is, using underlying fast C or C++ implementations.\n\n\n\n\nYou see that mathematical “tricks” become very important when we must implement formulae with finite precision and limited memory. Unfortunately getting acquainted with such tricks requires a separate course.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n§6.1 in Numerical Recipes\n\n\nNote that the solutions just discussed are not approximations. Even if we use different mathematical formulae, they are still equivalent to the original ones. The internal logic of our OPM agent is therefore still fully correct. In other situations the mathematical or computational solutions above may not be enough, and then we may need to resort to approximations, as it often happens with machine-learning algorithms.\n\n\n\n\n\n\nExercise 31.1\n\n\n\nTry to prove some of the mathematical identities above.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>[Implementing an OPM]{.red}</span>"
    ]
  },
  {
    "objectID": "code_design.html#sec-code-writing",
    "href": "code_design.html#sec-code-writing",
    "title": "31  Implementing an OPM",
    "section": "31.3 The code",
    "text": "31.3 The code\nWe implement the OPM agent and its inferences through three main functions. These and other helper functions are defined in the script OPM_nominal.R:\n\n\nbuildagent()\n\ncreates an “agent” object that stores the built-in information and the information learned from the training data. The built-in information consists in the choices of \\(k_{\\text{mi}}\\) and \\(k_{\\text{ma}}\\) parameters, the \\(\\operatorname{aux}(k)\\) parameters, and the list of variates and of their domains – the agent knows what are the possible values before seeing any data. The learned information consists in the set of counts \\(\\#(x,y,\\dotsc)\\) for all joint values of the variates.\n\n\nWe can used this function to build several agents, which differ in their background information or in the data they have learned.\nWe write this function with an option savememory for storing the learned information in a memory-efficient way, if needed.\n\ninfer()\n\nasks an agent to draw an inference for a new unit, specifying the desired predictands and the known predictors for the new unit, if any are available. It returns the agent’s degrees of belief about the predictands, using formulae (28.3)–(28.5).\n\n\nThe formulae must be implemented in slightly different ways, depending on whether the learned information is stored in a memory-efficient way. For this reason we actually have two implementations of this function, called infer.agent() and infer.agentcompressed().\nThis function can be used as often as we please, and with any agents we please.\n\ndecide()\n\nasks an agent to make a decision, specifying the list of possible decisions, the predictands and their probabilities, and the utilities for the different decisions and outcomes. We shall discuss this function more in detail in ch.  37.\n\n\n\n\n\n\n\n\n\nExercise 31.2\n\n\n\nOpen the script OPM_nominal.R and locate the functions buildagent() and infer.agent(). In each, identify the lines of code that implement the various calculations discussed above. Note that alpha stands for \\(2^k\\), and counts stands for the array or list of counts \\(\\#(y,x,\\dotsc)\\).\n\n\n\n\nBesides the three main functions above, we can write other functions that help us handling the inference task and calculate other quantities of interest:\n\n\nguessmetadata()\n\ntakes a dataset and builds a preliminary metadata file, encoding the information about variates and domain guessed from the dataset. Typically we have to correct this preliminary file to include values that may be missing from the learning data.\n\n\n\nrF()\n\ndraws one or more possible full-population frequency distribution \\(\\boldsymbol{f}\\), according to an agent’s degree of belief \\(\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\) updated after learning the data.\n\n\n\nplotFbelief()\n\nplots, as a generalized scatter plot, the possible full-population marginal frequency distributions for a single (not joint) predictand variate. If required it also plots the final probability distribution obtained with infer().\n\n\n\nmutualinfo()\n\nasks an agent to calculate the mutual information (§ 18.5) between any two sets of variates.\n\n\n\n\n\nIn the next chapter we give some more documentation on these functions and on how to use them, and in ch.  33 we use them in a concrete task.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>[Implementing an OPM]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html",
    "href": "prototype_code.html",
    "title": "32  Prototype code and workflow",
    "section": "",
    "text": "32.1 Function documentation\nA concise documentation is here given of the R functions designed in ch.  31 and described in § 31.3, together with an example of how they are used in a task.\nThe functions are collected in https://github.com/pglpm/ADA511/blob/master/code/OPM_nominal.R\nOptional arguments are written with = ..., which specifies their default values. Some additional optional arguments, mainly used for testing, are omitted in this documentation.\n( Further documentation will be added )",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html#sec-opm-docs",
    "href": "prototype_code.html#sec-opm-docs",
    "title": "32  Prototype code and workflow",
    "section": "",
    "text": "guessmetadata(data, file = NULL)\n\n\nArguments:\n\n\ndata: either a string with the file name of a dataset in .csv format (with header line), or a dataset given as a data.frame object.\nfile: a string specifying the file name of the metadata file. If no file is given and data is a file name, then file will be the same name as data but with the prefix meta_. If no file is given and data is not a string, then the metadata are output to stdout.\n\n\nOutput:\n\n\neither a .csv file containing the metadata, or a data.frame object as stdout.\n\n\n\n\n\n\n\n\n buildagent(metadata, data = NULL, kmi = 0, kma = 20, savememory = FALSE)\n\n\nArguments:\n\n\nmetadata: either a string with the name of a metadata file in .csv format, or metadata given as a data.frame.\ndata: either a string with the file name of a training dataset in .csv format (with header line), or a training dataset given as a data.frame.\nkmi: the \\(k_{\\text{mi}}\\) parameter.\nkma: the \\(k_{\\text{ma}}\\) parameter.\n\n\nOutput:\n\n\nan object of class agent or agentcompressed, consisting of a list of an array or list counts and three vectors alphas, auxalphas, palphas. The class agentcompressed saves the counts in a memory-efficient way.\n\n\n\n\n\n\n\n infer(agent, predictand, predictor = NULL)\n\n\nArguments:\n\n\nagent: an agent object.\npredictand: a vector of strings with the names of variates.\npredictor: either a list of elements of the form variate = value, or a corresponding one-row data.frame.\n\n\nOutput:\n\n\nthe joint probability distribution \\(\\mathrm{P}(\\mathit{predictand} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{predictor}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;values;} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}_{\\textrm{d}})\\) for all possible values of the predictands.\n\n\nNotes:\n\n\nIf predictors is present, the agent is acting as a “supervised-learning” algorithm. Otherwise it is acting as an “unsupervised-learning” algorithm. The obtained probabilities could be used to generate a new unit similar to the ones observed.\nThe variate names in the predictand and predictor inputs must match some variate names known to the agent. Unknown variate names are discarded. The function gives an error if predictand and predictor have variates in common.\n\n\n\n\n\n\n\n decide(probs, utils = NULL)\n\n\nArguments:\n\n\nprobs: a probability distribution for one or more variates.\nutils: a named matrix or array of utilities. The rows of the matrix correspond to the available decisions, the columns or remaining array dimensions correspond to the possible values of the predictand variates.\n\n\nOutput:\n\na list of elements EUs and optimal:\n\nEUs is a vector containing the expected utilities of all decisions, sorted from highest to lowest\noptimal is the decision having maximal expected utility, or one of them, if more than one, selected with equal probability\n\n\nNotes:\n\n\nIf utils is missing or NULL, a matrix of the form \\(\\begin{bsmallmatrix}1&0&\\dotso\\\\0&1&\\dotso\\\\\\dotso&\\dotso&\\dotso\\end{bsmallmatrix}\\) is assumed (which corresponds to using accuracy as evaluation metric).\n\n\n\n\n\n\n\n\n rF(n = 1, agent, predictand, predictor = NULL)\n\n(generate population-frequency samples)\n\n mutualinfo(agent, A, B, base = 2)\n\n(calculate mutual information)",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "prototype_code.html#sec-opm-workflow",
    "href": "prototype_code.html#sec-opm-workflow",
    "title": "32  Prototype code and workflow",
    "section": "32.2 Typical workflow",
    "text": "32.2 Typical workflow\nThe workflow discussed here is just a guideline and reminder of important steps to be taken when applying an optimal agent to a given task. There cannot be more than a guideline, because each data-science and engineering problem is unique. Literally following some predefined, abstract workflow typically leads to sub-optimal results. Sub-optimal results can be acceptable in some unimportant problems, but are unacceptable in important problems, where, say, people’s lives can be involved, such as medical ones.\nWe can roughly identify four main stages:\n\n\n\n\n\n\n\nDefine the task\n\nIn this stage we clarify what the task to be solved is – and why. Asking “why” often reveals the true needs and goals underlying the problem. If possible, the task is formalized. For example, the formal notions introduced in the parts Data I and Data II might be used: a specific statistical population is specified, with well-defined units and variates, and so on.\n\n\nWe often have to get back to his initial stage, as the side arrows in the above flow diagram indicate. New findings in the data or unavoidable limitations in the algorithms used may lead us to re-examine our assumptions and facts, or to re-define our goals (and sometimes to give up!).\n\n\n\n\n\nCollect & prepare background info\n\nBackground and metadata information, as well as auxiliary assumptions, are collected, examined, prepared. Remember that this kind of information is required in order to make sense of the data (§ 24.4). In this stage we ask questions such as “Is our belief about the task exchangeable?”, “Can the statistical population be considered infinite?”, and similar question that make clear which kinds of ready-made methods and approximations are acceptable or not. This stage also helps for correcting possible deficiencies in the training data used in the next stage. For instance, some possible variate values might not appear in the training data, owing to their rarity in the statistical population.\n\n\nIn this stage it is especially important to specify:\n\ndefinition of units (what counts as “unit” and can be used as training data?)\ndefinition of variates and their domains\ninitial probabilities\npossible decisions that may be required in the repeated task applications\nutilities associated with the decisions above\n\n\n\n\n\n\nCollect & prepare training data\n\nUnits similar to the units of our future inferences, but of which we have more complete information, are collected and examined. These are the “training data”. They are used in the next step to make the agent learn from examples. The problematic notion of similarity was discussed in § 20.2: what counts as “similar” is difficult to decide, and often we shall have to revise our decision. Sometimes no units satisfactorily similar to those of interest are available. In this case we must assess which of their informational relationships can be considered similar, which may lead us to use the agent in slightly different ways. We must also check whether training units with partially missing variates can be used by our agent or not.\n\n\n\n\n\n\n\n\n\n How many training data?\n\n\n\n“Data augmentation” is something necessary with particular machine-learning algorithms as a way to compensate or correct their internal background information, but does not apply to an optimal agent.\nThe question “how many training data should we use?” does not make sense for an optimal agent which works according to probability theory and decision theory. The answer is simply “as many as you have available”.\nIf no or few training data are available, then the optimal agent will automatically absorb as much information as possible from them, and combine it with its background information to draw optimal inferences and make optimal decisions.\nGiving artificial training data to the agent, just to increase the number of training data, is pointless and dangerous.\nPointless, because the agent is, in fact, automatically “simulating” artificial data internally as needed, from its background information and the real training data available. This is exactly what the belief distribution \\(\\mathrm{p}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}data},\\mathsfit{I}_{\\textrm{d}})\\) is doing: remember that the agent is internally considering all possible populations of data (chapter  27).\nDangerous, because artificial data may contain incorrect information, leading the agent to arrive at sub-optimal and potentially disastrously deceiving results.\n\n\n\n\n\n\nPrepare OPM agent\n\nThe background information and training data (if any available) are finally fed to the agent.\n\n\n\n\n\n\n\nRepeated application\n\nInferences are drawn, and decision made, for each new application instance. With our prototype agent, the inferences and the decisions can in principle be different from instance to instance.\n\n\n\n\n\nEvery new application can be broken down into several steps:\n\n\n\n\n\n\n\n( Remaining steps to be added soon )",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>[Prototype code and workflow]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html",
    "href": "example_opm1.html",
    "title": "33  Example application: adult-income task",
    "section": "",
    "text": "33.1 Define the task\nLet’s illustrate the example workflow described in § 32.2 with a toy, but not too simplistic, example, based on the adult-income dataset.\nAll code functions and data files are in the directory\nhttps://github.com/pglpm/ADA511/tree/master/code\nWe start loading the R libraries and functions needed at several stages. You need to have installed1 the packages extraDistr and collapse. Make sure you have saved all source files and data files in the same directory. We also set the seed for the sample generator, to reproduce our results if necessary.\nThe main task is to infer whether a USA citizen earns less (≤) or more (&gt;) than USD 50 000/year, given a set of characteristics of that citizen. In view of later workflow stages, let’s note a couple of known and unknown facts to delimit this task in a more precise manner:",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#define-the-task",
    "href": "example_opm1.html#define-the-task",
    "title": "33  Example application: adult-income task",
    "section": "",
    "text": "Given the flexibility of the agent we shall use, we can generalize the task: to infer any subset of the set of characteristics, given any other subset. In other words, we can choose the predictand and predictor variates for any new citizen. Later on we shall also extend the task to making a concrete decision, based on utilities relevant to that citizen.\nThis flexibility is also convenient because no explanation is given as to what purpose the income should be guessed.\nThe training data come from a 1994 census, and our agent will use an exchangeable belief distribution about the population. The value of the USD and the economic situation of the country changes from year to year, as well as the informational relationships between economic and demographic factors. For this reason the agent should be used to draw inferences about at most one or two years around 1994. Beyond such time range the exchangeability assumption is too dubious and risky.\nThe USA population in 1994 was around 260 000 000, and we shall use around 11 000 training data. The population size can therefore be considered approximately infinite.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#collect-prepare-background-info",
    "href": "example_opm1.html#collect-prepare-background-info",
    "title": "33  Example application: adult-income task",
    "section": "33.2 Collect & prepare background info",
    "text": "33.2 Collect & prepare background info\n\nVariates and domains\nThe variates to be used must be of nominal type, because our agent’s background beliefs (represented by the Dirichlet-mixture distribution) are only appropriate for nominal variates. In this toy example we simply discard all original non-nominal variates. These included some, such as age, that would surely be relevant for this task. As a different approach, we could have coarsened each non-nominal variate into three or four range values, so that treating it as nominal would have been an acceptable approximation.\nFirst, create a preliminary metadata file by running the function guessmetadata() on the training data train-income_data_example.csv:\n\nguessmetadata(data = 'train-income_data_example.csv',\n              file = 'preliminary.csv')\n\nInspect the resulting file preliminary.csv and check whether you can alter it to add additional background information.\nAs an example, note that domain of the \\(\\mathit{native\\_country}\\) variate does not include \\({\\small\\verb;Norway;}\\) or \\({\\small\\verb;Sweden;}\\). Yet it’s extremely likely that there were some native Norwegian or Swedish people in the USA in 1994; maybe too few to have been sampled into the training data. Let’s add these two values to the list of domain values (don’t forget to also add the names V41 and V42 in the new columns), and increase the domain size of \\(\\mathit{native\\_country}\\) from 40 to 42. The resulting, updated metadata file has already been saved as meta_income_data_example.csv.\n\n\nAgent’s parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\)\nHow many data should the agent learn in order to appreciably change its initial beliefs about the variates above, for the USA 1994 population? Let’s put an upper bound at around 1 000 000 (that’s roughly 0.5% of the whole population) with \\(k_{\\text{ma}}= 20\\), and a lower bound at 1 with \\(k_{\\text{mi}}= 0\\); these are the default values. We shall see later what the agent suggests might be a reasonable amount of training data.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#collect-prepare-training-data",
    "href": "example_opm1.html#collect-prepare-training-data",
    "title": "33  Example application: adult-income task",
    "section": "33.3 Collect & prepare training data",
    "text": "33.3 Collect & prepare training data\nThe 11 306 training data have been prepared by including only nominal variates, and discarding datapoints with partially missing data (although the function buildagent() discards such incomplete datapoints automatically). The resulting file is train-income_data_example.csv.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#sec-prepare-opm1",
    "href": "example_opm1.html#sec-prepare-opm1",
    "title": "33  Example application: adult-income task",
    "section": "33.4 Prepare OPM agent",
    "text": "33.4 Prepare OPM agent\nFor the sake of this example we shall prepare two agents that have the same background information, represented by the same \\(k_{\\text{mi}}, k_{\\text{ma}}\\) parameters, but that have learned from different amounts of training data:\n\nopm10, trained with 10 training datapoints;\nopmAll, trained with all 11 306 training datapoints.\n\nPrepare and train the first agent with the buildagent() function:\n\n## temporarily load all training data\ntraindata &lt;- tread.csv('train-income_data_example.csv')\n\n## feed first 10 datapoints to the agent\nopm10 &lt;- buildagent(\n    metadata = 'meta_income_data_example.csv',\n    data = traindata[1:10, ]\n)\n\nAnd then the second agent (we also remove the training data to save memory):\n\n## delete training data for memory efficiency\nrm(traindata)\n\n\nopmAll &lt;- buildagent(\n    metadata = 'meta_income_data_example.csv',\n    data = 'train-income_data_example.csv'\n)\n\n\n\nWe can peek into the internal structure of these “agent objects” with str()\n\nstr(opmAll)\n\nList of 6\n $ uniquedata: NULL\n $ counts    : 'table' int [1:7, 1:16, 1:7, 1:14, 1:6, 1:5, 1:2, 1:42, 1:2] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"dimnames\")=List of 9\n  .. ..$ workclass     : chr [1:7] \"Federal-gov\" \"Local-gov\" \"Private\" \"Self-emp-inc\" ...\n  .. ..$ education     : chr [1:16] \"10th\" \"11th\" \"12th\" \"1st-4th\" ...\n  .. ..$ marital_status: chr [1:7] \"Divorced\" \"Married-AF-spouse\" \"Married-civ-spouse\" \"Married-spouse-absent\" ...\n  .. ..$ occupation    : chr [1:14] \"Adm-clerical\" \"Armed-Forces\" \"Craft-repair\" \"Exec-managerial\" ...\n  .. ..$ relationship  : chr [1:6] \"Husband\" \"Not-in-family\" \"Other-relative\" \"Own-child\" ...\n  .. ..$ race          : chr [1:5] \"Amer-Indian-Eskimo\" \"Asian-Pac-Islander\" \"Black\" \"Other\" ...\n  .. ..$ sex           : chr [1:2] \"Female\" \"Male\"\n  .. ..$ native_country: chr [1:42] \"Cambodia\" \"Canada\" \"China\" \"Columbia\" ...\n  .. ..$ income        : chr [1:2] \"&lt;=50K\" \"&gt;50K\"\n $ variates  :List of 9\n  ..$ workclass     : chr [1:7] \"Federal-gov\" \"Local-gov\" \"Private\" \"Self-emp-inc\" ...\n  ..$ education     : chr [1:16] \"10th\" \"11th\" \"12th\" \"1st-4th\" ...\n  ..$ marital_status: chr [1:7] \"Divorced\" \"Married-AF-spouse\" \"Married-civ-spouse\" \"Married-spouse-absent\" ...\n  ..$ occupation    : chr [1:14] \"Adm-clerical\" \"Armed-Forces\" \"Craft-repair\" \"Exec-managerial\" ...\n  ..$ relationship  : chr [1:6] \"Husband\" \"Not-in-family\" \"Other-relative\" \"Own-child\" ...\n  ..$ race          : chr [1:5] \"Amer-Indian-Eskimo\" \"Asian-Pac-Islander\" \"Black\" \"Other\" ...\n  ..$ sex           : chr [1:2] \"Female\" \"Male\"\n  ..$ native_country: chr [1:42] \"Cambodia\" \"Canada\" \"China\" \"Columbia\" ...\n  ..$ income        : chr [1:2] \"&lt;=50K\" \"&gt;50K\"\n $ alphas    : num [1:21] 1 2 4 8 16 32 64 128 256 512 ...\n $ logauxk   : num [1:21] -160706 -157643 -154588 -151547 -148530 ...\n $ palphas   : num [1:21] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"class\")= chr \"agent\"\n\n\nthis shows the information contained in the agent as a list of several objects, among which:\n\nthe array counts, containing the counts \\(\\color[RGB]{34,136,51}\\# z\\);\nthe list variates, containing the variates’ names and domains;\nthe vector alphas, containing the values of \\(2^k\\);\nthe vector auxalphas, containing the (logarithm of) the multiplicative factors \\(\\operatorname{aux}(k)\\) (§ 31.2);\nthe vector palphas, containing the updated probabilities about the required amount of training data.\n\nThe agent has internally guessed how many training data should be necessary to affect its prior beliefs. We can peek at its guess by plotting the alphas parameters against the palphas probabilities:\n\nflexiplot(x = opmAll$alphas, y = opmAll$palphas, type = 'b',\n    xlim = c(0, 10000), ylim = c(0, NA),\n    xlab = 'required number of training data', ylab = 'probability')\n\n\n\n\n\n\n\n\nThe most probable amount seems to be of the order of magnitude of 2000 units.\nNote that you can see the complete list of variates and their domains by simply calling opmAll$variates (or any relevant agent-object name instead of opmAll). Here is the beginning of the list:\n\nhead(opmAll$variates)\n\n$workclass\n[1] \"Federal-gov\"      \"Local-gov\"        \"Private\"         \n[4] \"Self-emp-inc\"     \"Self-emp-not-inc\" \"State-gov\"       \n[7] \"Without-pay\"     \n\n$education\n [1] \"10th\"         \"11th\"         \"12th\"         \"1st-4th\"     \n [5] \"5th-6th\"      \"7th-8th\"      \"9th\"          \"Assoc-acdm\"  \n [9] \"Assoc-voc\"    \"Bachelors\"    \"Doctorate\"    \"HS-grad\"     \n[13] \"Masters\"      \"Preschool\"    \"Prof-school\"  \"Some-college\"\n\n$marital_status\n[1] \"Divorced\"              \"Married-AF-spouse\"     \"Married-civ-spouse\"   \n[4] \"Married-spouse-absent\" \"Never-married\"         \"Separated\"            \n[7] \"Widowed\"              \n\n$occupation\n [1] \"Adm-clerical\"      \"Armed-Forces\"      \"Craft-repair\"     \n [4] \"Exec-managerial\"   \"Farming-fishing\"   \"Handlers-cleaners\"\n [7] \"Machine-op-inspct\" \"Other-service\"     \"Priv-house-serv\"  \n[10] \"Prof-specialty\"    \"Protective-serv\"   \"Sales\"            \n[13] \"Tech-support\"      \"Transport-moving\" \n\n$relationship\n[1] \"Husband\"        \"Not-in-family\"  \"Other-relative\" \"Own-child\"     \n[5] \"Unmarried\"      \"Wife\"          \n\n$race\n[1] \"Amer-Indian-Eskimo\" \"Asian-Pac-Islander\" \"Black\"             \n[4] \"Other\"              \"White\"",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#sec-application-and-exploration",
    "href": "example_opm1.html#sec-application-and-exploration",
    "title": "33  Example application: adult-income task",
    "section": "33.5 Application and exploration",
    "text": "33.5 Application and exploration\n\nApplication: only predictands\nOur two agents are ready to be applied to new instances.\nBefore applying them, let’s check some of their inferences, and see if we find anything unconvincing about them. If we find something unconvincing, it means that the background information we provided to the agent doesn’t match the one in our intuition. Then there are two or three possibilities: our intuition is misleading us and need correcting; or we need to go back to stage Collect & prepare background info and correct the background information given to the agent; or a combination of these two possibilities.\nWe ask the opm10 agent to forecast the \\(\\mathit{income}\\) of the next unit, using the infer() function:\n\ninfer(agent = opm10, predictand = 'income')\n\nincome\n   &lt;=50K     &gt;50K \n0.506288 0.493712 \n\n\nThis agent gives a slightly larger probability to the \\({\\small\\verb;&lt;=50K;}\\) case. Using the function plotFbelief() we can also inspect the opm10-agent’s belief about the frequency distribution of \\(\\mathit{income}\\) for the full population. This belief is represented by a generalized scatter plot of 1000 representative frequency distributions, represented as the light-blue lines:\n\nplotFbelief(agent = opm10,\n    n = 1000, ## number of example frequency distributions\n    predictand = 'income',\n    ylim = c(0,1), ## y-axis range\n    main = 'opm10') ## plot title\n\n\n\n\n\n\n\n\nwhere the black line is the probability distribution previously calculated with the infer() function.\nThis plot expresses the opm10-agent’s belief that future training data might lead to even higher probabilities for \\({\\small\\verb;&lt;=50K;}\\). But note that the agent is not excluding the possibility of lower probabilities.\nLet’s visualize the beliefs of the opmAll-agent, trained with the full training dataset:\n\nplotFbelief(agent = opmAll, n = 1000,\n    predictand = 'income',\n    ylim = c(0,1), main = 'opmAll')\n\n\n\n\n\n\n\n\nThe probability that the next unit has \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\) is now above 70%. Also note that the opmAll-agent doesn’t believe that this probability would change appreciably if more training data were provided.\n\n\nWe can perform a similar exploration for any other variate. Let’s take the \\(\\mathit{race}\\) variate for example:\n\nplotFbelief(agent = opm10, n = 1000,\n    predictand = 'race',\n    ylim = c(0,1), main = 'opm10',\n    cex.axis = 0.75) ## smaller axis-font size\n\n\n\n\n\n\n\n\nNote again how the little-trained opm10-agent has practically uniform beliefs. But it’s also expressing the fact that future training data will probably increase the probability of \\(\\mathit{race}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\\).\nThis is corroborated by the fully-trained agent:\n\nplotFbelief(agent = opmAll, n = 1000,\n    predictand = 'race',\n    ylim = c(0,1), main = 'opmAll', cex.axis = 0.75)\n\n\n\n\n\n\n\n\n\n\nThese checks are satisfying, but it’s good to examine their agreement or disagreement with our intuition. Examine the last plot for example. The opmAll agent has very firm beliefs (no spread in the light-blue lines) about the full-population distribution of \\(\\mathit{race}\\). Do you think its beliefs are too firm, after 11 000 datapoints? would you like the agent to be more “open-minded”? In that case you should go back to the Collect & prepare background info stage, and for example modify the parameters \\(k_{\\text{mi}}, k_{\\text{ma}}\\), then re-check. Or you could even try an agent with a different initial belief distribution.\nIn making this kind of considerations it’s important to keep in mind what we learned and observed in previous chapters:\n\n\n\n\n\n\nNote\n\n\n\nOur goal: optimality, not “success”\nRemember (§ 2.4 and § 8.1) that a probability represents the rational degree of belief that an agent should have given the particular information available. We can’t judge a probability from the value it assigns to something we later learn to be true – because according to the information available it could be more rational (and optimal) to consider that something implausible (recall the example in § 8.1 of an object falling from the sky as we cross the street).\nFrom this point of view we should be wary of comparing the probability of something with our a-posteriori knowledge about it.\n\n\nThe data cannot speak for themselves\nWe could build an agent that remains more “open-minded” (more spread in the light-blue lines), having received exactly the same training data. This “open-mindedness” therefore cannot be determined by the training data. Once more this shows that data cannot “speak for themselves” (§ 24.4).\n\n\n\n\n\n\n\n\n\nApplication: specifying predictors\nLet’s now draw inferences by specifying some predictors.\nWe ask the opm10 agent to forecast the \\(\\mathit{income}\\) of a new unit, given that the unit is known to have \\(\\mathit{occupation}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Exec-managerial;}\\) and \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Male;}\\) (two predictor variates). What would you expect?\nThe opm10-agent’s belief about the unit – as well as about the full subpopulation (§ 22.1) of units having those predictors – is shown in the following plot:\n\nplotFbelief(agent = opm10, n = 1000,\n    predictand = 'income',\n    predictor = list(occupation = 'Exec-managerial', sex = 'Male'),\n    ylim = c(0,1), main = 'opm10')\n\n\n\n\n\n\n\n\nNote how the opm10-agent still slightly higher probability to \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\), but at the same time it is quite uncertain about the subpopulation frequencies; more than if the predictor had not been specified. That is, according to this little-trained agent there could be large variety of possibilities within this specific subpopulation.\nThe opmAll-agent’s beliefs are shown below:\n\nplotFbelief(agent = opmAll, n = 1000,\n    predictand = 'income',\n    predictor = list(occupation = 'Exec-managerial', sex = 'Male'),\n    ylim = c(0,1), main = 'opmAll')\n\n\n\n\n\n\n\n\nit believes with around 55% probability that such a unit would have higher, \\({\\small\\verb;&gt;50K;}\\) income. The representative subpopulation-frequency distributions in light-blue indicate that this belief is unlikely to be changed by new training data.\n\n\nLet’s now see an example of our agent’s versatility by switching predictands and predictors. We tell the opmAll-agent that the new unit has \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&gt;50;}\\), and ask it to infer the joint variate \\((\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex})\\); let’s present the results in rounded percentages:\n\nresult &lt;- infer(agent = opmAll,\n    predictand = c('occupation', 'sex'),\n    predictor = list(income = '&gt;50K'))\n\nround(result * 100, 1) ## round to one decimal\n\n                   sex\noccupation          Female Male\n  Adm-clerical         3.1  4.1\n  Armed-Forces         1.0  1.0\n  Craft-repair         1.1  9.4\n  Exec-managerial      3.6 16.0\n  Farming-fishing      1.0  2.1\n  Handlers-cleaners    1.0  1.6\n  Machine-op-inspct    1.1  3.1\n  Other-service        1.6  1.8\n  Priv-house-serv      1.0  1.0\n  Prof-specialty       4.8 14.7\n  Protective-serv      1.0  3.2\n  Sales                1.8 10.1\n  Tech-support         1.5  3.4\n  Transport-moving     1.1  3.7\n\n\nIt returns a 14 × 2 table of joint probabilities. The most probable combinations are \\(({\\small\\verb;Exec-managerial;}, {\\small\\verb;Male;})\\) and \\(({\\small\\verb;Prof-specialty;}, {\\small\\verb;Male;})\\).\n\n\nThe rF() function\nThis function generates full-population frequency distributions (even for subpopulations) that are probable according to the data. It is used internally by plotFbelief(), which plots the generated frequency distributions as light-blue lines.\nLet’s see, as an example, three samples of how the full-population frequency distribution for \\(\\mathit{sex} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{income}\\) (jointly) could be:\n\nfreqsamples &lt;- rF(n = 3, ## number of samples\n    agent = opmAll,\n    predictand = c('sex', 'income'))\n\nprint(freqsamples)\n\n, , 1\n\n        income\nsex         &lt;=50K      &gt;50K\n  Female 0.284734 0.0729671\n  Male   0.429640 0.2126587\n\n, , 2\n\n        income\nsex         &lt;=50K      &gt;50K\n  Female 0.280624 0.0690956\n  Male   0.432737 0.2175428\n\n, , 3\n\n        income\nsex         &lt;=50K      &gt;50K\n  Female 0.284728 0.0696334\n  Male   0.434569 0.2110701\n\n\nThese possible full-population frequency distributions can be used to assess how much the probabilities we find could change, if we collected a much, much larger amount of training data. Here is an example:\nWe generate 1000 frequency distributions for \\((\\mathit{occupation} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathit{sex})\\) given \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&gt;50K;}\\), and then take the standard deviations of the samples as a rough measure of how much the probabilities we previously calculated could change;\n\nfreqsamples &lt;- rF(n = 1000, agent = opmAll,\n    predictand = c('occupation', 'sex'),\n    predictor = list(income = '&gt;50K'))\n\nvariability &lt;- apply(freqsamples,\n    c('occupation','sex'), ## which dimensions to apply\n    sd) ## function to apply to those dimensions\n\nround(variability * 100, 1) ## round to one decimal\n\n                   sex\noccupation          Female Male\n  Adm-clerical         0.3  0.3\n  Armed-Forces         0.2  0.2\n  Craft-repair         0.2  0.4\n  Exec-managerial      0.3  0.6\n  Farming-fishing      0.2  0.2\n  Handlers-cleaners    0.2  0.2\n  Machine-op-inspct    0.2  0.3\n  Other-service        0.2  0.2\n  Priv-house-serv      0.2  0.2\n  Prof-specialty       0.3  0.6\n  Protective-serv      0.2  0.3\n  Sales                0.2  0.5\n  Tech-support         0.2  0.3\n  Transport-moving     0.2  0.3\n\n\nthe agent believes (at around one standard deviation, that is 68%) that the current probability wouldn’t change more than about ±0.5%.\n\n\nThe inferences above were partially meant as checks, but we see that we can actually ask our agent a wide variety of questions about the full population, and do all sorts of association studies.\n\n\n\n\n\n\n No “test” or “validation” datasets used or needed\n\n\n\nThe tests and explorations above were done without any “validation” or “test” datasets. This is because our agent is capable of calculating and showing its beliefs about the full population – and therefore about future data.\nThe need for validation or test datasets with common machine-learning algorithms arise from the fact that full-population beliefs are hidden or, more commonly, not computed at all, in order to gain speed. The application of the trained machine-learning algorithm to a validation dataset is an approximate way of extracting such beliefs.\n\n\n\n\n\n\nExploring the population properties: mutual information\nIn § 18.5 we introduced mutual information as the information-theoretic measure of mutual relevance and association of two quantities or variates. For the present task, the opmAll-agent can tell us the mutual information between any two sets of variates of our choice, with the function mutualinfo().\nFor instance, let’s calculate the mutual information between \\(\\mathit{occupation}\\) and \\(\\mathit{marital\\_status}\\).\n\nmutualinfo(agent = opmAll, A = 'occupation', B = 'marital_status')\n\n[1] 0.0827823\n\n\nIt is a very low association: knowing either variate decreases the effective number of possible values of the other only \\(2^{0.0827823\\,\\mathit{Sh}} \\approx 1.06\\) times.\nNow let’s consider a scenario where, in order to save resources, we can use only one variate to infer the income. Which of the other variates should we prefer? We can calculate the mutual information between each of them, in turn, and \\(\\mathit{income}\\):\n\n## list of all variates\nvariates &lt;- names(opmAll$variates)\n\n## list of all variates except 'income'\npredictors &lt;- variates[variates != 'income']\n\n## prepare vector to contain the mutual information\nrelevances &lt;- numeric(length(predictors))\nnames(relevances) &lt;- predictors\n\n## calculate, for each variate, the mutual information 'relevance' (in shannons)\n## between 'income' and that variate\nfor(vrt in predictors){\n    relevances[vrt] &lt;- mutualinfo(agent = opmAll, A = 'income', B = vrt)\n}\n\n## output the mutual informations in decreasing order\nsort(relevances, decreasing = TRUE)\n\nmarital_status   relationship      education     occupation      workclass \n    0.10074130     0.09046621     0.06332052     0.05506897     0.03002995 \nnative_country            sex           race \n    0.01925227     0.01456655     0.00870089 \n\n\nIf we had to choose only one variate to infer the outcome, on average it would be best to use \\(\\mathit{marital\\_status}\\). Our last choice should be \\(\\mathit{race}\\).\n\n\n\n\n\n\nExercise 33.1\n\n\n\nNow consider the scenario where we must exclude one variate from the eight predictors, or, equivalently, we can only use seven variates as predictors. Which variate should we exclude?\nPrepare a script similar to the one above: it calculates the mutual information between \\(\\mathit{income}\\) and the other predictors but with one omitted, omitting each of the eight in turn.\nWarning: this computation might require 10 or more minutes to complete.\n\nWhich single variate should not be omitted from the predictors? which single variate could be dropped?\nDo you obtain the same relevance ranking as in the “use-one-variate-only” scenario above?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#example-application-to-new-data",
    "href": "example_opm1.html#example-application-to-new-data",
    "title": "33  Example application: adult-income task",
    "section": "33.6 Example application to new data",
    "text": "33.6 Example application to new data\nLet’s apply the opmAll-agent to a test dataset with 33 914 new units. For each new unit, the agent:\n\ncalculates the probability of \\(\\mathit{income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;&lt;=50;}\\), via the function infer(), using as predictors all variates except \\(\\mathit{income}\\)\nchooses one of the two values \\(\\set{{\\small\\verb;&lt;=50K;}, {\\small\\verb;&gt;50K;}}\\), via the function decide() trying to maximizing utilities corresponding to the accuracy metric\n\nThe function decide() will be described in more detail in chapters 3.3 and 37.\nAt the end we plot a histogram of the probabilities calculated for the new units, to check for instance for how many of the agent was sure (beliefs around 0% or 100%) or unsure (beliefs around 50%). We also report the final utility/accuracy per unit, and the time needed for the computation:\n\n## Load test data\ntestdata &lt;- tread.csv('test-income_data_example.csv')\n\nntest &lt;- nrow(testdata) ## size of test dataset\n\n## Let's time the calculation\nstopwatch &lt;- Sys.time()\n\ntestprobs &lt;- numeric(ntest) ## prepare vector of probabilities\ntesthits &lt;- numeric(ntest) ## prepare vector of hits\n\nfor(i in 1:ntest){\n\n    ## calculate probabilities for 'income' given all other variates\n    probs &lt;- infer(agent = opmAll,\n        predictand = 'income',\n        predictor = testdata[i, colnames(testdata) != 'income'])\n\n    ## store the probability for &lt;=50K\n    testprobs[i] &lt;- probs['&lt;=50K']\n\n    ## decide on one value\n    chosenvalue &lt;- decide(probs = probs)$optimal\n\n    ## check if decision == true_value: hit! and store result\n    testhits[i] &lt;- (chosenvalue == testdata[i, 'income'])\n}\n\n## Print total time required\nprint(Sys.time() - stopwatch)\n\nTime difference of 5.55433 secs\n\n## Histogram and average accuracy (rounded to one decimal)\nthist(testprobs,\n    n = seq(0, 1, length.out = 10), # 9 bins\n    plot = TRUE,\n    xlab = 'P(income = \"&lt;=50K\")',\n    ylab = 'frequency density in test set',\n    main = paste0('accuracy: ', round(100 * mean(testhits), 1), '%'))",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm1.html#sec-compare-opm",
    "href": "example_opm1.html#sec-compare-opm",
    "title": "33  Example application: adult-income task",
    "section": "33.7 Comparison",
    "text": "33.7 Comparison\n\n\n\n\n\n\nExercise 33.2\n\n\n\nNow try to use a popular machine-learning algorithm for the same task, using the same training data, and compare it with the prototype optimal predictor machine. Examine the differences. For example:\n\nCan you inform the algorithm that \\(\\mathit{native\\_country}\\) has two additional values \\({\\small\\verb;Norway;}\\), \\({\\small\\verb;Netherlands;}\\) not present in the training data? How?\nCan you flexibly use the algorithm to specify any predictors and any predictands on the fly?\nDoes the algorithm inform you of how the inferences could change if more training data were available?\nWhich accuracy does the algorithm achieve on the test set?",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>[Example application: adult-income task]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html",
    "href": "slm.html",
    "title": "34  Example application: “small language model”",
    "section": "",
    "text": "34.1 Natural language as inference and decision\nIn § 2.3 we remarked that even a process like speaking involves a continuous decision-making. Each uttered word is chosen among an infinity of other possible ones. The choice depends not only on the object or idea that has to be communicated at the moment, but also on long-term goals and consequences. Is the conversation meant to be funny? Is it to explain something in the most precise way possible? What’s the probability that this particular choice of words offend the listener? What’s the probability that this possible sentence is misunderstood, and what would happen in that case?\nThe decision-making underlying speaking is remarkably complex, and can only be represented by a deep sequence of decisions and outcomes (we’ll discuss decision sequences in ch.  36), trying to forecast short-term and long-term future consequences. Most of this decision-making takes place almost unconsciously in humans, by means of approximate, heuristic procedures. But it’s a decision-making process nevertheless.\nBuilding an AI agent that can speak in the sense above is still out of reach. Speaking in that sense requires an agent to have long-term goals and a set of values. But speaking can be mimicked, at least for short time spans, by a different inference and decision process. In human speech the choice of words depends on future goals as much or more than past factors; but we can try to draw an inference based on past factors alone.\nIn the simplest inference of this kind, an agent can assign a degree of belief about the word that “should follow next”, based on the words that precede it. This is how the next-word predictors of some smartphone keyboards work, and it is essentially also the way Large Language Models (LLMs) and General Pretrained Transformers (GPTs) work. The inference is based on the frequencies of many different sequences of words, learned from a huge amount of written or spoken texts.\nThis way of operation is also the reason why we can’t really say that large language models understand language, despite some literature stating that they do. They don’t do any kind of forecast of the consequences of their word choice, nor any inferences about the intentions of their interlocutor; not even approximate, heuristic ones. They have simply learned word frequencies from a huge variety of contexts.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html#sec-language-inference",
    "href": "slm.html#sec-language-inference",
    "title": "34  Example application: “small language model”",
    "section": "",
    "text": "For the extra curious\n\n\n\nLarge language models as Markov chains",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html#sec-opm-as-slm",
    "href": "slm.html#sec-opm-as-slm",
    "title": "34  Example application: “small language model”",
    "section": "34.2 The OPM as a small language model",
    "text": "34.2 The OPM as a small language model\nAlgorithms that implement the kind of word inference just described, usually do so in an exchangeable way. They use the frequencies of different word sequences, without regard to whether most of those sequences occur, say, in older rather than newer texts.\nOur OPM is also designed to use built-in exchangeable beliefs, and in the previous chapter we saw its extreme versatility with exchangeable inferences of nominal variates – which is just what words, more or less, are. So let’s try to use it for word inference, and see if it can mimic speech. Of course we cannot expect spectacular results: large language models manage to make their remarkable inferences after learning more than tens of trillions (10 000 000 000 000) of words, which require huge amounts of memory and time. Owing to our limitations of memory and time, our OPM can only act as a “small language model”.\nTo use the OPM this way, we define a population of \\(n\\) variates, let’s say \\(n = 3\\). The three variates are defined as consecutive words in some specific set of texts. We can call them \\({\\small\\verb;word1;}\\), \\({\\small\\verb;word2;}\\), \\({\\small\\verb;word3;}\\), and so on. Instead of words it’s actually best to use tokens, which comprise not only words, but punctuation marks like ,.;:!? and other signs. (The exact definition of token depends on the specific application, and can even include parts of words.) Such a sequence of \\(n\\) tokens is often called an n-gram.\n\nPreparing the training data from some text\nIn order to prepare the training data for our OPM agent, let’s load some helper R functions defined in the SLMutilities.R file, as well as the OPM functions, from the usual directory\nhttps://github.com/pglpm/ADA511/blob/master/code:\n\nsource('tplotfunctions.R')\nsource('OPM_nominal.R')\nsource('SLMutilities.R')\n\nset.seed(400)\n\nThe function preparengramfiles() takes four input arguments:\n\ninputfile: the name of a text .txt file, used to train the agent;\nn: the \\(n\\) of the n-grams, default n = 3;\nmaxtokens: the maximum number of unique tokens to use – the “vocabulary” – in case we need to save memory; default is maxtokens = Inf, which uses all tokens from the input text;\nnumbersasx: convert any groups of digits to the token x? Default TRUE;\noutsuffix: the suffix for the output files (see below);\n\nThe function creates two csv files: one containing the training data with all n-grams from the input text, and one containing the metadata, consisting in the vocabulary. It outputs the file names as list with elements metadata and data. These files have exactly the same format used in the previous chapter; for our OPM agent these data and metadata aren’t qualitatively different from the ones of the adult-income task.\nFor simplicity, the helper function above converts all text to upper case and ASCII characters; it replaces every sequence of digits 0–9 with the single lowercase letter x; and only keeps the following punctuation: ,.;:?!%$&@+'/-.\nAs a concrete example let’s start with a simple text: the lyrics of Daft Punk’s song Around the World:\n\n\n\n\nAround the world, around the world.\nAround the world, around the world.\n\nThese two lines are stored in the file texts/around_the_world.txt. We shall use 3-grams to train our agent:\n\nngramfiles &lt;- preparengramfiles(\n    inputfile = 'texts/around_the_world.txt',\n    outsuffix = 'around',\n    n = 3,\n    maxtokens = Inf,\n    numbersasx = TRUE\n)\n\nUnique tokens:  5.\n\n\nData:  22  3-grams.\n\n\nFiles saved.\n\n\nThe helper function tells us that it used five unique tokens, and from the text it produced a training dataset of 22 3-grams. Let’s take a look at the training dataset:\n\nread.csv(ngramfiles$data)\n\n    word1  word2  word3\n1  AROUND    THE  WORLD\n2     THE  WORLD      ,\n3   WORLD      , AROUND\n4       , AROUND    THE\n5  AROUND    THE  WORLD\n6     THE  WORLD      .\n7   WORLD      . AROUND\n8       . AROUND    THE\n9  AROUND    THE  WORLD\n10    THE  WORLD      ,\n11  WORLD      , AROUND\n12      , AROUND    THE\n13 AROUND    THE  WORLD\n14    THE  WORLD      .\n15  WORLD      . AROUND\n16      . AROUND    THE\n17 AROUND    THE  WORLD\n18    THE  WORLD      ,\n19  WORLD      , AROUND\n20      , AROUND    THE\n21 AROUND    THE  WORLD\n22    THE  WORLD      .\n\n\nWe see that the token vocabulary consists of  {AROUND THE WORLD , .}.  The training data consist of all sequences, 22 in total, of three consecutive tokens present in the text. Note a specific word in the original text therefore usually appears in three different datapoints, except for the very first and very last two words.\nLet’s build an opmSLM agent and make it learn from these metadata and data:\n\nopmSLM &lt;- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data\n)\n\n\n\nToken inferences\nWe ask the opmSLM agent to draw an inference about \\({\\small\\verb;word1;}\\), and sort the results in order of decreasing probabilities:\n\nprobs &lt;- infer(agent = opmSLM, predictand = 'word1')\n\nsort(probs, decreasing = TRUE)\n\nword1\n  AROUND      THE    WORLD        ,        . \n0.262502 0.262502 0.223438 0.145311 0.106248 \n\n\nThese degrees of belief make sense: in the learning text, if we take any token that can be followed by two more, we find AROUND, THE, and WORLD with approximately equal frequencies, somewhat less for WORLD because it also occurs in second-last position. The two punctuation marks occur with around half those frequencies.\nThe agent learned from just two lines, though. Let’s inspect the agent’s beliefs about the frequencies in a possibly larger text, by means of the plotFbelief() function:\n\nplotFbelief(agent = opmSLM,\n    n = 200,\n    predictand = 'word1',\n    sort = +Inf,\n    ylim = c(0,1))\n\n\n\n\n\n\n\n\nso the agent is open to possible changes in the frequencies, if it were given more training data.\n\nLet’s use the agent as a small language model. If a sequence of two tokens is “AROUND THE”, what should the next word be?\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'AROUND', word2 = 'THE'))\n\nsort(probs, decreasing = TRUE)\n\nword3\n     WORLD          ,          .     AROUND        THE \n0.98125345 0.00468664 0.00468664 0.00468664 0.00468664 \n\n\nThe agent believes at 98% that it should be WORLD!\nOur agent can actually also do inferences that many large language models are not designed to do. For instance it can guess the token preceding a sequence of two tokens. What should come before “THE WORLD”?\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word1',\n    predictor = list(word2 = 'THE', word3 = 'WORLD'))\n\nsort(probs, decreasing = TRUE)\n\nword1\n    AROUND          ,          .        THE      WORLD \n0.98125345 0.00468664 0.00468664 0.00468664 0.00468664 \n\n\nIt can also guess the middle token. What should be between THE and ,?\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word2',\n    predictor = list(word1 = 'THE', word3 = ','))\n\nsort(probs, decreasing = TRUE)\n\nword2\n     WORLD          ,          .     AROUND        THE \n0.96336537 0.00915866 0.00915866 0.00915866 0.00915866",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html#sec-slm-large-example",
    "href": "slm.html#sec-slm-large-example",
    "title": "34  Example application: “small language model”",
    "section": "34.3 A more demanding example",
    "text": "34.3 A more demanding example\nThe simple text we chose was just for testing. Let’s now use a slightly larger text: the United Nation’s Universal Declaration of Human Rights, stored in texts/human_rights.txt. The core of this document is a set of Articles, for example:\n\nArticle 1\nAll human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\nArticle 2\nEveryone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. […]\n\nPrepare the metadata and training dataset:\n\nngramfiles &lt;- preparengramfiles(\n    inputfile = 'texts/human_rights.txt',\n    outsuffix = 'rights',\n    n = 3,\n    maxtokens = Inf,\n    numbersasx = TRUE\n)\n\nUnique tokens:  506.\n\n\nData:  1904  3-grams.\n\n\nFiles saved.\n\n\nThis time the vocabulary is over 500 tokens, and the agent will learn from around 2000  3-grams.\nBuild an agent from these metadata and data. We call it again opmSLM (rewriting the previous one). To avoid memory problems, we use the option savememory = TRUE, which stores the learning information in a memory-efficient way, at the cost of slightly longer computation times during inferences.\n\nopmSLM &lt;- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data,\n    savememory = TRUE\n)\n\nNow some inferences. Recall that the text is converted to uppercase, and all numbers are replaced with x. Many pieces of text thus begin with “ARTICLE x”. Ask the agent what should follow after these two tokens. Let’s just display the five most probable tokens:\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'ARTICLE', word2 = 'x'))\n\nsort(probs, decreasing = TRUE)[1:5]\n\nword3\n EVERYONE        NO       ALL       MEN   NOTHING \n0.7329435 0.1332633 0.0666322 0.0333166 0.0333166 \n\n\nRoughly 75% of Articles begin with “everyone…”, and this is roughly the agent’s belief.\n\nApparent grammar knowledge\nLarge language models seem to have good grammar knowledge. This knowledge comes, not from the study of grammar rules, but again simply from the frequencies of particular word combinations in their training texts. Let’s see if our small language model displays some rudimentary “knowledge” of grammar.\nWhich word should follow the noun “right”, as in “the right…”?\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word3',\n    predictor = list(word1 = 'THE', word2 = 'RIGHT'))\n\nsort(probs, decreasing = TRUE)[1:5]\n\nword3\n          TO           OF       FREELY          THE          AND \n0.9280423224 0.0356950206 0.0356950206 0.0000011285 0.0000011285 \n\n\nThe opmSLM agent believes at 93% that the preposition “to” should follow “the right”: “the right to…”. Well learned!\nAnother grammar question: which is correct, “everyone has”, or “everyone have”? Let’s check the agent’s beliefs about either sequence. To this purpose we use infer() to calculate the probabilities of all tokens after EVERYONE, and then select the probabilities of the particular tokens HAS and HAVE. We can compare the two probabilities against each other:\n\nprobs &lt;- infer(agent = opmSLM,\n    predictand = 'word2',\n    predictor = list(word1 = 'EVERYONE'))\n\nprobHas &lt;- probs['HAS']\n\nprobHave &lt;- probs['HAVE']\n\n## probability of each, assuming that one of them is true\n\nc(probHas, probHave) / (probHas + probHave)\n\n        HAS        HAVE \n0.999273886 0.000726114 \n\n\nA 99.9% belief that “everyone has” is the correct one between the two!\n\n\n\n\n\n\nExercise 34.1\n\n\n\nUsing the four fundamental rules or the shortcut rules, prove that\n\\[\n\\mathrm{P}\\bigl({\\small\\verb;has;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} ({\\small\\verb;has;} \\lor {\\small\\verb;have;}) \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}) =\n\\frac{\n\\mathrm{P}({\\small\\verb;has;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\mathrm{P}({\\small\\verb;has;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}({\\small\\verb;have;} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\ntaking into account that \\({\\small\\verb;has;}\\) and \\({\\small\\verb;have;}\\) are mutually exclusive. (In our case \\(\\mathsfit{I}\\coloneqq{\\small\\verb;everyone;} \\land D\\), where \\(D\\) is the learning and background information.)",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html#sec-amount-ngrams",
    "href": "slm.html#sec-amount-ngrams",
    "title": "34  Example application: “small language model”",
    "section": "34.4 How much training data should we use?",
    "text": "34.4 How much training data should we use?\nOur small-language-model agent made reasonable inferences so far, relying on sets of three tokens. What if we use 4-grams, 5-grams, or 1000-grams, instead of 3-grams? won’t our agent’s performance increase?\nNot necessarily. Using n-grams with higher \\(n\\) means that the agent is working with a population with more variates, and therefore it needs more training data in order to arrive at sharper beliefs.\nRecall from § 33.4 and § 28.1 that our Dirichlet-mixture agent has the nifty capability to estimate how many training datapoints should be necessary to concretely modify its initial beliefs. What’s the situation with the present agent? It learned from around 2000 datapoints; are they enough? Let’s plot the belief distribution about the necessary amount, as we did in § 33.4:\n\nflexiplot(x = opmSLM$alphas, y = opmSLM$palphas, type = 'b',\n    xlim = c(0, 10000), ylim = c(0, NA),\n    xlab = 'required number of training 3-gram data',\n    ylab = 'probability')\n\n\n\n\n\n\n\n\nOops! The agent is telling us that the 2000 datapoints we gave it are very probably not enough: it’d be better with at least 4000!\nAs we increase the order of n-grams, and also the vocabulary size, much much more datapoints are needed. To see this, let’s double \\(n\\), preparing a dataset of 6-gram from our text, and build a temporary agent to assess how many datapoints would be needed (pay attention to the new x-axis range!):\n\n## Prepare metadata and training data for 6-grams...\nngramfiles &lt;- preparengramfiles(\n    inputfile = 'texts/human_rights.txt',\n    outsuffix = 'temp',\n    n = 6,\n    maxtokens = Inf,\n    numbersasx = TRUE\n)\n\nUnique tokens:  506.\n\n\nData:  1901  6-grams.\n\n\nFiles saved.\n\n## ...Build a temporary agent from them...\nopmTemp &lt;- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data,\n    savememory = TRUE\n)\n\n## ...Check agent's belief about size of required dataset...\nflexiplot(x = opmTemp$alphas, y = opmTemp$palphas, type = 'b',\n    xlim = c(0, 100000), ylim = c(0, NA),\n    xlab = 'required number of training 6-gram data',\n    ylab = 'probability')\n\n\n\n\n\n\n\n## ...remove the agent from memory\nrm(opmTemp)\n\nThe most probable answer is roughly 65 000 training datapoints. So doubling the n-gram order requires a dataset around 16 times larger; and this is while keeping the vocabulary small, just 500 tokens. These numbers give us a glimpse of why commercial large language models need preposterously large amounts of data and computing power.\n\n\n\n\n\n\nExercise 34.2: [Idea by Martin Thunestveit]\n\n\n\nCan the OPM agent mimic simple maths knowledge, such as the multiplication table?\n\nPrepare data and metadata files appropriate for this kind of learning, and build an agent trained from them. The predictors should be, for instance, a sequence of tokens like “3 X 5”, and the predictand should be the result.\nIs it enough to provide, as training data, just one copy of the multiplication tables? Or do you need to repeat it two or more times?\nExperiment further with this idea!",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "slm.html#sec-preview-gentext",
    "href": "slm.html#sec-preview-gentext",
    "title": "34  Example application: “small language model”",
    "section": "34.5 Text generation: a preview",
    "text": "34.5 Text generation: a preview\nPresent-day large language models do not only give probabilities about the next word: they generate words. We can of course use our flexible OPM agent to generate text as well. Text generation, however, is a decision-making task, which also involves utilities. We’re going to discuss decision making in the next chapters, and then we’ll use the OPM for making decisions as well.\nFor the moment, let’s get a preview of some text generated by our opmSLM agent, using the function generatetext() which we’ll explain in a later chapter. Its arguments should be self-explanatory:\n\ntext &lt;- generatetext(\n    agent = opmSLM,\n    prompt = c('ARTICLE', 'x'),\n    stopat = 70,\n    online = FALSE\n)\n\nwrapprint(text, wrapat = 50)\n\n ARTICLE x NOTHING IN THIS DECLARATION, WITHOUT\n DISTINCTION OF ANY CRIMINAL CHARGE AGAINST HIM. ARTICLE x\n EVERYONE IS ENTITLED TO ALL ON THE BASIS OF MERIT.\n EDUCATION SHALL BE PROHIBITED IN ALL THEIR FORMS. ARTICLE x\n EVERYONE HAS THE RIGHT TO FREEDOM OF SPEECH AND BELIEF AND\n FREEDOM, JUSTICE AND PEACE IN THE GOVERNMENT OF HIS\n INTERESTS. ARTICLE x NO ONE SHALL BE MADE ...\n\n\nThe generated text above does not come from the Universal Declaration of Human Rights: as clear from the various grammar mistakes, it is a genuine creation of the OPM. It is just “inspired” by the original text, despite the subversive “EDUCATION SHALL BE PROHIBITED IN ALL THEIR FORMS”.",
    "crumbs": [
      "[**A prototype Optimal Predictor Machine**]{.red}",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>[Example application: \"small language model\"]{.red}</span>"
    ]
  },
  {
    "objectID": "utilities.html",
    "href": "utilities.html",
    "title": "35  Utilities",
    "section": "",
    "text": "35.1 From inferences to decisions\nAt long last we have seen how an agent can calculate the probabilities of any kind of sentence, given any facts and assumptions available to it. We studied this probability calculation for general problems, and then more in detail for problems enjoying special properties, for instance when the agent’s beliefs are exchangeable (chapter  25). Most important, we saw how previous data and background information can be used to determine, and at the same time affect, these probabilities. We have even built a real, prototype agent that can flexibly calculate probabilities in problems involving nominal variates.\nHaving rational degrees of belief about all possible hypotheses and unknowns is useful, and the first step in scientific research. But often it isn’t enough. Often the agent needs to act, to do something, to make a choice, even if its uncertainty has not disappeared and therefore it can’t be sure of what its choice will lead to. Our very first example of engineering problem (chapter  1) involved the decision on whether to accept or discard an electronic component just produced in an assembly line, not knowing whether it will fail within a year or not.\nIn chapter  3 we met the theory to deal with this kind of decision-making problem: Decision Theory.\nDecision Theory requires the decision-making agent to calculate the probabilities of the unknown outcomes. Now we know how to do that, at least in some kinds of problems! Most of our discussion so far focused on the calculation of those probabilities, which often is the most difficult part of the decision-making task.\nSo let’s face the decision-making problem again at last, and complete the construction of our prototype agent by implementing the final decision-making step.1",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#from-inferences-to-decisions",
    "href": "utilities.html#from-inferences-to-decisions",
    "title": "35  Utilities",
    "section": "",
    "text": "1 Please go back to chapter  3 and review the notions and terminology introduced there.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-decisions-utilities",
    "href": "utilities.html#sec-decisions-utilities",
    "title": "35  Utilities",
    "section": "35.2 Review of a basic decision problem: outcomes, decisions, utilities",
    "text": "35.2 Review of a basic decision problem: outcomes, decisions, utilities\nRecall the structure of a basic decision:\n\nIn order to make a decision, the agent needs:\n\n The set of possible decisions, which we represent as sentences like \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\).\n The set of possible outcomes, whose truth is unknown to the agent. In the kind of decision problems that we are examining, the outcomes correspond to the possible values \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\) of the predictand variate.\n The probabilities of the outcomes  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\) which are determined by the agent’s background information \\(\\mathsfit{I}\\), by any other available information, such as \\(\\mathsfit{\\color[RGB]{34,136,51}data}\\) about previously observed units and the values of some predictor variates \\(\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\) for the current unit; and possibly also by the agent’s decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\) (see below).\n The utilities of each pair of decision and outcome \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\).\n\nSome texts call the joint pair \\(({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{204,187,68}D})\\), or equivalently \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\), of a decision and an outcome, a consequence.\nLet’s not forget some important points about the notions above:\n\n\n\n\n\n\n\n\n\n\n\nWe are not assuming any temporal or “causal” relationship between decisions and outcomes: our framework works independently of these relationships. In some decision-making problems the outcomes happen after a decision is made, and may be “influenced” by it or not. In other decision-making problems the outcomes have already happened before a decision is made.\nIn a transportation-choice problem, for instance, the outcome “wet from rain” may happen after we make the decision “go on foot” rather than “go by bus”. In an image-classification problem, on the other hand, the outcome “true label is \\({\\small\\verb;cat;}\\)” was already determined before we made the decision “classify as \\({\\small\\verb;dog;}\\)”.\n\n\n\n\nIn connection with the warning above, in some problems the probabilities of the outcomes may depend on the decision; that is, knowledge about the decision is relevant to the knowledge about the outcome (see again § 18):\n\\[\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nFor instance, in the transportation-choice problem the probability of the outcome “wet from rain” is higher conditional on the decision “go on foot” than on the decision “go by bus”.\nIn many decision-making problems typical of machine learning, such as classification, information about the decision is irrelevant to the information about the outcome (which usually has already happened), and we have\n\\[\n  \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n  = \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n  \\]\n\n\n\nIn what follows we shall consider problems, such as classification, where knowledge of the agent’s decision is irrelevant to the outcome’s probability. We shall nevertheless keep the more general notation  \\(\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\).",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-what-utilities",
    "href": "utilities.html#sec-what-utilities",
    "title": "35  Utilities",
    "section": "35.3 What are utilities?",
    "text": "35.3 What are utilities?\nIn most decision-making problems the “gain” or “loss” or “satisfaction” or “desiderability” of the outcomes depends on many different aspects. A person purchasing some item may have to choose between something inexpensive but of low quality, or something of high quality but expensive. A clinical patient may have to choose between a treatment that increases life expectancy but worsens the quality of life, or a treatment that improves the quality of life but decreases life expectancy.\nDecision Theory says that whenever an agent makes a decision among alternatives having heterogeneous decision aspects, then it is implicitly using only one real number to summarize and bring together all those aspects. If this weren’t true, the agent would be deciding in an irrational way, which could even be exploited against the agent itself.\nSuch idea is not counter-intuitive in our culture. We are wont, for example, to exchange money for things of wildly different kinds: food, entertainment, health, work, transport, communication, life insurance, knowledge, political power. The monetary value of a human life (“value of statistical life”) for some governments is about USD 10 000 000.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nOn making life and death decisions\n\n\n\n2 With the opposite convention we speak of disutility or loss.Utility is the name we give to the real number that encodes together all heterogeneous desirabilities and gains of an outcome. The convention is that the higher the utility, the more desirable is the outcome.2\n\nNotation\nWe denote the utility of the outcome \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\) following from decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\) as\n\\[\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nThis notation reminds us that the utilities assigned by an agent depend not only on the outcome, but also on the decision and on the agent’s background information \\(\\mathsfit{I}\\).\nThe utilities for all outcomes and decisions can be encoded in a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\), having one row per decision and one column per outcome:\n\\[\n\\boldsymbol{\\color[RGB]{68,119,170}U}\\coloneqq\n\\begin{bmatrix}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n&\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n& \\dotso\n\\\\\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n&\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}'' \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n& \\dotso\n\\\\\n\\dotso&\\dotso&\\dotso\n\\end{bmatrix}\n\\]\n\n\n\nUtilities in sequences of apparently identical decisions\nKeep in mind that the background information \\(\\mathsfit{I}\\) also includes previous decisions and outcomes, if the present decision and outcome are part of a sequence. Because of this, when a decision problem occurs again, the utility of a particular decision and outcome may be different from the previous ones. This will be discussed in § 36.4.\nAs an example, suppose someone proposes you a sequence of bets. A coin will be tossed several times; before each toss you must decide between \\(\\mathsfit{\\color[RGB]{204,187,68}H}\\coloneqq{}\\)“bet on heads” or \\(\\mathsfit{\\color[RGB]{204,187,68}T}\\coloneqq{}\\)“bet on tails”; if the outcome is the one you bet on, you win $ 1, otherwise lose $ 1. The utility of a $ 1-loss at the first toss might be \\(\\color[RGB]{68,119,170}-1\\):\n\\[\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}H}_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}C_{\\color[RGB]{0,0,0}1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tail;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\color[RGB]{68,119,170}-1\n\\]\nBut the utility of a $ 1-loss at the second toss might be lower, say \\(\\color[RGB]{68,119,170}-1.1\\), if preceded by a loss:\n\\[\n\\begin{aligned}\n&\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}H}_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}C_{\\color[RGB]{0,0,0}2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tail;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{\\color[RGB]{204,187,68}H}_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}C_{\\color[RGB]{0,0,0}1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tail;}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}) = \\color[RGB]{68,119,170}-1.1\n\\\\&\\text{\\small whereas}\n\\\\\n&\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}H}_2 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}C_{\\color[RGB]{0,0,0}2}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;tail;}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\mathsfit{\\color[RGB]{204,187,68}H}_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}C_{\\color[RGB]{0,0,0}1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;head;}} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\mathsfit{I}) = \\color[RGB]{68,119,170}-1\n\\end{aligned}\n\\]\nand so on. The decreased utility, even if the consequences at each coin toss are the same, may for example arise from the importance of the accumulated loss compared to your savings.\n\n\nUtility of money\nThe outcomes of a decision may involve the winning or losing of money. As we have emphasized multiple times, a utility is not the same thing as a sum of money. This means that even if we have associated a utility of \\(1\\) to a money win of $1, the utility associated to a win of $100 may be different from $100, and the utility associated to a loss of $1 may be different from \\(-1\\). There is no definite, universal mathematical relation between utility and money; such a relation must be assessed case by case.\nA lot of misunderstanding about decision theory and about the principle of maximal expected utility (ch.  36) has come from the erroneous numerical identification of utility and money. Some researchers have even proposed alternative procedures, owing to such misunderstandings. Here is one example.\nSuppose a coin is going to be tossed; your degree of belief in the two outcomes is 50%/50%. You’re offered to choose between two bets:\n\nBet A: If the coin lands tails, you win $10 010; if heads, you lose $10 000.\nBet B: If the coin lands tails, you win $1; if heads, you lose $1.\n\nA naive application of the principle of maximal expected utility would say that your expected money win of bet A is $5, whereas the expected win of bet B is $0. So bet A should be chosen.\nBut many people would not choose bet A, because if the coin landed heads, the loss would be excessive. Some people would not even be able to pay such a loss. Some researchers have seen this as a failure of the principle of maximal expected utility, and have proposed alternative rules that, for instance, take into account a measure (standard deviation or similar) of the difference between the expected value and the maximal loss or win.\nBut the point here is that the utility of a $10 000 loss is not simply \\(-10 000\\) times the utility of a $1 win. The very explanations of people’s choices against bet A reveal this: they make clear that losing $10 000 may put their very lives at risk. Clearly the utility of a $10 000 loss is much lower than \\(-10 000\\).\nAn example of more reasonable formula for the utility of a money win of \\(\\$x\\) (negative if a loss) is\n\\[\n\\mathrm{U}(\\$x) = \\ln\\frac{\\$C + \\$x}{\\$C}\n\\]\nwhere \\(\\$C\\) is the total money capital, or a fraction thereof, owed by a person. Suppose that a person owing a capital \\(\\$C = \\$20 000\\) is offered the two bets above. The utilities of the various wins and losses for the bets above are\n\\[\n\\begin{aligned}\n\\mathrm{U}(\\$10 010) &\\approx 0.41\n\\\\\n\\mathrm{U}(-\\$10 000) &\\approx -0.69\n\\\\[1ex]\n\\mathrm{U}(\\$1) &\\approx 0.000 050\n\\\\\n\\mathrm{U}(-\\$1) &\\approx -0.000 050\n\\end{aligned}\n\\]\nThe expected utility of bet A is approximately \\(-0.14\\), and that of bet B is approximately \\(0\\). Thus the principle of maximal expected utility leads to bet B as the optimal choice! What’s needed are not alternative principles, but a correct utility assessment.\n\n\nContinuous case\nIn some decision-making problems the set of possible decisions can be considered as continuous.\nA power-plant operator, for example, may have to decide to supply an amount of power between 100 MW and 200 MW to a geographical region in the next hour. The unknown “outcome” \\(\\color[RGB]{238,102,119}Y\\) in this scenario may be the power demand, which could be in the same range. In this case we can represent a decision by a statement \\(\\color[RGB]{204,187,68}D\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}d\\), such as\n\\[\\color[RGB]{204,187,68}D\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}175\\,\\mathrm{MW}\\]\n(where “\\(\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\)” obviously stands for “set to”, not for “is observed to be equal to”; recall § 6.3?)\nIn such continuous cases we speak of a utility function\n\\[\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\]\nA typical, extremely abused utility function is the negative squared loss:\n\\[\n\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{sl}})\n= -\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2\n\\]\nstating that the utility decreases as the squared difference between \\(\\color[RGB]{204,187,68}d\\) and \\({\\color[RGB]{238,102,119}y}\\). In concrete problems it is worthwhile to think of more realistic and problem-specific utility functions. In the power-plant scenario, for example, the utility could be worse if the power output is below the power demand, than above. This could be expressed by a function like\n\\[\n\\mathrm{u}({\\color[RGB]{204,187,68}d} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\textrm{plant}})\n=\n\\begin{cases*}\n-2\\,\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2 & if ${\\color[RGB]{204,187,68}d} &lt; {\\color[RGB]{238,102,119}y}$\n\\\\[1ex]\n-\\abs{{\\color[RGB]{204,187,68}d} - {\\color[RGB]{238,102,119}y}}^2 &  if ${\\color[RGB]{204,187,68}d} \\ge {\\color[RGB]{238,102,119}y}$\n\\end{cases*}\n\\]\nor some other asymmetric function.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-whence-utilities",
    "href": "utilities.html#sec-whence-utilities",
    "title": "35  Utilities",
    "section": "35.4 How to determine utilities?",
    "text": "35.4 How to determine utilities?\nIt can be quite difficult to assess the utilities of the decisions and outcomes in a decision-making problems, because of reasons such as heterogeneity or uncertainty, discussed below. Yet, Decision Theory says that any decision is either implicitly using such a number, or is sub-optimal or logically inconsistent. Moreover, the specification, at some level, of utilities not derived by further analysis is simply unavoidable – just like the specification of some initial probabilities.\n\nHeterogeneous factors\nMany heterogeneous factors can enter the determination of utilities. In medical decision-making problems, for example, a clinician must choose one among several possible treatments for a patient, and the utilities of the outcome must take into account factors such as\n\ncost of the treatment\nexpected life length resulting from the treatment\nquality of life resulting from the treatment\npatient’s preferences and attitudes towards life\n\nIt can be very difficult to combine these factors into a single number.\nSeveral fields, such as medicine, have developed and refined several methodologies to arrive at utilities that account for all important factors. Unfortunately we shall not explore any of them in these notes.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nDecisions with Multiple Objectives\nChapter 8 of Sox & al. Medical Decision Making\n\n\n\n\n\nUncertainty\nThe assessment of utilities can also be affected by uncertainties, and therefore become an inference problem in itself. The utility of an outcome may depend on further decisions and further outcomes, whose utilities in turn depend on further decisions and outcomes. Our basic decision framework can in this case be applied repeatedly, as briefly discussed below. In the simplest case, if the agent is uncertain between utility values \\(\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}'\\) with probability \\(p'\\), and utility values \\(\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}''\\) with probability \\(p'' = 1-p'\\), then the utilities to use are the averages\n\\[p'\\,{\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}'} + p''\\,{\\color[RGB]{68,119,170}\\boldsymbol{\\color[RGB]{68,119,170}U}''}\\]\nBut the specification, at some level, of utilities not derived by further inferences is simply unavoidable – just like the specification of some initial probabilities.\n\n\nApproximate assignments\nAn approximate procedure of assigning utilities goes as follows:\n\n\n\n\n\n\n \n\n\n\n\nWrite down all possible consequences\nSort all consequences from best to worst. Ties are possible, that is, there may be several consequences considered to be equally good or equally bad.\nAssign utility \\(\\color[RGB]{68,119,170}1\\) to the best consequence (or to each of the best, if there’s a tie), and utility \\(\\color[RGB]{68,119,170}0\\) to the worst.\nDetermine the utility \\({\\color[RGB]{68,119,170}u}\\) of each intermediate consequence \\(({\\color[RGB]{238,102,119}y}, \\mathsfit{\\color[RGB]{204,187,68}D})\\) as follows:\n\nConsider a lottery where you can win the best consequence (that with utility \\(\\color[RGB]{68,119,170}1\\)) with some probability \\(p^{+}\\), or the worst consequence with probability \\(1-p^{+}\\).\nChoose one value of \\(p^{+}\\in [0,1]\\) such that it would be OK if you were forced to exchange the lottery with the consequence \\(({\\color[RGB]{238,102,119}y},\\mathsfit{\\color[RGB]{204,187,68}D})\\) under consideration, and vice versa, exchange the consequence under consideration with the lottery.\nThat probability value is the utility \\({\\color[RGB]{68,119,170}u}= p^{+}\\) of the consequence under consideration.\n\nCheck whether the utilities determined through lotteries respect your initial sorting. If they don’t, then you have reasoned inconsistently somewhere. Repeat the steps above thinking more thoroughly about your sorting and lotteries.\nAs an additional check, take any three consequences with utilities \\({\\color[RGB]{68,119,170}u}_1 \\ge {\\color[RGB]{68,119,170}u}_2 \\ge {\\color[RGB]{68,119,170}u}_3\\). Consider again a lottery where you can win the first consequence with probability \\(p\\) or the third with probability \\(1-p\\), and choose \\(p\\) so that it would be OK to be forced to exchange this lottery with the second consequence, and vice versa. Then you should find, at least approximately, that\n\\[{\\color[RGB]{68,119,170}u}_2 = p\\cdot {\\color[RGB]{68,119,170}u}_1 + (1-p)\\cdot {\\color[RGB]{68,119,170}u}_3\\]\nIf you don’t, then there are again inconsistencies or irrational biases in your judgements, and you should review them.\n\n\n\nIn the procedure above, the values \\(\\color[RGB]{68,119,170}1\\) and \\(\\color[RGB]{68,119,170}0\\) for the best and worst consequences are arbitrary: they correspond to setting a zero and a measurement unit of your utility scale. You can choose any other pair of values \\({\\color[RGB]{68,119,170}u_{\\textrm{max}}} &gt; {\\color[RGB]{68,119,170}u_{\\textrm{min}}}\\). The procedure applies in the same way, but the utility corresponding to \\(p^{+}\\) is then given by\n\\[{\\color[RGB]{68,119,170}u}= p^{+}\\cdot{\\color[RGB]{68,119,170}u_{\\textrm{max}}} +\n(1-p^{+})\\cdot {\\color[RGB]{68,119,170}u_{\\textrm{min}}}\n\\]\n\n\nThe assessment of initial utilities constitutes a field under active development (see references below), usually called utility elicitation.\n\n\n\n\n\n\n Study reading\n\n\n\n\n§15.3.1 of Artificial Intelligence\nFinal Summary (pp. 234–235) of chapter 8 in Sox & al. Medical Decision Making\n§§ 9.14–9.17 of Lindley: Making Decisions\n§4.2 of Keeney & al. 1976/1993: Decisions with Multiple Objectives\nSkim through Dawes & al. 1985: Attitude and opinion measurement",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-utilities-metric",
    "href": "utilities.html#sec-utilities-metric",
    "title": "35  Utilities",
    "section": "35.5 Utilities as evaluation metric",
    "text": "35.5 Utilities as evaluation metric\nThe utilities of the consequences not only allow the agent to determine the optimal decision, as we shall see in the next chapter. They also allow us to quantify how much utility an agent yielded in a concrete application or sequence of applications of a specific decision-making task.\nSince the possible gains and losses of a specific problem are encoded in the problem-specific utilities \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\), these utilities quantify by definition how much has been gained or lost in solving the problem.\nSuppose that in the first instance of a decision-making task the agent makes decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}_1\\), and in that instance the outcome \\({\\color[RGB]{238,102,119}Y}_1\\) turns out to be \\({\\color[RGB]{238,102,119}y}_1\\) (this may be discovered a long time after the decision was made). The utility (possibly a loss) gained at that instance is then, by definition,\n\\[\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_1 \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}1}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nAssuming that the utilities are additive over instances3, then the total utility yield for instances \\(i=1,2,\\dotsc,M\\) is\n3 if they weren’t, the whole decision-making scheme of the next chapter should be changed, but a similar approach would still apply\\[\n\\sum_{i=1}^{M}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nand the average utility yield per instance is\n\\[\n\\frac{1}{M}\n\\sum_{i=1}^{M}\n\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_i \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\n\nUse in a test set\nThe utility yield can be calculated in a test run of the agent, to check whether its operation meets its specifications and expectations, or even to compare the performance of different agents.\nFor the test run we need a set of data \\(i=1,2,\\dotsc,M\\) (which should come from the same population underlying the real application, see § 23.3) for which the actual outcomes \\({\\color[RGB]{238,102,119}Y_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\color[RGB]{0,0,0}i}}\\) are known, and for which any predictors \\({\\color[RGB]{34,136,51}X_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\color[RGB]{0,0,0}i}}\\) are also available, so that they can be used by the agents under evaluation.\nEach agent is then applied to these data: it is given the predictors \\({\\color[RGB]{34,136,51}X_{\\color[RGB]{0,0,0}i}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\color[RGB]{0,0,0}i}}\\), and from these it will determine and possibly execute the optimal decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}_i\\), for all \\(i\\). The total or average utility yield generated by the agent is then given by the formula above.\nThe test utility yield of an agent can be examined to uncover possible design flaws (say, wrong background information, or programming bugs). The yields of different agents can be compared to decide which is most appropriate to the task.\nWe will return to this use of the utility matrix in chapter  42, when we discuss some evaluation metrics typical of present-day machine-learning methodology.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "utilities.html#sec-indep-axiom",
    "href": "utilities.html#sec-indep-axiom",
    "title": "35  Utilities",
    "section": "35.6 Utilities and probabilities must be independent",
    "text": "35.6 Utilities and probabilities must be independent\nThe independence (or “sure-thing”) axiom of Decision Theory says that the utilities cannot be functions of the probabilities. In other words, an agent cannot assign higher or lower utility to some outcome just because its probability is higher or lower (or vice versa) than the probability of another outcome. The converse also holds: the probability of an outcome cannot be judged higher or lower just because the outcome is more or less desirable (or vice versa). Note that there may be some kind of relation between utilities and probabilities, but only because they refer to the same sentences, not because they are determined by each other’s numerical values.\nA dependence of probabilities on utilities we recognize immediately as “wishful thinking”. But some researchers have from time to time objected that the dependence of utilities on probabilities could be rationally justified, and have proposed alleged counterexamples (usually called “paradoxes”) to prove their objection. The most famous are Allais’s and Ellsberg’s paradoxes.\nExamination of these would-be counterexamples show that they actually contain logical inconsistencies of various kind. Here we want to emphasize one particular kind of mistake: they base utilities on particular aspects of the decision-making problem, but then use the probabilities of different aspects. Let’s show this inconsistency with an extreme example that illustrates it clearly.\nSuppose a person is asked to make a decision between two bets or lotteries \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) and \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\):\n\n\\(\\mathsfit{\\color[RGB]{204,187,68}A}\\): 50% probability of winning or losing nothing, and 50% probability of losing 10 000 $ (or an amount that’s high for the person’s economy)\n\\(\\mathsfit{\\color[RGB]{204,187,68}B}\\): 90% probability of winning 100 $, and 10% probability of winning or losing nothing\n\nBefore applying decision theory to this problem we need to assess which factors affect the utilities of this person. It turns out that this person is a gambler: she only cares about the “thrill & risk” of a consequence, and she doesn’t care about losing money.\nA hasty and naive application of decision theory could represent the problem with the following tree:\n\nand the decision with maximal expected utility would be \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\). But the gambler obviously prefers \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\). A critic of Decision Theory would then say that this happens because, contrary to the axiom of independence, we should allow the utilities to depend on the probabilities, which are more uncertain for \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) than for \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\).\nBut the above application is wrong and illogical. In the representation above, the probabilities refer to the monetary outcome; but we said that the gambler doesn’t care about losing money. If “thrill & risk” is the factor that determines the utilities, then the probabilities should be about that same factor.\nFor the gambler, choosing \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\) leads for certain to a situation with little “thrill” (the winning outcome is almost sure) and no risk (no money will be lost in any case). Choosing \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) instead leads for certain to a situation with high “thrill” (completely uncertain outcome) and high risk (huge money loss). The second situation has higher relative utility than the first. The correct representation of the decision problem is therefore like this:\n\nNote that the utilities do not depend on the probabilities, exactly as required by the independence axiom. The principle of maximal expected utility leads to decision \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\), the gambler’s favourite. Also note that the relevant probabilities are not the ones (about money winnings) mentioned in the initial statement of the problem. Just because we read or hear the word “probability” doesn’t mean that that’s the probability we need.\nThis would-be counterexample therefore vindicates Decision Theory. The problem was not in the axiom of independence, but in the fact that the framework was illogically applied. The “need” to break the axiom of independence to recover the intuitively correct solution (basically correcting an error with another error) was actually a warning sign that some illogical reasoning was taking place.\nIn more realistic situations, both utilities and probabilities must refer to a combination of monetary value and other factors, such as emotional ones. What’s important in any case is that they refer to the same factors. So to speak: if you say that you like oranges and don’t care about apples, then you should worry about how many oranges, not apples, there are.\nThis example, even if somewhat exaggerated, reminds us of two caveats that we have repeated several times in these notes:\n\n\nIt is important to enquire what the exact goals and whys of an engineering or data-science problem really are. Otherwise you may end up wasting a lot of time developing the correct solution to the wrong problem.\nDon’t let yourself be deceived by words and technical terms. Try to understand the essence of the problem that lies beyond its verbal description.\n\n\nIn chapter  42 we shall see that some common evaluation metrics in machine learning actually break the independence axiom, and should therefore be avoided.\n\n\n\n\n\n\n Study reading\n\n\n\n\nA theory of human behavior (pp. 30-37) in chapter 2 of Eells: Rational Decision and Causality\nMorrison 1967: On the consistency of preferences in Allais’ paradox\nSkim through §9 (pp. 80–86) in chapter 4 of Raiffa: Decision Analysis",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>[Utilities]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html",
    "href": "making_decisions.html",
    "title": "36  Making decisions",
    "section": "",
    "text": "36.1 Maximization of expected utility\nIn the previous chapter we associated utilities to consequences, that is, pairs \\((\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y})\\) of decisions and outcomes. We can also associate utilities to the decisions alone – and these are used to determine the optimal decision.\nThe expected utility of a decision \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\) in a basic decision problem is calculated as a weighted average over all possible outcomes, the weighs being the outcomes’ probabilities:\n\\[\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n= \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nAccording to Decision Theory the agent’s final decision is determined by the\nIt may happen that there are several decisions which have equal, maximal expected utility. In this case any one of them can be chosen. A useful strategy is to choose one among them with equal probability. Such strategy helps minimizing the loss from possible small errors in the specification of the utilities, or from the presence of an antagonist agent which tries to predict what our agent is doing.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-max-exp-utilities",
    "href": "making_decisions.html#sec-max-exp-utilities",
    "title": "36  Making decisions",
    "section": "",
    "text": "Principle of maximal expected utility for simple decision problems\n\n\n\n\nThe optimal decision which should be made by the agent is the one having maximal expected utility:\n\\[\n\\mathsfit{\\color[RGB]{204,187,68}D}_{\\text{optimal}} =\n\\operatorname{argmax}\\limits\\limits_{\\mathsfit{\\color[RGB]{204,187,68}D}} \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nwhere, in some tasks, the probabilities may not depend on \\(\\mathsfit{\\color[RGB]{204,187,68}D}\\).\nThis is also called the principle of expected-utility maximization and other similar names.\n\n\n\n\n\nIn the formula above, “\\(\\operatorname{argmax}\\limits\\limits_z G(z)\\)” is the value \\(z^*\\) which maximizes the function \\(G(z)\\). Note the difference:  \\(\\max\\limits_z G(z)\\)  is the value of the maximum itself (its \\(y\\)-coordinate, so to speak), whereas  \\(\\operatorname{argmax}\\limits\\limits_z G(z)\\)  is the value of the argument that gives the maximum (its \\(x\\)-coordinate). For instance\n\\[\\max\\limits_z (1-z)^2 = 0 \\qquad\\text{\\small but}\\qquad \\operatorname{argmax}\\limits\\limits_z (1-z)^2 = 1\\]\n\n\n\n\nNumerical implementation in simple cases\nThe principle of maximal expected utility is straightforward to implement in many important problems.\nIn § 35.3 we represented the set of utilities by a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\). If the probabilities of the outcomes do not depend on the decisions, we represent them as a column matrix \\(\\boldsymbol{\\color[RGB]{34,136,51}P}\\), having one entry per outcome:\n\\[\n\\boldsymbol{\\color[RGB]{34,136,51}P}\\coloneqq\n\\begin{bmatrix}\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y}'' \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\\n\\dotso\n\\end{bmatrix}\n\\]\nThen the collection of expected utilities is a column matrix, having one entry per decision, given by the matrix product \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\boldsymbol{\\color[RGB]{34,136,51}P}\\).\nAll that’s left is to check which of the entries in this final matrix is maximal.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-max-exp-util-example",
    "href": "making_decisions.html#sec-max-exp-util-example",
    "title": "36  Making decisions",
    "section": "36.2 Concrete example: targeted advertisement",
    "text": "36.2 Concrete example: targeted advertisement\nAs a concrete example application of the principle of maximal expected utility, let’s keep on using the adult-income task from chapter  33, in a typical present-day scenario.\nSome corporation, which offers a particular phone app, wants to bombard its users with advertisements, because advertisement generates much more revenue than making the users pay for the app. For each user the corporation can choose one among three ad-types, let’s call them \\({\\color[RGB]{204,187,68}A}, {\\color[RGB]{204,187,68}B}, {\\color[RGB]{204,187,68}C}\\). The revenue obtained from these ad-types depends on whether the target user’s income is \\({\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}\\) or \\({\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}\\). A separate study run by the corporation has shown that the average revenues (per user per minute) depending on the three ad-types and the income levels are as follows:\n\n\n\n\n\n\n\nTable 36.1: Revenue depending on ad-type and income level\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}income}\\)\n\n\n\\({\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}\\)\n\\({\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}\\)\n\n\nad-type\n\\({\\color[RGB]{204,187,68}A}\\)\n\\(\\color[RGB]{68,119,170}-1\\,\\$\\)\n\\(\\color[RGB]{68,119,170}3\\,\\$\\)\n\n\n\\({\\color[RGB]{204,187,68}B}\\)\n\\(\\color[RGB]{68,119,170}2\\,\\$\\)\n\\(\\color[RGB]{68,119,170}2\\,\\$\\)\n\n\n\\({\\color[RGB]{204,187,68}C}\\)\n\\(\\color[RGB]{68,119,170}3\\,\\$\\)\n\\(\\color[RGB]{68,119,170}-1\\,\\$\\)\n\n\n\n\n\n\n\n\n\n\nAd-type \\({\\color[RGB]{204,187,68}B}\\) is a neutral advertisement type that leads to revenue independently of the target user’s income. Ad-type \\({\\color[RGB]{204,187,68}A}\\) targets high-income users, leading to higher revenue from them; but it leads to a loss if shown to the wrong target (more money spent on making and deploying the ad than what is gained from users’ purchases). Vice versa, ad-type \\({\\color[RGB]{204,187,68}B}\\) targets low-income users, with a reverse effect.\nThe corporation doesn’t have access to its users’ income levels, but it covertly collects, through some other app, all or some of the eight predictor variates \\(\\color[RGB]{34,136,51}\\mathit{workclass}\\), \\(\\color[RGB]{34,136,51}\\mathit{education}\\), \\(\\color[RGB]{34,136,51}\\dotsc\\), \\(\\color[RGB]{34,136,51}\\mathit{sex}\\), \\(\\color[RGB]{34,136,51}\\mathit{native\\_country}\\) from each of its users. The corporation has also access to the adult-income dataset (or let’s say a more recent version of it).\nIn this scenario the corporation would like to use an AI agent that can choose and show the optimal ad-type to each user.\n\n\nOur prototype agent from chapters 31, 32, 33 can be used for such a task. It has already been trained with the dataset, and can use any subset (possibly even empty) of the eight predictors to calculate the probability for the two income levels.\nAll that’s left is to equip our prototype agent with a function that outputs the optimal decision, given the calculated probabilities and the set of utilities. In our code this is done by the function decide() described in chapter  32 and reprinted here:\n\n\n decide(probs, utils=NULL)\n\n\nArguments:\n\n\nprobs: a probability distribution for one or more variates.\nutils: a named matrix or array of utilities. The rows of the matrix correspond to the available decisions, the columns or remaining array dimensions correspond to the possible values of the predictand variates.\n\n\nOutput:\n\na list of elements EUs and optimal:\n\nEUs is a vector containing the expected utilities of all decisions, sorted from highest to lowest;\noptimal is the decision having maximal expected utility, or one of them, if more than one, selected with equal probability.\n\n\nNotes:\n\n\nIf utils is missing or NULL, a matrix of the form \\(\\begin{bsmallmatrix}1&0&\\dotso\\\\0&1&\\dotso\\\\\\dotso&\\dotso&\\dotso\\end{bsmallmatrix}\\) is assumed (which corresponds to using accuracy as evaluation metric).\n\n\n\n\n\n\n\nExample\nA new user logs in; all eight predictors are available for this user:\n\n\\[\\color[RGB]{34,136,51}\n\\begin{aligned}\n&\\mathit{workclass} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Private;}\n&& \\mathit{education} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Bachelors;}\n\\\\ & \\mathit{marital\\_status} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Never-married;}\n&& \\mathit{occupation} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Prof-specialty;}\n\\\\ & \\mathit{relationship} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Not-in-family;}\n&& \\mathit{race} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;White;}\n\\\\ & \\mathit{sex} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Female;}\n&& \\mathit{native\\_country} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;United-States;}\n\\end{aligned}\n\\]\n\nThe agent calculates (using the infer() function) the probabilities for the two income levels, which turn out to be\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{238,102,119}\\mathit{\\color[RGB]{238,102,119}income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;&lt;=50K;}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}predictor}, \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}) = 83.3\\%\n\\\\&\\mathrm{P}({\\color[RGB]{238,102,119}\\mathit{\\color[RGB]{238,102,119}income}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;&gt;50K;}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{34,136,51}predictor}, \\mathsfit{\\color[RGB]{34,136,51}data}, \\mathsfit{I}) = 16.7\\%\n\\end{aligned}\n\\]\n\nand can be represented by the column matrix\n\\[\\boldsymbol{\\color[RGB]{34,136,51}P}\\coloneqq\n\\color[RGB]{34,136,51}\\begin{bmatrix}\n0.833\\\\0.167\n\\end{bmatrix}\n\\]\nThe utilities previously given can be represented by the matrix\n\\[\\boldsymbol{\\color[RGB]{68,119,170}U}\\coloneqq\n\\color[RGB]{68,119,170}\\begin{bmatrix}\n-1&3\\\\2&2\\\\3&-1\n\\end{bmatrix}\n\\]\nMultiplying the two matrices above we obtain the expected utilities of the three ad-types for the present user:\n\n\\[\n\\boldsymbol{\\color[RGB]{68,119,170}U}\\boldsymbol{\\color[RGB]{34,136,51}P}=\n\\color[RGB]{68,119,170}\\begin{bmatrix}\n-1&3\\\\2&2\\\\3&-1\n\\end{bmatrix}\n\\,\n\\color[RGB]{34,136,51}\\begin{bmatrix}\n0.833\\\\0.167\n\\end{bmatrix}\\color[RGB]{0,0,0}\n=\n\\begin{bmatrix}\n{\\color[RGB]{68,119,170}-1}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ {\\color[RGB]{68,119,170}3}\\cdot{\\color[RGB]{34,136,51}0.167}\n\\\\\n{\\color[RGB]{68,119,170}2}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ {\\color[RGB]{68,119,170}2}\\cdot{\\color[RGB]{34,136,51}0.167}\n\\\\\n{\\color[RGB]{68,119,170}3}\\cdot{\\color[RGB]{34,136,51}0.833}\n+ ({\\color[RGB]{68,119,170}-1})\\cdot{\\color[RGB]{34,136,51}0.167}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.332\\\\\n2.000\\\\\n\\boldsymbol{2.332}\n\\end{bmatrix}\n\\]\n\nThe highest expected utility is that of ad-type \\({\\color[RGB]{204,187,68}C}\\), which is therefore shown to the user.\n\n\nPowerful flexibility of the optimal predictor machine\nIn the previous chapters we already emphasized and witnessed the flexibility of the optimal predictor machine with regard to the availability of the predictors: it can draw an inference even if some or all predictors are missing.\nNow we can see another powerful kind of flexibility: the optimal predictor machine can in principle use different sets of decisions and different utilities for each new application. The decision criterion is not “hard-coded”; it can be customized on the fly.\nThe possible number of ad-types and the utilities could even be a function of the predictor values. For instance, there could be a set of three ad-types targeting users with \\(\\color[RGB]{34,136,51}\\mathit{education}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Bachelors;}\\), a different set of four ad-types targeting users with \\(\\color[RGB]{34,136,51}\\mathit{education}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;Preschool;}\\), and so on.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-unpred-decisions",
    "href": "making_decisions.html#sec-unpred-decisions",
    "title": "36  Making decisions",
    "section": "36.3 “Unsystematic” decisions",
    "text": "36.3 “Unsystematic” decisions\nIn § 35.6 we emphasized the need to give appropriate utilities to outcomes, and to think about utilities in more general terms or with a more “open mind”, not simply equating them to monetary value, but considering all factors involved.\nA similar open mind must be kept in determining the set of available decisions, considering the circumstances and also considering the decisions made by other agents. This leads to the idea of unsystematic decisions, more often called randomized decisions in the literature. Let’s illustrate the idea with an example.\nPolice Dispatch has received intel that a band of robbers is going to rob either location \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) or location \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\). Dispatch can send a patrol to either of these places, but not both, owing to momentarily limited patrolling resources. The police patrol awaits orders from Dispatch on the radio.\nDispatch assigns a utility of, say, \\(\\color[RGB]{68,119,170}+1\\) if the robbery is diverted and possibly the robbers caught, and a utility of \\(\\color[RGB]{68,119,170}0\\) otherwise. As things stand, dispatching the patrol to either location has a 50% probability of catching the robbers, so each location decision has an expected utility of \\(\\color[RGB]{68,119,170}0.5\\). But there’s a catch.\nDispatch has also received intel that the robbers are managing to intercept the radio transmission to the police patrol. So if Dispatch instructs the patrol to “go to location \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\)”, the robbers will know that and rob location \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\) instead. And vice versa if Dispatch sends the patrol to \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\). The updated decision problem has now an expected utility of \\(0\\) for either choice.\nWhat can Dispatch do? Well there’s at least one more possible decision. Dispatch can radio the patrol and instruct: “Toss a coin; if tails, go to \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\); if heads, go to \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\)”. If Dispatch does so, then the robbers, even intercepting the radio instruction, can’t to predict for certain where the police patrol will go. Should Dispatch make this third decision?\n\n\n\n\n\n\nExercise 36.1\n\n\n\nDraw the diagram for the basic decision problem above, with all three decisions, the outcomes of each, and the corresponding utilities and probabilities.\n\n\nThe third decision, “Toss a coin…” has an expected utility of \\(0.5\\), whereas the other two, “Go to \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\)” or “Go to \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\)”, have each \\(0\\) expected utility. The third decision is optimal among these three.\nThe crucial feature of the third decision is that it has an element of unpredictability for the robbers; it also has one for Dispatch, but that’s a side-effect.\n\nThis idea extends to more complex situations where an agent has an available sequence of decisions, the outcomes of each depending in turn on the decisions of another, antagonist agent. Such a sequence is usually called a strategy. The antagonist agent can in this case manipulate to some degree the outcomes, and alter its own strategy to minimize the first agent’s expected utilities and maximize its own. The first agent can remedy this by introducing a new set of decisions whose outcomes are instead unpredictable to the antagonist (and possibly to itself).\n\n\n\n\n\n\n The final decision is not unsystematic or unpredictable\n\n\n\nNote that even when we apply such ideas, we are still nevertheless following the principle of maximal expected utility. We haven’t changed or contradicted that principle in any way.\nAnd we are not making decisions “at random”. What we’ve done is simply to consider a different set of decisions; the final choice is never “random”.\nTo see this, it’s important to make a clear separation between decisions on one side, and objects or results coming from those decisions on the other side. We often make a hasty but incorrect identification of the two. In the police example, there are two locations, \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\) and \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\), involved. But a location is not a decision – “location \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\)” is not even a sentence (recall ch.  6). The initial possible decisions of Dispatch are, properly speaking,\n#1. “Say ‘go to location \\(\\mathsfit{\\color[RGB]{204,187,68}A}\\)’ on the radio”.\n#2. “Say ‘go to location \\(\\mathsfit{\\color[RGB]{204,187,68}B}\\)’ on the radio”.\nTo which then Dispatch adds one more possible decision:\n#3. “Say ‘toss a coin; …’ on the radio”.\nDispatch determines that this latter decision #3 has higher expected utility than decisions #1 and #2, and therefore follows it. It isn’t making a decision “at random” among the three possibilities. The final location becomes unpredictable, to Dispatch and to the robbers, but the decision is not unpredictable: it’s #3, determined by maximization of expected utility.\nUnfortunately many texts oversimplify this idea, and seem to imply that in some cases it’s best to make decisions “at random”, in apparent contradiction with the principle of maximal expected utility. As we have seen, this is not the case.\n\nIt’s also important to emphasize that the unpredictability is desired on the antagonist agent’s side, not on the side of the agent making the decision.\n\n\n\n\nIn a decision-making problem, several choices may happen to have equal, maximal expected utilities. In such situations it is usually recommended to “choose randomly” among those choices; this is what the function decide() (§ 36.2) does. This recommendation is an application of the ideas just discussed.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-decision-trees",
    "href": "making_decisions.html#sec-decision-trees",
    "title": "36  Making decisions",
    "section": "36.4 Multiple decisions: decision trees",
    "text": "36.4 Multiple decisions: decision trees\n\nCharacteristics and graphical representation of a decision tree\nIn § 2.5 we discussed one important feature of Decision Theory: it can be applied in a sequential, recursive, or modular way. The typical case for such application is when further decisions are available if a specific outcome occurs. It would take several chapters to develop this more general application in full; here we summarize its main features and method.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\n§15.5 and chapter 16 of Artificial Intelligence\nChapter 2 of Decision Analysis\n\n\n\nConsider a set of decisions and outcomes, which depend on the occurrence of other outcomes and decisions, which in turn may depend on the occurrence of yet more outcomes and decisions, and so on. Graphically this situation can be represented by a decision tree, which depicts the sequences of decisions and outcomes in tree-like fashion. Here is an example:\n\n\n\nAs usual, decision nodes are represented by squares, and inference nodes by circles. The root decision node is on the left. It presents a number of decisions which we can denote\n\\[\\mathsfit{\\color[RGB]{204,187,68}D}_{1,1},\\quad \\mathsfit{\\color[RGB]{204,187,68}D}_{1,2},\\quad\\dotsc,\\quad\\mathsfit{\\color[RGB]{204,187,68}D}_{1,\\dotsc},\\quad\\dotsc\\]\nThe first index  “\\({}_{1,}\\)”  indicates that these are root decisions; the second index enumerates these decisions. Each decision has a set of possible outcomes which we can denote, all together,\n\\[\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1},\\quad \\mathsfit{\\color[RGB]{34,136,51}O}_{1,2},\\quad\\dotsc,\\quad\\mathsfit{\\color[RGB]{34,136,51}O}_{1,\\dotsc},\\quad\\dotsc\\]\nwith the same indexing convention. Each outcome has an associated degree of belief, which generally depends on the outcome and on the respective decision. For instance, the probability for outcome \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1}\\) must be written\n\\[\\mathrm{P}(\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}_{1,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s background information.\n\nSo far this looks almost like a basic decision tree (§ 3.1). Why “almost”? because we don’t associate utilities to these outcomes. Not yet. Instead, we see that some outcomes, like \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1}\\), open new decision nodes, with associated outcomes. We can keep the same notation convention as for the root decision node, but now using  “\\({}_{2,}\\)”  as first index:\n\\[\\mathsfit{\\color[RGB]{204,187,68}D}_{2,1},\\quad \\mathsfit{\\color[RGB]{204,187,68}D}_{2,2},\\quad\\dotsc,\\quad\\mathsfit{\\color[RGB]{204,187,68}D}_{2,\\dotsc},\\quad\\dotsc\\]\nand for the their outcomes as well:\n\\[\\mathsfit{\\color[RGB]{34,136,51}O}_{2,1},\\quad \\mathsfit{\\color[RGB]{34,136,51}O}_{2,2},\\quad\\dotsc,\\quad\\mathsfit{\\color[RGB]{34,136,51}O}_{2,\\dotsc},\\quad\\dotsc\\]\nNote that this is just a notation we use here; you can use any other convenient notation you like.\nEach outcome of the second level has an associated degree of belief. Note that the degree of belief in an outcome generally depends on all previous decisions and outcomes leading to that outcome. For instance, the probability for outcome \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{2,2}\\) must be written\n\\[\\mathrm{P}(\\mathsfit{\\color[RGB]{34,136,51}O}_{2,2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}_{2,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{204,187,68}D}_{1,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\n\nWhat about utilities? In this particular decision tree, the outcomes \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{2,1}\\) and \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{2,2}\\) do not lead to any subsequent possible decisions: they are among the tips of the tree. Then we do associate utilities to them: utilities are only associated to all outcomes at the tips of the decision tree. And it’s important to note that the utility associated to an outcome generally depends on all previous decisions and outcomes leading to that outcome. For instance, the utility associated to outcome \\(\\mathsfit{\\color[RGB]{34,136,51}O}_{2,2}\\) must be written\n\\[\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}_{1,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}O}_{1,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{204,187,68}D}_{2,1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}O}_{2,2} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\]\n\nLet’s summarize these important features of a general decision tree:\n\n\n\n\n\n\nCharacteristics of a decision tree\n\n\n\n\nSome outcomes lead to further decision nodes with associated outcomes.\nDegrees of belief are associated to each outcome. The degree of belief in an outcome generally depends on all previous decisions and outcomes leading to that outcome.\nUtilities are associated to outcomes at the tips (not to intermediate ones). The utility associated to an outcome generally depends on all previous decisions and outcomes leading to that outcome.\nAt each level of the tree, it may happen that some outcomes lead to further decisions, while other outcomes are tips.\n\n\n\n\n\nDecision making with a decision tree\nNow to the important question: how do we choose the optimal decision at each decision node? The procedure, which is quite intuitive once one understands its general principle, is called averaging out and folding back or backwards induction. It starts from the tips, and step-wise associates a utility to each decision and each outcome, moving from the tips to the root of the decision tree:\n\n\n\n\n\n\nBackwards induction\n\n\n\n\nTake all outcomes at the **tips* which originate from the same inference node (circle).\nAt that inference node, calculate the expected utility, using the probabilities and utilities associated with the tip outcomes.\nNote that this inference node stems from a particular decision. The calculated expected utility is associated to that decision.\nRepeat the previous step until all tip outcomes have been exhausted.\n \nTake all decisions which have an associated expected utility (at the corresponding inference node), and which originate from the same decision node (square).\nAt that decision node, calculate the maximal expected utility among the expected utilities of the decisions originating at that node.\nNote that this decision node stems from a particular outcome. The calculated maximal expected utility is associated to that outcome.\nRepeat the previous step until all outcomes closest to tip outcomes have been exhausted.\n \n\nNow we have that some outcomes which initially didn’t have any associated utility, because they were not tips of the decision tree, have obtained an associated utility.\nStarting from these, repeat the four steps above, and so on recursively. In the end you arrive at the association of expected utilities to all decisions originating from the root decision node.\n\n\n A concrete example will be added \n\n\n\n\n\n\n Misunderstandings and misuses of backwards induction\n\n\n\nIt’s important to be aware of some characteristics and consequences of backwards induction. Misunderstanding of these is often the origin of misapplications and misunderstandings of this basic method of Decision Theory.\n\nOutcome utilities may depend on outcome frequencies\n\nWe have seen that the utility of a final outcome depends on the whole sequence of decisions and outcomes leading to it. This means that such utility may depend on the frequencies with which particular outcomes occur in that sequence. These are objectively observable frequencies, not degrees of beliefs, so there is no contradiction with the basic principle that the utility of an outcome cannot depend on the probability for that outcome.\nSome people confuse or simplify a sequential decision problem with a basic decision, trying to interpret the frequencies of the sequence as probabilities in the basic decision. Then they declare that the basic principle of Decision Theory – that the utility of an outcome cannot depend on the degree of belief on that outcome – is broken. Their erroneous conclusion comes from a double error: (1) they confuse a sequential problem with a basic one; (2) they confuse frequencies with degrees of belief.\n\nUtilities need not be additive\n\nWe have seen that utilities are associated only to the outcomes at the tip of the decision tree. So the utility is a property of the whole history leading to that outcome, so to speak. In some situations this history may include outcomes that are similar, in some sense, to the final outcome. For example, in a problem of 100 sequential coin bets and tosses, the final outcome might be heads, preceded by a sequence of 44 heads and 55 tails.\nIn such situations, some people associate a utility to heads and one to tails, and then consider the utility of a sequence leading to 45 heads and 55 tails to be the sum of 45 heads-utilities and 55 tails-utilities. But Decision Theory does not say that the final utility should be calculated this way. In fact, Decision Theory does not even associate any utility to intermediate outcomes; only to the whole history. The summing procedure above is only a special assumption, which may be valid or reasonable in some cases, but unreasonable and invalid in others.\n\n\n\n\nSo be careful in your decision analyses; do not take shortcuts or make erroneous simplifications!",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "making_decisions.html#sec-DT-generalization",
    "href": "making_decisions.html#sec-DT-generalization",
    "title": "36  Making decisions",
    "section": "36.5 The full extent of Decision Theory",
    "text": "36.5 The full extent of Decision Theory\nThe simple decision-making problems and framework that we have discussed in these notes are only the basic blocks of Decision Theory. This theory covers more complicated decision problems. We only mention some examples:\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nFull accounts of Decision Theory can for example be found in:\n\nDecision Analysis\nIntroduction to Statistical Decision Theory\n\n\n\n\n\nSequential decisions. Many decision-making problems involve sequences of possible decisions, alternating with sequences of possible outcomes. These sequences can be represented as decision trees. Decision theory allows us to find the optimal decision sequence for instance through the “averaging out and folding back” procedure.\n\n\n\n\nUncertain utilities. It is possible to recast Decision Theory and the principle of maximal expected utility in terms, not of utility functions \\(\\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), but of probability distributions over utility values:\n\\[\\mathrm{P}({\\color[RGB]{68,119,170}U\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}u} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\\]\nFormally the two approaches can be shown to be equivalent.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapter 8 of Rational Descriptions, Decisions and Designs\nBox 11.1 (p. 351) of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\n\n\n\n\n\nAcquiring more information. In many situations the agent has one more possible choice: to gather more information in order to calculate sharper probabilities, rather than deciding immediately. This kind of decision is also accounted for by Decision Theory, and constitutes one of the theoretical bases of “reinforcement learning”.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\n§15.6 of Artificial Intelligence\n§11.5 of Risk Assessment and Decision Analysis with Bayesian Networks\n\n\n\n\n\n\n\nMulti-agent problems. To some extent it is possible to consider situations (such as games) with several agents having different and even opposing utilities. This area of Decision Theory is apparently still under development.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapter 17 of Artificial Intelligence",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>[Making decisions]{.blue}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html",
    "href": "example_opm2.html",
    "title": "37  The prototype Optimal Predictor Machine makes decisions",
    "section": "",
    "text": "37.1 Initialization and build of OPM agent\nIt is straightforward to implement decision-making in our prototype Optimal Predictor Machine. Let’s continue with the example from chapter  33.\nLoad the necessary libraries and functions, including the decide() function, and train the agent as we did previously:\nsource('tplotfunctions.R')\nsource('OPM_nominal.R')\n\nset.seed(100)\n\nopmAll &lt;- buildagent(\n    metadata = 'meta_income_data_example.csv',\n    data = 'train-income_data_example.csv'\n)",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#decision-matrix",
    "href": "example_opm2.html#decision-matrix",
    "title": "37  The prototype Optimal Predictor Machine makes decisions",
    "section": "37.2 Decision matrix",
    "text": "37.2 Decision matrix\nWe use the targeted-advertisement scenario of § 36.2, with the following utility matrix for the three ad-types:\n\nadUtilities &lt;- matrix(\n    c(-1, 3,\n        2, 2,\n        3,-1),\n    nrow = 3, byrow = TRUE,\n    dimnames = list(ad_type = c('A','B','C'),\n        income = c('&lt;=50K', '&gt;50K')))\n\nprint(adUtilities)\n\n       income\nad_type &lt;=50K &gt;50K\n      A    -1    3\n      B     2    2\n      C     3   -1",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#example-application",
    "href": "example_opm2.html#example-application",
    "title": "37  The prototype Optimal Predictor Machine makes decisions",
    "section": "37.3 Example application",
    "text": "37.3 Example application\nFirst let’s apply the principle of maximal expected utility step-by-step.\nConsider the example from § 36.2. The agent calculates the probabilities for the predictand income from the given predictor values:\n\nuserpredictors &lt;- list(\n    workclass = 'Private',\n    education = 'Bachelors',\n    marital_status = 'Never-married',\n    occupation = 'Prof-specialty',\n    relationship = 'Not-in-family',\n    race = 'White',\n    sex = 'Female',\n    native_country = 'United-States'\n)\n\nprobs &lt;- infer(agent = opmAll,\n    predictand = 'income',\n    predictor = userpredictors)\n\nprint(probs)\n\nincome\n   &lt;=50K     &gt;50K \n0.833333 0.166667 \n\n\nFind the expected utilities of the three possible ad-types by matrix multiplication:\n\nadUtilities %*% probs\n\n       \nad_type     [,1]\n      A -0.33333\n      B  2.00000\n      C  2.33333\n\n\nAnd we see that ad-type C is optimal.\n\n\nThe function decide() does the previous calculations. It outputs a list with elements:\n\nEUs: the expected utilities of the decisions, sorted from highest to lowest\noptimal: one decision unsystematically chosen among the optimal ones (if more than one)\n\n\noptimalAd &lt;- decide(utils = adUtilities, probs = probs)\n\nprint(optimalAd)\n\n$EUs\n       C        B        A \n 2.33333  2.00000 -0.33333 \n\n$optimal\n[1] \"C\"",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "example_opm2.html#performance-on-test-set",
    "href": "example_opm2.html#performance-on-test-set",
    "title": "37  The prototype Optimal Predictor Machine makes decisions",
    "section": "37.4 Performance on test set",
    "text": "37.4 Performance on test set\nFinally let’s apply our prototype agent to a test set, as a demonstration, and see how much utility it yields. This procedure will be discussed in more detail in § 43.\nLoad the test dataset; ntest is the number of test data:\n\ntestdata &lt;- read.csv('test-income_data_example.csv',\n    header = TRUE, na.strings = '',\n    stringsAsFactors = FALSE, tryLogical = FALSE)\n\nntest &lt;- nrow(testdata)\n\nWe build the analogous of a “confusion matrix” (§ 43), telling us how many times the agent chooses the three ad-types for both income levels.\n\nconfusionmatrix &lt;- adUtilities * 0\n\n## Use a for-loop for clarity\nfor(i in 1:ntest){\n    userpredictors &lt;- testdata[i, colnames(testdata) != 'income']\n\n    probs &lt;- infer(agent = opmAll,\n        predictand = 'income',\n        predictor = userpredictors)\n\n    decision &lt;- decide(utils = adUtilities, probs = probs)$optimal\n\n    trueincome &lt;- testdata[i, 'income']\n\n    confusionmatrix[decision, trueincome] &lt;-\n        confusionmatrix[decision, trueincome] + 1\n}\n\nprint(confusionmatrix)\n\n       income\nad_type &lt;=50K &gt;50K\n      A   769 2149\n      B 11768 5093\n      C 12961 1174\n\n\nThe total utility yield is the total sum of the element-wise product of the confusionmatrix and the adUtilities matrix\n\ntotalYield &lt;- sum(adUtilities * confusionmatrix)\n\naverageYield &lt;- totalYield/ntest\n\ncat('\\nTotal yield =', totalYield,\n    '\\nAverage yield =', averageYield,\n    '\\n')\n\n\nTotal yield = 77109 \nAverage yield = 2.27366 \n\n\nNote that:\n\nThis yield is higher than what would be obtained by just choosing the neutral ad-type B for all test units (the average yield would be exactly 2).\nThis yield is also higher than would be obtained by always choosing ad-type C, targeting the majority of units, which have income = '&lt;=50K'. This strategy would yield 2.00737.\n\n\n\n\n\n\n\nExercise 37.1\n\n\n\n\nTry to use some common machine-learning algorithm to perform the same task of choosing between the three ad-types. Is it difficult? why?\nIf you manage to do this, then compare the performances of the machine-learning algorithm and the opmAll agent.\nConstruct a scenario where the utility matrix is different depending on the sex predictor variate. Write a script to apply the opmAll agent on the test set according to this new scenario.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>[The prototype Optimal Predictor Machine makes decisions]{.red}</span>"
    ]
  },
  {
    "objectID": "slm2.html",
    "href": "slm2.html",
    "title": "38  The Optimal Predictor Machine generates text",
    "section": "",
    "text": "38.1 Agents that generate text: what’s the underlying decision-making problem?\nIn § 34.1 we saw that present-day Large Language Model determine their belief in what the next word or token should be, only based on the sequence of words seen so far, and on the frequencies of such sequences in a huge collection of texts. These beliefs are not determined on possible future outcomes of their choice of words, as instead is the case in human conversation. We managed to use our Optimal Predictor Machine in the same way in a simplified setting.\nLarge Language Models do not output probabilities, however: they output words. So at every step they are effectively choosing one of the possible words about which they calculated their beliefs. This is decision, and to be optimal and self-consistent it should be based on some outcomes and their utilities.\nWhat are the outcomes and utilities underlying word-choice in Large Language Models? This is still an open question. The approaches followed so far in the literature have been based more on intuition and on “playing around” rather than framing the problem in a systematic way. This means that there’s a lot of room for an AI engineer to bring forth a better understanding and major improvements.\nLet’s consider our OPM agent used as a “small language model” in § 34.2. It was only calculating degrees of belief about the \\(n\\)th token of a string of \\(n\\) tokens. First of all let’s ask again: beliefs about what? We could say: the belief that this \\(n\\)th token is the correct one, in this particular sequence. Keep in mind that this is a suspicious point of view; can we really say that there’s a “correct” token?\nIf we want our OPM agent to also choose one of the possible tokens, we need to find the appropriate set of outcomes and their utilities for this decision problem.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The Optimal Predictor Machine generates text]{.red}</span>"
    ]
  },
  {
    "objectID": "slm2.html#sec-what-decision-llm",
    "href": "slm2.html#sec-what-decision-llm",
    "title": "38  The Optimal Predictor Machine generates text",
    "section": "",
    "text": "Exercise 38.1\n\n\n\nOn your own or in group, think about the problem above.\n\nWhat kind of meaningful outcomes are there in this problem?\nCan they be easily assessed? Do they depend on the choice of present token alone, or in future ones as well?\nWhat are the utility values for the outcomes? How to assess them?\nCan we find alternative points of view about outcomes and utilities – points of view that do not reflect natural language but may make sense somehow in the present context?",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The Optimal Predictor Machine generates text]{.red}</span>"
    ]
  },
  {
    "objectID": "slm2.html#sec-tentative-decision-llm",
    "href": "slm2.html#sec-tentative-decision-llm",
    "title": "38  The Optimal Predictor Machine generates text",
    "section": "38.2 A tentative decision-making point of view",
    "text": "38.2 A tentative decision-making point of view\nA tentative and somewhat vague point of view is the following: the agent should generate, on the long run, text that “looks natural”. Now if we assign a given utility, say \\(+1\\), to the “correct” choice of token, the agent should at each step choose the token that always has highest probability. This, however, eventually leads to a circular repetition, as soon as a string of \\(n-1\\) token appears again. Let’s see an example of this phenomenon with our OPM agent.\nLet’s build again an agent that does inference about 3-grams from the United Nation’s Universal Declaration of Human Rights:\n\n## Load main functions\nsource('tplotfunctions.R')\nsource('OPM_nominal.R')\nsource('SLMutilities.R')\n\n## Set seed for reproducibility\nset.seed(700)\n\n## Prepare metadata and training data\nngramfiles &lt;- preparengramfiles(\n    inputfile = 'texts/human_rights.txt',\n    outsuffix = 'rights',\n    n = 3,\n    maxtokens = Inf,\n    numbersasx = TRUE\n)\n\nUnique tokens:  506.\n\n\nData:  1904  3-grams.\n\n\nFiles saved.\n\n## Create the OPM agent\nopmSLM &lt;- buildagent(\n    metadata = ngramfiles$metadata,\n    data = ngramfiles$data,\n    savememory = TRUE\n)\n\nNow let’s operate the agent as follows:\n\nWe give an initial prompt of two tokens: ARTICLE, x.\nWe let the agent calculate the degrees of belief about the next token.\nThe next token is chosen as the one having highest probability. This corresponds to a decision process with a unit utility matrix: correct token choices have utility \\(+1\\); and wrong choices, \\(0\\).\nThe first token is discarded, the second becomes first, and the last token generated becomes the second.\nRepeat from 1.\n\n\n## Starting tokens\nword1 &lt;- 'ARTICLE'\nword2 &lt;- 'x'\n\n## Convenience variable to wrap text\ntextlength &lt;- 0\n\nfor(i in 1:100){\n    ## Print the starting tokens if first iteration\n    if(i == 1){ cat('\\n', word1, word2, '') }\n\n    ## Calculate beliefs about next token\n    probs &lt;- infer(agent = opmSLM,\n        predictor = list(word1 = word1, word2 = word2),\n        predictand = 'word3')\n\n    ## Decision: next token\n    ## is the one with max prob.\n    word3 &lt;- names(which.max(probs))\n\n    ## Wrap text if it'll get too long\n    textlength &lt;- textlength + nchar(word3) + 1\n    if(textlength &gt; 50){\n        cat('\\n')\n        textlength &lt;- 0\n    }\n\n    ## Print chosen token\n    cat(word3, '')\n\n    ## Use last two tokens for new prediction\n    word1 &lt;- word2\n    word2 &lt;- word3\n}\n\n\n ARTICLE x EVERYONE HAS THE RIGHT TO FREEDOM OF OPINION AND \nEXPRESSION ; THIS RIGHT INCLUDES FREEDOM TO CHANGE HIS \nRELIGION OR BELIEF , AND THE RIGHT TO FREEDOM OF OPINION \nAND EXPRESSION ; THIS RIGHT INCLUDES FREEDOM TO \nCHANGE HIS RELIGION OR BELIEF , AND THE RIGHT TO FREEDOM \nOF OPINION AND EXPRESSION ; THIS RIGHT INCLUDES \nFREEDOM TO CHANGE HIS RELIGION OR BELIEF , AND THE RIGHT \nTO FREEDOM OF OPINION AND EXPRESSION ; THIS RIGHT \nINCLUDES FREEDOM TO CHANGE HIS RELIGION OR BELIEF , AND \nTHE RIGHT TO FREEDOM OF OPINION AND EXPRESSION ; THIS \nRIGHT INCLUDES FREEDOM TO \n\n\nAs you see, the agent started looping at “THE RIGHT”. The text doesn’t look natural from this point of view. The set of decisions and of utilities we chose for the agent are not appropriate.\n\nLet’s try to define a little more precisely what we mean by “look natural”.\n\nOne condition is the absence of text loops like the one above. The text should be to us somewhat unpredictable.\nAlso, the frequencies with which 3-grams appear in the output text should reflect those of natural language – like the frequencies in the texts used to train the agent.\n\nThese conditions suggest that we use an “unpredictable decision”, in the sense discussed in § 36.3.\nLet’s introduce the following decision:\n“Unpredictably select and output the next token according to its belief”.\nIn the long run, making this decision will produce text that is unpredictable, and also whose 3-gram frequencies reflect those of the training texts, because the agent bases its beliefs on those (though beliefs and frequencies are not exactly equal). Therefore it seems that, in the long run, this decision will lead to the highest utility.\n\nThe function generatetext() in the file SLMutilities.R applies the heuristic strategy above and outputs the resulting text. We saw an example output in § 34.5. Let’s see another example output:\n\ntext &lt;- generatetext(\n    agent = opmSLM,\n    prompt = c('ARTICLE', 'x'),\n    stopat = 70,\n    online = FALSE\n)\n\nwrapprint(text, wrapat = 50)\n\n ARTICLE x EVERYONE HAS THE RIGHT TO JUST AND\n FAVOURABLE CONDITIONS OF WORK AND TO SHARE IN SCIENTIFIC\n ADVANCEMENT AND ITS BENEFITS. EVERYONE HAS THE RIGHT TO\n FREEDOM OF MOVEMENT AND RESIDENCE WITHIN THE BORDERS OF\n EACH STATE. EVERYONE HAS THE RIGHT TO BE PRESUMED\n INNOCENT UNTIL PROVED GUILTY ACCORDING TO LAW IN A\n DEMOCRATIC SOCIETY. THESE RIGHTS AND FREEDOMS, EVERYONE\n SHALL BE GIVEN TO THEIR ...\n\n\n\n\n\n\n\n\n We’re approximating\n\n\n\nThe discussion above is not a rigorous application of the principle of maximal expected utility. We did not list properly the decisions, or the outcomes, or the utility values. The reasoning was purely heuristic and intuitive.\nFor this reason it is quite possible that the strategy adopted is not optimal.\n\n\n\n\n\n\n\n\nExercise 38.2\n\n\n\n\nTry to formalize our heuristic reasoning above and to derive the strategy rigorously, from the principle of maximal expected utility. (This is not an easy task, but why not try?)\nReverse-engineer the function generatetext() and verify that it indeed implements the decision discussed above.\nTry generating text with the OPM agent using other prompts. You should notice that sometimes the output text suddenly becomes gibberish, or is gibberish right from the start. It is also possible that gibberish output suddenly becomes more coherent again.\nCan you explain why these transitions from between sensible and insensible text occur?\nUse other text to train the OPM agent and explore its output. You can prepare texts yourself, or use the ones in the directory code/texts. In size order:\n\nhuman_rights.txt: Universal Declaration of Human Rights.\nEUparliament.txt: sample of European Parliament Proceedings.\nsherlock_holmes.txt: The Complete Sherlock Holmes.\nABC.txt: Australian Broadcasting Commission.",
    "crumbs": [
      "[**Decision-making**]{.blue}",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>[The Optimal Predictor Machine generates text]{.red}</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html",
    "href": "machine_learning_overview.html",
    "title": "39  Introduction to machine learning",
    "section": "",
    "text": "39.1 Hyperparameters and model complexity\nSo far we have learned about the principles behind drawing inferences and making decisions, and about various properties of data. Using only the four fundamental probability rules that drive the “thinking” of a perfect AI agent, we arrived at a couple of formulae that underlie many different kinds of inference, prediction, and data-generation tasks, in situations where the agent believe the data to be exchangeable (ch.  25). One example is\nWe saw that the agent needs to have a built-in belief about frequencies, \\(\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\).  Once it’s given some learning data, it can in principle draw all sorts of inferences and make all sorts of decisions. Concrete applications were given in chapters 33, 34, 37.\nWe shall now discuss about some present-day approximate procedures to draw inferences about data. They are approximate because they draw inferences or make decisions by doing approximate probability calculations and approximate maximizations of expected utilities, rather than exact ones. This approximating character makes these procedures very fast, but also sub-optimal. In any given problem it is therefore important to decide whether our priority is speed or optimality.\nMachine learning methods can be wildly different both in terms of operation and complexity. But they all behave as functions that take in a data point \\(x\\), a set of values \\(\\mathbf{w}\\) which we call parameters, and returns the prediction \\(\\hat{y}\\):\n\\[\n    \\hat{y} = f(x, \\mathbf{w}).\n\\tag{39.2}\\]\nThe prediction \\(\\hat{y}\\) can be any quantity discussed in section 12.2 – most commonly we want to classify the data (and we say we are doing classification), or we want to estimate some value (in which case we call it regression).\nIt has been pointed out already – and we will see it also in the coming chapters – that often there is actually not a functional relationship between the inputs \\(\\mathbf{x}\\), and the property we want to predict. This leaves us with two possibilities for a solution:\nBecause of the second point above, when we describe equation 39.2, we will use the word “function” in the wide sense where we allow it to be non-deterministic. An example is ChatGPT, which will answer you differently if you ask it the same exact question twice. If you prefer more mathematical rigour, we could call \\(f\\) a process instead, which is a more loose term.\nLet us break down equation 39.2. As engineers, when faced with the task of using data to model the behaviour of some system, our job is twofold. First, we need to select a suitable method to serve as the function \\(f\\), and second, we need to find the optimal values for the parameters \\(\\mathbf{w}\\). In ADA501 we will see how to choose an analytical \\(f\\) that corresponds to certain physical systems, while in our course, we will look at methods where \\(f\\) can be practically anything.\nFinding the parameters \\(\\mathbf{w}\\) is what takes us from a general method, to the specific solution to a problem. As we will see, the way of finding them differs between the various machine learning methods, but the principle is always the same: We need to choose a loss function, and then iteratively adjust \\(\\mathbf{w}\\) so that the loss, when computed on known data, becomes as small as possible. The loss function should represent the difference between what the model outputs, and the correct answer – the better the model, the smaller the difference. The choice of this loss function depends on what kind of problem we wish to solve, and we will look at the common ones shortly. But at this point we can already define the three main types of machine learning:\nHaving decided on a method to represent \\(f\\) and found a set of parameters \\(\\mathbf{w}\\), we say that these combined now constitute our model. The model should have internalised the important correlations in the data and thereby allows us to make predictions, i.e. do modelling. If we do decision-making based on the output of the model as well, we typically call in an agent, since there is now some level of autonomy involved. In this chapter, however, we will stick to modelling problems in a supervised fashion.\nYou may have heard the quote by statistician George Box:\nAlthough coming off as a somewhat negative view on things, the quote still captures an important point about statistical modelling – our goal is not to make a complete, 100% accurate description of reality, but rather a simplified description that is meaningful in the context of our task at hand. The “correct” level of simplicity, in other words the optimal number of parameters \\(\\mathbf{w}\\), can be hard to find. Often it will be influenced by practical considerations such as time and computing power, but it is always governed by the amount of data we have available. We will look at the theory of model selection later, but let us first consider a visual example, from which we can define some important concepts.\nImagine that you don’t have a thermometer that shows the outside temperature. Never knowing whether you should wear a jacket or not when leaving the house, you get a great idea: If you can construct a mathematical model for the outside temperature as a function of the time of day, then a look at the clock would be enough to decide for or against bringing a jacket. You manage to borrow a thermometer for a day, and make ten measurements at different times, which will form the basis for the model:\nThe next step is to choose a function \\(f\\). For one-dimensional data like this, we could for instance select among the group of polynomials, of the form\n\\[\n    f (x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M = \\sum_{j=0}^{M} w_j x^j \\,,\n\\]\nwhere \\(M\\) is the order of the polynomial. Recall that a zero’th order polymonial is just a constant value, so such a model would be represented with one single parameter. A first-order polynomial is a linear function (two parameters), and the higher in order (and number of parameters) we go, the more “curves” the function can have. This already presents us with a problem. Which order polynomial (i.e. which value of \\(M\\)) should we choose? Let us try different ones, and for each case, fit the parameters, meaning we find the parameter values that yield the smallest difference from the measured data points:\nObviously, the constant function does a poor job of describing our data, and likewise for the linear function. A fourth-order polynomial, on the other hand, looks very reasonable. Now consider the ninth-order polynomial: it matches the measurements perfectly, but surely, this does not match our expectation for what the temperature should look like.\nWe say that the first and second model underfit the data. This can happen for two reasons: Either the model has too little complexity to be able to describe the data (which is the case in this example), or, potentially, the optimal parameter values have not been found. The opposite case is overfitting, as shown for the last model, where the complexity is too high and the model adapts to artifacts in the data.\nThis concept is also called the bias-variance tradeoff. We will not go into too much detail on this yet, but qualitatively, we can say that bias (used in this setting) is the difference between the predicted value and the true value, when averaging over different data sets. Variance (again when used in this setting) indicates how big the change in parameters, and hence in model predictions, we get from fitting to different data sets. Let us say you measure the temperature on ten different days, and for each day, you fit a model, like before. These may be the results:\nThe blue dots are our “original” data points, plotted for reference, while the red lines corresponds to models fitted for each day’s measurements. Due to fluctuations in the measurements, they are different, but note how the difference is related to the model complexity. Our linear models (\\(M=1\\)) are all quite similar, but neither capture the pattern in the data very well, so they all have high bias. The overly complex models (\\(M=9\\)) have zero bias on their repective dataset, but high variance. The optimal choice is likely somewhere in-between (hence the tradeoff), as for the \\(M=4\\) models, which perform well without being overly sensitive to fluctuations in data. Since the value of \\(M\\) is chosen by us, we call it a hyperparameter, to separate it from the regular parameters which are optimised by minimising the loss function.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "href": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "title": "39  Introduction to machine learning",
    "section": "",
    "text": "All models are wrong, but some are useful.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 39.1: Temperature measurements over the course of 24 hours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zeroth-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n(b) First-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Fourth-order polymonial\n\n\n\n\n\n\n\n\n\n\n\n(d) Ninth-order polymonial\n\n\n\n\n\n\n\nFigure 39.2: Fitting polymonials (red lines) of different orders to the set of measurements (blue circles).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Large bias, low variance\n\n\n\n\n\n\n\n\n\n\n\n(b) Resonably low bias and variance\n\n\n\n\n\n\n\n\n\n\n\n(c) Low bias, large variance\n\n\n\n\n\n\n\nFigure 39.3: Red lines: Different models fitted to separate datasets, where each dataset is drawn from the same distribution as the original one. The original dataset shown in blue circles.\n\n\n\n\n\n\n\n\n\n\n\nExercise 39.1\n\n\n\n\nOur simple temperature model relies on a series of assumptions, some which might be good, and some which might be (very) bad. State the ones you can think of, and evaluate if they are sensible. Hints: are polynomials a good choice for \\(f\\)? Is the data representative?\nFor the different models in figure 39.2 we started by choosing polymonial degree \\(M\\), and then computed the parameters that minimized the difference between the data points and the model. Then we had a look at the results, compared them to our expectations, and decided that \\(M=0\\) and \\(M=9\\) were both unlikely. Can you think of a way to incorporate our initial expectations into the computation?",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#model-selection-in-practice",
    "href": "machine_learning_overview.html#model-selection-in-practice",
    "title": "39  Introduction to machine learning",
    "section": "39.2 Model selection (in practice)",
    "text": "39.2 Model selection (in practice)\nAs alluded to in the exercises above, there are ways of including both the data and our prior expectations when building a model, but it is, in fact, not very common to do so. In this section we will have a look at the typical machine learning approach, which relies on splitting the data in different sets. Starting from all our available data on a given problem that we wish to model, we divide it into\n\nthe training set, which is the data we will use to determine the model parameters \\(\\mathbf{w}\\),\nthe validation set, which is used to evaluate the model complexity (i.e. finding the optimal bias-variance tradeoff), and\nthe test set, which is used to evaluate the final performance of the model.\n\nThe benefit of this approach is that it is very simple to do. The downside, on the other hand, is that each set is necessarily smaller than the total, which subjects us to increased statistical uncertainty. The final parameters will be slightly less optimal than they could have been, and the measurement of the performance will be slightly less accurate. Still, it is common practice, and for the “standard” machine learning methods there is no direct way of simultaneously optimising for the model complexity.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#sec-why-fitting-issues",
    "href": "machine_learning_overview.html#sec-why-fitting-issues",
    "title": "39  Introduction to machine learning",
    "section": "39.3 Overfitting and underfitting: why? where from?",
    "text": "39.3 Overfitting and underfitting: why? where from?\nUp to now we have never met the problems of overfitting and underfitting. The OPM agent that we applied in chapters 33, 34, 37 did not suffer from this kind of problems. In fact, in § 33.5 we saw that the agent trained with just 10 datapoints did not try to fit them perfectly, but kept more conservative beliefs. The agent also warned us that further learning data might substantially change its beliefs. When a substantial amount of learning data was given to the agent, it aligned its beliefs with the observed frequencies, which was a reasonable thing to do. No issues about over- or under-fitting appeared at any stage; we did not even think about them.\nWhy do the machine-learning models above suffer from these issues instead? Where do these issues originate from?\nThe origins are various and may differ from one machine-learning algorithm to another. In the cases above there are two main culprits:\n\na particular approximation underlying the algorithms;\nunreasonable prior beliefs built into the algorithms.\n\nLet’s explain each in broad terms.\n\nThe “max instead of sum” approximation\nA machine-learning algorithm like one of the above is an agent that tries to predict the \\(\\mathit{temp}\\)erature from the \\(\\mathit{time}\\) by assuming that there’s a function from time to temperature; this could be a reasonable assumption. In loose probability notation we could write it like this:\n\\[\n\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\nwhere \\(F\\) represents the assumed function.\nBut the agent doesn’t know what the true function \\(F\\) is: it could be the function \\(F_{1}\\)  (say, a line like \\(\\mathit{temp} = A \\cdot \\mathit{time} + B\\)), or \\(F_{2}\\)  (say, a sine like \\(\\mathit{temp} = A \\cdot \\sin(B \\cdot \\mathit{time}) + C\\)), or an infinity of other possibilities. Each function is a hypothesis that the agent is entertaining. Therefore what the agent is trying to calculate is actually something like this:\n\\[\n\\mathrm{P}\\bigl(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n(F_{1} \\lor F_{2} \\lor \\dotsb)\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\]\nA perfect agent, reasoning according to the four fundamental rules, would in this case apply the law of extension of the conversation (§ 9.5), with a sum over the possible function hypotheses:1\n1 For simplicity we assume that \\(\\mathit{time}\\) alone is irrelevant for the belief about \\(F\\).\n\\[\n\\mathrm{P}\\bigl(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n(F_{1} \\lor F_{2} \\lor \\dotsb)\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr) =\n\\sum_i\n\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F_{i} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr) \\cdot\n\\mathrm{P}(F_{i} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr)\n\\]\n\nThis law in fact underlies formula 39.1, which we used for our OPM agent. In that formula the sum over hypotheses appears as “\\(\\sum_{\\boldsymbol{f}}\\)”, with the difference that the hypotheses in that case were frequencies.\nThe sum over hypotheses can be computationally expensive, and the machine-learning agents above approximate it with something simpler: they search for, and keep only the largest term in the sum, rather than performing the sum:\n\n\\[\n\\sum_i\n\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F_{i} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr) \\cdot\n\\mathrm{P}(F_{i} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr)\n\\mathrel{\\color[RGB]{238,102,119}\\pmb{\\approx}}\n\\max_i\\Bigl\\{\n\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F_{i} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr) \\cdot\n\\mathrm{P}(F_{i} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr)\n\\Bigr\\}\n\\]\n\nYou will see the appearance of this maximization, disguised as a minimization (of minus the logarithms of the sum terms), in the next machine-learning chapters.\nIn a manner of speaking we can say that a perfect agent makes a prediction by appropriately balancing all unknown alternatives, whereas the approximate machine-learning agent only considers the most “attention-calling” alternative. This is actually a form of cognitive bias, related to the so-called anchoring and availability biases.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSearch for “anchoring” and “availability heuristic” in Hastie & Dawes: Rational Choice in an Uncertain World\n\n\nTo make things worse, the factor \\(\\mathrm{P}(F_{i} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\), which should express prior information about the possible hypothesis \\(F_{i}\\), is often poorly chosen or not chosen at all, and approximately the same for all hypotheses. So the maximum is effectively determined only by the factor \\(\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F_{i} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\\), which expresses how well hypothesis \\(F_{i}\\) fits the existing data. This leads to overfitting.\nOne way to mitigate this problem is by not using the maximum among the sum terms, but a somewhat smaller term than the maximum. But how much smaller should it be? In order to evaluate that, a validation dataset is usually introduced, to get an idea at how badly the approximate agent predicts new data while trying to overfit the old data. This is also an approximate make-do solution, however, which still does not lead to the optimal result.\nAnother way to mitigate the problem is by providing a non-constant prior-belief factor \\(\\mathrm{P}(F_{i} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\); this is what regularization procedures do. However, these procedures do not choose a value based on prior information; the choice is based on blind, trial-and-error attempts to reduce the influence of the data-fit term \\(\\mathrm{P}(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}F_{i} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\\). The result is again not optimal.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMacKay: A practical Bayesian framework for backpropagation networks\n\n\n\n\nThese are the reasons why a perfect agent, such as the OPM of the previous chapters, does not need a validation dataset or regularization procedures: it does not approximate the required sum over hypotheses by a maximization, and it uses meaningful prior information.\n\n\nUnreasonably restrictive built-in beliefs\nFor a reasonable prediction it is of course important that the actual function \\(F\\) is actually among the hypotheses built into the agent. No amount of learning data will increase the agent’s belief in a hypothesis that it isn’t even considering:\n\\[\n\\mathrm{P}\\bigl(\\mathit{temp} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{time} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n(\\underbracket{\\color[RGB]{0,0,0}F_{1} \\lor F_{2} \\lor \\dotsb}_{\\color[RGB]{119,119,119}\\text{true $F$ missing!}})\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I}\\bigr)\n\\]\nNo matter how many learning data are used, such an agent will keep on making poor predictions. This leads to underfitting.\nRestricting an agent’s set of beliefs in this way is often used to mitigate the “max instead of sum” approximation. We can put this idea in a funny way: since the approximating agent is not balancing all alternatives but only choosing the most attention-calling one, let’s at least give it unreasonable alternatives to maximize from.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_overview.html#machine-learning-taxonomy",
    "href": "machine_learning_overview.html#machine-learning-taxonomy",
    "title": "39  Introduction to machine learning",
    "section": "39.4 Machine learning taxonomy",
    "text": "39.4 Machine learning taxonomy\nMachine learning has been around for many decades, and the list of methods that fit under our simple definition of looking like equation 39.2 and learning their parameters from data, is very long. For a (non-exhaustive) systematic list, have a look at the methods that are implemented in scikit-learn, a Python library dedicated to data analysis.\nIn this course we will not try to go through all of them, but rather focus on the fundamentals of what they all have in common. With this fundamental understanding, learning about new methods is like learning a new programming language – each has their specific syntax and specific uses, but the underlying mechanism is the same. We will look at two important methods, that are inherently very different, but still accomplish the same end result. The first is decision trees (and the extension into random forests), while the second is neural networks.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Introduction to machine learning</span>"
    ]
  },
  {
    "objectID": "decision_trees.html",
    "href": "decision_trees.html",
    "title": "40  Machine-learning decision trees",
    "section": "",
    "text": "40.1 Decision trees for classification on categorical values\n“Decision trees” is the name given simple and intuitive machine learning models, which can be expanded and elaborated into a surprisingly powerful and general method for both classification and regression. They are based on the idea of performing a series of yes-or-no tests on the data, which finally lead to decision. Note that they are not the same as the “decision trees” of § 36.4.\nWe have already seen the tree-like structure for illustrating decision problems in Chapter 3, but in this section, we are doing similar yet different things. The name of “decision trees” still pertains to both, but it should hopefully be clear from context when we are talking about what.\nFor a trivial example, lets us say you want to go and spend the day at the beach. There are certain criteria that should be fullfilled for that to be a good idea, and some of them depend on each other. A decision tree could look like this:\nG\n\n\n\nA\n\n  Go to the beach?  \n\n\n\nB\n\nWorkday?\n\n\n\n\nC\n\nDon't go\n\n\n\nB-&gt;C\n\n\nYes\n\n\n\nD\n\nSunny?\n\n\n\nB-&gt;D\n\n\nNo\n\n\n\nE\n\nTemp &gt; 18ºC\n\n\n\nD-&gt;E\n\n\nYes\n\n\n\nF\n\nTemp &gt; 23ºC\n\n\n\nD-&gt;F\n\n\nNo\n\n\n\nG\n\n Go to the beach \n\n\n\nE-&gt;G\n\n\nYes\n\n\n\nH\n\n Don't go \n\n\n\nE-&gt;H\n\n\nNo\n\n\n\nI\n\n Go to the beach \n\n\n\nF-&gt;I\n\n\nYes\n\n\n\nJ\n\n Don't go \n\n\n\nF-&gt;J\n\n\nNo\n\n\n\n\n\n\nFigure 40.1: A very simple decision tree.\nDepending on input data such as the weather, we end up following a certain path from the root node, along the branches, down to the leaf node, which returns the final decision for these given observations. The botany analogies are not strictly necessary, but at least we see where the name decision tree comes from.\nStudying the above tree structure more closely, we see that there are several possible ways of structuring it, that would lead to the same outcome. We can choose to first split on the Sunny node, and split on Workday afterwards. Drawing it out on paper, however, would show that this structure needs a larger total number of nodes, since we always need to split on Workday. Hence, the most efficient tree is the one that steps through the observables in order of descending importance.\nThe basic algorithm for buiding a decision tree (or growing it, if you prefer) on categorical data, can be written out quite compactly. Consider the following pseudo-code:\nThis is the original ID3 algorithm [@quinlan1986induction]. Note how it works recursively – for each new feature, the function calls itself to build a subtree.\nWe start by creating a node, which becomes a leaf node either if it classifies all examples correctly (no reason to split), or if there are no more features left (not possible to split). Otherwise, we find the most important feature by calling Importance(examples), and proceed to make all possible splits. Now, the magic happens in the Importance function. How can we quantify which feature is best to discriminate on? We have in sections Section 23.1 and Section 18.5 met a useful definition from information theory, which is the Shannon entropy:\n\\[\n    H(f) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\\ \\mathrm{Sh}\n    \\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nwhere the \\(f_i\\) are frequency distributions. If we stick to the simple example of our target features being “yes” or “no”, we can write out the summation like so:\n\\[\n    H = -(f_{\\mathrm{yes}} \\log_2 f_{\\mathrm{yes}} + f_{\\mathrm{no}} \\log_2 f_{\\mathrm{no}})\\ \\mathrm{Sh}\n\\]\nLet us compute the entropy for two different cases, to see how it works. In the first case, we have 10 examples: 6 corresponding to “yes”, and four corresponding to “no”. The entropy is then\n\\[\n    H(6\\;\\text{yes}, 4\\;\\text{no}) = -[(6/10) \\log_2 (6/10) + (4/10) \\log_2 (4/10)]\\ \\mathrm{Sh} = 0.97\\ \\mathrm{Sh}\n\\]\nIn the second case, we still have 10 examples, but nearly all of the same class: 9 examples are “yes”, and 1 is “no”:\n\\[\n    H(9\\;\\text{yes}, 1\\;\\text{no}) = - [(9/10) \\log_2 (9/10) + (1/10) \\log_2 (1/10)]\\ \\mathrm{Sh} = 0.47\\ \\mathrm{Sh}\n\\]\nInterpreting the entropy as a measure of impurity in the set of examples, we can guess (or compute, using \\(0\\cdot \\log_2 0 = 0\\)) that the lowest possible entropy occurs for a set where all are of the same class. When doing classification, this is of course what we aim for – separating all examples into those corresponding to “yes” and those corresponding to “no”. A way of selecting the most important feature is then to choose the one where we expect the highest reduction in entropy, caused by splitting on this feature. This is called the information gain, and is generally defined as\n\\[\n    Gain(A, S) \\coloneqq H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) \\,,\n\\tag{40.1}\\]\nwhere \\(A\\) is the feature under consideration, \\(Values(A)\\) are all the possible values that \\(A\\) can have. Further, \\(S\\) is the set of examples, and \\(S_v\\) is the subset containing examples where \\(A\\) has the value \\(v\\). Looking again at the binary yes/no case, it looks a little simpler. Using the feature Sunny as \\(A\\), we get:\n\\[\n    Gain(\\texttt{Sunny}, S) = H(S) - \\left(\\frac{S_{\\texttt{Sunny=yes}}}{S} H(S_{\\texttt{Sunny=yes}}) + \\frac{S_{\\texttt{Sunny=no}}}{S} H(S_{\\texttt{Sunny=no}})\\right) \\,.\n\\]\nThis equation can be read as “gain equals the original entropy before splitting on Sunny, minus the weighted entropy after splitting”, which is what we were after. One thing to note about equation 40.1: while it allows for splitting on an arbitrary number of values, we typically always want to split in two, resulting in binary trees. Non-binary trees tend to quickly overfit, which why few of the successors to the ID3 algorithm allow this. The extreme case would be if a feature is continuous instead of categorical. For a continuous feature it is unlikely that the data will contain values that are identical – probably many values are similar, but not identical to e.g. ten digits precision. ID3 would potentially split such a feature into as many branches as there are data points, which is maximal overfitting. Binary trees can of course overfit too (we will get back to this shortly), but first, let us introduce a similar algorithm that can deal with both continuous inputs, and continuous output.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Machine-learning decision trees</span>"
    ]
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-classification-on-categorical-values",
    "href": "decision_trees.html#decision-trees-for-classification-on-categorical-values",
    "title": "40  Machine-learning decision trees",
    "section": "",
    "text": "function BuildTree(examples, target_feature, features)\n  \n  # examples is the training data\n  # target_feature is the feature we want to predict\n  # features is the list of features present in the data\n  \n  tree ← a single node, so far without any label\n  if all examples are of the same classification then\n    give tree a label equal to the classification\n    return tree\n  else if features is empty then\n    give tree a label equal the most common value of target_feature in examples\n    return tree\n  else\n    best_feature ← the feature from features with highest Importance(examples)\n\n    for each value v of best_feature do\n      examples_v ← the subset of examples where best_feature has the value v\n      subtree ← BuildTree(examples_v, target_feature, features - {best_feature})\n      add a branch with label v to tree, and below it, add the tree subtree\n    \n    return tree\n\n\n\n\nThe same algorithm is shown and explained in section 19.3 in Russell and Norvig, although they fail to specify that this is ID3.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Machine-learning decision trees</span>"
    ]
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-regression-in-addition-to-classification",
    "href": "decision_trees.html#decision-trees-for-regression-in-addition-to-classification",
    "title": "40  Machine-learning decision trees",
    "section": "40.2 Decision trees for regression (in addition to classification)",
    "text": "40.2 Decision trees for regression (in addition to classification)\nThe problem of continuous features can be solved by requiring only binary splits, and then searching for the optimal threshold value for where to split. Each node will then ask “is the value of feature \\(A\\) larger or smaller than the threshold \\(x\\)”? Finding the best threshold involves going through all the values in data and computing the expected information gain. One could initially think that we need to consider all possible values that the data could take, but luckily we need only to consider the values the data does take, since the expected information gain has discrete steps for value where we move a data point from one branch to the other.\nA second thing we would like to solve, is to not only have categorical outputs (i.e. do classification), but also continuous values (i.e. do regression). Looking again at the pseudocode for the ID3 algorithm, we see that once we have “used up” all the features, the label assigned to the final branch will be the majority label among the remaining examples. For continuous target values, the fix is relatively simple – we instead just take the average of the values in the examples. These two solutions form the basis for the CART (Classification and Regression Trees) algorithm, which creates decision trees for any kind of inputs and outputs.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Machine-learning decision trees</span>"
    ]
  },
  {
    "objectID": "decision_trees.html#preventing-overfitting",
    "href": "decision_trees.html#preventing-overfitting",
    "title": "40  Machine-learning decision trees",
    "section": "40.3 Preventing overfitting",
    "text": "40.3 Preventing overfitting\nAgain from the pseudocode of the ID3 algorithm, we see that the basic rules for building a tree will keep making splits until we have perfect classification, or until no more features are available. Perfect classification surely sounds good, but is in practice rarely attainable, and these rules will typically create too many splits and thereby overfit to the training data.\n\nPruning and hyperparameter choices\nThe common approach to avoid this is in fact to just let it happen – and then afterwards, go in and remove the branches that give the least improvement in prediction. Sticking with the biologically inspired jargon, we call this pruning. We will not go into the details of this, but leave it as an illustration of how well-defined machine learning methods often need improvised heuristics to work. Other tweaks that are used in parallel include\n\nrequiring a minimum number of training events in each end node, and\nenforcing a maximum depth, i.e. allowing only a certain number of subsequent splits.\n\n\n\nRandom forests\nTraining several decision trees on similar data tends to end up looking like figure 39.3 (c): they are sensitive to small variations in the data and hence have high variance. But while each individual tree can be far off the target, the average is still good, since the variations often cancel out. This can be the case for many types of machine learning methods. We can improve performance by defining a new ensemble model \\(f(\\mathbf{x})\\) composed of several separate models \\(f_m(\\mathbf{x})\\),\n\\[\nf(\\mathbf{x}) = \\sum_{m=1}^M \\frac{1}{M} f_m(\\mathbf{x}) \\,,\n\\]\nwhere the \\(f_m\\) are trained on a randomly selected subset of the total data. Necessarily, the \\(f_m\\) models will be highly correlated, so one can also train them on randomly selected subsets of features, in order to reduce this correlation. In the case of decision trees, the ensemble is (obviously) called a random forest.\n\n\nBoosting\nA final trick for decision trees, which is boosting. This is an important method that all high-performing implementations of random forests use, but it also relies on a lot of tedious math, so we will mostly gloss over it. The point is that we can create the ensemble iteratively by adding new trees one at a time, where each new addition tries to improve on the prediction by the existing trees.\nStarting with a single tree \\(f_1\\), its predictions might be good, but not perfect. So when evaluated on the training data \\(\\mathbf{x}\\), there will be a difference between the targets \\(\\mathbf{y}\\) and the predictions \\(f(\\mathbf{x})\\), and this difference we typically call residuals \\(r\\):\n\\[\n\\mathbf{r}_1 = \\mathbf{y} - f_1(\\mathbf{x})\n\\]\nor if we re-write:\n\\[\n\\mathbf{y} = f_1(\\mathbf{x}) + \\mathbf{r}_1\n\\]\nWe want \\(\\mathbf{r}\\) to be as small as possible, but with a single tree, there is only so much we can do. The magic is to train a second tree \\(f_2\\), not to again predict \\(y\\), but to predict the residual \\(r_1\\). Then we get\n\\[\n\\mathbf{y} = f_1(\\mathbf{x}) + f_2(\\mathbf{x}) + \\mathbf{r}_2\n\\]\nwith \\(r_2 &lt; r_1\\) (hopefully), resulting in an improved prediction. This stagewise additive modelling can be done until we see no further improvement, resulting in an emsemble model that typically outperforms a standard random forest approach. Several variants of the boosting algorithm exists, some of which are discussed in chapter 16.4 of the Murphy book. Here, the explanation of the different boosting variants are based on arguments about loss functions. We have not started looking into loss funtions yet, but this will be a topic in the next chapter. It can be useful to come back to the Murphy book after having gone through next chapter’s material.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Machine-learning decision trees</span>"
    ]
  },
  {
    "objectID": "decision_trees.html#software-frameworks",
    "href": "decision_trees.html#software-frameworks",
    "title": "40  Machine-learning decision trees",
    "section": "40.4 Software frameworks",
    "text": "40.4 Software frameworks\nWe are about to start the programming exercises, so let us briefly discuss our options for the implementation. While coding a decision tree algorithm from scratch is not too technically challenging, it is an ineffective way to learn aboubt the its behaviour in practice, and we also do not have the time to so. Hence we will use ready-made frameworks.\nFor decent-performing and easy-to-use frameworks, we suggest to use scikit-learn for Python, and tidymodels for R. There are, however, plenty of alternatives, and you are free to choose whichever you like.\nFor state-of-the-art boosted decision trees, we suggest XGBoost, which regularly ranks among the best tree-based algorithms on the machine learning competitions at Kaggle, and is available for both Python and R.\n\n\n\n\n\n\nExercise 40.1\n\n\n\n: In these exercises we will start out with simple 2-dimensional data, just so we can visualise what is going on, and then move to real-world dataset afterwards.\n\nClassification\nLet us try to classify samples from two different populations, which we simply call positive class and negative class. Follow the code examples in this notebook: decision_tree_examples.ipynb. The exercises themselves are listed in the notebook, but are included here too:\n\nRecreate the data with either very big separation between the classes, or very small, and observe how the decision surface changes.\nIn the documentation for DecisionTreeClassifier, lots of options are described. You’ll noticed that we have already specified a non-default criterion. Try changing the other parameters, and again observe how the results differ. In particular, try setting max_depth and min_samples_split to small or big values.\nGenerate some separate test data, and plot those too. Does the default decision tree parameters give good results on the test data? Can you find better parameters to improve the class prediction for this example?\nFinally, print out the splitting thresholds and the leaf contents for the entire tree. Does it match your expectation from looking at the decision boundary?\n\n\n\nRegression\nNow, we want to try out decision trees for predicting continuous target values. We will leave you alone from the start, and only give you the recipe for generating the 1-dimensional data we want to predict:\n\nrng = np.random.default_rng()   # If not done already\nX = np.sort(5 * rng.uniform(size=(80, 1)), axis=0)\nY = np.sin(X).ravel()\nY[::5] += 3 * (0.5 - rng.uniform(size=16))\n\nThe execise is similar to the classification one:\n\nUsing DecisionTreeRegressor, vary the values of its parameters and observe the result.\nGenerate test data following the prescription above, and find optimal parameters that account for the fact that our data is prone to outliers.\n\n\n\nReal-world dataset classification\nNow that we understand how the tree-based models work, it is time to use them on an actual, higher-dimensional dataset. We will use the Adult Income dataset, which you have met already, to predict the binary outcome of people making more than $50,000 a year, depending on their education, line of work, and so on.\nThe training data are here: https://github.com/pglpm/ADA511/raw/master/datasets/train-income_data_nominal_nomissing.csv\nAs measure of how good the model’s predictions are, use the ratio of correct predictions (correct predictions divided by the number of examples), which is also known as accuracy.\nOnce you have trained your model and computed the accuracy on training data, compute the accuracy also for test data: https://github.com/pglpm/ADA511/raw/master/datasets/test-income_data_nominal_nomissing.csv\nWhich performs better – the DecisionTreeClassifier, the RandomForestClassifier or the GradientBoostingClassifier?\nOptional: To optimise the performance as far as possible, try one of the “latest” tree-based algoritm implementations, such as XGBoost, LightGBM, or TensorFlow Decision Forests.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Machine-learning decision trees</span>"
    ]
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "41  Neural networks",
    "section": "",
    "text": "41.1 Training neural networks\nNeural networks are performing extremely well on complex tasks such as language modelling and realistic image generation, although the principle behind how they work, is quite simple.\nThe basic building block is a node, which receives some values as input, and computes a single output:\nThe output is computed from the inputs \\(x_0, x_1, \\dots, x_N\\), each of which is multiplied by a weight \\(w_1, w_2, \\dots w_N\\), and summed together along with an additional parameter \\(b\\), which is typically called a bias. You can probably identify this step as good old linear regression:\n\\[\na = b + \\sum_{i=0}^N w_i x_i \\,.\n\\]\nA key property to neural networks, however, is to introduce nonlinear relationships. This is done by evaluating the output from each node by an activation function \\(h\\). The final output \\(z\\) from the node is then\n\\[\nz = h(a) = h \\left( b + \\sum_{i=0}^N w_i x_i \\right)\n\\]\nThe activation function is typically rather simple – the most popular is the rectified linear unit (ReLU), which propagates positive values but sets all negative values to zero:\n\\[\n\\text{ReLU}(x) = \\max(0, x) \\,.\n\\]\nVarious other possibilities for choice of activation function exists, as we will explore in the exercises later.\nArmed with our simple node, let us assemble several of them into a network. Starting with, say, four nodes, all the input data will be used by each of them: (todo) mention layers\nNeural networks are great for several reasons. They can be arranged to work with practically any type of data, including unstructured data such as images or text, which is not neatly organised into a table of explicit feature values. Granted, this flexibility does not appear by (FIGURE) alone, but is due to clever additions to the network structure that is beyond the scope of our lectures. A more theoretical argument for neural networks comes from the universal approximation theorem, which states that\nThis is a very powerful property, which is explained in understandable terms here (the proof (CITE) is rather technical). But, as we know already, this only helps if we are trying to model something which has a functional relationship.\nFinding the optimal values of the model’s parameters \\(\\mathbf{w}\\) is usually called to train the model. When we looked at polynomials in the machine learning introduction (Chapter 39) we did not talk about this yet, partly because polynomial models have a closed-form solution for the best parameters, meaning they can be computed directly. Since neural networks are nonlinear by design, analytic solutions are not available and we need a numerical approach; for instance: start with unsystematically chosen parameter values, then iteratively try to improve them.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#training-neural-networks",
    "href": "neural_networks.html#training-neural-networks",
    "title": "41  Neural networks",
    "section": "",
    "text": "Loss functions\nThe first step towards improving the parameters, is to define what improvement is. Ultimately, our goal is to make predictions about the data that equal the true values, which is to say, we want to minimise the difference between predictions \\(f(\\pmb{x}, \\pmb{w},  \\text{true values }y)\\). This difference can be formulated in several different ways, but in the case of regression, the most common is the sum of squared errors:\n\\[\nL(\\mathbf{w}) = \\sum_{\\text{data points } i} (f(\\mathbf{x}_i, \\mathbf{w}) - y_i)^2\n\\]\n\\(L\\) is called a loss function, alternatively a cost function or an error function. With this in place, the training process becomes a minimisation problem:\n\\[\n\\underset{\\mathbf{w}}{\\mathrm{arg\\,min}}\\, L(\\mathbf{w}) \\,.\n\\tag{41.1}\\]\nTo minimise the loss function we still need to apply it to some data \\(\\mathbf{x}\\), but we have not made this dependence explicit, since the data are “unchanged” throughout the minimisation process.\nAs for any bounded function, the minimum can be found either where the gradient \\(\\nabla L(\\mathbf{w})\\) is zero, or where it does not exist. Solving this analytically is usually impossible, so we resort to a numerical solution – iteratively taking steps in the direction of smaller loss. The crucial point in neural network training is that the loss function is differentiable with respect to the network parameters, meaning we can compute \\(\\nabla L(\\mathbf{w})\\) and take steps in the negative (downwards) direction:\n\\[\n    \\mathbf{w}^{n+1} = \\mathbf{w}^{n} - \\eta \\nabla L(\\mathbf{w}^{n}) \\,.\n\\tag{41.2}\\]\nThis is the method of gradient descent. Here we have introduced a new hyperparameter \\(\\eta\\) called the learning rate, which controls how large each step will be.\nThe process of actually adjusting the parameters in the correct direction is called backpropagation, and involves first computing the value of \\(f(\\mathbf{x}, \\mathbf{w})\\), and then stepping backwards through each layer of the network, recursively updating the parameters by using the derivative. This sounds very tedious, but can be done efficiently by automatic differentiation, i.e. letting a computer do it. Modern frameworks for neural network models require only to know the layout of the network, and will, as we shall see in the exercises, figure out the rest automatically.\n\n\nGradient descent\nWhile we did not get into it earlier, the concept of defining a loss function and doing gradient descent, is in fact how the majority of machine learning algorithms are trained. Even for decision trees, which had their dedicated algorithm, we ended up with a loss minimisation task once we introduced boosting.\nStraight-forward gradient descent as shown in equation {41.2} can work fine for relatively simple models, but will stop at the first minimum it encounters. For a reasonably complicated network, the loss function landscape can be expected to have several local minima or saddle points, causing the method to get stuck in places with suboptimal parameters. Several improved algorithms aim to tackle this.\n\nStochastic gradient descent updates the parameters through equation {41.2} for only a subset of data at a time. This is more computationally efficient, and the stochastic element helps against getting stuck in a local minimum, since a local minimum for some subset of data might not be a minimum for a different subset.\nAdaptive gradient methods use different learning rates per parameter, which is updated for each iteration.\nMomentum methods remember the previous gradients and keeps moving in the same direction even through flat or uphill parts, like a massive rolling ball.\n\nAll of the above can be combined, and the most common method of doing so is the Adaptive Moment Estimation (Adam) algorithm.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html",
    "href": "limitations_ML.html",
    "title": "42  Decisions: limitations of present-day machine-learning algorithms",
    "section": "",
    "text": "42.1 The omnipresence of decision-making in data science and machine learning\nMany machine-learning textbooks say that\nSuch statement is misleading, because it suggests that there is a functional relationship from input to output, or from predictor to predictand – a functional relationship that’s only waiting to be discovered and “learned”. But as we discussed in chapter  39, in many important tasks and applications this is actually not true: there isn’t any functional relationship between input and output at all.1 Even if any possible “noise” were removed, there would still not be any functional relationship between the denoised predictor and predictand (here is an example from image classification).\nIn many important tasks there’s only a statistical relationship between predictands and predictors. “Statistical” means that whenever a predictor has value \\(\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\\), the predictand value may turn out to be \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y'\\) in some units, but also \\(\\color[RGB]{34,136,51}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y''\\) in some other units, and so on. As a simple example, consider the Norwegian population in 2022. If our predictand is \\(\\mathit{sex}\\) and we take as predictor that a person’s \\(\\mathit{age}\\) is between \\(85\\)–\\(89\\) years, then a proportion \\(43 542/(43 542 + 28 220) \\approx 61\\%\\) of those persons have \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;female;}\\), and the remaining \\(39\\%\\) proportion has \\(\\mathit{sex}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\small\\verb;male;}\\). This doesn’t mean that, say, \\({\\small\\verb;female;}\\) is the “true” output, and \\({\\small\\verb;male;}\\) is just the effect of noise, or vice versa. That would be nonsense.\nEven in tasks where there actually is a functional relation from predictors to predictands, the agent typically doesn’t know what is the function’s output for particular predictor values, because no such values have been observed in the training data. It must interpolate or extrapolate what it has learned. Also in this case there are several possibilities to choose from.\nA decision-making step also appears in tasks where the agent must generate a new unit. Obviously there are many candidates for generation, which agree with what was observed in the training data. If the agent generates one unit, then it must internally have chosen among the possible candidates.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-decision-everywhere",
    "href": "limitations_ML.html#sec-decision-everywhere",
    "title": "42  Decisions: limitations of present-day machine-learning algorithms",
    "section": "",
    "text": "in supervised learning the algorithm learns a functional relationship between some kind of input and some kind of output\n\n\n1 This is one more reason why we use the more general terms “predictor” & “predictand”, rather that “input” & “output”.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-where-prob-util",
    "href": "limitations_ML.html#sec-where-prob-util",
    "title": "42  Decisions: limitations of present-day machine-learning algorithms",
    "section": "42.2 Where are the probabilities and the utilities? How are they calculated?",
    "text": "42.2 Where are the probabilities and the utilities? How are they calculated?\nThe remarks above have a very important consequence. If a machine-learning algorithm outputs just one predictand value (or generates one unit), among the possible ones that are consistent with the predictor, then it means that the algorithm is internally choosing one of the possibilities.\nSuch a choice is obviously a decision-making problem. We know that the optimal, logically consistent choice must be determined by Decision Theory, and its determination requires:\n\n  Some kind of background knowledge\n  the probabilities of the possible predictand values\n  a list of possible decisions\n  the utilities of the decisions, depending on the predictand’s true value\n\nThe algorithm internally must – at least approximately – be calculating probabilities and maximizing expected utilities. In principle its internal workings should be susceptible to an explanation or interpretation from this point of view.\nFor some, or maybe many, machine-learning algorithms such an interpretation is not readily available, and is or can be a very interesting area of research, leading to improvements or to completely new algorithms.\nAvailable interpretations of machine-learning algorithms are mostly from the point of view of Probability Theory; unfortunately very little from the point of view of Decision Theory. For this reason we now discuss some limitations of present-day algorithms from the decision-theoretic viewpoint.\n\n\n\n\n\n\n For the extra curious\n\n\n\nThe book Machine Learning and Part V of the book Information Theory, Inference, and Learning Algorithms discuss interpretations of several machine-learning algorithms from the point of view of Probability Theory, and in few cases also of Decision Theory.",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "limitations_ML.html#sec-limit-class",
    "href": "limitations_ML.html#sec-limit-class",
    "title": "42  Decisions: limitations of present-day machine-learning algorithms",
    "section": "42.3 Limitations of machine-learning classifiers (and estimators)",
    "text": "42.3 Limitations of machine-learning classifiers (and estimators)\n\nUnknowns vs decisions\nDecision Theory makes a distinction between what is unknown to an agent (“outcomes”), and what the agent has to choose (“decisions”). This distinction is common in everyday problems. We may wonder whether it will rain in the next hour. The point of our wonder, however, is not (except in some situations) the rain phenomenon per se, but its implications about what clothes or shoes we should wear, or about staying indoors or going out, or about going on foot or by car, and so on.\nAn important reason for this distinction is that the set of possible decisions often does not have a correspondence with the set of unknown possibilities. In fact, the two sets often have different numbers of elements. You can think of many situations in which you are unsure whether some event will happen or not, and you have three plans: one if you are almost sure the event will happen, one if you are almost sure the event will not happen, and a third “safe” plan if you are about 50%/50% uncertain. The “safe” plan typically has consequences that are neither too bad or too good, so that losses and gains are kept to a minimum. This example also shows why the probabilities of the outcomes are important.\n\n\nMany present-day machine-learning algorithms are quite limited in this respect:\n\ntheir output is typically one of the unknown values, not one of the decisions\nthey don’t give the probabilities of the unknown values\n\nThis limitation may not be important in some tasks, for instance when you are classifying some images as “cat” or “dog” for the purpose of a photo album. But it is extremely important and impairing in serious applications, such as clinical ones. Let’s illustrate this with a simple example.\n\n\nA typical decision-making problem in medicine\nA clinician may be uncertain about the presence or absence of some medical condition, let’s say a disease. This uncertainty cannot be fully removed. The clinician’s task is not simply to guess about the disease, but to choose among different available treatments. Imagine you may have broken a bone, and the clinician simply tells you: “I guess it’s broken, though I’m not fully sure. Goodbye!”.\nTwo crucial points about treatment choice are these:\n\nthere may be more than two possible treatments, even if the uncertainty is binary (disease present vs absent)\nwhich treatment is optimal depends on the probability that the disease is present, not on a simple “yes/no guess”\n\nNeglecting both points can lead to disastrous consequences.\nFor example, suppose the clinician has three treatments available: \\({\\color[RGB]{204,187,68}A}\\), \\({\\color[RGB]{204,187,68}B}\\), \\({\\color[RGB]{204,187,68}C}\\). They have different efficacies against the disease, if it is present; and different damaging side-effects for the patient, if the disease is not present. Suppose that efficacy and damage can be measured together as decrease in life expectancy for the patient. The effects are as follows:\n\n\n\n\n\n\n\nTable 42.1: Change in life expectancy depending on treatment and medical condition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}disease}\\)\n\n\n\n\n\\({\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\)\n\\({\\color[RGB]{238,102,119}{\\small\\verb;no;}}\\)\n\n\ntreatment\n\\({\\color[RGB]{204,187,68}A}\\)\n\\(\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}0\\,\\mathrm{yr}\\)\n\n\n\\({\\color[RGB]{204,187,68}B}\\)\n\\(\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}\\)\n\n\n\\({\\color[RGB]{204,187,68}C}\\)\n\\(\\color[RGB]{204,187,68}0\\,\\mathrm{yr}\\)\n\\(\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}\\)\n\n\n\n\n\n\n\n\n\n\n\ntreatment \\({\\color[RGB]{204,187,68}A}\\) is mild (it could actually be a no-treatment option): under it, the patient is expected to live four years shorter if the disease is present, but the life expectancy is unaltered if the disease is not present\ntreatment \\({\\color[RGB]{204,187,68}B}\\) is intermediate: under it, the patient is expected to live only one year shorter if the disease is present, but also if the disease is not present, owing to the damage caused by this treatment\ntreatment \\({\\color[RGB]{204,187,68}C}\\) is intensive: under it, the patient is expected not to lose extra years if the disease is present, but will lose four years if the disease is not present, owing to the heavy damage caused by this treatment.\n\nWhich of the treatments above should the clinician choose? We now know how the clinician should make the optimal decision, but let’s explore the possible consequences of not making it, in three different scenarios. In each scenario, the clinician has prescribed several clinical tests (the predictors) for the patient, and obtained their results.\n\nScenario 1: 10%/90%\nGiven the results of the clinical tests, the clinician knows that the patient is typical of a subpopulation of patients where 10% have the disease, and 90% don’t. The present patient could be one among the 10%, or one of among the 90%.\n\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}A}\\) for this subpopulation, including the present patient, then these patients’ lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-40\\,\\mathrm{yr}}\n\\]\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}B}\\) for this subpopulation, including the present patient, then these patients’ lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nIf the clinician always chooses treatment \\({\\color[RGB]{204,187,68}C}\\), then the lives will be shortened in total by\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90}\n= \\boldsymbol{-360\\,\\mathrm{yr}}\n\\]\n\nClearly the best decision is treatment \\({\\color[RGB]{204,187,68}A}\\). It is possible that, unfortunately, the present patient’s life will be shortened; but this treatment was the patient’s and clinician’s best bet.\nIn this scenario, a clinician that doesn’t choose the optimal treatment is on average taking away from each patient between 7 months and 3 years of life more than was necessary or unavoidable.\n\n\nScenario 2: 50%/50%\nGiven the results of the clinical tests, the clinician knows that the patient is typical of a subpopulation of patients where 50% have the disease, and 50% don’t. In this case the present patient could be one of the first 50%, or one of the other 50%.\nCalculations similar to those of scenario 1 leads to these results:\n\nTreatment \\({\\color[RGB]{204,187,68}A}\\)\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-200\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}B}\\):\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}C}\\):\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}50}\n= \\boldsymbol{-200\\,\\mathrm{yr}}\n\\]\n\nThe best decision is treatment \\({\\color[RGB]{204,187,68}B}\\).\nIn this scenario, a clinician who chooses treatments \\({\\color[RGB]{204,187,68}A}\\) or \\({\\color[RGB]{204,187,68}C}\\) for patients having the same predictors as the present patient is on average taking away one extra year of life from each patient.\n\n\nScenario 3: 90%/10%\nIn this scenario 90% of patients in the subpopulation with the observed predictors have the disease, and 10% don’t. The present patient could belong to either group.\n\nTreatment \\({\\color[RGB]{204,187,68}A}\\)\n\\[\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-360\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}B}\\):\n\\[\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}-1\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-100\\,\\mathrm{yr}}\n\\]\nTreatment \\({\\color[RGB]{204,187,68}C}\\):\n\\[\n{\\color[RGB]{204,187,68}0\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}90} +\n{\\color[RGB]{204,187,68}-4\\,\\mathrm{yr}}\\cdot{\\color[RGB]{34,136,51}10}\n= \\boldsymbol{-40\\,\\mathrm{yr}}\n\\]\n\nTreatment \\({\\color[RGB]{204,187,68}C}\\) is the best decision in this scenario, for reasons complementary to those of scenario 1.\nIn this scenario, like in the first, a clinician that doesn’t choose the optimal treatment is on average taking away, from each patient, between 7 months and 3 years of life more than was necessary or unavoidable.\n\n\nIn each scenario, note that any decision strategy different from “sticking to the optimal decision” leads to suboptimal results – that is, lives shortened more than what was unavoidable. In scenario 1, for instance, a decision strategy such as “choose treatment \\({\\color[RGB]{204,187,68}A}\\) most of the time, and treatment \\({\\color[RGB]{204,187,68}B}\\) from time to time” leads to an additional life shortening of several months. This is clear from the following graph, which shows the average reduction in life expectancy for each treatment, depending on the percentage of patients with the disease:\n\nConsider the vertical line corresponding to probability 0.1. Any strategy that mixes treatment \\({\\color[RGB]{204,187,68}A}\\) with any of the other two will only reduce the life expectancy. This is true for any other probability values and their corresponding optimal treatments.\nFrom the plot we can see that treatment \\({\\color[RGB]{204,187,68}A}\\) is optimal if the probability that the disease is present is below \\(25\\%\\), treatment \\({\\color[RGB]{204,187,68}C}\\) is optimal if the probability is above \\(75\\%\\), and treatment \\({\\color[RGB]{204,187,68}B}\\) for intermediate probabilities.\n\n\n\n\n\n\nExercise 42.1\n\n\n\nAlthough the intuitive reasoning above has somewhat been phrased in terms of frequency, it’s the probability – the clinician’s degree of belief – that counts. Try to reason about this point through the following exercise:\nConsider a clinician uncertain with 50% probability that the present patient belongs to a 10%/90% frequency subpopulation, and with 50% that the patient belongs to a 90%/10% frequency subpopulation. And unfortunately this uncertainty cannot be removed by further clinical tests. Which treatment should this clinician choose? why?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nChapter 1 of Sox & al.: Medical Decision Making\nSkim through chapter 6 of Sox & al.: Medical Decision Making\n§§ 1.5–1.6 of Lindley: Making Decisions\n\n\n\n\n\n\nSub-optimality of a typical machine-learning algorithm\nThe clinical example above shows that there isn’t any one-to-one connection between decisions and unknowns. We cannot say, for instance, that treatment \\({\\color[RGB]{204,187,68}C}\\) “corresponds” to the presence of the disease, because the best treatment is actually \\({\\color[RGB]{204,187,68}B}\\), not \\({\\color[RGB]{204,187,68}C}\\), if the probability for \\(\\mathit{\\color[RGB]{238,102,119}disease}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) is above 50% but below 75%.\nNow imagine that the clinician inputs the patient’s predictors into a neural network, trained to give an output about the presence or absence of the disease. The neural network outputs yes, but it’s known that the neural network can err. Which treatment should the clinician choose?\n\nDoes the output yes mean that the probability for \\(\\mathit{\\color[RGB]{238,102,119}disease}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) was above 50%? Then the clinician doesn’t know whether it’s above or below 75%, and can’t make the optimal choice between \\({\\color[RGB]{204,187,68}C}\\) and \\({\\color[RGB]{204,187,68}B}\\). At best the clinician can unsystematically alternate between these two treatments, but as we saw above this mixture is sub-optimal.\nIs the output yes produced with a particular probability? But what is this probability? The output could be \\({\\color[RGB]{238,102,119}{\\small\\verb;yes;}}\\) even if the probability were less than 50% or 25%. Then neither in this case can the clinician make the optimal choice between all three treatments. And a mixture of all three is again sub-optimal.\n\n\n\n\n\n\n\nExercise 42.2\n\n\n\nCould the clinician deduce an approximate probability by looking at the statistics (confusion matrix) of the neural network on some test set?\nExplore and maybe even implement this possibility.\n\n\n\n\n\n\n  Output scores or weights are not probabilities!\nSome machine-learning algorithms are capable of giving continuous “scores” or “weights” between 0 and 1 – for instance the output of a softmax function – instead of a simple answer such as “yes” or “no”. But unfortunately such scores are not probabilities or frequencies, even if there may be some association between them and a probability or frequency.\nA real-life example of this mismatch is evident in the plot below (see “for the extra curious” below for references). It shows the 0–1 output score given by a random forest in a binary-classification task on a test set, versus the probability of the corresponding class, calculated by an agent that has learned from further data and from the random forest’s score:\n\nClearly the output score is different from the probability; if they were the same the graphs would be straight lines with ±45° slopes.\nImagine what would happen if the clinician from our example above mistook the output score for a probability:\n\n  Output score in the range \\(0\\)–\\(0.25\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}A}\\). Luckily the upper boundary of this score range approximately corresponds to a frequency of \\(25\\%\\), so in this case the clinician would be choosing the optimal treatment.\n  Output score in the range \\(0.25\\)–\\(0.5\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}B}\\). Luckily the upper boundary of this score range approximately corresponds to a frequency of \\(75\\%\\) (note the mismatch), so in this case the clinician would be choosing the optimal treatment.\n  Output score in the range \\(0.5\\)–\\(0.75\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}B}\\). But the corresponding frequency is approximately between \\(75\\%\\)–\\(90\\%\\), so the optimal treatment is actually \\({\\color[RGB]{204,187,68}C}\\). As calculated in Scenario 3 above, the clinician would be then be shortening the patient’s life by 7 months more than was unavoidable.\n  Output score in the range \\(0.75\\)–\\(1\\): the clinician would choose treatment \\({\\color[RGB]{204,187,68}C}\\). Luckily this corresponds to an approximate frequency range \\(90\\%\\)–\\(92\\%\\), where treatment \\({\\color[RGB]{204,187,68}C}\\) is indeed optimal.\n\nNote that we cannot say “the suboptimal treatment is chosen one out of four times”, because in this example we don’t know how many patients end up having a score in the \\(0.5\\)–\\(0.75\\) range. In a concrete case it could be that 90% of the patients end up in this range, which would mean that the misuse of the output score would lead to a suboptimal treatment in 90% of cases.\n\n\nAnother real example of the difference between weights or scores and probabilities is shown in the plot below, obtained by applying a neural network to the same binary-classification task on a test set. The x-axis shows the internal output-layer weight that the neural network assigns to one class (more precisely, the diagonal where the two output-layer weights have equal values). The y-axis shows the probability of that class. Typically a “softmax” or sigmoid function is applied to the output-layer weights, in order to obtain positive, normalized scores that are often miscalled “probabilities”. The softmax in the present case is shown as the dashed grey line:\n\nWe can observe an even worse mismatch than for the random forest. For instance, if the softmax is between \\(0\\)–\\(0.25\\), then the output-layer weight must have been less than approximately \\(-0.5\\), which in turn means that the probability could be anywhere between \\(0\\%\\) and \\(50\\%\\). For a softmax between \\(0.75\\)–\\(1\\), the probability could be anywhere between \\(50\\%\\) and \\(92\\%\\).\nIn both plots above, notice how the probability for each class can never be higher than around \\(92\\%\\), yet the weights or scores go up to \\(1\\).\n\n\n\n\n\n\n Don’t use misleading terminology\n\n\n\nPlease don’t call the score or weight output of a machine-learning algorithm a “probability” or “frequency”, unless you’ve first made sure that it actually is a probability or frequency. Just because some numbers are positive and sum up to one doesn’t mean that they are a probability or frequency distribution.\nUsing this kind of mistaken terminology shows downright scientific incompetence, and its consequences, as you can see from the medical examples above, are borderline scientific malpractice.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDon’t guess what’s true: choose what’s optimal. A probability transducer for machine-learning classifiers",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>[Decisions: limitations of present-day machine-learning algorithms]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html",
    "href": "utilities_evaluation.html",
    "title": "43  Evaluation practices and utilities",
    "section": "",
    "text": "43.1 Confusion matrices and evaluation metrics\nMachine-learning methodology uses a disconcerting variety of evaluation metrics to try to quantify, compare, rank the performances of one or more algorithms.\nFor machine-learning classifiers many of these metrics are constructed from the so-called “confusion matrix”. The basic idea behind it is to test the algorithms of interest on a test dataset (the same for all), and then count for how many units the algorithm outputs value \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{238,102,119}y^{*}}\\) when the true value of the unknown is \\(\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\\), for all combinations of possible \\({\\color[RGB]{238,102,119}y^{*}}\\) and \\({\\color[RGB]{238,102,119}y}\\).\nImagine for instance a binary-classification task with classes \\({\\color[RGB]{238,102,119}\\alpha}\\) and \\({\\color[RGB]{238,102,119}\\beta}\\). It could be the electronic-component scenario we met in the first chapters, with class \\({\\color[RGB]{238,102,119}\\alpha}\\) being “the electronic component will function for at least a year”, and class \\({\\color[RGB]{238,102,119}\\beta}\\) “the electronic component will fail within a year of use”.\nApplication of two algorithms \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) to the same 100 test units results in the following confusion matrices:\nThese matrices can also be normalized, dividing every entry by the total number of test units. Various aspects can be read from the confusion matrices above. Algorithm \\(\\mathcal{B}\\), for example, seems better than \\(\\mathcal{A}\\) at inferring class \\({\\color[RGB]{238,102,119}\\alpha}\\), but slightly worse at inferring class \\({\\color[RGB]{238,102,119}\\beta}\\).\nMost evaluation metrics for classification combine the entries of a confusion matrix in a mathematical formula that yields a single number.\nSuch metrics and formulae can be quite opaque. Their definitions and their motivations are often arbitrary or very case-specific. It’s common to find works where several metrics are used because it’s unclear which single one should be used. And the only hope is that most or all of them will agree at least on the rankings they lead to. However,…",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-confusion",
    "href": "utilities_evaluation.html#sec-confusion",
    "title": "43  Evaluation practices and utilities",
    "section": "",
    "text": "Table 43.1: Confusion matrix for \\(\\mathcal{A}\\)\n\n\n\n\n\n\n\n\n\n\n\ntrue \\({\\color[RGB]{238,102,119}\\alpha}\\)\ntrue \\({\\color[RGB]{238,102,119}\\beta}\\)\n\n\noutput \\({\\color[RGB]{238,102,119}\\alpha}\\)\n27\n15\n\n\noutput \\({\\color[RGB]{238,102,119}\\beta}\\)\n23\n35\n\n\n\n\n\n\n\n\n\n\n\n\nTable 43.2: Confusion matrix for \\(\\mathcal{B}\\)\n\n\n\n\n\n\n\n\n\n\n\ntrue \\({\\color[RGB]{238,102,119}\\alpha}\\)\ntrue \\({\\color[RGB]{238,102,119}\\beta}\\)\n\n\noutput \\({\\color[RGB]{238,102,119}\\alpha}\\)\n43\n18\n\n\noutput \\({\\color[RGB]{238,102,119}\\beta}\\)\n7\n32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMajority votes are not a criterion for correctness\nHere are the scores that some popular metrics assign to algorithms \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), based on their confusion matrices above. The better algorithm according to each metric is indicated in green bold. Almost all these evaluation metrics seem to agree that \\(\\mathcal{B}\\) should be the best of the two:\n\n\n\n\n\n\n\n\n\n\n\n\nmetric\n\\(\\mathcal{A}\\)\n\\(\\mathcal{B}\\)\n\n\nAccuracy\n0.62\n0.75\n\n\nPrecision\n0.64\n0.70\n\n\nBalanced Accuracy\n0.62\n0.75\n\n\n\\(F_1\\) measure\n0.59\n0.77\n\n\nMatthews Correlation Coefficient\n0.24\n0.51\n\n\nFowlkes-Mallows index\n0.59\n0.78\n\n\nTrue-positive rate (recall)\n0.54\n0.86\n\n\nTrue-negative rate (specificity)\n0.70\n0.64\n\n\n\n\n\n\n\nYet, when put to actual use, it turns out that \\(\\mathcal{B}\\) actually leads to a monetary loss of 3.5 $ per unit, whereas \\(\\mathcal{A}\\) leads to a monetary gain of 3.5 $ per unit!",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-dt-util-evaluation",
    "href": "utilities_evaluation.html#sec-dt-util-evaluation",
    "title": "43  Evaluation practices and utilities",
    "section": "43.2 Decision theory and utilities as the basis for evaluation",
    "text": "43.2 Decision theory and utilities as the basis for evaluation\nHow is the surprising result above possible? Decision Theory tells us how, and can even correctly select the best algorithm from the confusion matrices above.\nAs discussed in the preceding sections, each application to a new unit is a decision-making problem. Neither making nor evaluating a decision is possible unless we specify the utilities relevant to the problem. It turns out that the consequences in the present problem have the following monetary utilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\({\\color[RGB]{238,102,119}\\alpha}\\) is true\n\\({\\color[RGB]{238,102,119}\\beta}\\) is true\n\n\n\\({\\color[RGB]{238,102,119}\\alpha}\\) is chosen\n\\(\\color[RGB]{68,119,170}15 \\$\\)\n\\(\\color[RGB]{68,119,170}-335 \\$\\)\n\n\n\\({\\color[RGB]{238,102,119}\\beta}\\) is chosen\n\\(\\color[RGB]{68,119,170}-35 \\$\\)\n\\(\\color[RGB]{68,119,170}165 \\$\\)\n\n\n\n\n\n\n\nThese utility values may be determined by the combination of sale revenue, disposal costs, warranty refunds, and similar factors.\nThe utility matrix above says that for every test unit of true class \\({\\color[RGB]{238,102,119}\\alpha}\\) (the unit will function for at least a year) that the algorithm classifies as \\({\\color[RGB]{238,102,119}\\alpha}\\) (and therefore sends for sale), the production company gains \\(\\color[RGB]{68,119,170}15 \\$\\); for every test unit of class \\({\\color[RGB]{238,102,119}\\alpha}\\) that the algorithm classifies as \\({\\color[RGB]{238,102,119}\\beta}\\) (and therefore discards), the production company gains \\(\\color[RGB]{68,119,170}-35 \\$\\) (so actually a loss); and so on. The total yield from each algorithm on the 100 test units can therefore be calculated by multiplying the four utilities above by the corresponding counts in the confusion matrix, and then taking the total. The average yield per unit is obtained dividing the total by the number of units, or directly using the normalized confusion matrix:\n\n\\[\n\\begin{aligned}\n\\text{$\\mathcal{A}$'s average yield } &=\n\\bigl[27 \\cdot {\\color[RGB]{68,119,170}15 \\$} +\n23 \\cdot ({\\color[RGB]{68,119,170}-35 \\$}) +\n15 \\cdot ({\\color[RGB]{68,119,170}-335 \\$}) +\n35 \\cdot {\\color[RGB]{68,119,170}165 \\$}\\bigr]/100\n\\\\[1ex]\n&= \\boldsymbol{\\color[RGB]{34,136,51}+3.5 \\$}\n\\\\[3ex]\n\\text{$\\mathcal{B}$'s average yield } &=\n\\bigl[43 \\cdot {\\color[RGB]{68,119,170}15 \\$} +\n7 \\cdot ({\\color[RGB]{68,119,170}-35 \\$}) +\n18 \\cdot ({\\color[RGB]{68,119,170}-335 \\$}) +\n32 \\cdot {\\color[RGB]{68,119,170}165 \\$}\\bigr]/100\n\\\\[1ex]\n&= {\\color[RGB]{170,51,119}-3.5 \\$}\n\\end{aligned}\n\\]\n\nThis calculation tells us that \\(\\mathcal{A}\\) is better, and also gives us an idea of the actual utilities that the two algorithms would yield.\n\nUtilities as evaluation metric\nThe calculation above is exactly the one discussed in § 35.5, where we logically motivated the use of utilities as an evaluation metric. In the case of common machine-learning classifiers the logically correct evaluation metric and procedure are is thus straightforward:\n\nfind the utilities relevant to the specific problem under consideration, collect them into a utility matrix \\(\\boldsymbol{\\color[RGB]{68,119,170}U}\\)\napply the classifier to a test set and build the confusion matrix \\(\\boldsymbol{\\color[RGB]{238,102,119}C}\\)\nmultiply confusion and utility matrices element-wise and take the total\n\nWhat’s remarkable in this procedure is that it is not only logically well-founded, but also mathematically simple. The formula for the correct evaluation metric is just a linear combination of the confusion-matrix entries. This linearity is actually a subtle consequence of the axiom of independence discussed in § 35.6.\nNote something very important:\n\n\n\n\n\n\n Don’t do class balancing!\n\n\n\nThis procedure requires that the test set be a representative, unsystematic sample of the actual population of interest. In particular, the proportions of classes in the test set should reflect their frequencies in real application. No “class balancing” should be performed.\n\n\nThe evaluation based on decision theory automatically takes into account “class balance” and its interaction with the utilities relevant to the task.\nThere are problems for which the simple strategy “always choose class \\(\\dotso\\)” is optimal, given the predictors available in the problem. So this strategy cannot be beaten by “improving” or finding a “better” algorithm: such endeavour is only a waste of time. The reason is that the maximal information that the predictors can give about the predictand is not enough to sharpen probabilities above the thresholds determined by the utilities. This maximal information is an intrinsic properties of predictors and predictands, so it cannot be improved by fiddling with algorithms. The only way for improvement is to find other predictors.\n“Class balancing” does not solve this problem. It transforms the actual task into a different task, where maybe some algorithm can show improvement, simply because we have changed the population statistics and therefore the mutual information between predictors and predictands. But as soon as we get back to reality, to the actual task, the situation will be as before.\n\n\n\n\n\n\n Study reading\n\n\n\nDrummond & al. 2005: Severe Class Imbalance: Why Better Algorithms Aren’t the Answer",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "utilities_evaluation.html#sec-good-bad-metrics",
    "href": "utilities_evaluation.html#sec-good-bad-metrics",
    "title": "43  Evaluation practices and utilities",
    "section": "43.3 Popular evaluation metrics: the good and the bad",
    "text": "43.3 Popular evaluation metrics: the good and the bad\nIf an evaluation metric can be rewritten as a linear combination of the confusion-matrix entries, then it can be interpreted as arising from a set of utilities (although they might not be the ones appropriate to the problem).\nThis is the case, for instance, of\n\naccuracy, which turns out to correspond to the utility matrix \\(\\begin{bsmallmatrix}\n1&0\\\\0&1\n\\end{bsmallmatrix}\\) or its non-binary analogues;\ntrue-positive rate, which corresponds to \\(\\begin{bsmallmatrix}\n1&0\\\\0&0\n\\end{bsmallmatrix}\\)\ntrue-negative rate, which corresponds to \\(\\begin{bsmallmatrix}\n0&0\\\\0&1\n\\end{bsmallmatrix}\\)\n\nIf an evaluation metric cannot be rewritten in such a linear form, then it is breaking the axioms of Decision Theory, and is therefore guaranteed to carry some form of cognitive bias. The axiom of independence is specifically broken, because non-linearities imply some functional dependence of utilities on probabilities or vice versa.\nSome quite popular evaluation metrics turn out to break Decision Theory in this way:\n\nprecision\n\\(F_1\\)-measure\nMatthews correlation coefficient\nFowlkes-Mallows index\nbalanced accuracy\n\nThe area under the curve of the receiving operating characteristic (typically denoted “AUC”) is also an evaluation metric that breaks the axioms of Decision Theory, although it is not based on the confusion matrix.\nThis fact is actually funny, because the first papers (in the 1960s–1970s, referenced below) that discussed an evaluation method based on the receiver operating characteristic actually derived it from Decision Theory. The papers gave the correct procedure to use the receiver operating characteristic, and pointed out the “area under the curve” only as a quick but possibly erroneous heuristic procedure.\nIt goes without saying that you should stay away from the cognitive-biased metrics above.\n\n\n\n\n\n\n Study reading\n\n\n\n\nSkim through Dyrland & al. 2022: Does the evaluation stand up to evaluation?\nSkim through Swets & al. 1961: Decision processes in perception\nSkim through Metz 1978: Basic principles of ROC analysis",
    "crumbs": [
      "[**Approximations: machine learning**]{.midgrey}",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>[Evaluation practices and utilities]{.darkgrey}</span>"
    ]
  },
  {
    "objectID": "whither.html",
    "href": "whither.html",
    "title": "What next?",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have finally reached the end of this course. Congratulations!\n…Or maybe we should say: *Good luck on your new journey! – Because this is just the beginning.\nWhat we hope you have taken from this course is a big picture of the science underneath data science and data-driven engineering, with a clear idea of its main (and few!) principles. You can now apply these principles to engineering problems similar to those explored in this course, and to other, more challenging problems. The principles you have learned are exactly the same.\n\n\nNow it is up to you in which directions to continue your journey as a data scientist. Maybe you want to…\n\n\n  engineer “optimal predictor machines” that can deal with more complex kind of data\n  improve existing machine-learning algorithms by analysing how they approximate an optimal predictor machine\n  use your understanding of the foundations to interpret and explain how present-day algorithms work\n  look for new technologies that may allow us to do the complicated computations required by an optimal predictor machine\n  disseminate what you have learned here, or explore its foundations, or find ways to make it more understandable\n\n\n…and many other possibilities.\nIt’s important to be aware that most of these directions will require more difficult mathematics, in order to write working code, to face more realistic problems, and to find actual solutions to them. In the part “A prototype Optimal Predictor Machine*” you saw that we needed to bring up Dirichlet distributions, factorials, integrals, and other mathematics in order to build a concrete, working prototype of an optimal agent. And that agent can only work in a limited and somewhat simple class of problems. Solving more complicated problems will, inevitably, require more complicated mathematics. For some this is actually a fun challenge. In any case, don’t forget the ever-positive side: the basic principles are few and intuitively understandable in their essence.\n\nUsing Probability Theory and Decision Theory as thinking and organizational frameworks\nWe hope that you will use basic probability theory, decision theory, and their notation as tools to frame and organize inference, prediction, and decision problems.\nIt does not matter whether the problem can then be solved exactly according to the rules of probability & decision theory, or whether only a crude approximation is available. You have seen that these two theories are extremely useful even just in the beginning stage, when we ask questions like “what do I need to find?”, “why do I need to find it?” “what do I know?”, “what am I assuming?”, “what’s are the gains and costs of success and failure?” – and similar questions.\nFor example, see again how the basic probability notation helped us classify different types of inference in chapter  24  A categorization of inferences. The notation even suggested at once how to correctly deal with partially missing data (§ Flexible categorization using probability notation).\n\n\nThe basic, universal formula behind all supervised- and unsupervised-learning algorithms\nWe also hope that you will not forget, and actually use as much as possible, the basic formula (chapter  24  A categorization of inferences) that represents how an agent doing any kind of supervised- or unsupervised-learning works. This formula is what a neural network or a random forest are doing under the hood, even if just in an approximate way:\n\n\n\n\n\n\n\n \n\n\n\n\nAll previous predictors and predictands known (supervised learning)\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\qquad{}=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}y}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\n\n“Guess all variates” (unsupervised learning, generative algorithms):\n\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n=\n\\frac{\n\\mathrm{P}(\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{\\color[RGB]{170,51,119}z}\n\\mathrm{P}(\n\\color[RGB]{238,102,119}\nZ_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}z}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nZ_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nZ_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}z_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\n\n\n\nPrevious predictors known, previous predictands unknown (unsupervised learning, clustering)\n\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\\\[2ex]\n&\\quad{}=\n\\frac{\n\\sum_{\\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\n\\sum_{{\\color[RGB]{170,51,119}y}, \\color[RGB]{204,187,68}y_{N}, \\dotsc, y_{1}}\n\\mathrm{P}(\\color[RGB]{238,102,119}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}{\\color[RGB]{170,51,119}y}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{34,136,51}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\color[RGB]{0,0,0}\\, \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\,\n\\color[RGB]{204,187,68}\nY_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{N}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{N}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\color[RGB]{204,187,68}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\color[RGB]{34,136,51}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\end{aligned}\n\\]\n\n\nAll these formulae, even for hybrid tasks, involve sums and ratios of only one distribution:\n\\[\\boldsymbol{\n\\mathrm{P}(\\color[RGB]{68,119,170}\nY_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{\\text{new}}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{\\text{new}}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\dotsb \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}\n\\]\nand if the problem is exchangeable, for instance without time dependence or memory effects, the distribution can be calculated in a simpler way:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl(\n\\color[RGB]{68,119,170}Y_{\\text{new}} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{\\text{new}} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\n\\dotsb\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nY_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1}\n\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\nX_{1} \\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}\n\\color[RGB]{0,0,0}\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}} \\mathsfit{I}\\bigr)\n\\\\[2ex]\n&\\qquad{}=\n\\sum_{\\boldsymbol{f}}\nf({\\color[RGB]{68,119,170}Y_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{\\text{new}}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x}) \\cdot\n\\, \\dotsb\\, \\cdot\nf({\\color[RGB]{68,119,170}Y_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y_{1} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}X_{1}\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x_{1}})\n\\cdot\n\\mathrm{P}(F\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}\\boldsymbol{f}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\nPossibly there is a final decision about the output (if a single output is required), using some utilities and the principle of maximal expected utility:\n\\[\n\\mathsfit{\\color[RGB]{204,187,68}D}_{\\text{optimal}} =\n\\operatorname{argmax}\\limits\\limits_{\\mathsfit{\\color[RGB]{204,187,68}D}} \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{U}(\\mathsfit{\\color[RGB]{204,187,68}D}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}{\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\cdot\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathclose{}\\mathord{\\nonscript\\mkern 0mu\\textrm{\\small=}\\nonscript\\mkern 0mu}\\mathopen{}x} \\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{\\color[RGB]{34,136,51}data}\\mathbin{\\mkern-0.5mu,\\mkern-0.5mu}\\mathsfit{I})\n\\]\n\n\n\n\n\n\n\nFurther texts\nIf you are looking for further texts to deepen your understanding of the probability calculus and decision theory, we recommend the following – but it’s a good idea to explore on your own! Try and skim through texts you find, you may stumble onto very interesting good ones.\n\nE. T. Jaynes (1994): Probability Theory: The Logic of Science\nD. J. C. MacKay (1995): Information Theory, Inference, and Learning Algorithms\nJ.-M. Bernardo, A. F. Smith (1994): Bayesian Theory\nH. Raiffa (1968): Decision Analysis: Introductory Lectures on Choices under Uncertainty\nS. J. Russell, P. Norvig (1995): Artificial Intelligence: A Modern Approach Parts I–IV (chapters 1–19)\nP. E. Rossi (2014) Bayesian Non- and Semi-parametric Methods and Applications",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[What next?]{.lightblue}"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Further reading",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n\nFoundations of the probability calculus\nThe four fundamental rules of the Probability Calculus have been at least since Laplace in the 1700s, essentially in their present form. Laplace used them to infer properties of planets and their orbit (with results still valid today). Proof of their logical foundation and necessity started to appear in the 1940s, a formal milestone being the proof by R. T. Cox in 1946. They have been tightened and reformulated in different ways since. Here are some old and recent works on the foundations (as opposed to works that simply mention the rules and apply them). Cox’s and Jaynes’s are probably the first ones to be checked:\n\nJ. M. Keynes (1921): A Treatise on Probability.\nW. E. Johnson (1924): Logic. Part III: The Logical Foundations of Science.\nW. E. Johnson (1932): *Probability: The relations of proposal to supposal, Axioms, The deductive and inductive problems.\nH. Jeffreys (1939): Theory of Probability.\nR. T. Cox (1946): Probability, Frequency, and Reasonable Expectation.\nG. Pólya (1949): Preliminary remarks on a logic of plausible inference.\nG. Pólya (1954): Mathematics and Plausible Reasoning. Vol. II: Patterns of Plausible Inference.\nM. Tribus (1969): Rational Descriptions, Decisions and Designs.\nE. T. Jaynes (1994): Probability Theory: The Logic of Science.\nJ. B. Paris (1994): The Uncertain Reasoner’s Companion: A Mathematical Perspective.\nT. Hailperin (1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications.\nP. Snow (1998): On the correctness and reasonableness of Cox’s theorem for finite domains.\nP. Snow (2001): The reasonableness of possibility from the perspective of Cox.\nK. S. Van Horn (2003): Constructing a logic of plausible inference: a guide to Cox’s theorem.\nM. J. Dupré, F. J. Tipler (2009): New axioms for rigorous Bayesian probability.\n\n\n\n\n\nFoundations of Decision Theory\nDecision Theory is much younger than the Probability Calculus, and its foundations probably still needs to be tightened here and there. Here are old and recent works on its foundations:\n\nJ. von Neumann, O. Morgenstern (1953): Theory of Games and Economic Behavior.\nD. Luce, H. Raiffa (1957): Games and Decisions: introduction and critical survey.\nL. J. Savage (1954/1972): The Foundations of Statistics.\nE. Eells (1982/2016): Rational Decision and Causality.\nR. Pettigrew (2011/2019): Epistemic Utility Arguments for Probabilism.\nR. A. Briggs (2014/2019): Normative Theories of Rational Choice: Expected Utility.",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[Further reading]{.lightblue}"
    ]
  },
  {
    "objectID": "thanks.html",
    "href": "thanks.html",
    "title": "Thanks",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\nWe would like to thank, in unsystematic order:\n\nThe master students of this course, for their enthusiasm and interest in the course goal and material, and for their constructive feedback. You rock!\nThe members of the Artificial Intelligence Engineering group at the Western Norway University of Applied Sciences (HVL) for endorsement of the present course and encouragement during its construction.\nSoledad Gonzalo Cogno and Iván Davidovich and their course Concepts in Data Analysis, organized by the Gonzalo Cogno Group at the Kavli Institute for Systems Neuroscience and at the Norwegian University of Science and Technology (NTNU), where some of the present course material was initially designed and tested.\nThe members of the Medical AI group at the Mohn Medical Imaging and Visualization Centre, in particular Ingrid Rye, Alexandra Vik, Marek Kociński, Arvid Lundervold, Astri Lundervold, Alexander Lundervold, for the many group meetings, discussions, and projects where the core of the present course material was tested.\nThe Bulletin of the International Society for Bayesian Analysis and its editor Gregor Kastner for kindly allowing us to share the present course in the “Teaching Highlight” section.\nLars Michael Kristensen for his great work on the Master in Applied Computer Science at HVL, of which this course is part.\nThe developers and maintainers of Quarto for help in the customization of this course’s web pages.\nOur beloved, families, friends for their continuous encouragement and support.\nPGLPM thanks Saitama for being a constant source of awe and inspiration.",
    "crumbs": [
      "[**Conclusion**]{.lightblue}",
      "[Thanks]{.lightblue}"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "\\(\\DeclarePairedDelimiter{\\set}{\\{}{\\}}\\) \\(\\DeclarePairedDelimiter{\\abs}{\\lvert}{\\rvert}\\) \n\n(Note to ADA511 students: in the course’s Canvas page) you can find further information on how to access these references.\n\nBelieve nothing, O monks, merely because you have been told it, or because it is traditional, or because you yourselves have imagined it. Do not believe what your teacher tells you merely out of respect for the teacher.     (Attributed to Gautama Buddha)\n\n\nBut in the natural sciences, whose conclusions are true and necessary and have nothing to do with human will, one must take care not to place oneself in the defense of error; for here a thousand Demostheneses and a thousand Aristotles would be left in the lurch by every mediocre wit who happened to hit upon the truth for himself.     (Galileo Galilei)\n\n\n\nThe notions, ideas, and rules that you have learned in this course have been presented in such a way as to appear plausible and intuitively understandable. In some places we gave sketch of proofs.\nBut that is not enough.\nA “data mechanic” (see preface) might be excused for using incorrect formulae, and might simply say “this is the procedure I was taught”. You instead, as a data engineer and data scientist, have the duty to check the validity of the theory and principles that you use in developing new algorithms, code, solutions.\nIn particular, you cannot accept theories or methods simply because\n\n\n  they are commonly used, or used by the majority of some community;\n  some “authority” or known scientist says they are correct.\n\n\nIn fact, Science began when the two criteria above were discarded as not valid. Galileo’s quote above says this very explicitly. Imagine if Einstein had said “all scientists see no problem with the notion of simultaneity, so it must be correct”, or “great scientists like Maxwell or Poincaré did not see any problem with the notion of simultaneity, so it must be correct”. There is no scientific progress with this kind of reasoning.\nInstead, the only two scientific criteria you have to decide on the validity of a method or theory are\n\n\n  logical proof,\n  experimental corroboration.\n\n\nwhich you must do as much as possible by yourself. The more verification you delegate to others, to majority or “authority”, the less you are doing science.\n\n\nFor this reason you have, at some point, go and check for yourself the validity of what you’ve learned in this course. You might in fact find out that something was not correct! Then you’ll correct it and make science advance. Throughout the course We have given references where many proofs can be found. Here are some final references containing the main proofs of what you have learned here; you should check and validate them at some point.\n\n\n\n\nR. A. Briggs (2014/2019): Normative Theories of Rational Choice: Expected Utility.\nM. Ben-Ari (1993/2012): Mathematical Logic for Computer Science.\nT. M. Cover, J. A. Thomas (1991/2006): Elements of Information Theory.\nR. T. Cox (1946): Probability, Frequency and Reasonable Expectation.\nM. Cox, A. O’Hagan (2022): Meaningful expression of uncertainty in measurement.\nR. M. Dawes, T. L. Smith (1985): Attitude and opinion measurement, pp. 509–566 in G. Lindzey, E. Aronson: Handbook of Social Psychology. Vol. I: Theory and Method.\nP. Diaconis, S. Holmes, R. Montgomery (2007): Dynamical Bias in the Coin Toss.\nC. Drummond, R. C. Holte (2005): Severe Class Imbalance: Why Better Algorithms Aren’t the Answer.\nK. Dyrland, A. S. Lundervold, P.G.L. Porta Mana (2022): Does the evaluation stand up to evaluation?: A first-principle approach to the evaluation of classifiers.\nE. Eells (1982/2016): Rational Decision and Causality.\nN. Fenton, M. Neil (2019): Risk Assessment and Decision Analysis with Bayesian Networks.\nG. Galilei (1632/1967): Dialogue concerning the two chief world systems – Ptolemaic & Copernican\nS. J. Gould (1985/2013): The Median Isn’t the Message.\nP. C. Gregory (2005): Bayesian Logical Data Analysis for the Physical Sciences.\nT. Hailperin (1965): Best Possible Inequalities for the Probability of a Logical Function of Events.\nT. Hailperin (1996): Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications.\nR. Hastie, R. M. Dawes (2001/2010): Rational Choice in an Uncertain World: The Psychology of Judgment and Decision Making.\nD. Heath, W. Sudderth (1976): De Finetti’s Theorem on Exchangeable Variables.\nM. R. A. Huth, M. D. Ryan (2000/2004): Logic in Computer Science.\nM. Ingham (2012): No More Band-Aids: Integrating FM into the Onboard Execution Architecture.\nE. T. Jaynes (1994/2003): Probability Theory: The Logic of Science.\nR. L. Keeney, H. Raiffa (1976/1993): Decisions with Multiple Objectives: Preferences and Value Tradeoffs.\nW. Kruskal, F. Mosteller (1979): Representative Sampling, I: Non-scientific Literature.\nD. V. Lindley (1971/1988): Making Decisions.\nD. V. Lindley, M. R. Novick (1981): The role of exchangeability in inference.\nD. J. C. MacKay (1992): A practical Bayesian framework for backpropagation networks.\nD. J. C. MacKay (1995/2005): Information Theory, Inference, and Learning Algorithms.\nG. Malinas, J. Bigelow (2004/2016): Simpson’s paradox.\nC. E. Metz (1978): Basic principles of ROC analysis.\nD. G. Morrison (1967): On the consistency of preferences in Allais’ paradox.\nA. O’Hagan (1988): Probability: Methods and measurement.\nM. Ono, A. Nicholas, F. Alibay, J. Parrish (2015): SMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission.\nF. J. Pelletier, A. Hazen (2021/2023): Natural Deduction Systems in Logic.\nJ. W. Pratt, H. Raiffa, R. Schlaifer (1995/1996): Introduction to Statistical Decision Theory.\nH. Raiffa (1968/1970): Decision Analysis: Introductory Lectures on Choices under Uncertainty.\nS. J. Russell, P. Norvig (1995/2022): Artificial Intelligence: A Modern Approach.\nD. S. Sivia (1996/2006): Data Analysis: A Bayesian Tutorial.\nH. C. Sox, M. C. Higgins, D. K. Owens (1988/2013): Medical Decision Making.\nK. Steele, H. O. Stef{'a}nsson (2015/2020): Decision Theory.\nJ. A. Swets, W. P. Tanner, Jr., T. G. Birdsall (1961): Decision processes in perception.\nG. Takeuti (1975/1987): Proof Theory.\nB. C. Williams, M. D. Ingham, S. H. Chung, P. H. Elliott (2003): Model-based programming of intelligent embedded systems and robotic space explorers.\nO. Zekri, A. Odonnat, A. Benechehab, L. Bleistein, N. Boullé, I. Redko (2024): Large language models as Markov chains.",
    "crumbs": [
      "[References]{.lightblue}"
    ]
  }
]