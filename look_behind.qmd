# [A look behind]{.red} {#sec-look-behind}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

In the past chapters we have learned a lot of theory and fundamentals, illustrated with some simple examples. Now we shall finally put the theory into practice! We shall build a prototype AI agent that implements the theory as closely as possible.

Before proceeding, let's take a quick look back at the road behind us and recall the most important milestones:

::::{.column-page-inset-right .green}
- {{< fa traffic-light >}}\ \ In order to make decisions and to act in an optimal way, an agent needs to maximize expected utility. The agent must quantify its "values" (*utilities*) and its degrees of belief (*probabilities*) about the consequences of the problem.
\
\

- {{< fa traffic-light >}}\ \ In order to calculate its degrees of belief in a logically consistent way, an agent must use the **four fundamental rules of inference**:
    
    $\P(\lnot X \| Z) = 1 - \P(X \| Z)$  
    $\P(X \and Y \| Z) 
= \P(X \| Y \and Z) \cdot \P(Y \| Z) 
= \P(Y \| X \and Z) \cdot \P(X \| Z)$  
    $\P(X \lor Y \| Z) = 
    \P(X \| Z) + \P(Y \| Z) - \P(X \and Y \| Z)$  
    $\P(X \| X \and Z) = 1$
    
    Any departure from these rules will lead to small or large logical errors.
\
\

- {{< fa traffic-light >}}\ \ When an agent must draw inferences about populations, having observed $N$ units (*training data*) from the population, the four rules lead to the general formula
    
    $\P( Z_{N+1}\mo z \| 
    Z_{N}\mo z_{N} \and \dotsb \and Z_{1}\mo z_{1} \and \yI) =
    \frac{
    \P( Z_{N+1}\mo z \and
    Z_{N}\mo z_{N} \and \dotsb \and Z_{1}\mo z_{1} \| \yI)
    }{
\sum_z \P( Z_{N+1}\mo z \and
    Z_{N}\mo z_{N} \and \dotsb \and Z_{1}\mo z_{1} \| \yI)
    }$
    
    and some slight variations of it.
    
    The probability distribution $\P( Z_{N+1}\mo z \and \dotsb \and Z_{1}\mo z_{1} \| \yI)$ must be built-in in the agent.
\
\

- {{< fa traffic-light >}}\ \ When an agent must draw inferences about an approximately infinite population, having observed $N$ units (*training data*) from the population, and the agent has *exchangeable* beliefs about the population, the four rules lead to the general formula
    
    $\P( Z_{N+1}\mo z \| 
    Z_{N}\mo z_{N} \and \dotsb \and Z_{1}\mo z_{1} \and \yI) =
	\frac{
\sum_{\vf}
f(Z\mo z) \cdot
f(Z\mo z_{N}) \cdot
\, \dotsb\, \cdot
f(Z\mo z_{1})
\cdot
\P(F\mo\vf \| \yI)
}{
\sum_{\vf}
f(Z\mo z_{N}) \cdot
\, \dotsb\, \cdot
f(Z\mo z_{1})
\cdot
\P(F\mo\vf \| \yI)
}$
    
    and some slight variations of it.
    
    The probability distribution $\P(F\mo\vf \| \yI)$ must be built-in in the agent.
::::
\

Pay attention to the fact that all inference formulae about populations came straight from the four fundamental rules of inference. We did not use intuition, and **we did not use any "models"**.

In the next chapter we address the practical question of implementing the formulae above into code.
