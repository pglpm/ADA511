# [Inference about frequencies]{.green} {#sec-inference-exch}
{{< include macros.qmd >}}
{{< include macros_exchangeability.qmd >}}

## Inference when population frequencies aren't known {#sec-freq-not-known}

In [chapter @sec-inference-from-freqs] we considered an agent that has exchangeable beliefs and that knows the full-population frequencies. The degrees of belief of such an agent have a very simple form: products of frequencies. But for such an agent the observations of units *doesn't give any useful information* for drawing inferences about new units: such observations provide frequencies which the agent already knows.

Situations where we have complete frequency knowledge can be common in engineering problems, where the physical laws underlying the phenomena involved are known and computable. They are far less common in data-science and machine-learning applications: here we must consider agents that do not know the full-population frequencies.

How does such an agent calculate probabilities about units? The answer is actually a simple application of the "extension of the conversation" ([§@sec-extension-conversation], which boil down to applications of the `and` and `or` rules). A probability given that the frequency distribution is not known is equal to the average of the probabilities given each possible frequency distribution, weighted by the probabilities of the frequency distributions:

$$
\begin{aligned}
&\P(
\bZ_{u'}\mo {\blue z'} \and 
\bZ_{u''}\mo {\blue z''} \and 
\dotsb
\| \yI
)
\\[2ex]
&\qquad{}=
\sum_{\vf}
\P(
\bZ_{u'}\mo {\blue z'} \and 
\bZ_{u''}\mo {\blue z''} \and 
\dotsb
\| F\mo\vf \and \yI
)
\cdot
\P(F\mo\vf \| \yI)
\end{aligned}
$$

But we saw in [§@sec-moreunit-freq-known] that the probability for a sequence of values given a known frequency is just the product of the value's frequencies. We thus have our long-sought formula:


:::::{.column-page-inset-right}
:::{.callout-note style="font-size:120%"}
## de Finetti's representation theorem
::::{style="font-size:120%"}

If an agent has background information $\yI$ about a population saying that

- beliefs about units are exchangeable
- the population size is practically infinite

then

$$
\P(
\bZ_{u'}\mo {\blue z'} \and 
\bZ_{u''}\mo {\blue z''} \and 
\dotsb
\| \yI
)
\approx
\sum_{\vf}
f(\bZ\mo {\blue z'}\black) \cdot
f(\bZ\mo {\blue z''}\black) \cdot
\,\dotsb\ 
\cdot
\P(F\mo\vf \| \yI)
$$

for any (different) units $u', u'', \dotsc$ and any (even equal) values $\blue z', z'', \dotsc$.

In the sum above, $\vf$ runs over all possible frequency distributions for the full population.

[Properly speaking the sum is an integral, because $F$ is a continuous quantity. We should write $\P(
\bZ_{u'}\mo {\blue z'} \and 
\dotsb
\| \yI
) = \int 
f(\bZ\mo {\blue z'})  \cdot
\,\dotsb\ 
\cdot
\p(\vf \| \yI)\,\di\vf$
]{.small}
::::
:::
:::::

This result is called [**de Finetti's representation theorem**]{.blue} for exchangeable belief distributions. It must be emphasized that **this result is actually independent of any real or imaginary population frequencies**. We took a route to it through the idea of population frequencies only to help our intuition. If for any reason you find the idea of a "limit frequency for an infinite population" somewhat suspicious, then don't worry: the formula above actually does not rely on it. The formula results from the assumption has exchangeable beliefs about a collection of units that can potentially be continued without end.

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
[*Foresight: Its logical laws, its subjective sources*](references.html). This essay gives much insight on our reasoning process in making forecasts and learning from experience.
:::
::::

\

Let's see how this formula works in the simple Mars-prospecting example (with 3 million rocks or more) from [§@sec-moreunit-freq-known]. Suppose that the agent:

:::{.column-margin}
![](mars_crater2.jpg){width=50%}
:::

- knows that the rock collection consists of:
    
    + either a proportion 2/3 of $\yy$-rocks and 1/3 of $\yn$-rocks; denote these frequencies [with $\vfa$]{.m}
    
    + or a proportion 1/2 of $\yy$-rocks and 1/2 of $\yn$-rocks; denote these frequencies [with $\vfb$]{.m}

- assigns a $75\%$ degree of belief to the first hypothesis, and $25\%$ to the second (so the sentence $(F\mo\vfa) \lor (F\mo\vfb)$ has probability $1$):
    
    $$
\P(F\mo\vfa \| \yul) = 75\%
\qquad 
\P(F\mo\vfb \| \yul) = 25\%
$$

What is the agent's degree of belief that rock #1 contains haematite? According to the derived rule of extension of the conversation, that is, the main formula written above, we find:

$$
\begin{aligned}
\P(\yR_{1} \mo \yy \| \yul)
&=
\sum_{\vf}
f(\yR\mo\yy) \cdot \P(F\mo\vf \| \yul)
\\[1ex]
&=
f'(\yR\mo\yy) \cdot \P(F\mo\vfa \| \yul) +
f''(\yR\mo\yy) \cdot \P(F\mo\vfb \| \yul)
\\[1ex]
&=
{\lblue\frac{2}{3}}\cdot 75\% +
{\lblue\frac{1}{2}}\cdot 25\%
\\[1ex]
&= \boldsymbol{62.5\%}
\end{aligned}
$$

In an analogous way we can calculate, for instance, the agent's belief that rock #1 contains haematite, rock #2 doesn't, and rock #3 does:

:::{.column-page-inset-right}
$$
\begin{aligned}
\P(\yR_{1} \mo \yy \and \yR_{2} \mo \yn \and \yR_{3} \mo \yy \| \yul)
&\approx
\sum_{\vf}
f(\yR\mo\yy) \cdot f(\yR\mo\yn) \cdot f(\yR\mo\yy) \cdot
\P(F\mo\vf \| \yul)
\\[2ex]
&=
f'(\yR\mo\yy) \cdot f'(\yR\mo\yn) \cdot  f'(\yR\mo\yy) \cdot 
\P(F\mo\vfa \| \yul) + {}
\\[1ex]
&\qquad
f''(\yR\mo\yy) \cdot f''(\yR\mo\yn) \cdot  f''(\yR\mo\yy) \cdot 
\P(F\mo\vfb \| \yul)
\\[2ex]
&=
{\lblue\frac{2}{3}}\cdot {\yellow\frac{1}{3}}\cdot {\lblue\frac{2}{3}}\cdot 75\% +
{\lblue\frac{1}{2}}\cdot {\yellow\frac{1}{2}}\cdot {\lblue\frac{1}{2}}\cdot 25\%
\\[2ex]
&\approx \boldsymbol{14.236\%}
\end{aligned}
$$
:::

\

This formula generalizes to any population, any variates, and any number of hypotheses about the frequencies.

Mathematical and, even more, computational complications arise when we consider *all possible* frequency distributions, since there is a practically infinite number of them; they form a continuum in fact. But do not let these practical difficulties affect the intuitive picture behind them, which is simple to grasp once you've considered some simple examples.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Consider a state of knowledge $\se{K}'$ according to which:

::::{}
- The rock collection may have a proportion $0/10$ of $\yy$-rocks (and $9/10$ of $\yn$-rocks); call this frequency distribution $\vf_0$
- The rock collection may have a proportion $1/10$ of $\yy$-rocks; call this $\vf_1$
- and so on... up to
- a proportion $10/10$ of $\yy$-rocks; call this $\vf_{10}$
::::

- The probability of each of these frequency hypotheses is $1/11$, that is:
    
	$$
	\begin{aligned}
	&\P(F\mo\vf_{0} \| \se{K}') = 1/11 \\[1ex]
	&\P(F\mo\vf_{1} \| \se{K}') = 1/11 \\[1ex]
	&\dotso\\[1ex]
	&\P(F\mo\vf_{10} \| \se{K}') = 1/11
	\end{aligned}
	$$

Calculate the probabilities

$$
\P(\yR_1\mo\yy \and \yR_2\mo\yy \|\se{K}') \qquad
\P(\yR_1\mo\yy \and \yR_2\mo\yn \|\se{K}') \qquad
\P(\yR_1\mo\yn \and \yR_2\mo\yn \|\se{K}')
$$
<!-- 0.35 0.15 0.35 -->

Do they all have the same value? Try to explain why or why not.
:::


## Learning from observed units {#sec-learning-general}

Staying with the same Mars-prospecting scenario, let's now ask what's the agent's degree of belief that rock #1 contains haematite, *given that the agent has found that rock #2 doesn't contain haematite*. In the case of an agent that knows the full-population frequencies we saw [§@sec-no-learn-freqs] that this degree of belief is actually unaffected by other observations. What happens when the population frequencies are not known?

The calculation is straightforward:

$$
\begin{aligned}
\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and \yul)
&=
\frac{
\P(\yR_{1} \mo \yy \and \yR_{2} \mo \yn \| \yul)
}{
\P(\yR_{2} \mo \yn \| \yul)
}
\\[2ex]
&\approx
\frac{
\sum_{\vf}
f(\yR\mo\yy) \cdot f(\yR\mo\yn) \cdot
\P(F\mo\vf \| \yul)
}{
\sum_{\vf}
f(\yR\mo\yn) \cdot
\P(F\mo\vf \| \yul)
}
\\[2ex]
&=
\frac{
{\lblue\frac{2}{3}}\cdot {\yellow\frac{1}{3}}\cdot 75\% +
{\lblue\frac{1}{2}}\cdot {\yellow\frac{1}{2}}\cdot 25\%
}{
{\yellow\frac{1}{3}}\cdot 75\% +
{\yellow\frac{1}{2}}\cdot 25\%
}
\\[2ex]
&\approx\frac{
22.9167\%
}{
37.5000\%
}
\\[2ex]
&= \boldsymbol{61.111\%}
\end{aligned}
$$

Knowledge that $\yR_{2}\mo\yn$ thus *does* affect the agent's belief about $\yR_{1}\mo\yy$:

$$
\P(\yR_{1} \mo \yy \|  \yul) = 62.5\%
\qquad
\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and \yul) \approx 61.1\%
$$

In particular, the observation of one $\yn$-rock has somewhat decreased the probability of observing a new $\yy$-rock.

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
- Calculate the minimal number of $\yn$ observations needed for lowering the agent's degree of belief of observing a $\yy$ to $55\%$ or less.
<!-- 0.611111 0.595238 0.578431 0.562016 0.547198 -->
    
	Does it seem possible to lower the agent's belief to less than $50\%$? Explain why.

- Calculate the minimal number of $\yy$ observations needed for increasing the agent's degree of belief of observing a $\yy$ to $65\%$ or more.
<!-- 0.633333 0.640351 0.646119 0.650766 -->
    
	Does it seem possible to increase the agent's belief to more than $2/3$? Explain why.
:::

## How learning works: learning about frequencies {#sec-learn-freqs}

An agent having full-population frequency information does not learn^[remember the warning of [§@sec-no-learn-freqs] about "learning"] from observation of units, whereas an agent not having such information does learn from observation of units. This fact shows how learning from observed to unobserved units actually works. Crudely speaking, observations do not directly affect the beliefs about unobserved units, but instead affect the **beliefs about the population frequencies**. And these in turn affect the beliefs about unobserved units. Graphically this could be represented as follows:

![](learning_flow.png)
<!-- ```{mermaid} -->
<!-- flowchart LR -->
<!--   A([observed units]) -\-> B([population frequencies]) -\-> C([unobserved units]) -->
<!--   linkStyle 0 stroke:#47a,fill:#47a -->
<!--   linkStyle 1 stroke:#47a,fill:#47a -->
<!--   style A fill:#283,color:#fff,stroke-width:0px -->
<!--   style B fill:#bbb,color:#000,stroke-width:0px -->
<!--   style C fill:#e67,color:#fff,stroke-width:0px -->
<!-- ``` -->

as opposed to this:

:::{.columns}
::::{.column width="15%"}

::::

::::{.column width="70%"}
![](learning_flow2.png)
<!-- ```{mermaid} -->
<!-- flowchart LR -->
<!--   A([observed units]) -\-> B([population frequencies]) -\-> C([unobserved units]) -->
<!--   A --\-> C -->
<!--   linkStyle 0 stroke:#47a -->
<!--   linkStyle 1 stroke:#47a -->
<!--   linkStyle 2 stroke:#47a -->
<!--   style A fill:#283,color:#fff,stroke-width:0px -->
<!--   style B fill:#bbb,color:#000,stroke-width:0px -->
<!--   style C fill:#e67,color:#fff,stroke-width:0px -->
<!-- ``` -->
::::

:::

:::{.callout-important}
## {{< fa exclamation-triangle >}} Information connections
The graphs above represent [**informational**]{.purple} connections, not "causal". The directed arrows roughly mean "...provides information about..."; they do not mean "...causes...".

In the first graph, the lack of an arrow from [*observed units*]{.green} to [*unobserved units*]{.red} means that all information provided by the observed units for the unobserved ones is fully contained in the information about frequencies.
:::

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
[*On the notion of cause*](references.html)
:::
::::

\

The informational relation between observed units, frequencies, and unobserved units becomes clear if we check how the agent's beliefs about the frequency hypotheses change as observations are made. In the Mars-prospecting example of [§@sec-freq-not-known], the agent has initial probabilities

$$
\P(F\mo\vfa \| \yul) = 75\%
\qquad 
\P(F\mo\vfb \| \yul) = 25\%
$$

where $\vfa$ gives frequency $2/3$ to $\yy$, and $\vfb$ gives frequency $1/2$ to $\yy$. How do these probabilities change, conditional on the agent's observing that rock #2 doesn't contain haematite? We just need to use Bayes's theorem. For the first hypothesis $F\mo\vfa$:

:::{.column-page-right}
$$
\begin{aligned}
\P(F\mo\vfa \| \yR_2\mo\yn \and \yul) &=
\frac{
\P(\yR_2\mo\yn \| F\mo\vfa \and \yul) \cdot
\P(F\mo\vfa \| \yul)
}{
\P(\yR_2\mo\yn \| F\mo\vfa \and \yul) \cdot
\P(F\mo\vfa \| \yul)
+
\P(\yR_2\mo\yn \| F\mo\vfb \and \yul) \cdot
\P(F\mo\vfb \| \yul)
}
\\[1ex]
&=
\frac{
f'(\yR\mo\yn) \cdot
\P(F\mo\vfa \| \yul)
}{
f'(\yR\mo\yn) \cdot
\P(F\mo\vfa \| \yul)
+
f''(\yR\mo\yn) \cdot
\P(F\mo\vfb \| \yul)
}
\\[1ex]
&=
\frac{
\frac{1}{3} \cdot
75\%
}{
\frac{1}{3} \cdot
75\%
+
\frac{1}{2} \cdot
25\%
}
\\[2ex]
&=
\boldsymbol{66.667\%}
\end{aligned}
$$
:::

and an analogous calculation yields $\P(F\mo\vfb \| \yR_2\mo\yn \and \yul)=33.333\%$.

This result makes sense, because according to the hypothesis $F\mo\vfb$ there is a higher proportion of $\yn$-rocks than according to $F\mo\vfa$, and a $\yn$-rock has been observed. The hypothesis $F\mo\vfb$ therefore becomes slightly more plausible, and $F\mo\vfa$ slightly less.


The updated degree of belief above for the frequencies also gives us an alternative (yet equivalent) way to calculate the conditional probability $\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and \yul)$. Use the derived rule of "extension of the conversation" in a different manner:

:::{.column-page-inset-right}
$$
\begin{aligned}
\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and \yul)
&=
\sum_{\vf}
\P(\yR_{1} \mo \yy \| \yR_{2} \mo \yn \and F\mo\vf \and \yul)
\cdot
\P(F\mo\vf \| \yR_{2} \mo \yn \and \yul)
\\[2ex]
\text{\grey\scriptsize(no learning if frequencies are known)}\enspace
&=\sum_{\vf}
\P(\yR_{1} \mo \yy \|  F\mo\vf \and \yul)
\cdot
\P(F\mo\vf \| \yR_{2} \mo \yn \and \yul)
\\[2ex]
&=
\P(\yR_{1} \mo \yy \|  F\mo\vfa \and \yul)
\cdot
\P(F\mo\vfa \| \yR_{2} \mo \yn \and \yul)
+{}
\\[1ex]
&\qquad\P(\yR_{1} \mo \yy \|  F\mo\vfb \and \yul)
\cdot
\P(F\mo\vfb \| \yR_{2} \mo \yn \and \yul)
\\[2ex]
&=
f'(\yR \mo \yy)
\cdot
\P(F\mo\vfa \| \yR_{2} \mo \yn \and \yul)
+{}
\\[1ex]
&\qquad
f''(\yR \mo \yy)
\cdot
\P(F\mo\vfb \| \yR_{2} \mo \yn \and \yul)
\\[2ex]
&=
{\lblue\frac{2}{3}} \cdot 66.667\%
+
{\lblue\frac{1}{2}} \cdot 33.333\%
\\[2ex]
&= \boldsymbol{61.111\%}
\end{aligned}
$$
:::

The result is exactly as in [§@sec-learning-general] -- as it should be: remember from [chapter @sec-probability] that the four rules of inference are built so as to mathematically guarantee this kind of logical self-consistency.



### An intuitive interpretation of population inference

The general expression for the updated belief about frequencies has a very intuitive interpretation. Again using Bayes's theorem, but omitting the proportionality constant,

:::{.column-page-right}
$$
\begin{aligned}
\P(F\mo\vf \| \green Z_1\mo z_1 \and \dotsb \and Z_N\mo z_N \black \and \yul) 
&\propto
\P(\green Z_1\mo z_1 \and \dotsb \and Z_N\mo z_N \black 
\| F\mo\vf \and \yul) 
\cdot
\P(F\mo\vf \| \yul)
\\[2ex]
&\propto
\overbracket[0.1ex]{f(\green Z_1\mo z_1\black)  \cdot \,\dotsb\, 
\cdot f(\green Z_N\mo z_N \black )}^{\midgrey\mathclap{\text{how well the frequency "fits" the data}}} 
\ \cdot \ 
\underbracket[0.1ex]{\P(F\mo\vf \| \yul)}_{\midgrey\mathclap{\text{how reasonable the frequency is}}}
\end{aligned}
$$
:::

This product can be interpreted as follows.

Take a hypothetical frequency distribution $\vf$. If the data have high frequencies according to it, then the product

$$f(\green Z_1\mo z_1\black)  \cdot \,\dotsb\, 
\cdot f(\green Z_N\mo z_N \black )$$

has a large value. Vice versa, if the data have low frequency according to it, that product has a small value. This product therefore expresses how well the hypothetical frequency distribution $\vf$ "fits" the observed data.

On the other hand, if the factor

$$\P(F\mo\vf \| \yul)$$

has a large value, then the hypothetical $\vf$ is probable, or "reasonable", according to the background information $\yK$. Vice versa, if that factor has a low value, then the hypothetical $\vf$ is improbable or "unreasonable", owing to reasons expressed in the background information $\yK$.

The agent's belief in the hypothetical $\vf$ is a balance between these two factors, the "fit" and the "reasonableness". This has a very important consequence:

:::{.callout-note style="font-size:120%"}
##  
::::{style="font-size:120%"}

Probability inference does **not** need any "regularization methods" or any procedures against "over-fitting" or "under-fitting".

In fact, *the very notions of over- or under-fitting refer to the background information $\yK$ and the initial belief $\P(F\mo\vf \| \yul)$*.

Think about it: How can we judge that an algorithm is over- or under-fitting, given that we do not know the "ground truth"? (If we knew the ground truth we wouldn't be making inferences.) Such judgement reveals that we have some *preconceived* notion of what a reasonable distributions would look like -- that's exactly what $\P(F\mo\vf \| \yul)$ encodes.

::::
:::

The agent's belief about new data is then an *average* of what the frequency of the new data would be *for all possible frequency distributions* $\vf$:

:::{.column-body-outset-right}
$$
\begin{aligned}
&\P(\red Z_{N+1}\mo z_{N+1} \black \| \green Z_1\mo z_1 \and \dotsb \and Z_N\mo z_N \black \and \yul) 
\\[2ex]
&\qquad{}=
\sum_{\vf}
f({\red Z_{N+1}\mo z_{N+1}}) \cdot
\P(F\mo\vf \| \green Z_1\mo z_1 \and \dotsb \and Z_N\mo z_N \black \and \yul)
\end{aligned}
$$
:::

Each possible $\vf$ is weighed by its credibility, which takes into account the fit of the possible frequency to observed data, and its reasonableness against the agent's background information.



### Other uses of the belief distribution about frequencies

The fact that the agent is actually learning about the full-population frequencies allows it to draw improved inferences not only about units, but also about characteristics intrinsic to the population itself, and also about its own performance in future inferences. For instance, the agent can even forecast the maximal accuracy that can be obtained in future inferences. We shall quickly explore these possibilities in a later chapter.

::: {.callout-warning}
## {{< fa book >}} Study reading
- Ch. 4 of Jaynes: [*Probability Theory*](references.html)

- §§8.1--8.6 of O'Hagan: [*Probability*](references.html)

- Skim through Heath & Sudderth 1976: [*De finetti's theorem on exchangeable variables*](references.html)
:::

\

## How to assign the probabilities for the frequencies? {#sec-prob-for-freqs}

The general formula we found for the joint probability:

$$
\P(
\bZ_{u'}\mo {\blue z'} \and 
\bZ_{u''}\mo {\blue z''} \and 
\dotsb
\| \yI
)
\approx
\sum_{\vf}
f(\bZ\mo {\blue z'}\black) \cdot
f(\bZ\mo {\blue z''}\black) \cdot
\,\dotsb\ 
\cdot
\P(F\mo\vf \| \yI)
$$

allows us to draw many kinds of predictions about units, which we'll explore in the next chapter.

But how does the agent assign\ \ [$\P(F\mo\vf \| \yI)$ ,]{.m}\ \ that is, the probability distribution (in fact, a density) over all possible frequency distributions? There is no general answer to this important question, for two main reasons.

First, a proper answer is obviously problem-dependent. In fact\ \ [**$\P(F\mo\vf \| \yI)$\ \ is the place where the agent encodes any background information relevant to the problem**]{.blue}.

Take the simple example of the tosses of a coin. If you (the agent) examines the coin and the tossing method and they seem ordinary to you, then you might assign probabilities like these:

$$
\begin{aligned}
&\P(F\mo\pr{always heads} \| \yi[o]) \approx 0
\\
&\P(F\mo\pr{always tails} \| \yi[o]) \approx 0
\\
&\P(F\mo\pr{50\% heads 50\% tails} \| \yi[o]) \approx \text{\small very high}
\end{aligned}
$$

But if you are told that the coin is a magician's one, with either two heads or two tails, and you don't know which, then you might assign probabilities like these:

$$
\begin{aligned}
&\P(F\mo\pr{always heads} \| \yi[m]) = 1/2
\\
&\P(F\mo\pr{always tails} \| \yi[m]) = 1/2
\\
&\P(F\mo\pr{50\% heads 50\% tails} \| \yi[m]) = 0
\end{aligned}
$$

:::{.callout-caution}
## {{< fa user-edit >}} Exercise
Assume the state of knowledge $\yi[m]$ above and calculate:

- $\P(\pr{heads 1st toss} \| \yi[m])$, the probability of heads at the first toss.

- $\P(\pr{heads 2nd toss} \| \pr{heads 1st toss} \and \yi[m])$, the probability of heads at the second toss, given that heads was observed at the first.

Explain your findings.
:::

Second, for complex situations with many variates of different types it is may be mathematically and computationally difficult to write down and encode\ \ [$\P(F\mo\vf \| \yI)$ .]{.m}\ \ Moreover, the multidimensional characteristics and quirks of this belief distribution can be difficult to grasp and understand.

Yet it is a result of probability theory ([§@sec-inference-origin]) that **we cannot avoid specifying $\P(F\mo\vf \| \yI)$**. Any "methods" that claim to avoid the specification of that probability distribution *are* covertly specifying one instead, and hiding it from sight. It is therefore best to have this distribution at least open to inspection rather than hidden.

Luckily, if\ \ $\P(F\mo\vf \| \yI)$\ \ is "open-minded", that is, if it doesn't exclude a priori any frequency distribution $\vf$, or in other words if it doesn't assign strictly zero belief to any $\vf$, then with enough data the updated belief distribution \ \ $\P(F\mo\vf \| \se{data} \and \yI)$\ \ will actually converge to the true frequency distribution of the full population. The tricky word here is "enough". In some problems a dozen observed units might be enough; in other problems a million observed units might not be enough yet.

::::{.column-margin}
::: {.callout-tip}
## {{< fa rocket >}} For the extra curious
[*Bayesian statistical inference for psychological research*](references.html)
:::
::::

\

In [chapter @sec-dirichlet-mix] we shall discuss and implement a mathematically concrete belief distribution $\P(F\mo\vf \| \se{D})$ appropriate to task involving nominal variates.

